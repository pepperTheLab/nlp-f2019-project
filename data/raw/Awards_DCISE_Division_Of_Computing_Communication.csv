"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1740741","Foundations of Model Driven Discovery from Massive Data","CCF","TRIPODS Transdisciplinary Rese, INSPIRE, EPSCoR Co-Funding","09/01/2017","06/18/2018","Bjorn Sandstede","RI","Brown University","Standard Grant","Tracy Kimbrel","08/31/2020","$1,482,177.00","Stuart Geman, Jeffrey Brock, Eli Upfal, Bjorn Sandstede, Joseph Hogan","Bjorn_Sandstede@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","041Y, 8078, 9150","047Z, 060Z, 062Z, 9150","$0.00","This project creates an institute at Brown University that brings together the disciplines of mathematics, statistics, and theoretical computer science to define and refine the foundational landscape of the emerging area of data science.  The institute sponsors focused activities organized by small groups of researchers that cut across disciplinary boundaries. It connects with and informs a broad range of students at the undergraduate and graduate levels, working in a wide area of domain areas, from neuroscience, to genomics, to climate modeling, to public policy. Theoretical developments can improve diagnostic imaging and tumor classification, can develop improved models for neural structure, and can even inform findings regarding food stamps and recidivism in Rhode Island. <br/> <br/>The mission of the institute is to foster development and principled application of theory and methods of big data to discover, refine, and validate underlying theoretical models that govern a system or data-generating process, which in turn improve predictions of new outcomes.  Scientific projects in ""causal and model-based inference,"" ""data analysis on massive networks,"" and ""geometric and topological methods to analyze and visualize complex data"" drive home the role of the model, and its continuous refinement, in data analysis. Rather than seeking better ""black-boxes"" for analysis, the institute will emphasize the role of the ""investigator-in-the-loop"" interrogating the entirety of the data pipeline, seeking theoretical improvements and implications.  It connects to the Brown Data Science Initiative and the Institute for Computational and Experimental Research in Mathematics (ICERM).  Funds for the project come from CISE Computing and Communications Foundations, MPS Division of Mathematical Sciences, Growing Convergent Research, and EPSCoR. (Convergence can be characterized as the deep integration of knowledge, techniques, and expertise from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities.  This project promotes Convergence by bringing together communities representing many disciplines including mathematics, statistics, and theoretical computer science as well as engaging communities that apply data science to practical research problems.)"
"1740855","TRIPODS: Berkeley Institute on the Foundations of Data Analysis","CCF","TRIPODS Transdisciplinary Rese","09/01/2017","08/24/2018","Michael Mahoney","CA","University of California-Berkeley","Continuing grant","Nandini Kannan","08/31/2020","$999,999.00","Richard Karp, Bin Yu, Michael Jordan, Peter Bartlett, Fernando Perez","mmahoney@icsi.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","041Y","047Z, 062Z","$0.00","In response to NSF's TRIPODS Phase I initiative, the PIs, with expertise in theoretical and applied statistics, computer science, and mathematics at the University of California, Berkeley, will create a Foundations of Data Analysis (FODA) Institute to address cutting-edge foundational issues in interdisciplinary data science.  The Institute will advance foundational research and the application of foundational methods through an intensive program of cross-disciplinary outreach to application domains in and beyond the campus research community.  In parallel with the massive technological and methodological advances in the underlying disciplines over the past decade, a thriving array of data-related research and training programs has emerged across campus.  Yet none of these programs within the campus data science ecosystem are devoted to addressing the interdisciplinary foundations of data analysis in a focused, mission-driven manner.  The FODA Institute will address this crucial unmet need.  This interdisciplinary project will lay the groundwork for more productive and fruitful interactions between theoretically-inclined data science researchers and researchers in diverse domains that rely upon, but do not always explicitly appreciate, foundational concepts.  Advances in this area will lead to more principled extraction of insights from data across a wide range of domains.  The three-year Phase I pilot will pave the way for institutionalization of the project as a larger center that will be the subject of a potential Phase II application.<br/><br/>The technical research component of the project addresses four fundamental challenges in data science: the characterization of what is, and what is not, possible in terms of upper and lower bounds for inferential optimization problems; probing more deeply the notion of stability as a computational-inferential principle; exploring the complementary role of randomness as a statistical resource, as an algorithmic resource, and as a tool for data-driven computational mathematics; and developing methods to combine science-based with data-driven models in a principled manner.  Each of these challenges addresses old questions in light of new needs, each has important synergies with the other challenges, and each is situated squarely at the interface of theoretical computer science, theoretical statistics, and applied mathematics.  The project will bridge the underlying interdisciplinary gaps to address some of the most important questions at the heart of data science today.  Funds for the project come from CISE Computing and Communications Foundations and MPS Division of Mathematical Sciences."
"1740707","TRIPODS: Institute for Foundations of Data Science","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, INSPIRE","09/01/2017","08/23/2017","Stephen Wright","WI","University of Wisconsin-Madison","Standard Grant","Nandini Kannan","08/31/2020","$1,499,523.00","Michael Newton, Robert Nowak, Rebecca Willett, Sebastien Roch","swright@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","041Y, 1253, 8078","047Z, 060Z, 062Z","$0.00","As data continues to accumulate at an ever-increasing rate, so does the need for powerful and novel methods to extract information from data, in a form that is useful to individuals, society, researchers,and commerce. This project establishes a new entity at the University of Wisconsin-Madison: the Institute for Foundations of Data Science (IFDS). Building on the foundational work of earlier generations of researchers, IFDS will serve as a hub for people across campus with expertise in mathematics, statistics, and computer science to explore new approaches to the formulation and solution of problems in data analysis, as well as to epitomize the possibilities of a collaborative approach to investigating fundamental issues in data science. IFDS will integrate with the broader UW-Madison agenda for data science research, creating a new home for research of a fundamental, theoretical nature.  It will play a vital role in establishing graduate degree programs in data science and in outreach to industrial partners with interests in fundamental data science research.<br/><br/>The new Institute for Foundations of Data Science will bring together researchers across the UW-Madison campus in mathematics, statistics, and theoretical computer science for a transdisciplinary effort organized around three themes: Algebra and Optimization in Data Science, Graphs and Networks in Data Science, and Data Acquisition Theory and Methods. All topics represent areas of significant current interest in data science due to their intrinsic fundamental import and their wide applicability. The collaborations within these themes will involve fourteen senior researchers, together with postdoctoral and graduate student researchers. The IFDS will lay the foundations for a larger future effort in transdisciplinary data science research, possibly involving other universities and institutes.  Funds for the project come from CISE Computing and Communications Foundations, MPS Division of Mathematical Sciences, MPS Office of Multidisciplinary Activities, and Growing Convergent Research. (Convergence can be characterized as the deep integration of knowledge, techniques, and expertise from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. This project promotes convergence by developing transdisciplinary data science solutions to problems in important application domains such as functional brain networks in cognitive neuroscience, understanding causal effects in gene regulatory networks, phylogenetic reconstruction in computational evolutionary biology, and the spread of information or disease across a network.)"
"1740858","TRIPODS: UA-TRIPODS - Building Theoretical Foundations for Data Sciences","CCF","TRIPODS Transdisciplinary Rese, ALGORITHMIC FOUNDATIONS","09/01/2017","08/29/2018","Hao Zhang","AZ","University of Arizona","Continuing grant","Tracy Kimbrel","08/31/2020","$916,499.00","Joseph Watkins, Stephen Kobourov, David Glickenstein","hzhang@math.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","041Y, 7796","047Z, 062Z, 7926, 9251","$0.00","UA-TRIPODS aims to build a transformative and highly interactive data science hub in the Southwestern United States for the integration of research, education, and outreach in the theoretical foundations of data science.  Through research, education, and outreach targeted to high school students through postdoctoral fellows, UA-TRIPODS will help establish and encourage a diverse population to pursue careers in quantitative and computational data science.  The intellectual merits of the project include identifying fundamental areas where progress in the theoretical foundations is imperative, developing mathematical and algorithmic principles of data science, and basic research on topics ranging from the theory of large scale networks and optimization to statistical modeling for natural language processing and Bayesian methods.  This is accomplished by extending long-standing collaborations with experts in the foundational disciplines - theoretical computer science, mathematics, and statistics - and motivated by problems in the domain sciences and industry.  New mathematical, statistical, and algorithmic principles will strengthen the foundations of theoretical data science, while driving discovery and innovation for emerging applications.  UA-TRIPODS will also cultivate new partnerships with local industries engaged in data-centric enterprises in astronomy, environmental science, genomics, lunar and planetary sciences, medicine, transportation, and optical sciences.<br/><br/>Research Working Groups will pursue fundamental questions in a number of different areas in the theoretical foundations of data science.  By ensuring each group includes researchers from each area of the foundational disciplines, while also pairing these groups with science and industry partners, UA-TRIPODS will ensure that these projects will benefit from a truly transdisciplinary collaboration.  Progress in collaboration will be measured by specialized program evaluation metrics.  Yearly workshops, seminars, and brainstorming/visioning activities will enable UA-TRIPODS to lay the foundation for the future of training and research in data science.  In addition, this project will support the development of a new undergraduate degree in Statistics and Data Science at the University of Arizona, expanded online offerings, and facilitation of cooperation in graduate education (both among the foundational disciplines and with the domain sciences).  Students will be provided with opportunities to experience first-hand data analysis and interdisciplinary research and education.  UA-TRIPODS will also engage partnering colleges and universities across the Southwest in the institute activities and then utilize these connections to support novel and effective inter-university and intra-university research and educational programs.  Partnerships with Hispanic Serving Institutions and Tribal Colleges in southern California, Arizona, New Mexico, and western Texas will be strengthened."
"1740833","TRIPODS: From Foundations to Practice of Data Science and Back","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC","09/01/2017","08/28/2018","John Wright","NY","Columbia University","Continuing grant","Nandini Kannan","08/31/2020","$1,000,000.00","Daniel Hsu, Alexandr Andoni, David Blei, Qiang Du","jw2966@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","041Y, 1253","047Z, 062Z","$0.00","In recent decades, scientific and technological fields have experienced ""data moments"" as researchers recognized the potential of drawing new types of inferences by applying techniques from computational statistics and machine learning to ever-growing datasets. At the same time, everyday life is increasingly saturated with products of data analysis: search engines, recommendation systems, autonomous vehicles, etc. These developments raise fundamental methodological questions, including how to collect and pre- pare data for analysis, and how to transform statistical inferences into effective action and new statistical inquiries. To address these questions, it is necessary to develop theoretical foundations for the practice of data science, and to provide practitioners with sound and practically relevant methodological training. The Columbia TRIPODS Institute pursues these goals through an integrated program of research in data science foundations, curriculum development, and center-building activities. The research program seeks to provide theoretical understanding of practical heuristics, develop modular and well-structured toolkits of computational primitives for data science, and to support the entirety of the data science cycle, from data collection and annotation, to the assessment of the analysis product. <br/><br/>The Institute pursues programs of research, education and center-building aimed at articulating theoretical foundations for data science. Its activities aim to have a major impact in shaping this emerging field. The research directions include understanding tractable classes of optimization problems, developing primitives that support efficient computation on data, and developing methodological foundations for interactive protocols in data science. These directions address challenging problems at the interface between theory and practice, the solutions of which require ideas spanning mathematics, statistics, and computing. The educational activities articulate model curricula in data science at the MS/professional and PhD levels, including interdisciplinary courses aimed at building a common language for a new generation of scientists and engineers. Center building activities are organized around cross-disciplinary themes and structured to encourage interaction across disciplines and to develop a common methodological community in Foundations of Data Science. These research and educational activities-including workshops, summer schools, distinguished lecture series, long-term visits, and outreach- help to further define and disseminate a common language for foundational research and education, and to increase diverse participation in data science. Located within the Center for Foundations of Data Science in the Data Science Institute at Columbia University, the Institute is at the center of the Northeast Big Data Hub. This position supports expansion of activities within Columbia, and also with other Big Data Hub members, TRIPODS Institutes, and research/industry organizations.  Funds for the project come from CISE Computing and Communications Foundations, CISE Information Technology Research, MPS Division of Mathematical Sciences, and MPS Office of Multidisciplinary Activities."
"1740850","TRIPODS: Towards a Unified Theory of Structure, Incompleteness & Uncertainty in Heterogeneous Graphs","CCF","TRIPODS Transdisciplinary Rese","09/01/2017","08/17/2018","Lise Getoor","CA","University of California-Santa Cruz","Continuing grant","Nandini Kannan","08/31/2020","$1,049,829.00","C. Seshadhri, Abel Rodriguez","getoor@soe.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","041Y","047Z, 062Z","$0.00","This project brings together researchers from mathematics, statistics, and computer science to develop a unified theory of data science applied to uncertain and heterogeneous graph and network data.  Most real-world applications of networks involve complex phenomena, such as socio-behavioral interactions, biological and/or chemical processes, technical systems like data centers, and communication systems for smart cities. These data are heterogeneous, including multiple modalities and multiple scales. Crucially, the data observed is often incomplete and very noisy.  A new foundation for data science needs to be built in order to address these challenges in the context of graph and network data.  Similarly, we lack a clear unified theory that allows us to understand how to quantify the uncertainty in the system that arises from the uncertainty in the relationships among its actors.  This is a fertile area for transdisciplinary collaboration between statisticians, mathematicians, and computer scientists, with strong impacts on industry, academia, government and broader society. <br/><br/>This project centers around two research themes. In the first theme, the PIs will investigate models of algorithms on uncertain network data, and specifically combine techniques from sub-linear algorithms with Bayesian methods.  The second theme focuses on how algorithms can benefit from data uncertainty, in the context of privacy, disclosure, and robustness to noise. In both these themes, technical advances will be achieved by marrying computational approaches to uncertainty with statistical and mathematical approaches for uncertainty.  In addition to the research agenda, the project involves an ambitious vision for data science capabilities spanning academia to industry. The education aspects of this vision include a series of themed workshops and the development of comprehensive educational resources spanning secondary, undergraduate and advanced graduate materials. The project also involves collaboration between UC Santa Cruz and Silicon Valley companies that will ground our proposed theoretical and algorithmic advances with practical applications to real-world problems and data. Furthermore, these collaborations with industrial partners will lead to specialized workforce development.  Looking forward towards Phase II, the project aims to develop collaborations with industry partners and various academic institutions in the area to develop a Silicon Valley/Greater Bay Area Institute on Foundations of Data Science, potentially to be located at University of California Santa Cruz Silicon Valley campus in Santa Clara.  Funds for the project come from CISE Information Technology Research and MPS Division of Mathematical Sciences."
"1839308","TRIPODS+X:RES: Investigations at the Interface of Data Science and Neuroscience","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, COGNEURO, IntgStrat Undst Neurl&Cogn Sys","10/01/2018","09/10/2018","Nicholas Turk-Browne","CT","Yale University","Standard Grant","Christopher W. Stark","09/30/2021","$599,992.00","Damon Clark, John Lafferty, Jeffrey Brock","nicholas.turk-browne@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","041Y, 1253, 1699, 8624","047Z, 062Z, 8089, 8091","$0.00","This project will build a transformative bridge between data science and neuroscience. These two young fields are driving cutting-edge progress in the technology, education, and healthcare sectors, but their shared foundations and deep synergies have yet to be exploited in an integrated way - a new discipline of ""data neuroscience."" This integration will benefit both fields: Neuroscience is producing massive amounts of data at all levels, from synapses and cells to networks and behavior. Data science is needed to make sense of these data, both in terms of developing sophisticated analysis techniques and devising formal, mathematically rigorous theories. At the same time, models in data science involving AI and machine learning can draw insights from neuroscience, as the brain is a prodigious learner and the ultimate benchmark for intelligent behavior. Beyond fundamental scientific gains in both fields, the project will produce additional outcomes, including: new collaborations between universities, accessible workshops, graduate training, integration of undergraduate curricula in data science and neuroscience, research opportunities for undergraduates that help prepare them for the STEM workforce, academic-industry partnerships, and enhanced high-performance computing infrastructure.<br/><br/>The overarching theme of this project is to develop a two-way channel between data science and neuroscience. In one direction, the project will investigate how computational principles from data science can be leveraged to advance theory and make sense of empirical findings at different levels of neuroscience, from cellular measurements in fruit flies to whole-brain functional imaging in humans. In the reverse direction, the project will view the processes and mechanisms of vision and cognition underlying these findings as a source for new statistical and mathematical frameworks for data analysis. Research will focus on four related objectives: (1) Distributed processing: reconciling work on communication constraints and parallelization in machine learning with the cellular neuroscience of motion perception to develop models of distributed estimation; (2) Data representation: examining how our understanding of the different ways that the brain stores information can inform statistically and computationally efficient learning algorithms in the framework of exponential family embeddings and variational inference; (3) Attentional filtering: incorporating the cognitive concept of selective attention into machine learning as a low-dimensional trace through a high-dimensional input space, with the resulting models used to reconstruct human subjective experience from brain imaging data; (4) Memory capacity: leveraging cognitive studies and natural memory architectures to inform approaches for reducing/sharing memory in artificial learning algorithms. The inherently cross-disciplinary nature of the project will provide novel theoretical and methodological perspectives on both data science and neuroscience, with the goal of enabling rapid, foundational discoveries that will accelerate future research in these fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740551","TRIPODS: Algorithms for Data Science: Complexity, Scalability, and Robustness.","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC","09/01/2017","08/23/2017","Sham Kakade","WA","University of Washington","Standard Grant","Christopher W. Stark","08/31/2020","$1,500,000.00","Zaid Harchaoui, Dmitriy Drusvyatskiy, Yin Tat Lee, Maryam Fazel","sham@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","041Y, 1253","047Z, 062Z","$0.00","Award: CCF 1740551, Principal Investigator: Sham Kakade<br/><br/>Algorithmic tools underpin the ways in which modern data science methods glean insights from data, manipulate their environments, and estimate underlying statistical properties in the world. With increasing computational resources and an unprecedented growth of large datasets, there is an increased need for scalable and robust algorithmic tools which can provide insights into data in an automated manner, and thus, help to accelerate the pace of science and engineering. The modern challenges that a range of fields now face are no longer easily handled by ideas from a single discipline. A central goal of this project is to provide a common language and unifying methods for addressing contemporary data science challenges. At their core, each of the three disciplines of computer science, mathematics, and statistics has rich theories of complexity and robustness.  These theories have influenced the design of the available tools that are used to address real world computational problems. Going forward, this project seeks new algorithms and design principles that unify ideas and provide a common language for addressing contemporary data science challenges. The PIs will draw from their expertise in computer science, mathematics, and statistics to aid in providing these unifying approaches. In parallel, aiming for a strong educational impact of the work, the aim is to train students an postdoctoral scholars to be well-versed in different areas underpinning data science and will incorporate appropriate theoretical ideas into a data science curriculum. The PIs will also organize events that help train students (including a hackathon and a bootcamp) and a research workshop that bring together researchers from the three disciplines for discussion and collaboration.<br/><br/>In particular, the research objectives of this project are in unifying basic abstractions and techniques in order to yield not only further breakthroughs in all three fields, but also to impact societal and technological growth. The complexity and algorithmic questions this work seeks to address include: (i) how to unify various notions of complexity (which range from information theoretic to computational to black box oracle models), (ii) how to unify notions of robustness and adaptivity (e.g., how solutions and methods change as oracle models are corrupted by random or adversarial noise), (iii) how to address optimization challenges due to nonconvexity, and (iv) how to use these unified approaches to design more effective scalable tools, in theory and practice. These foundations will directly draw from the PIs close collaborations with various technological and scientific practitioners. Funds for the project come from CISE Computing and Communications Foundations, CISE Information Technology Research, MPS Division of Mathematical Sciences, and MPS Office of Multidisciplinary Activities."
"1740776","TRIPODS: Transdisciplinary Research Institute for Advancing Data Science (TRIAD)","CCF","TRIPODS Transdisciplinary Rese","09/01/2017","08/18/2018","Xiaoming Huo","GA","Georgia Tech Research Corporation","Continuing grant","Nandini Kannan","08/31/2020","$1,017,342.00","Dana Randall, Prasad Tetali, C. F. Jeff Wu, Srinivas Aluru","xiaoming@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","041Y","047Z, 062Z","$0.00","This project creates the Transdisciplinary Research Institute for Advancing Data Science (TRIAD) at the Georgia Institute of Technology.  TRIAD aims to integrate research and education in mathematical, statistical, and algorithmic foundations for data science. Analysis of massive, dynamic, noisy, and complex data arising in virtually every sphere of human activity is a pressing challenge of our time, and an area of great importance for its economic and societal impact. TRIAD will address the growing challenges in establishing the foundations of data science, much of which lies at the intersection of computer science, statistics, and mathematics. TRIAD's intellectual focus is to design and build transdisciplinary research programs that provide an enabling and cross-fertilizing platform of ideas and stakeholders (including theoreticians/scientists from domain sciences and users of technology). TRIAD hosts focused working groups, national and international workshops, and organized innovation labs. Participants include senior, mid-career, and junior faculty members, postdoctoral fellows, graduate and senior undergraduate students, and data science practitioners at large. All TRIAD activities involve interdisciplinary personnel from the three foundational disciplines. TRIAD deploys information technology and communication infrastructure to quickly and efficiently disseminate its research and activities, while the research community at large can easily access and comment/critique TRIAD's choice of research programs and topics. The institute aims to create an intellectual atmosphere that connects theoreticians and practitioners, scientists, and engineers from across the nation and worldwide on a regular basis.<br/><br/>TRIAD enriches careers of participants ranging from undergraduate students to senior researchers from around the nation. Postdoctoral fellows and graduate students are introduced to collaborative research in the institute activities and through workshops. TRIAD makes prudent efforts to reach out to diverse communities, including participants from smaller colleges and institutions serving under-represented minorities. TRIAD actively engages in outreach through public lectures, press releases, and dissemination via other internet channels. TRIAD works with associated professional societies to provide stimulus to data-science-related initiatives. Additional activities (such as customized workshops) will combine interactive projects and field trips to acquaint undergraduate and/or high school students from all over the U.S. with data-science-related techniques and the themes of TRIAD's year-long programs. Every effort will be made to make products and lectures available online and to enable remote participation.  Funds for the project come from CISE Computing and Communications Foundations and MPS Division of Mathematical Sciences."
"1740822","TRIPODS: Data Science for Improved Decision-Making: Learning in the Context of Uncertainty, Causality, Privacy, and Network Structures","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, INSPIRE","10/01/2017","08/23/2017","Kilian Weinberger","NY","Cornell University","Standard Grant","Tracy J. Kimbrel","09/30/2020","$1,496,655.00","Steven Strogatz, Giles Hooker, Jon Kleinberg, David Shmoys","kilianweinberger@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","041Y, 1253, 8078","047Z, 060Z, 062Z","$0.00","The researchers propose to create a center of data science for improved decision-making that combines expertise from computer science, information science, mathematics, operations research, and statistics. Their goal is to pursue basic research that will contribute to the theoretical foundations of data science.  The research topics chosen have applications that can benefit society as a whole and integrate the perspectives of the disciplines that the project brings together. The five concrete research directions proposed are: Privacy and Fairness, Learning on Social Graphs, Learning to Intervene, Uncertainty Quantification, and Deep Learning. The aim of the Center is to advance knowledge in these areas and to broaden the range of disciplines and perspectives that can provide contributions to these challenging issues. The researchers plan to incorporate the community beyond Cornell through online seminars, workshops, and student conferences. <br/><br/>The research findings will provide an urgently needed foundation for data science in several topic areas of importance to society. As the center is placed at the intersection of multiple disciplines, the intellectual merit spans all disciplines involved and findings may translate to new algorithms and approaches in each one of them.  <br/><br/>The research focus spans five core areas. <br/><br/>1. Privacy and Fairness.  As data science becomes pervasive across many areas of society, and as it is increasingly used to aid decision-making in sensitive domains, it becomes crucial to protect individuals by guaranteeing privacy and fairness.  The investigators propose to research the theoretical foundations to providing such guarantees and to surface inherent limitations. <br/><br/>2. Learning on Social Graphs. Many of the fundamental questions in applying data science to the interactions between individuals and larger social systems involve the social networks that underpin the connections between individuals. The researchers will develop new techniques for understanding both the structure of these networks and the processes that take place within them.<br/><br/>3. Learning to Intervene. Data-driven approaches to learning good interventions (including policies, recommendations, and treatments) inspire challenging questions about the foundations of sequential experimental design, counterfactual reasoning, and causal inference.<br/><br/>4. Uncertainty Quantification. Quantifying uncertainty about specific predictions or conclusions represents a key need in data science, especially when applied to decision-making with potential consequences to human subjects. The researchers will develop statistical tools and theoretical guarantees to assess the uncertainties of predictions made by popular algorithms in data science. <br/><br/>5. Deep Learning. Deep Learning algorithms have made impressive advances in practical settings.  Although their basic building blocks are well understood, there is still ambiguity about what they learn and why they generalize so well. There are indications that they may learn data manifolds and that the type of optimization algorithm influences generalization. <br/><br/>Advances in our theoretical understanding of these phenomena requires combined efforts from optimization, statistics, and mathematics but could lead to insights for all aspects of data science.<br/><br/>Funds for the project come from CISE Computing and Communications Foundations, MPS Division of Mathematical Sciences, MPS Office of Multidisciplinary Activities, and Growing Convergent Research. (Convergence can be characterized as the deep integration of knowledge, techniques, and expertise from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. This project promotes Convergence by bringing together communities representing many disciplines including mathematics, statistics, and theoretical computer science as well as engaging communities that apply data science to practical research problems.)"
"1659488","REU Site: Data Science of Risk and Human Activity","CCF","RSCH EXPER FOR UNDERGRAD SITES","06/01/2017","03/10/2017","George Mohler","IN","Indiana University","Standard Grant","Rahul Shah","05/31/2020","$287,377.00","","georgemohler@gmail.com","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","1139","9250","$0.00","This Research Experiences for Undergraduates (REU) program at Indiana University Purdue University Indianapolis (IUPUI) will provide eight undergraduate students from across the United States with the opportunity to conduct research on the data science of human activity.  The students will spend ten weeks during the summer working with IUPUI faculty on projects related to predictive modeling and estimation of risk with applications to crime, conflict, political instability and daily routine activity.  The students will also attend a data science bootcamp as part of the program that will provide training in the foundations of data science (statistics, machine learning, and software development).  Data science is a rapidly growing field due to increases in the volume and variety of data being generated.  Graduates with skill sets at the intersection of mathematics, statistics, computing, data analysis, and data modeling are in high demand, both in industry and academia.  The REU site will increase undergraduate student awareness, preparation, and interest in pursuing graduate degrees in STEM fields where data science is becoming a larger focus.  Students choosing to pursue a career in industry upon graduation will also be better prepared to meet the data challenges of the increasing number of companies where data science is a high priority. <br/>  <br/>The REU projects address new challenges in the data science of risk and human activity.  Students working in problem area 1 will focus on learning to rank problems for space-time crime prediction.  Machine learning models for ranking crime hotspots according to risk will be developed that will be tailored to ranking based loss functions that arise in criminology.  The second project will focus on classifying types of activity from mobile sensor time series.  The students will compare existing techniques to deep learning algorithms employing architectures developed specifically for 3d data from the accelerometer and gyroscope.  The third area of research will focus on point process models of heterogeneous grievance data.  In particular, students will develop spatio-temporal models for conflict data in Sub-Saharan Africa coupled with geolocated, relevant Tweets from the same time period."
"1619448","CIF: SMALL: Information Theoretic Foundations of Data Science","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2016","09/19/2017","Alon Orlitsky","CA","University of California-San Diego","Continuing grant","Phillip Regalia","07/31/2019","$323,629.00","","alon@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7797, 7923, 7935","$0.00","The advent of modern data generation, collection, communication, retention, and application has brought about an important new discipline termed data science. It draws upon diverse techniques from statistics, machine learning, computer science, and information theory to analyze, understand, and most importantly, utilize, data. It therefore significantly impacts diverse fields ranging from medicine to marketing, manufacturing, finance, and security. Much of the initial work in this area has centered on simple models to to facilitate analysis. This project is focused on bringing these results closer to real-world applications such as natural language processing. The results of this work are expected to lead to a better ability to predict future events, detect anomalies, and classify data.<br/><br/>The research in this project is focused on three technical thrusts: (i) extending data science concepts to practical regimes, (ii) utilizing structure, and (iii) understanding the effect of memory in data sources. With regards to (i), the objective is to develop optimal distribution estimators not just for the traditional Kullback-Leibler divergence, but also for other metrics, and to do so for arbitrary parameter values, not just restricted asymptotic regimes. With regards to (ii), the objective is to utilize structure to improve distribution estimates and to resolve a conundrum where theorists and practitioners employ opposing distribution estimation techniques. With regards to (iii), the general objective is to study the problem of estimating distributions with memory. Preliminary results have uncovered an interesting dichotomy between compression and learning when memory is present. Through these technical thrusts, this project will study complex and realistic data models and derive results with important theoretical implications as well as applications in a variety of fields."
"1740761","TRIPODS: Topology, Geometry, and Data Analysis (TGDA@OSU):Discovering Structure, Shape, and Dynamics in Data","CCF","TRIPODS Transdisciplinary Rese","10/01/2017","08/17/2018","Tamal Dey","OH","Ohio State University","Continuing grant","Tracy Kimbrel","09/30/2020","$1,000,000.00","Yusu Wang, David Sivakoff, Sebastian Kurtek, Facundo Memoli","tamaldey@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","041Y","047Z, 062Z","$0.00","This project will advance the methodological and theoretical foundations of data analytics by considering the geometric and topological aspects of complex data from mathematical, statistical and algorithmic perspectives, thus enhancing the synergy between the Computer Science, Mathematics, and Statistics communities. Furthermore, this project will benefit a range of impactful scientific areas including medicine, neuronanatomy, machine learning, geographic information systems, mechanical engineering designs, and political science.  The research products will be implemented and disseminated through software packages and tutorials, allowing widespread application by industrial and academic practitioners.  Through this project, the PIs will develop curricula for cross-disciplinary, undergraduate and graduate education.  There is already extant data science curriculum offered jointly between Statistics and Computer Science and Engineering at The Ohio State University (OSU), including the recent Data Analytics undergraduate major, providing a platform to develop new courses and an opportunity to engage future industry leaders in basic research.  Additionally, this project aims to develop partnerships with the Translational Data Analytics and the Mathematical Biosciences Institutes at OSU, as well as other internal and external research and education centers.  Plans for workshops and summer schools are included for outreach and training purposes.<br/><br/>In the past few decades, a large number of models, methods, and algorithmic frameworks have been developed for data science.  However, as data become increasingly more complex, the field faces new challenges.  In particular, the non-Euclidean nature, the higher order connectivity, the hidden global cues, and the dynamics regulating the data pose further challenges to existing methods.  This project will explore and leverage the geometric and topological structures inherent in the data to tackle some of these problems.  The main aims are to discover, model and reveal information in the form of (i) structures in data, (ii) shapes from data, and (iii) dynamics underlying data.  This project leverages concepts from mathematical areas of differential and algebraic topology and geometry, applied statistics and combinatorics, and computational areas of algorithms, graph theory, and statistical/machine learning.  Research in geometric and topological data analysis has brought forth the need to recast and reinvestigate classical concepts in statistics and mathematics in the context of finite data, approximations, and noise.  This project investigates explicit or hidden structures behind data, such as cluster trees, which are the basis for understanding and efficient processing of data.  Additionally, the PIs aim to model the precise shape behind data globally or locally, which are essential for providing a platform where various statistical analyses can be carried out.  Particular examples include the shape space of surface models and the tree space of phylogenetic trees.  Finally, this project will consider dynamics in the data, where the interplay between temporal and topological/geometric features can lead to deeper insights.  All of these areas will inevitably be enriched by new applications."
"1763665","AF: Medium: Collaborative Proposal: Foundations of Adaptive Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","03/01/2018","03/02/2019","Cynthia Dwork","MA","Harvard University","Continuing grant","Tracy Kimbrel","02/28/2021","$196,862.00","","dwork@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7924, 7926","$0.00","Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. <br/><br/>The technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of ""over-fitting"" and ""false discovery."" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763314","AF: MEDIUM: Collaborative Research: Foundations of Adaptive Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","03/01/2018","03/02/2019","AARON ROTH","PA","University of Pennsylvania","Continuing grant","Tracy Kimbrel","02/28/2021","$249,299.00","Weijie Su","aaroth@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7796","7924, 7926","$0.00","Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. <br/><br/>The technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of ""over-fitting"" and ""false discovery."" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763896","CIF: Medium: Multi-Agent Consensus Equilibrium: Modular Methods for Integrating Disparate Sources of Expertise","CCF","COMM & INFORMATION FOUNDATIONS","05/01/2018","04/23/2018","Charles Bouman","IN","Purdue University","Continuing grant","Phillip Regalia","04/30/2022","$316,000.00","Gregery Buzzard, Garth Simpson, Stanley Chan","bouman@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7924, 7935, 9251","$0.00","Over the past decade, two major trends have reshaped data science: (i) a growing tidal wave of imaging and sensing devices and (ii) the rapid adoption of break-through machine learning technologies. From autonomous vehicles to medical devices, the ability to collect and analyze large quantities of data is changing the world. The goal of this research project is to create a new mathematical and computational framework for integrating distributed and multimodal sensor data into multiple types of emerging models in data science and machine learning so as to extract more information from data. The project team includes researchers from diverse disciplines who will address problems ranging from physical science and medicine to consumer imaging and industrial inspection. The research will result in theories, algorithms, and open software that can be used to integrate information from heterogeneous sensing systems to estimate and reconstruct signals and images.<br/><br/>The framework for model-data integration is based on a new theory of Multi-Agent Consensus Equilibrium (MACE). MACE allows for modular integration of multi-modal physical sensor information with information derived from data science models. At the core of this approach is the computational solution of the consensus equilibrium equations. These equations balance distributed sensor information with prior knowledge provided by machine learning models. The MACE framework is a generalization of the more traditional Bayesian or regularized inverse approach, but it allows for the use of non-traditional data science models such as deep convolutional neural networks in the solution of sensing and imaging problems. This project's contributions are in four areas: Thrust 1: Foundational Theoretical Methods; Thrust 2: Robust Sensor and Data Model Integration; Thrust 3: Multimodal and Networked MACE; and Thrust 4: Automated Experimentation. The project also includes integrated educational activities, engaging both graduate and undergraduate students in this research, as well as the development of new courses on consensus equilibrium and nonlinear optical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763786","AF: Medium: Collaborative Research: Foundations of Adaptive Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","03/01/2018","02/14/2018","Adam Smith","MA","Trustees of Boston University","Continuing grant","Tracy J. Kimbrel","02/28/2021","$86,913.00","","ads22@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7796","7924, 7926","$0.00","Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. <br/><br/>The technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of ""over-fitting"" and ""false discovery."" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763191","AF: MEDIUM: Collaborative Research: Foundations of Adaptive Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","03/01/2018","02/14/2018","James Zou","CA","Stanford University","Continuing grant","Tracy J. Kimbrel","02/28/2021","$89,310.00","","jamesz@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","7924, 7926","$0.00","Classical tools for rigorously analyzing data make the assumption that the analysis is static: the models and the hypotheses to be tested are fixed independently of the data, and preliminary analysis of the data does not feed back into the data gathering procedure. On the other hand, modern data analysis is highly adaptive. Large parts of modern machine learning perform model selection as a function of the data by iteratively tuning hyper-parameters, and exploratory data analysis is conducted to suggest hypotheses, which are then validated on the same data sets used to discover them. This kind of adaptivity is often referred to as p-hacking, and blamed in part for the surprising prevalence of non-reproducible science in some empirical fields. This project aims to develop rigorous tools and methodologies to perform statistically valid data analysis in the adaptive setting, drawing on techniques from statistics, information theory, differential privacy, and stable algorithm design. <br/><br/>The technical goals of this project include coming up with: 1) information-theoretic measures that characterize the degree to which a worst-case data analysis can over-fit, given an interaction with a dataset; 2) models for data analysts that move beyond the worst-case setting, and; 3) empirical investigations that bridge the gap between theory and practice. The problem of adaptive data analysis (also called post-selection inference, or selective inference) has attracted attention in both computer science and statistics over the past several years, but from relatively disjoint communities. Part of the aim of this project is to integrate these two lines of work. The team of researchers on this project span departments of computer science, statistics, and biomedical data science. In addition to attempting to unify these two areas, the broader impacts of this research will be to make science more reliable, and reduce the prevalence of ""over-fitting"" and ""false discovery."" The project also has a significant outreach and education component, and will educate graduate students, organize workshops, and produce expository materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1513936","CIF: Medium: Data Science: Analytics for Unstructured and Distributed Data","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2015","04/16/2018","Jose Moura","PA","Carnegie-Mellon University","Continuing grant","Phillip Regalia","09/30/2019","$1,197,412.00","Soummya Kar","moura@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7924, 7935, 7936, 9251","$0.00","CIF: Medium: Data Science: Analytics for Unstructured and Distributed Data<br/>Jos M F Moura and Soummya Kar<br/><br/>Abstract<br/><br/>There has been a perfect storm of convergent technologies leading to an onslaught of data in digital format, ready to be acquired, stored, accessed, transmitted, and processed. This research develops quantitative analytics that are appropriate for data arising in many novel areas like social networks or urban environments, outside traditional engineering or science. This data is unstructured?it is no longer a single time series or a single image and cannot be naturally arranged in a vector or table. The data is distributed?it originates from many different agents, possibly scattered over a large physical space (e.g., a metro area). <br/><br/>To process unstructured and distributed Big Data, the research extends traditional signal processing methods to distributed signal processing on graphs (DSPG) by associating two graphs with the data: 1) the ?physical graph? whose nodes index the data and whose physical edges capture the relations or dependencies among the data; and 2) the ?cyber? graph, possibly different from the physical graph, whose nodes index distributed processing units and whose cyber edges represent (local) communication channels among these units. To extend DSPG analytics to process distributed Big Data analytics, the investigators address the following challenges: 1) discover the structure of the underlying physical graph; and 2) design consensus+innovations scalable distributed algorithms to accurately process the distributed Big Data. Examples of important analytics include: forward filtering that computes y = Hx, where H is a graph filter or graph transform and x is the data associated with the agents; and inverse problems that compute solutions to linear systems Hx = y, where H and y are given, while x is reconstructed."
"1846218","CAREER: Scalable Algorithmic Primitives for Data Science","CCF","ALGORITHMIC FOUNDATIONS","07/01/2019","02/01/2019","Yang Peng","GA","Georgia Tech Research Corporation","Continuing grant","Rahul Shah","06/30/2024","$70,367.00","","rpeng@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","1045, 7926","$0.00","This project aims to improve some of the most fundamental algorithms in computing: solving systems of linear equations, optimization on graphs, and maintaining dynamically changing networks. Tools for solving these problems are integral components of high-level programming languages such as MATLAB and Julia, and are frequently taught as basic programming constructs in courses on machine learning, statistics, and data science. Improved algorithms for these problems can provide faster, more robust, and easier-to-use algorithmic primitives, which would in turn enable computing on larger and more diverse data sets in areas such as data mining, image processing, scientific computing, and network science. The proposed works will actively involve graduate students, and their results will be incorporated into courses at both graduate and undergraduate levels. The project will also support the PI's long-time involvement with algorithmic problem-solving outreach activities, with a focus on making these activities more accessible to underrepresented groups, and institutionalizing the involvement of graduate students.<br/> <br/>The problems that this project proposes to study, linear system solvers and optimization on graphs, are some of the most well-studied problems in algorithm design. Previous work on these topics led to many widely-used algorithms and data structures. The main approach of this project is motivated by progress on combining numerical and combinatorial algorithmic primitives through the graph Laplacian matrix, known as the `Laplacian paradigm' for designing graph algorithms. Recent and ongoing work by the PI and collaborators led to the current best algorithms for many key problems involving graph Laplacians, and more importantly, significantly broadened the scope of problems addressed. An underlying theme in these results is that the most powerful tools work with intermediate algorithmic states, and the focus of this project is a more in-depth study of this phenomenon using ideas from data structures, which also construct and reuse intermediate algorithmic states. These directions of investigation will lead to new algorithmic constructs, give improved tools for computing on static and dynamic data, and enable new applications based on computations on graphs and matrices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1553075","CAREER: Advancing Multidimensional Data Science via New Algebraic Models and Scalable Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","05/15/2016","09/14/2018","Shuchin Aeron","MA","Tufts University","Continuing grant","Phillip Regalia","04/30/2021","$425,438.00","","shuchin.aeron@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7797","1045, 7797, 7936, 9251","$0.00","Building upon the rapid advancements in monitoring, networking and sensing technologies, most modern data collected are inherently multidimensional in nature, that is, each datum is influenced by a variety of factors. For example, a pixel in a remote sensing video is a function of time, color (wavelength) and spatial location. Another contemporary example comes from the widely used rating and recommendation systems, where a rating depends on the user, user demographic, product being rated and time. In fact, this is the case for numerous other applications such as cellular network performance statistics, geophysical systems for earth sciences, and education statistics in interactive learning and collaboration environments. This research addresses the need to advance data science for reliable and scalable information acquisition and processing for these complex and large-scale multidimensional data.<br/> <br/>In particular, the research builds and investigates a novel linear and multilinear algebraic framework to model multidimensional data. Using this framework one can tap into the well-developed body of vector space methods and adapt them to process multidimensional data in a principled manner to realize orders of magnitude performance gains over current methods.  The research also addresses the challenges, which arise in deploying the framework for large-scale applications and investigates numerical and memory efficient algorithms. Integration of research and education is enabled through new cross-cutting curriculum development, continued undergraduate mentoring, and through integration of the research outcomes with existing undergraduate curriculum. The broader impacts are realized through a number of collaborative efforts involving, Tufts Interactive Learning and Collaboration Environment (InterLACE) program, Brigham and Women?s Hospital and AT&T research."
"1750430","CAREER: Foundations of Information Theory: Information Inequalities and Dimension-Free Phenomena","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2018","02/02/2018","Thomas A Courtade","CA","University of California-Berkeley","Continuing grant","Phillip Regalia","07/31/2023","$174,282.00","","courtade@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7797","1045, 7935","$0.00","Information inequalities stemming from entropy and mutual information form the basis for Shannon's mathematical theory of communication, data transmission and storage, the vast consequences of which have ushered in the modern information age. While originally bounding theoretically the rate at which data may be transmitted over an imperfect channel, or how far an information source may be compressed, the foundational nature of these inequalities has led to their widespread applications in quantitative fields ranging from computer science to physics. This may be attributed, in part, to these inequalities being dimension-free, that is, the sharpness of the estimate does not degrade with the data dimension, thus making them suitable for analysis and inference in high-dimensional settings that are characteristic of modern problems in statistics, optimization and data science.  Broadly speaking, this project seeks to further elucidate these foundational underpinnings of information theory, and to extend their applicability to modern problems in statistics and data analysis that seek to uncover information from large data sets. The research is coupled with a plan to integrate research and education at multiple levels: the project will train researchers at the interface of statistics, information theory and mathematics, preparing them to enter academic and industrial careers in data science, and skillfully adapt to new fields as national priorities change. Other aims are to promote collaboration within the broader research community through development of thematic workshops and tutorials. <br/><br/>This project will undertake a systematic investigation of information inequalities. This goal will be achieved through an integrated research agenda that seeks to: (i) quantify high-dimensional statistical phenomena; (ii) characterize extremal properties of information inequalities; and (iii) discover new concentration and isoperimetric phenomena through linking information- and transportation-based quantities on graphs and other discrete spaces.  Through these aims, this project will promote the influence of information theory across traditional boundaries and enrich the set of intellectual questions addressed.  Given the intrinsic importance of quantifying statistical and probabilistic phenomena throughout science and engineering, the results will have broad and lasting impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1846300","CAREER: Statistical Inference Under Information Constraints: Efficient Algorithms and Fundamental Limits","CCF","COMM & INFORMATION FOUNDATIONS","02/01/2019","12/11/2018","Jayadev Acharya","NY","Cornell University","Continuing grant","Phillip Regalia","01/31/2024","$85,491.00","","acharya@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","1045, 7935","$0.00","Data science and machine learning systems have to optimize constraints on the availability of data, computation time, memory for storage, and privacy concerns. For example, while performing web search on mobile devices, one would like the applications to be small in size, communicate as little data as possible, and leak as little about the user as possible. These constraints are often at odds with each other. A system that provides strong privacy guarantees might require more data and computation, and a system that uses little data might require more computation. A fundamental understanding of the limits and trade-offs between constrained resources such as samples, time, memory, communication, and privacy is critical for tackling the many challenges in data science that lay ahead. In spite of many success stories of data science, these trade-offs are poorly understood even in some of the simplest settings. This project aims to establish the fundamental trade-offs between these resources, as well as design efficient schemes that achieve them. The project outcomes can help design faster, communication-frugal, privacy-preserving, and space-efficient learning systems. The project seeks to involve the participation of a diverse group of researchers in this project through outreach activities that target undergraduate students and under-represented communities.<br/><br/>The investigator will formulate and study fundamental statistical inference tasks such as distribution estimation, hypothesis testing, and distribution property estimation under the information constraints mentioned above. A particular direction of interest is the impact of the availability of shared randomness on the other constraints for distributed machine learning systems. While the role of randomness has been studied in problems in communication complexity, its role in machine learning systems is often overlooked. The project will integrate ideas from computer science, information theory, machine learning, and statistics, seeking to bridge researchers from these communities. All findings of this project will be disseminated through publications, and will be made publicly available on the investigator's website.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1526513","AF: Small: Analyzing Complex Data with a Topological Lens","CCF","ALGORITHMIC FOUNDATIONS","09/01/2015","08/17/2015","Yusu Wang","OH","Ohio State University","Standard Grant","Rahul Shah","08/31/2019","$399,999.00","Facundo Memoli, Tamal Dey","yusu@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7796","7923, 7929","$0.00","In the modern data-centric era, one is constantly faced with the task of extracting intelligent summaries out of diverse, complex data. This task is becoming increasingly challenging as the data becomes more complex.   Recent work has demonstrated that topological ideas and concepts can be powerful in extracting essential structures/features that are hidden in data.  Although existing topological methods are promising and powerful, they are limited when analyzing data that is laced with complex maps (e.g, non-real valued functions) and temporal components. <br/><br/>This project aims to broaden the scope of topological techniques and methodologies for analyzing such complex data.  Specifically, the PIs will investigate novel methodologies and computational issues to address key challenges caused by complexity in modern data:  the diverse properties/information associated with data, the dynamic/time-varying behavior of data, and the sheer volume of the data. The project will provide a theoretical understanding of a recently proposed framework, called Mapper, and its extension to a multiscale formulation. It will explore the use of persistence methodologies, including zigzag constructions, for understanding the time-varying aspects of data. <br/><br/>The geometric and topological ideas behind this project will bring new perspectives to the important field of computational data analysis. A successful algorithmic theory for summarizing and characterizing complex and dynamic data with topological techniques can provide a powerful tool for data exploration and analysis in various fields of science and engineering. The educational impact of this project is in a large synergy between mathematics and computer science motivated by real applications. The findings from the project are planned to be part of the course materials that the PIs develop. This project will train graduate students who will develop skills in mathematics and theoretical computer science, most notably in algorithms and topology, in writing efficient software, and its application to analyzing data sets. The combination of such skills is becoming increasingly essential in modern data science."
"1619081","AF: Small: Redundancy exploiting algorithms for high throughput genomics","CCF","ALGORITHMIC FOUNDATIONS","08/01/2016","07/11/2016","Cenk Sahinalp","IN","Indiana University","Standard Grant","Rahul Shah","07/31/2019","$400,000.00","Funda Ergun","cenksahi@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7796","7923, 7926, 7931","$0.00","Determining the genomic makeup of individuals is crucial for understanding how certain genomic variants ultimately lead to disease (such as cancer). Determining genomic makeup of agriculturally important plants, trees, farm animals and wild life help improve agriculture, forestry, veterinary medicine and environmental science. Since the introduction of ""next generation sequencing technologies"" in 2008, the cost of genome sequencing has dropped by a factor of 1000. This has led to an increase in the speed genomic data is generated that far outpaces the improvements in our computing and data storage capability. With the advent of these cheap, and fast genome sequencing technologies, the scientific community has been able to launch mega-projects such as The Pan Cancer Analysis of Whole Genomes Project, which aim to determine the genome sequences of thousands of cancer patients. Our project aims to address the imminent data size challenges in these large scale genomic studies through new genomic data compression methods that aim to reduce the redundancy in how genomic sequences are represented. The source of this redundancy is the high similarity among genome sequences of individual patients, as well as the high similarity between regions across the genome of a single human genome. Since the main difficulty in extracting information from genome sequences is computational, reduction in the computational resources needed to manage and analyze genomic data through the compression methods will help genomics improve human life and the environment. <br/><br/>The impact of this project on student and personnel training will be in terms of two new graduate courses at Indiana University:  a course on data management, access and processing for genomic data by PI Sahinalp, and a course on compressed algorithms with a focus on genomic data, emphasizing the effects of new big data paradigms compression, by PI Ergun. Both courses will fit into the CS PhD program, as well as into the existing Bioinformatics and Data Science Master's programs; they are also intended to attract the more curious undergraduates.<br/><br/>The rapid advancement of nucleic acid sequencing technology has re-shaped almost every field of life science, from agriculture to bioenergy, and from environmental science to biomedicine. Large-scale genome projects are producing petabyte-scale data from thousands of patients or by mobile sensors collecting environmental samples. As the technology marches forward, most people who visit hospitals will eventually have their (possibly tissue-specific) genomes sequenced. Genomic data will be collected from thousands to millions of non-model organisms and their populations in order to assess the biodiversity within the corresponding ecosystem. Complex microbial communities will be sampled from thousands of geographic locations to study the influence of environmental conditions. Furthermore, these studies will involve continuous data collection efforts, for the purpose of monitoring the dynamic changes in biosystems by the use of genome-wide or transcriptome-wide sequencing. As a result, genomic data generation is to occur at an unprecedented pace, necessitating the development of novel algorithms to help reduce the burden of genomic sequence data on computational, storage and transmission systems. <br/><br/>This project combines the unique strengths of the two investigators at Indiana University, bringing a principled, algorithmic approach to critical infrastructure problems in genomics. The project will address the needs of the next stage of genomic data generation by mega cancer projects, portable devices collecting environmental samples, and even smaller sensors to be embedded in the human body, through the use of new compression tools and compressed data structures for communicating, storing, managing, and accessing large collections of (streaming) genome data. For this purpose, we will employ and expand the existing algorithmic repertoire involving approximation algorithms, sublinear algorithms, lossless data compression, I/O efficient, memory hierarchy aware/oblivious and compressed data structures."
"1822279","Conference: Machine Learning in Science and Engineering","CCF","STATISTICS, PROJECTS, ALGORITHMIC FOUNDATIONS, Big Data Science &Engineering, Materials Eng. & Processing","06/01/2018","05/18/2018","Dana Randall","GA","Georgia Tech Research Corporation","Standard Grant","Tracy J. Kimbrel","05/31/2019","$29,999.00","Newell Washburn","randall@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1269, 1978, 7796, 8083, 8092","047Z, 062Z, 7556","$0.00","This award supports the first annual Symposium on Machine Learning in Science and Engineering (MLSE), held in Pittsburg, Pennsylvania, June 6-8, 2018.  The meeting, initially organized by Carnegie Mellon University and Georgia Tech, is the first comprehensive and open annual conference bringing together leading researchers in science and engineering whose work benefits from advances in machine learning and data science.  While machine learning has revolutionized many areas of biological and biomedical research, its impact across the sciences and engineering is at an early stage. This symposium will bring together researchers in a diversity of Science, Technology, Engineering, and Math (STEM) areas focused on applying machine learning to problems of fundamental or applied nature. Presentations will focus on adapting existing machine learning methods to current research areas, developing new machine learning algorithms specific to science and engineering, and identifying new frontiers of research that may only be pursued using a data-driven approach. The symposium will offer attendees focused short courses taught by experts in machine learning on a variety of cutting-edge tools that are critical in advancing these fields. The MLSE symposium will help catalyze machine learning methodologies and collaborations across the sciences and engineering, bringing together researchers in a diversity of STEM areas focused on applying machine learning to fundamental and applied problems. Presentations will focus on adapting existing machine learning methods to current research areas, developing new machine learning algorithms specific to science and engineering, and identifying new frontiers of research that may only be pursued using a data-driven approach.<br/><br/>The symposium is anticipated to reach, in its first year, at least 400 direct participants and attendees, including under-represented minorities, those attending Minority Serving Institutions (MSIs), and low income students local to the conference venues, or selected to travel to the event based on merit and need. Several groups within the research community are involved, including a diverse group of students, early career researchers and faculty. Information will be widely disseminated on a continuing basis through news items published via community-specific and broad news release venues. Partial support is being provided primarily to enable participation by students and young researchers, in addition to a limited number of tutorial and plenary speakers.  The organizers are committed to promoting participation among underrepresented groups, junior researchers and students, and including tutorials to widen accessibility to as large a group of attendees, as possible. The organizers will have an open competition for these travel awards, selected by a diverse committee, and the opportunity to apply will be widely disseminated across the relevant disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740751","TRIPODS: Institute for Foundations of Data Science (IFDS)","CCF","TRIPODS Transdisciplinary Rese","10/01/2017","08/29/2018","Piotr Indyk","MA","Massachusetts Institute of Technology","Continuing grant","Tracy J. Kimbrel","09/30/2020","$868,500.00","Devavrat Shah, Jonathan Kelner, Philippe Rigollet, Ronitt Rubinfeld","indyk@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","041Y","047Z, 062Z","$0.00","The aim of this project is to plant the seeds for an interdisciplinary institute devoted to foundations of data science at MIT. During the three years of Phase I, the project goal is to stimulate research and educational interactions between mathematics, statistics and theoretical computer science, both within MIT and in the research community at large. On the way, the team will also develop the organizational capabilities and visibility needed for the (potential) Phase II activities.<br/><br/>The project activities in Phase I will be organized around 5 semester-long themes. Each theme will be devoted to a topic at the intersection of at least two (and often three) TRIPODS areas, and will focus on catalyzing interactions between them. The specific themes are: statistical and computational tradeoffs, sub-linear algorithms and distribution testing, learning with complex structures, graphical models and exchangeability, and non-convex optimization. Within each theme, the PIs will organize numerous activities, including a workshop, a fall/spring school covering introductory material, and regular seminars. They will also support knowledge transfer activities, including office hours that will offer free help and consultation on algorithmic, mathematical and statistical aspects of data science. To facilitate the interactions between the participants, the project will support two project postdocs, who will play the role of connectors between different disciplines."
"1762363","SHF: Medium: Collaborative Research: Computer-Aided Programming for Data Science","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","04/23/2018","Ruben Goncalves Martins","PA","Carnegie-Mellon University","Continuing grant","Nina Amla","05/31/2022","$74,416.00","","rubenm@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7924, 8206","$0.00","The goal of this project, named DataWizard, is to dramatically simplify the effort that is currently required for data analytics through the use of computer-aided programming. Specifically, this project aims to semi-automate data collection, querying, and wrangling tasks by automatically generating programs from informal specifications. As a result, the DataWizard project will allow domain scientists to focus on more interesting data analytics and visualization tasks, leaving the ""grunt work"" of data science to computer-aided programming tools. The project will also advance the state-of-the-art in automated program synthesis and natural language processing and apply these techniques to the burgeoning field of big data analytics. <br/><br/>From a technical perspective, the goals of the DataWizard project are three-fold. First, this project develops novel programming-by-example and information extraction techniques to address challenges that arise in data collection, including consolidation of different data sources, transformations between hierarchical and relational data, and extraction of information from unstructured data sources. Second, this project explores new techniques for querying data using natural language descriptions. In particular, this project considers data extraction from relational and noSQL databases as well as semi-structured data sources, such as XML and JSON. Third, this project develops novel program synthesis methods for automating data wrangling, cleaning, and imputation tasks that commonly arise in data analytics. Overall, these techniques  make it significantly easier for data scientists to gain insights from messy data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1850052","CRII: AF: Enriched Topological Summaries for Inverse Problems","CCF","ALGORITHMIC FOUNDATIONS","07/01/2019","01/30/2019","Justin Curry","NY","SUNY at Albany","Standard Grant","Rahul Shah","06/30/2021","$174,074.00","","jmcurry@albany.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","CSE","7796","7796, 7929, 8228","$0.00","With Big Data comes the need for succinct data summaries that are easy to visualize and understand. Topological Data Analysis (TDA) offers a particular suite of computational tools for summarizing and visualizing shape in high-dimensional data sets. These tools have been used in recent years to guide new discoveries in cancer research, neuroscience, materials science, image analysis, and many areas where shape, broadly construed, is of importance. However, by summarizing data in a succinct way certain important differences between data sets can often go undetected. This project provides a new mathematical framework for precisely measuring how lossy the most popular methods in TDA are. By developing new ways for quantifying how large-scale differences go undetected, the project offers enrichments of current topological methods to obtain novel data science tools with greater distinguishing power. The awarded funds will go primarily to fund a graduate student to aid the PI in basic research and development of these enriched topological summaries. Computational experiments on data sets of public interest, e.g. time-varying socio-economic indicators and weather data, will be carried out to test performance of these tools against current state of the art methods. Additionally, the PI will incorporate these novel methods into the data science curriculum at the PI's host institution and recruit students from historically under-represented groups to be trained as the nation's next generation of data scientists.<br/><br/>This project is an ambitious extension of earlier work undertaken by the PI and provides a targeted attack on the inverse problem for the main objects of study in topological data analysis: the merge tree, which tracks how connected components of the sub-level set of a function evolves; Reeb graphs, which tracks connected components of the fiber of a function; and the barcode/persistence diagram, which is a collection of intervals/points in the plane that represent an algorithmic pairing of critical points of a function on a space. The PI showed in earlier work how merge trees determine the associated barcode and provided a precise enumeration of how many distinct merge trees have the same barcode. By identifying functions on the real line that are related by an orientation preserving coordinate transformation, the PI showed how a novel enrichment of the merge tree---the chiral merge tree---faithfully captures these equivalence classes of functions and offers exponential distinguishing power over the barcode for time-series analysis. This project aims to carry out similar analysis for functions on surfaces, with an eye toward improving the classification performance of persistent homology in image analysis, and for Reeb graphs, to better integrate with level-set persistent homology. By carrying out a careful study of inverse problems in each of these settings, novel enrichments analogous to the chiral merge tree will be developed. Additionally, to make these enriched topological summaries useful for data classification tasks, novel metrics will be defined for comparing each of these summaries and algorithms will be developed for the efficient computation of these metrics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1351108","CAREER: Algorithms for understanding data","CCF","ALGORITHMIC FOUNDATIONS","07/01/2014","01/22/2014","Gregory Valiant","CA","Stanford University","Standard Grant","Rahul Shah","06/30/2019","$500,000.00","","gvaliant@cs.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","1045, 7926","$0.00","Given samples from some unknown distribution, what can one infer about the underlying distribution, and how efficiently can these inferences be made? In many of the most fundamental settings, our understanding of the computational and information theoretic possibilities and barriers is still startlingly poor. This project tackles two broad research objectives: developing efficient algorithms for probing data, and understanding how to efficiently estimate properties of distributions. The first line of research seeks to understand which questions about a dataset can be answered extremely efficiently, requiring computational resources (time, or memory) that are sublinear in the size of the dataset or distribution. The second research objective is to understand the minimal amount of information necessary to ascertain, with high probability, whether or not a distribution or dataset possesses a given property. In the context of statistical property estimation, this problem asks how few samples are needed to estimate the property in question to a desired accuracy, with high probability. This research pursues both new estimation algorithms, and new information theoretic tools and lower bounds.<br/><br/>With vast and important datasets emerging across many disciplines, from genetic, biological, and medical databases, to databases documenting our economic and social behaviors, the challenge of how to make sense of them has particular immediate relevance and has rapidly become the bottleneck in scientific understanding.  The specific problems investigated in this project arise in the analysis of these datasets; algorithmic advances on these problems have the potential to very quickly be adopted and transform ongoing data analysis efforts.  Beyond the immediate implications for the data sciences, these questions are extremely basic and foundational. As such, new techniques, perspectives, and insights gleaned from their study are likely to have broad implications for other problems throughout computer science, statistics, information theory, and the data sciences."
"1762299","SHF: Medium: Collaborative Research: Computer-Aided Programming for Data Science","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","04/23/2018","Isil Dillig","TX","University of Texas at Austin","Continuing grant","Nina Amla","05/31/2022","$409,971.00","Gregory Durrett","isil@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7924, 8206","$0.00","The goal of this project, named DataWizard, is to dramatically simplify the effort that is currently required for data analytics through the use of computer-aided programming. Specifically, this project aims to semi-automate data collection, querying, and wrangling tasks by automatically generating programs from informal specifications. As a result, the DataWizard project will allow domain scientists to focus on more interesting data analytics and visualization tasks, leaving the ""grunt work"" of data science to computer-aided programming tools. The project will also advance the state-of-the-art in automated program synthesis and natural language processing and apply these techniques to the burgeoning field of big data analytics. <br/><br/>From a technical perspective, the goals of the DataWizard project are three-fold. First, this project develops novel programming-by-example and information extraction techniques to address challenges that arise in data collection, including consolidation of different data sources, transformations between hierarchical and relational data, and extraction of information from unstructured data sources. Second, this project explores new techniques for querying data using natural language descriptions. In particular, this project considers data extraction from relational and noSQL databases as well as semi-structured data sources, such as XML and JSON. Third, this project develops novel program synthesis methods for automating data wrangling, cleaning, and imputation tasks that commonly arise in data analytics. Overall, these techniques  make it significantly easier for data scientists to gain insights from messy data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1637534","AitF:Collaborative Research: Bridging the Gap between Theory and Practice for Matching and Edge Cover Problems","CCF","Algorithms in the Field","09/01/2016","08/22/2016","Alex Pothen","IN","Purdue University","Standard Grant","Tracy J. Kimbrel","08/31/2020","$399,401.00","","apothen@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7239","","$0.00","Every year the 19,000 students who graduate from medical schools in the U.S. are  matched with the hospitals where they will do their residency training. Both students and hospitals rank their choices, and an algorithm is used to find a matching that gives each student their best available choice. These problems also arise in other contexts: matching organ donors to recipients whose bodies will not reject the transplanted organ, matching advertisements to web surfers based on their interests, etc. Variations of matching problems, and related edge cover problems, are formalized in computer science on combinatorial objects called graphs, and involve beautiful mathematics, sophisticated algorithms, efficient implementations of these algorithms on modern desk-top computers and supercomputers, and empirical evaluation on a number of problems that arise in application areas such as computational science and engineering, data science, network science, etc.  In this project, the two PIs will develop new algorithms and software for matching and edge cover problems, and make implementations available for practitioners in various fields of science, engineering and industry. The PIs will also train two PhD students in this project, and develop teaching resources to make these developments accessible to undergraduate and graduate students in computer science. <br/><br/>The problem of computing a matching that maximizes some objective function has been actively investigated for decades, driven by many high-profile industrial and medical applications.  This project focuses on the design, theoretical analysis, and implementation of matching algorithms that meet the needs of modern applications, and considers generalized matching problems such as b-matching, b-edge cover, and metric matching.  Classical serial algorithms that compute exactly optimum matchings are not always suited to massive graph data sets, which can contain billions of edges.  Fortunately, in many applications it suffices to have nearly optimum matchings rather than exactly optimum ones.  One goal of this project is to design simple and efficient matching algorithms that are both highly parallel, and produce provably good approximate solutions.<br/><br/>This project will examine several open problems on the approximability of generalized weighted matching problems, particularly on which matching-type problems admit linear time algorithms with approximation factor arbitrarily close to one.  To that end, the PIs will study how relaxing standard linear programming formulations of generalized weighted matching problems allows for more efficient algorithms. These and other algorithms will be modified to make them efficient on modern processors that support parallel computing. <br/><br/>Two PhD students will be trained in this project. Generalized graph matching algorithms are now applied in numerical linear algebra software, for preconditioning, graph clustering, anonymizing data, and network alignment.  The PIs will evaluate the performance of new and existing algorithms on these applications.  The PIs will make freely available all code of matching algorithms developed under this project.<br/><br/>Basic matching algorithms from the mid-20th century are firmly established in the canon of computer science education, but few modern matching algorithms are taught at the undergraduate level.  The PIs will incorporate modules on modern matching and applications into their courses at Purdue University and the University of Michigan, and make these materials publicly available."
"1637546","AitF:Collaborative Research: Bridging the Gap between Theory and Practice for Matching and Edge Cover Problems","CCF","Algorithms in the Field","09/01/2016","08/22/2016","Seth Pettie","MI","University of Michigan Ann Arbor","Standard Grant","Tracy J. Kimbrel","08/31/2020","$400,000.00","","pettie@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7239","","$0.00","Every year the 19,000 students who graduate from medical schools in the U.S. are  matched with the hospitals where they will do their residency training. Both students and hospitals rank their choices, and an algorithm is used to find a matching that gives each student their best available choice. These problems also arise in other contexts: matching organ donors to recipients whose bodies will not reject the transplanted organ, matching advertisements to web surfers based on their interests, etc. Variations of matching problems, and related edge cover problems, are formalized in computer science on combinatorial objects called graphs, and involve beautiful mathematics, sophisticated algorithms, efficient implementations of these algorithms on modern desk-top computers and supercomputers, and empirical evaluation on a number of problems that arise in application areas such as computational science and engineering, data science, network science, etc.  In this project, the two PIs will develop new algorithms and software for matching and edge cover problems, and make implementations available for practitioners in various fields of science, engineering and industry. The PIs will also train two PhD students in this project, and develop teaching resources to make these developments accessible to undergraduate and graduate students in computer science. <br/><br/>The problem of computing a matching that maximizes some objective function has been actively investigated for decades, driven by many high-profile industrial and medical applications.  This project focuses on the design, theoretical analysis, and implementation of matching algorithms that meet the needs of modern applications, and considers generalized matching problems such as b-matching, b-edge cover, and metric matching.  Classical serial algorithms that compute exactly optimum matchings are not always suited to massive graph data sets, which can contain billions of edges.  Fortunately, in many applications it suffices to have nearly optimum matchings rather than exactly optimum ones.  One goal of this project is to design simple and efficient matching algorithms that are both highly parallel, and produce provably good approximate solutions.<br/><br/>This project will examine several open problems on the approximability of generalized weighted matching problems, particularly on which matching-type problems admit linear time algorithms with approximation factor arbitrarily close to one.  To that end, the PIs will study how relaxing standard linear programming formulations of generalized weighted matching problems allows for more efficient algorithms. These and other algorithms will be modified to make them efficient on modern processors that support parallel computing. <br/><br/>Two PhD students will be trained in this project. Generalized graph matching algorithms are now applied in numerical linear algebra software, for preconditioning, graph clustering, anonymizing data, and network alignment.  The PIs will evaluate the performance of new and existing algorithms on these applications.  The PIs will make freely available all code of matching algorithms developed under this project.<br/><br/>Basic matching algorithms from the mid-20th century are firmly established in the canon of computer science education, but few modern matching algorithms are taught at the undergraduate level.  The PIs will incorporate modules on modern matching and applications into their courses at Purdue University and the University of Michigan, and make these materials publicly available."
"1637576","AitF: Collaborative Research: Modeling movement on transportation networks using uncertain data","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS","09/01/2016","09/06/2016","Carola Wenk","LA","Tulane University","Standard Grant","James Donlon","08/31/2020","$317,681.00","","cwenk@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","7239, 7796","9150, 9251","$0.00","In the current data-centered era, there are many highly diverse data sources that provide information about movement on transportation networks. Examples include GPS trajectories, social media data, and traffic flow measurements. Much of this movement data is challenging to utilize due to the inherent uncertainty caused by infrequent sampling and sparse coverage. The goal of this project is to develop a unified framework that uses as many available data sources as possible to extract meaningful traffic and movement information automatically from the data. Probabilistic network movement models will be developed that capture movement probabilities and traffic volume on a network over time. The results will impact a range of applications that rely on capturing population movements, such as urban planning, geomarketing, traffic management, and emergency management. Educational activities will be integrated throughout the project. Students will be closely involved in research and practical implementations, and will be trained in spatio-temporal data management, algorithms development, and (trajectory) data analysis. The combination of such skills is increasingly important in spatial data science. Topics involved in this project will enrich the course material and curriculum development at both institutions. <br/><br/>The objective of this project is to create a unified framework for aggregating and analyzing diverse and uncertain movement data on road networks, with the aim to provide tools for querying and predicting traffic volume and movement. Probabilistic movement models on the network will be developed that can handle heterogeneous data sources, including GPS trajectories, geo-tagged social media data, bike-share data, public transport data, and traffic volume data. The diversity and spatio-temporal uncertainty of this data will be addressed with a Bayesian traffic pattern learning approach that first trains the movement models with the more certain data, which in turn will be used to fill gaps in the more uncertain data. The project will advance the state-of-the-art in theoretical communities (computational geometry, data mining) as well as in applied communities (spatial databases, location science). The results of the research will available on the project website (movementanalytics.org), and will be disseminated in prestigious venues through presentations and demonstrations at conferences, and through publications in journals."
"1566281","CRII: CIF: Towards Linear-Time Computation of Structured Data Representations","CCF","","04/15/2016","04/12/2016","Chinmay Hegde","IA","Iowa State University","Standard Grant","Phillip Regalia","03/31/2019","$173,282.00","","chinmay@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","026y","7797, 7936, 8228, 9150","$0.00","Estimating an unknown object from noisy, nonlinear, and incomplete observations constitutes a basic problem in data science. Standard solutions first assume that the unknown object obeys a structured mathematical representation, and then develop optimization algorithms for recovering the parameters of the representation. Moreover, rigorous analysis reveals that several such algorithms are statistically optimal. However, despite these advances in statistical understanding, the role of computation is far less well understood; in many cases, even the best methods incur a computation time proportional to a high-degree polynomial in terms of the data size. Therefore, to reap the full benefits of these methods in applications involving massive data, new algorithmic approaches are necessary. This research project introduces new theory and computational tools for learning structured data representations in linear running time. <br/><br/><br/>The new algorithmic approaches are based on the intuition that challenging optimization problems encountered in data analysis can be circumvented if the answers are merely approximate, rather than exact. Establishing precise tradeoffs between statistical performance, approximation quality, and running time is a key focus. With this intuition, the project addresses three specific problems: (i) Reconstructing signals and images from nonlinear observations. (ii) Recovering graphs from partial observations, such as linear sketches or inter-node distance measurements. (iii) Estimating multidimensional probability distributions from random samples. Algorithms developed within the scope of the project impact applications ranging from medical imaging, computer network monitoring, social network analysis, and non-destructive evaluation (NDE). All publications, data, and source code are publicly available. The project involves the active participation of both graduate and undergraduate students, and exposes them to a span of areas including mathematics, statistics, computer science, and optimization."
"1453432","CAREER: Privacy-preserving learning for distributed data","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace","07/01/2015","07/17/2018","Anand Sarwate","NJ","Rutgers University New Brunswick","Continuing grant","Phillip Regalia","06/30/2020","$556,200.00","","anand.sarwate@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","2878, 7797, 8060","1045, 7434, 7797, 7935, 9251","$0.00","Medical technologies such as imaging and sequencing make it possible to gather massive amounts of information at increasingly lower cost. Sharing data from studies can advance scientific understanding and improve healthcare outcomes. Concern about patient privacy, however, can preclude open data sharing, thus hampering progress in understanding stigmatized conditions such as mental health disorders.  This research seeks to understand how to analyze and learn from sensitive data held at different sites (such as medical centers) in a way that quantifiably and rigorously protects the privacy of the data.  <br/><br/>The framework used in this research is differential privacy, a recently-proposed model for measuring privacy risk in data sharing.  Differentially private algorithms provide approximate (noisy) answers to protect sensitive data, involving a tradeoff between privacy and utility.  This research studies how to combine private approximations from different sites to improve the overall quality or utility of the result. The main goals of this research are to understand the fundamental limits of private data sharing, to design algorithms for making private approximations and rules for combining them, and to understand the consequences of sites having more complex privacy and sharing restrictions.  The methods used to address these problems are a mix of mathematical techniques from statistics, computer science, and electrical engineering.<br/><br/>The educational component of this research will involve designing introductory university courses and material on data science, undergraduate research projects, curricular materials for graduate courses, and outreach to the growing data-hacker community via presentations, tutorial materials, and open-source software. <br/><br/>The primary aim of this research is bridge the gap between theory and practice by developing algorithmic principles for practical privacy-preserving algorithms. These algorithms will be validated on neuroimaging data used to understand and diagnose mental health disorders. Implementing the results of this research will create a blueprint for building practical privacy-preserving learning for research in healthcare and other fields.  The tradeoffs between privacy and utility in distributed systems lead naturally to more general questions of cost-benefit tradeoffs for learning problems, and the same algorithmic principles will shed light on information processing and machine learning in general distributed systems where messages may be noisy or corrupted."
"1849883","CRII: CIF: Approximate Message Passing Algorithms for High-Dimensional Estimation","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2019","03/05/2019","Cynthia Rush","NY","Columbia University","Standard Grant","Phillip Regalia","05/31/2021","$153,597.00","","cynthia.rush@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7797","7935, 8228, 9102","$0.00","Due a remarkable increase in computational power over the last few decades, the amount of data collected in fields such as biology, astronomy, and finance has expanded considerably. Because of this explosion of new data, many modern scientific and engineering applications require analysis of and learning on larger datasets and more complex problems than the field has ever considered before.  A major challenge for researchers is to understand how much data, or information, is necessary to solve such complex statistical problems, and given that data, how to most effectively use it to gain insight about the real-world application at hand.  This project explores these questions by developing and analyzing the performance of computationally-efficient algorithms and statistical procedures for these settings.  The research will bring together tools and ideas from information theory, statistical physics, and applied probability to use as a framework for understanding modern, high-dimensional statistics problems and complex machine learning tasks that are core challenges in engineering and data science.  Just as the research in this project is interdisciplinary, so are the educational activities pursued, which focus on making research outcomes accessible to non-experts, increasing opportunities for students from underrepresented communities in computing and data science, and training a new generation of data scientists with multi-disciplinary skillsets and research interests.<br/><br/>This project studies a class of computationally-efficient algorithms, referred to as approximate message passing or AMP, that are used for high-dimensional statistical inference and estimation tasks that underlie many practical applications such as imaging in healthcare and security or building autonomous vehicles using artificial intelligence. Moreover, because AMP allows for exact characterization of its asymptotic performance, such algorithms have been used to establish theory for estimation problems in machine learning and statistics. In many of these applications, AMP outperforms the best competing algorithms in both accuracy and runtime. Drawing on techniques from information theory, signal processing, machine learning, probability, and statistical physics, the goal of the project is to significantly expand and improve the theoretical foundations of AMP algorithms in order to (i) greatly extend the algorithm's capabilities in high-dimensional estimation, (ii) characterize the theoretical properties of the existing AMP algorithms for more general problem settings, and (iii) create new application areas for AMP algorithms and their supporting theory.  The work will lead to the introduction or greater use of AMP in burgeoning fields like machine learning and artificial intelligence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815361","CIF: Small: Collaborative Research: Generative Adversarial Privacy: A Data-driven Approach to Guaranteeing Privacy and Utility","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","02/20/2019","Lalitha Sankar","AZ","Arizona State University","Standard Grant","Phillip Regalia","09/30/2021","$308,000.00","","lalithasankar@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7797","7923, 7935, 9102, 9178, 9251","$0.00","There is a growing need to publish datasets for both public benefit (via data-driven research) and private gains (enterprise data sharing). However, consumer privacy concerns have largely stymied such efforts since large datasets also contain confidential information about participating individuals. This project leverages recent advancements in learning generative models directly from the datasets to introduce a novel framework called generative adversarial privacy (GAP). GAP formalizes adversarial learning as a game between a privatizer that wishes to learn the optimal privacy mechanism and any statistical adversary intent on learning the confidential features. This formalization is crucial to evaluate data-driven approaches against adversaries with strong inferential capabilities. This project will include interactions with Honeywell Labs as well as outreach and dissemination with Stanford industry partners in the electricity and smart cities sector. Outreach programs include exposing middle- and high-school girls to social network privacy challenges at ASU and K-12 teacher training on data science through the Stanford Office of Science Outreach Program.<br/><br/>The project will focus on three foundational problems. The first two ensure privacy of confidential features in the published data and involve developing: (i) theoretical limits of the GAP formulation for a large class of loss functions that capture a range of adversarial capabilities; and (ii) convergence guarantees of the proposed GAP model. The third problem focuses on guaranteeing identity privacy via synthetic datasets using a combination of generative models (to generate synthetic data from training data) and classes of statistical adversaries to understand the efficacy of generating synthetic datasets with both utility and privacy guarantees. A key element of this project involves testing on both publicly available datasets as well as proprietary data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844234","CAREER:Foundation of Communication-Efficient Distributed Computation and Monitoring","CCF","ALGORITHMIC FOUNDATIONS","06/01/2019","02/01/2019","Qin Zhang","IN","Indiana University","Continuing grant","Rahul Shah","05/31/2024","$111,254.00","","qzhangcs@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7796","1045, 7926, 7934","$0.00","Through the massive use of mobile devices, data clouds, and the rise of the Internet of Things, large amounts of data have been generated, digitized, and analyzed for the benefit of society. As data are often collected and maintained at different sites, communication has become necessary for nearly every computational task. Moreover, decision makers naturally want to maintain a centralized view of all the data in a timely manner, which requires frequent queries on the distributed data and, in the extreme, continuous monitoring of the query output.  The cost of communication has naturally become the bottleneck for such applications. This project aims to develop communication-efficient solutions for distributed computation and monitoring. The products will be integrated into a trilogy of courses in the foundations of data science. The project will involve training students at all levels, with an emphasis on gender diversity and participation of underrepresented groups. <br/><br/>This project targets three fundamental aspects of distributed computation: (1) the tradeoffs between the communication cost and the number of rounds of the computation in distributed one-shot computation, (2) the power of data partitioning, and (3) the connections between distributed one-shot computation and continuous monitoring.  The PI will approach these directions via the study of fundamental algorithmic problems in databases, data mining, networking, and machine learning. A systematic theory of communication-efficient computation and monitoring has the potential to impact a wide range of rapidly developing areas in theoretical foundations of big data, including streaming algorithms, sketching algorithms, and parallel and distributed computing. It will also deepen our understanding of communication complexity and information theory in theoretical computer science and mathematics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1637541","AitF: Collaborative Research: Modeling movement on transportation networks using uncertain data","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS","09/01/2016","09/06/2016","Dieter Pfoser","VA","George Mason University","Standard Grant","James Donlon","08/31/2020","$507,852.00","Andreas Zuefle","dpfoser@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7239, 7796","9251","$0.00","In the current data-centered era, there are many highly diverse data sources that provide information about movement on transportation networks. Examples include GPS trajectories, social media data, and traffic flow measurements. Much of this movement data is challenging to utilize due to the inherent uncertainty caused by infrequent sampling and sparse coverage. The goal of this project is to develop a unified framework that uses as many available data sources as possible to extract meaningful traffic and movement information automatically from the data. Probabilistic network movement models will be developed that capture movement probabilities and traffic volume on a network over time. The results will impact a range of applications that rely on capturing population movements, such as urban planning, geomarketing, traffic management, and emergency management. Educational activities will be integrated throughout the project. Students will be closely involved in research and practical implementations, and will be trained in spatio-temporal data management, algorithms development, and (trajectory) data analysis. The combination of such skills is increasingly important in spatial data science. Topics involved in this project will enrich the course material and curriculum development at both institutions. <br/><br/> The objective of this project is to create a unified framework for aggregating and analyzing diverse and uncertain movement data on road networks, with the aim to provide tools for querying and predicting traffic volume and movement. Probabilistic movement models on the network will be developed that can handle heterogeneous data sources, including GPS trajectories, geo-tagged social media data, bike-share data, public transport data, and traffic volume data. The diversity and spatio-temporal uncertainty of this data will be addressed with a Bayesian traffic pattern learning approach that first trains the movement models with the more certain data, which in turn will be used to fill gaps in the more uncertain data. The project will advance the state-of-the-art in theoretical communities (computational geometry, data mining) as well as in applied communities (spatial databases, location science). The results of the research will available on the project website (movementanalytics.org), and will be disseminated in prestigious venues through presentations and demonstrations at conferences, and through publications in journals."
"1527104","AF: Small: Collaborative Research: Mathematical Theory and Fast Algorithms for Rayleigh Quotient-type Optimizations","CCF","COMPUTATIONAL MATHEMATICS, ALGORITHMIC FOUNDATIONS","07/15/2015","07/08/2015","Ren-Cang Li","TX","University of Texas at Arlington","Standard Grant","Balasubramanian Kalyanasundaram","06/30/2019","$138,995.00","","rcli@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","1271, 7796","7923, 7933, 9263","$0.00","Many modern data analysis techniques and applications in machine <br/>learning try to learn what input data has the largest effects on the <br/>outputs. Rayleigh Quotients (RQ), or, more generally, RQ-type <br/>objective functions, are the basis of a mathematical technique that <br/>captures this information. This project conducts in-depth theoretical <br/>and algorithmic studies of three RQ-type optimizations: robust RQ <br/>optimizations that can handle data uncertainty, constrained RQ-type <br/>optimizations that can incorporate prior information from image <br/>segmentation or data clustering, and trace ratio optimizations that <br/>can perform multi-view spectral clustering. This project improves <br/>understanding of this practically important and user-oriented <br/>mathematical theory, creating computational methods that are embodied <br/>in open-source software. It not only advances mathematical theory <br/>and optimization algorithms in data science, but trains computer <br/>science and computational mathematics graduate students in <br/>interdisciplinary knowledge and tools necessary to undertake the <br/>project successfully. The PIs also involve undergraduate students <br/>in all aspects of this research project. <br/><br/>The PIs expect to produce a unified view of RQ-type optimizations, <br/>reformulating them into linear and nonlinear eigenvalue problems for <br/>which new variational principles can characterize the optimal <br/>solutions. These new principles should expose the numerical linear <br/>algebra characteristics of the underlying problems, supporting the <br/>development of fast algorithms that exploint the mathematical <br/>properties and sparse data structure."
"1527091","AF: Small: Collaborative Research: Mathematical Theory and Fast Algorithms for Rayleigh Quotient-type Optimizations","CCF","ALGORITHMIC FOUNDATIONS","07/15/2015","07/08/2015","Zhaojun Bai","CA","University of California-Davis","Standard Grant","Balasubramanian Kalyanasundaram","06/30/2019","$159,999.00","","bai@cs.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7796","7923, 7933","$0.00","Many modern data analysis techniques and applications in machine<br/>learning try to learn what input data has the largest effects on the<br/>outputs. Rayleigh Quotients (RQ), or, more generally, RQ-type<br/>objective functions, are the basis of a mathematical technique that<br/>captures this information. This project conducts in-depth theoretical<br/>and algorithmic studies of three RQ-type optimizations: robust RQ<br/>optimizations that can handle data uncertainty, constrained RQ-type<br/>optimizations that can incorporate prior information from image<br/>segmentation or data clustering, and trace ratio optimizations that<br/>can perform multi-view spectral clustering.  This project improves<br/>understanding of this practically important and user-oriented<br/>mathematical theory, creating computational methods that are embodied<br/>in open-source software.  It not only advances mathematical theory<br/>and optimization algorithms in data science, but trains computer<br/>science and computational mathematics graduate students in<br/>interdisciplinary knowledge and tools necessary to undertake the<br/>project successfully. The PIs also involve undergraduate students<br/>in all aspects of this research project.<br/><br/>The PIs expect to produce a unified view of RQ-type optimizations,<br/>reformulating them into linear and nonlinear eigenvalue problems for<br/>which new variational principles can characterize the optimal<br/>solutions.  These new principles should expose the numerical linear<br/>algebra characteristics of the underlying problems, supporting the<br/>development of fast algorithms that exploit the mathematical<br/>properties and sparse data structure."
"1814880","CIF: Small: Collaborative Research: Generative Adversarial Privacy: A Data-driven Approach to Guaranteeing Privacy and Utility","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/18/2018","Ram Rajagopal","CA","Stanford University","Standard Grant","Phillip Regalia","09/30/2021","$200,000.00","","ramr@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7797","7923, 7935, 9102","$0.00","There is a growing need to publish datasets for both public benefit (via data-driven research) and private gains (enterprise data sharing). However, consumer privacy concerns have largely stymied such efforts since large datasets also contain confidential information about participating individuals. This project leverages recent advancements in learning generative models directly from the datasets to introduce a novel framework called generative adversarial privacy (GAP). GAP formalizes adversarial learning as a game between a privatizer that wishes to learn the optimal privacy mechanism and any statistical adversary intent on learning the confidential features. This formalization is crucial to evaluate data-driven approaches against adversaries with strong inferential capabilities. This project will include interactions with Honeywell Labs as well as outreach and dissemination with Stanford industry partners in the electricity and smart cities sector. Outreach programs include exposing middle- and high-school girls to social network privacy challenges at ASU and K-12 teacher training on data science through the Stanford Office of Science Outreach Program.<br/><br/>The project will focus on three foundational problems. The first two ensure privacy of confidential features in the published data and involve developing: (i) theoretical limits of the GAP formulation for a large class of loss functions that capture a range of adversarial capabilities; and (ii) convergence guarantees of the proposed GAP model. The third problem focuses on guaranteeing identity privacy via synthetic datasets using a combination of generative models (to generate synthetic data from training data) and classes of statistical adversaries to understand the efficacy of generating synthetic datasets with both utility and privacy guarantees. A key element of this project involves testing on both publicly available datasets as well as proprietary data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816874","AF: Small: Foundations for Data-driven Algorithmics","CCF","SPECIAL PROJECTS - CCF","06/15/2018","06/04/2018","Yaron Singer","MA","Harvard University","Standard Grant","Rahul Shah","05/31/2021","$499,947.00","","yaron@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","2878","075Z, 7923, 7926","$0.00","The traditional approach in optimization assumes that the underlying objective is known, but in many real-life applications, the true objectives are not known and learned from data. This gap between theory and practice turns out to be quite dramatic, and leaves us without guarantees on the performance of optimization algorithms in such applications. The goal of this project is to develop a theory for algorithms whose input (i.e., the objective) is learned from data, and design algorithms that perform well in these settings. The technical challenges in this space are highly non-trivial, but their solution would dramatically impact our thinking in computer science and result in major advancements in AI. The project develops courses in optimization and data science that foster an interdisciplinary approach. The project will involve mentoring undergraduate and graduate students from underrepresented groups and promote an open access research culture. The investigator will develop new interdisciplinary connections through courses, seminars, and workshops with the goal of promoting a discipline of researchers working on algorithms for the information age.<br/><br/>In light of a recent line of impossibility results initiated by the investigator, the goal of this project is to investigate alternative notions of optimization that can facilitate desirable guarantees for data-driven optimization. The first direction in this project considers optimization from adaptive samples. The general notion of adaptivity is surprisingly under-explored, and advancement on this front can have a tremendous impact both on theory and applications. A complementary direction is to consider algorithms that are given samples on a training datasets, and seek to approximate the optimal solution of the testing dataset, drawn from the same distribution. Finally, the last direction considered is that of optimization from pairwise comparisons.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763514","CCF: Medium: Enabling Real-Time Quantitative Decision Making over Streaming Data","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","04/25/2018","Rajeev Alur","PA","University of Pennsylvania","Continuing grant","Nina Amla","05/31/2022","$683,237.00","Sanjeev Khanna, Zachary Ives, Boon Thau Loo","alur@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7798","7924, 8206","$0.00","A key component of an effective Internet of Things (IoT) system is the ability to make decisions in real-time in response to data it receives. While the exact logic for making decisions in different applications requires domain-specific insights, it typically relies on computing quantitative summaries of large data streams in an efficient and incremental manner. Programming the desired logic is challenging due to the enormous volume of data and hard constraints on available memory and response time. This project aims to assist IoT programmers in meeting this challenge by designing a query language with natural and high-level constructs suitable for processing streaming data, supported by a compiler and run-time system to facilitate deployment while meeting the constraints of desired accuracy, memory footprint, and real-time response.<br/><br/>The design of the query language draws upon insights from two distinct paradigms: extensions of relational query languages for handling streaming data, and state-based languages for runtime monitoring and synchronous programming. The novel integration of linguistic constructs allows the programmer to impart input data stream a logical hierarchical structure  and also employ relational constructs to partition the input data by keys and to merge data streams from different sensors. While theory of approximation and streaming algorithms allows exploration of trade-offs among language features, accuracy, and processing time during compilation, distributed platforms such as Apache Storm are used for high performance processing.  Two application domains, device monitoring for physiological patient data and dynamic monitoring of network traffic for anomalies, are used for experimental evaluation. A course module on predictable real-time decision making is being developed, and can be used in an undergraduate course on data science, in graduate courses on database systems and cyber-physical systems, in summer schools, and as tutorials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839356","TRIPODS+X:EDU:  An MBI TGDA+Neuro Program for Undergraduates","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, IntgStrat Undst Neurl&Cogn Sys","10/01/2018","09/10/2018","Janet Best","OH","Ohio State University","Standard Grant","Christopher W. Stark","09/30/2021","$199,983.00","Yusu Wang, Sebastian Kurtek, Yune Lee, Facundo Memoli","jbest@math.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","041Y, 1253, 8624","047Z, 062Z, 8089, 8091","$0.00","This project develops an educational program for undergraduate students, introducing them to methods at the forefront of modern mathematical and statistical data analysis through application to problems in neuroscience. The program consists of a one-semester interactive online course followed by an in-person research experience held at the Mathematical Bioscience Institute in Columbus, OH. Faculty from Ohio State University (OSU) and Pennsylvania State University (PSU) will lecture in the online course; participating student cohorts will reside at OSU, PSU, and a diverse collection of six additional US institutions - including two liberal arts colleges, two universities in Puerto Rico, an historically black university (HBCU), and a regional university with a primarily commuter student body. The project will introduce a modern area of research -- topological and geometric data analysis, to undergraduate students with different backgrounds and educational experiences, better preparing these students for graduate education and/or entry into the workforce. A flexible and accessible undergraduate curriculum in topological and geometric methods for the analysis of neuroscience data will be developed and made broadly available. A diverse community of educators will be trained to facilitate the delivery of the curriculum at their home institution, and their own research opportunities will be enhanced; an important outcome of the project is the engagement of experimental and mathematical neuroscientists in the emerging field of topological and geometric data analysis.<br/><br/>In this project, researchers in topological and geometric data analysis at Ohio State University (OSU) will collaborate with an experimental neuroscientist, director of the Speech, Language, and Music Lab (SLAM Lab) at OSU to develop scientific methods, curricular materials and educational projects.  Research in the SLAM lab involves the use of structural and functional MRI data to identify anatomical structures and networks underlying varied abilities in speech processing, as well as the nature of brain plasticity. A flexible and accessible undergraduate curriculum in topological and geometric methods for the analysis of neuroscience data will be developed and made broadly available. An interactive online course, using the broadband facilities of the Mathematical Biosciences Institute (MBI) will be given to undergraduates at eight institutions with participation by local faculty. A following workshop at the MBI will bring together the OSU researchers, the remote faculty and the undergraduates to work on research projects. This project provides the opportunity for two cohorts of undergraduates from eight colleges and universities to participate in a unique education/research experience and for the development of a novel curriculum in topological and geometric methods for neuroscience applications. In addition, it will allow faculty from participating institutions to learn the principles of topological and geometric data analysis, providing the opportunity to broaden their research programs to include this modern area of data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1748764","EAGER: A Python Program Analysis Infrastructure to Facilitate Better Data Processing","CCF","CI REUSE, SOFTWARE & HARDWARE FOUNDATION","09/15/2017","09/11/2017","Xiangyu Zhang","IN","Purdue University","Standard Grant","Sol J. Greenspan","08/31/2019","$147,000.00","","xyzhang@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","6892, 7798","7433, 7916, 7944, 8004","$0.00","Python is the third most popular programming language, after C and Java, and the most widely used language in Machine Learning and Data Science. Applications in Python are prone to human errors as much as those in other languages, or maybe more so due to the dynamic nature of Python. Therefore, tools to analyze, test, verify, and optimize Python applications are in a pressing need. Such tools are lagging or non-existent for Python. The root cause is the lack of infrastructure to support building practical and effective tools, which entails addressing the dynamic features of Python, such as dynamic typing, dynamic code loading/execution, and pervasive invocations to external library functions implemented in other languages.<br/><br/>This project aims to explore the feasibility of building a Python program analysis infrastructure by developing two sample tools that rely upon a common set of infrastructural capabilities including the instrumentation, static analysis and symbolic analysis capabilities. The two sample tools are a data provenance tracking tool for machine learning applications and a bug finding tool to detect data format inconsistencies, which are the most dominant type of bugs in data processing. The provenance tool will demonstrate the importance of static analysis and program instrumentation, and the bug finding tool will demonstrate the importance of symbolic analysis. Both tools will illustrate the great benefits that can be brought to datascientists by advanced tools. In addition, they will illustrate that the aforementioned capabilities cannot be simply ported from existing infrastructures for other languages such as C and Java. The infrastructure will meet the pressing need of comprehensive tool building support for Python. A lot of cutting-edge synergistic research will be enabled across the CISE research community to serve data application programmers, data scientists and even end users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757970","REU Site: Collaborative Research: BigDataX: From theory to practice in Big Data computing at eXtreme scales","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/15/2018","02/05/2018","Kyle Chard","IL","University of Chicago","Standard Grant","Rahul Shah","01/31/2021","$35,000.00","","chard@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1139","9250","$0.00","This award extends the BigDataX program, a Research Experiences for Undergraduates (REU) site at Illinois Institute of Technology (IIT) and the University of Chicago (UChicago). The BigDataX site focuses on undergraduate research in both the theory and practice of big data computing at extreme scales. BigDataX includes a diverse group of 10 undergraduate students, who will conduct research over a 10 week period, in the area of big data and how it will impact the design, analysis, and implementation of run-time systems and storage systems to support big data applications. This research will make extreme scale computing more tractable, touching every branch of computing in high-end computing and datacenters. These advancements will impact scientific discovery and economic development at the national level, and they will strengthen a wide range of research activities enabling efficient access, processing, storage, and sharing of valuable scientific data from many disciplines. The proposed work will place students in the middle of a technological revolution which will revolutionize the computing domain. <br/><br/>The primary objective of this proposal is to promote a data-centric view of scientific and technical computing, at the intersection of distributed systems theory and practice. This proposal includes 6 mentors, with a variety of complementing expertise from theory to systems, distributed systems, storage management, and data science. The mentors have extensive experience in mentoring undergraduate students, collectively having worked with over one hundred students in research activities, publishing dozens peer-reviewed workshop/conference/journal papers at top venues together with undergraduate students. The students that will be part of the BigDataX program will be exposed to big data applications, large data sets, and various distributed systems such as the Mystic reconfigurable testbed at IIT, Chameleon cloud and Theta supercomputer at Argonne National Laboratory, the XSEDE national cyberinfrastructure, and the Amazon Web Services cloud."
"1845076","CIF: CAREER: Robust, Interpretable, and Efficient Unsupervised Learning with K-set Clustering","CCF","COMM & INFORMATION FOUNDATIONS","05/01/2019","02/21/2019","Laura Balzano","MI","University of Michigan Ann Arbor","Continuing grant","Phillip Regalia","04/30/2024","$118,738.00","","girasole@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7797","1045, 7936, 9102","$0.00","Modern machine learning techniques aim to design models and algorithms that allow computers to learn efficiently from vast amounts of previously unexplored data. These problems are called 'unsupervised' because no human-provided information about the data is available to guide the machine learning process. Arguably the two most important unsupervised machine learning tools are dimensionality-reduction and clustering. In dimensionality-reduction, the algorithm seeks a simple low-dimensional structure that captures the interesting behavior in the data. In clustering, the algorithm seeks to group data points together into meaningful clusters. As increasingly higher-dimensional data are collected about progressively more elaborate physical, biological, and social phenomena, algorithms that aim at both dimensionality reduction and clustering are often highly applicable. However, joint formulations in the literature are often ad-hoc and fundamentally unable to operate on real data that have missing elements, corruptions, and heterogeneity --- critical machine learning challenges for modern data problems. This research project is expected to have broad applicability in data science, and will be demonstrated in two applications: genetics and computer vision. <br/><br/>The joint clustering and dimensionality reduction formulation used in this project, called K-set clustering, seeks K ""central sets"" constrained to have some low-dimensional representation, each of which represents one of K clusters in the data. The formulation is  a generalization of K-means, K-subspaces, and principal component analysis, and it naturally leads to several novel problem instances. Given a defined set geometry, the corresponding problem instance is approached from two perspectives: understanding the geometry of that instance of the problem formulation, and learning those geometric models from data. Three specific examples of the problem formulation will be studied: subspace clustering, variety clustering, and polyhedral set clustering. While each example presents intrinsic and unique challenges, these are just examples of a larger paradigm that is limited only by one's ability to define sets amenable to modeling the geometric structure in data. The formulation allows for interpretable data analysis, with a framework that can readily incorporate missing data and heterogeneous data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816577","SHF: Small: Hyperscaling Data Analytics for High-Performance Computers","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","07/02/2018","Spyros Blanas","OH","Ohio State University","Standard Grant","Almadena Chtchelkanova","06/30/2021","$460,000.00","","blanas.2@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7798","7923, 7942","$0.00","Data analytics extracts insights from massive datasets, often with the assistance of machine learning techniques. The goal of this project is to allow domain experts, including data scientists, to analyze massive datasets quickly using the most powerful supercomputing systems in the world. The problem is that state-of-the-art data processing algorithms that filter data, summarize results and combine information from different sources have inherent scalability bottlenecks. This project designs hyperscalable data processing algorithms that harness the unprecedented compute, storage and networking concurrency of a high-performance computer. This project also develops an open-source data processing engine to disseminate prototype implementations of these algorithms to the public. Another contribution is the creation of a massively parallel data processing module and associated teaching materials for undergraduate data science curricula, such as the diverse Data Analytics undergraduate major at The Ohio State University.<br/><br/>The confluence of extreme compute parallelism, fast networking and growing memory capacities in the modern datacenter presents an opportunity to design a hyperscalable data processing kernel for warehouse-scale computers. This project sits at the intersection of data management and high-performance computing; it develops scalable join and aggregation algorithms, topology-conscious query planning and optimization techniques, and interference-aware data access methods for shared cold storage. This is accomplished by carefully overlapping communication and computation, identifying and avoiding unscalable all-to-all communication, accounting for network path congestion and variability in remote memory access latency, and judiciously using inter-process coordination to accelerate data ingestion from a massively parallel shared file system. These research activities lay the intellectual foundation to make data analytics scalable and efficient in warehouse-scale computers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1737785","Collaborative Research: Phylanx: Python based Array Processing in HPX","CCF","SOFTWARE & HARDWARE FOUNDATION, ","08/15/2017","08/03/2017","Hartmut Kaiser","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Almadena Chtchelkanova","07/31/2019","$373,200.00","Steven Brandt","hkaiser@cct.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7798, Q212","7942, 9150","$0.00","The availability and size of data sets has increased significantly over the course of the past decade. To enable the analysis of large data sets on High Performance Computing (HPC) resources while minimizing time- and energy-to-solution requires incorporating static and runtime information to determine the best possible data layout of the large data arrays used by an application to minimize data movement. The goal of this proposal is to deliver Phylanx, a general purpose framework supporting a variety of data science, machine learning, and statistically oriented applications. Phylanx is designed such that a user?s code will be able to perform efficiently on current and future architecture as long as the runtime system is maintained. This greatly reduces the maintenance burden and will increase the productivity of domain scientists. Phylanx lays a solid foundation for technology transfer from academia to industry and fills the gap between academic innovation and commercial application, by creating a software layer that industrial partners can feel confident relying upon. <br/>Phylanx is a scalable, array-based and distributed framework targeting HPC systems using the HPX, dynamic asynchronous task-based parallel runtime system. The dataflow-style capabilities exposed by HPX guarantee the preservation of all data-dependencies even for complex distributed workflows. This project overcomes some of the limitations of existing Big Data solutions such as Hadoop, Spark, and Flink by providing users the ability to: implement NumPy-styled expression-graphs using Python or C/C++, optimize these graphs for optimal data layout, distribution, tiling, and minimal communication overheads, and evaluate those graphs with high efficiency on a runtime interpreter targeting distributed HPC systems. Additionally, Phylanx uses greedy sub-modular techniques on the expression tree to provide a mathematically provable guarantee of optimal performance in machine learning domains and in data placement problems. The platform will provide implementations of 6 benchmarks which have been selected for their domain specificity in text, image, and graph applications."
"1837131","FMitF: A Novel Framework for Learning Formal Abstractions and Causal Relations from Temporal Behaviors","CCF","FMitF: Formal Methods in the F, SPECIAL PROJECTS - CCF","11/01/2018","09/07/2018","Jyotirmoy Deshmukh","CA","University of Southern California","Standard Grant","Nina Amla","10/31/2022","$1,000,000.00","Yan Liu, Paul Bogdan","jdeshmuk@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","094Y, 2878","062Z, 071Z, 075Z, 8206","$0.00","Formal methods consist of a collection of techniques that help developers rigorously reason about the behaviors of software and hardware systems with the help of mathematical logic. While formal logic has been used for articulating system specifications for the purpose of verification or software synthesis, this project introduces a logic-based framework to address machine-learning problems such as classification (which category should a new datum be put into?), clustering (how should a collection of data points be grouped together into categories?) and discovery of causal relations (when should an earlier data observation be deemed to cause the appearance of a later data observation?) for time-series data, in which repeated observations are made over time. The use of formal logic opens new avenues such as enhancing the interpretability of machine-learning models, the explainability of learning results, and articulation of formal guarantees on the behavior of learning algorithms. The societal impact of this work targets discovery of latent information in time-series data in diverse domains such as healthcare, autonomous systems, and security. The research impacts education by providing cross-disciplinary training of undergraduate and graduates students in areas of data science, machine learning, formal methods, and introducing students to methods from statistical physics on a number of real-world systems.<br/><br/>This project explores the intersection between the logical inference based on real-time temporal logics and statistical inference prevalent in machine learning. The algorithms developed in this project allow users to express domain knowledge in the form of signal predicates or chance constraints, and output the results of classification, clustering or causal discovery as formulas in specific real-time temporal logics. This allows the results of the machine-learning algorithms to be human-interpretable, and also improves the explainability of learning algorithms by answering the question of why a particular time-series datum is classified or clustered in a specific fashion.  These techniques are able to model uncertainty in time-series data by creating a new class of non-parametric learning methods that combine concepts from statistical physics, information theory, and statistical inference. The use of a logic-based framework allows providing formal guarantees on the learning process itself by applying ideas such as probably-approximately-correct learning (from computational learning theory) to the inference of real-time temporal logic formulas from data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1453073","CAREER: Signal Processing Through the Lens of Geometry","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2015","09/14/2018","Waheed Bajwa","NJ","Rutgers University New Brunswick","Continuing grant","Phillip Regalia","06/30/2020","$550,000.00","","waheed.bajwa@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7797","1045, 7797, 7936","$0.00","This project addresses many of the challenges that pertain to the following trends in data gathering and information processing. First, recent technological advances are pushing our society toward generation of massive quantities of data. Second, this massive data generation and collection is having an unintended consequence: the fraction of dirty data, defined as incomplete, grossly erroneous or mislabeled data, within these data sets is increasing. Third, there is an increasing shift toward relying on interconnected sets of geographically-distributed data for inference and decision making. Collectively, these three trends portend an inevitable transition to a data-driven world rife with big, dirty, and distributed data. Information processing in this new age of big, dirty, distributed data demands novel mathematical data models and robust computational and statistical tools.<br/><br/>The intellectual merit of this project lies in the ways it addresses the challenges of information processing for big, dirty, distributed data. First, it deals with the challenge of processing for big, dirty data by developing theoretical and algorithmic foundations of a novel geometric signal/data model that results in improved inference from big data, even in the presence of dirty data, because of the model?s ability to faithfully capture the ?ambient geometry? of big data. Second, it develops and analyzes novel collaborative processing algorithms that build on top of the developed model for improved inference from big, dirty data distributed across the world. The research agenda of this project impacts nearly every discipline that relies on advances in information processing for improved inference and decision making. In addition, it impacts the society and the US healthcare system through its applications to early cancer detection, activity recognition in chaotic trauma bays, and collaborative digital pathology. The education agenda of this project impacts the society and the US economy through K-12 and college level outreach activities within New Jersey, modernization of the Rutgers signal processing curriculum, and training of undergraduate and graduate students for careers in data science."
"1712867","CIF: AF: Small: Foundations of Multimodal Information Integration","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","06/26/2017","Guillermo Sapiro","NC","Duke University","Standard Grant","Phillip Regalia","08/31/2020","$431,728.00","","guillermo.sapiro@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7797","7923, 7935, 7936","$0.00","Data comes in all forms, including visual, text, medical records, and comments on social medial. This diverse (multimodal) data is critical when data is scarce, noisy, and uncertain. Different modalities can improve joint inference and decision making and allow producing (at extremely low-cost) results that were possible only with high-end devices and techniques before. In addition, inferring a condition from unexpected data sources is of paramount importance in disciplines ranging from marketing to health-care and defense. This project addresses these fundamental challenges with new mathematical and computational tools. Through new collaborations, the project has access to unique data and problems of significant impact in human well-being. A related online class also continues to grow and develop, with over 120,000 students so far. A unique summer immersion program will also involve undergraduate students in multimodal data science research.<br/> <br/>The exploitation of multimodal data is one of the unifying themes of this project. A further unifying theme is the underlying mathematical foundation: subspace modeling and embedding. Tools from subspace modeling in the form of learning multimodal low-rank representations, modeling multimodal sparse networks, and solving for big data matrix decompositions are here developed. A third unifying motif of this work is the ubiquitous consideration of computational efficiency. All the above is developed in three major components: classification and recognition, data augmentation, and network analysis. The project addresses critical problems such as multimodal face recognition, dynamic multimodal graph inference, gaze analysis, multimodal network analysis, and non-negative matrix factorization. The overall goal is to efficiently exploit and integrate multimodal data to help in joint inference and decision making."
"1618605","AF: Small: Collaborative Research: Geometric and Topological Algorithms for Analyzing Road Network Data","CCF","ALGORITHMIC FOUNDATIONS, EPSCoR Co-Funding","07/01/2016","04/25/2017","Brittany Fasy","MT","Montana State University","Standard Grant","Rahul Shah","06/30/2019","$171,824.00","","brittany@cs.montana.edu","309 MONTANA HALL","BOZEMAN","MT","597172470","4069942381","CSE","7796, 9150","7923, 7929, 9150, 9251","$0.00","The project aims to develop theoretically grounded, effective methods for analyzing data associated with road networks -- using graphs that represent road networks as a framework for analyzing network data. Thanks to the spread of GPS-enabled devices, trajectory data has become ubiquitous.  Many other sources, including census data and crime statistics, have addresses or geographic locations that link to an underlying road network. <br/><br/>Algorithms with mathematical guarantees will be developed to align trajectories to the network under natural and realistic properties of true trajectories, to reconstruct road networks from trajectory and density data. It will also provide two frameworks for comparing data-endowed networks at different levels. While the problems of trajectory alignment, map reconstruction, and map comparison have attracted a lot of attention in the GIS community, most approaches are ad-hoc, provide no quality guarantees, and are limited to post-hoc analysis. This project will provide novel theoretical foundations combining approaches from computational topology and geometry, and will further advance the state-of-the-art of the field of topological / geometric data analysis. <br/><br/>The PIs will continue to combine educational and research activities through this project. Students will be tightly integrated into the research and practical implementation of this project, and will be trained in integrating geometric thinking, algorithms development, and (trajectory) data analysis. The combination of such skills is increasingly important in data science. Topics involved in this project will enrich the course material and curriculum development at each of the three institutions."
"1618469","AF: Small: Collaborative Research: Geometric and Topological Algorithms for Analyzing Road Network Data","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","04/30/2017","Carola Wenk","LA","Tulane University","Standard Grant","Rahul Shah","06/30/2019","$166,827.00","","cwenk@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","7796","7923, 7929, 9150, 9251","$0.00","The project aims to develop theoretically grounded, effective methods for analyzing data associated with road networks -- using graphs that represent road networks as a framework for analyzing network data. Thanks to the spread of GPS-enabled devices, trajectory data has become ubiquitous.  Many other sources, including census data and crime statistics, have addresses or geographic locations that link to an underlying road network. <br/><br/>Algorithms with mathematical guarantees will be developed to align trajectories to the network under natural and realistic properties of true trajectories, to reconstruct road networks from trajectory and density data. It will also provide two frameworks for comparing data-endowed networks at different levels. While the problems of trajectory alignment, map reconstruction, and map comparison have attracted a lot of attention in the GIS community, most approaches are ad-hoc, provide no quality guarantees, and are limited to post-hoc analysis. This project will provide novel theoretical foundations combining approaches from computational topology and geometry, and will further advance the state-of-the-art of the field of topological / geometric data analysis. <br/><br/>The PIs will continue to combine educational and research activities through this project. Students will be tightly integrated into the research and practical implementation of this project, and will be trained in integrating geometric thinking, algorithms development, and (trajectory) data analysis. The combination of such skills is increasingly important in data science. Topics involved in this project will enrich the course material and curriculum development at each of the three institutions."
"1618247","AF: Small: Collaborative Research:Geometric and topological algorithms for analyzing road network data","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","06/27/2016","Yusu Wang","OH","Ohio State University","Standard Grant","Rahul Shah","06/30/2019","$189,099.00","","yusu@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7796","7923, 7929, 9150","$0.00","The project aims to develop theoretically grounded, effective methods for analyzing data associated with road networks -- using graphs that represent road networks as a framework for analyzing network data. Thanks to the spread of GPS-enabled devices, trajectory data has become ubiquitous.  Many other sources, including census data and crime statistics, have addresses or geographic locations that link to an underlying road network. <br/><br/>Algorithms with mathematical guarantees will be developed to align trajectories to the network under natural and realistic properties of true trajectories, to reconstruct road networks from trajectory and density data. It will also provide two frameworks for comparing data-endowed networks at different levels. While the problems of trajectory alignment, map reconstruction, and map comparison have attracted a lot of attention in the GIS community, most approaches are ad-hoc, provide no quality guarantees, and are limited to post-hoc analysis. This project will provide novel theoretical foundations combining approaches from computational topology and geometry, and will further advance the state-of-the-art of the field of topological / geometric data analysis. <br/><br/>The PIs will continue to combine educational and research activities through this project. Students will be tightly integrated into the research and practical implementation of this project, and will be trained in integrating geometric thinking, algorithms development, and (trajectory) data analysis. The combination of such skills is increasingly important in data science. Topics involved in this project will enrich the course material and curriculum development at each of the three institutions."
"1452961","CAREER: Algorithmic Foundations for Social Data","CCF","ALGORITHMIC FOUNDATIONS","05/01/2015","05/07/2018","Yaron Singer","MA","Harvard University","Continuing grant","Tracy J. Kimbrel","04/30/2020","$412,348.00","","yaron@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","1045, 7926, 7932","$0.00","In the past several years, there has been a great deal of exposure to the opportunities and promise that lie in large-scale data. Although massive data sets have been collected and analyzed in well over a decade, the excitement is largely due to the relatively recent availability of social data: massive digital records of human interactions. This provides a unique system-wide perspective of collective human behavior which poses fundamental challenges and opportunities. Despite the tremendous progress made in recent years, very few algorithmic frameworks to-date have been purposefully developed for analyzing social data sets. The goal of this project is to develop frameworks that enable analysis of large-scale social data. This project seeks novel models that are rich in problems, raise deep questions about computation, and can lead to long-lasting impact on sociology and data science.<br/><br/>From a technical perspective, the goal of the project is to develop appropriate algorithmic machinery with strong theoretical guarantees that translate to results in practice. The project consists of three main lines of research. The first line of research seeks to develop a theory to optimize events in the future given a distribution on the consequences of actions we take in the present. The second line of research considers learnability and scalability of social data, and its interpretation for optimization. The third line of research considers design of robust optimization algorithms for noisy data. The methodology includes experimentation on real data sets to develop appropriate algorithmic machinery with strong theoretical guarantees that translate to results in practice.  Both undergraduate and graduate curriculum will benefit from the development of courses in this interdisciplinary area."
"1757964","REU Site: Collaborative Research: BigDataX: From theory to practice in Big Data computing at eXtreme scales","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/15/2018","02/05/2018","Ioan Raicu","IL","Illinois Institute of Technology","Standard Grant","Rahul Shah","01/31/2021","$323,106.00","Kyle Hale","iraicu@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","1139","9250","$0.00","This project extends the BigDataX program, a Research Experiences for Undergraduates (REU) site at Illinois Institute of Technology (IIT) and the University of Chicago (UChicago). The BigDataX site focuses on undergraduate research in both the theory and practice of big data computing at extreme scales. BigDataX includes a diverse group of 10 undergraduate students, who will conduct research over a 10 week period, in the area of big data and how it will impact the design, analysis, and implementation of run-time systems and storage systems to support big data applications. This research will make extreme scale computing more tractable, touching every branch of computing in high-end computing and datacenters. These advancements will impact scientific discovery and economic development at the national level, and they will strengthen a wide range of research activities enabling efficient access, processing, storage, and sharing of valuable scientific data from many disciplines. The proposed work will place students in the middle of a technological revolution which will revolutionize the computing domain. <br/><br/>The primary objective of this proposal is to promote a data-centric view of scientific and technical computing, at the intersection of distributed systems theory and practice. This proposal includes 6 mentors, with a variety of complementing expertise from theory to systems, distributed systems, storage management, and data science. The mentors have extensive experience in mentoring undergraduate students, collectively having worked with over one hundred students in research activities, publishing dozens peer-reviewed workshop/conference/journal papers at top venues together with undergraduate students. The students that will be part of the BigDataX program will be exposed to big data applications, large data sets, and various distributed systems such as the Mystic reconfigurable testbed at IIT, Chameleon cloud and Theta supercomputer at Argonne National Laboratory, the XSEDE national cyberinfrastructure, and the Amazon Web Services cloud."
"1649448","SHF: Small: Asked and Answered: Intelligent Data Science for Software Projects","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","07/21/2016","Jane Huang","IN","University of Notre Dame","Standard Grant","Sol J. Greenspan","05/31/2019","$515,902.00","","JaneClelandHuang@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7798","7923, 7944, 9251","$0.00","Software and systems engineering projects accrue large amounts of development data including requirements, design, code, test cases, and fault logs. When combined with the power of software analytics this data could be used to provide actionable intelligence to project stakeholders. For example, a developer might ask to view all ""safety-related code which is likely to exhibit runtime faults."" The proposed work will deliver a solution named Asked and Answered for Software Intensive Projects (AA) and will support a broad range of analytic queries.   To foster the transition of AA to practice, the researchers will partner with industry collaborators throughout the project and develop an open-source framework facilitating the deployment of AA technology into an industrial environment. A series of natural language Query Challenges will be designed and disseminated and used to train Software Engineering students in a broad spectrum of software analytics.<br/> <br/>Delivering the AA solution requires several non-trivial challenges to be addressed. First, a natural language (NL) query interface will be developed allowing stakeholders to issue queries in their own words and from their own perspective on the project. These queries will then be transformed into a structured, executable format. Heuristics and statistical inferencing techniques will be adopted and interactive mechanisms will be designed to seek clarification from the user when the query cannot be disambiguated automatically. Software analytics will be integrated into the query mechanism so that AA can respond to a wide range of analytical questions. AA will support the dynamic composition of primitive functions into execution flows in order to service a wide range of analytical queries. Finally, AA will deliver a query engine capable of generating optimized query execution plans which take into account the nuances of the domain - namely its heterogeneous data formats, distributed tools, and the dynamic runtime requirements of analytic functions."
"1657477","CRII: AF: Algorithms for Noise-Tolerant Function Testing with Applications to Deep Learning","CCF","CRII CISE Research Initiation","03/01/2017","02/23/2017","Grigory Yaroslavtsev","IN","Indiana University","Standard Grant","Rahul Shah","02/29/2020","$174,553.00","","gyarosla@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","026Y","7796, 7926, 8228","$0.00","Machine learning has emerged as an important area of computer science, which has a potential significantly to change our lives and society. In deep learning, one needs to rely on being able to quickly test the properties of objective functions. The goal of this project is to develop algorithms for testing analytic properties of high-dimensional functions. Better understanding of properties of optimization objectives used in deep learning will enable researchers in the field to make more educated decisions regarding the choice of optimization methods. It will simplify and introduce rigor in the art of parameter tuning that plays key role in achieving high performance in training deep neural nets. The framework for approximate algorithmic functional analysis (Lp-testing) developed by the PI that forms the starting point for this research has been taught in courses on learning theory and algorithms for big data at the University of Pennsylvania and University of Buenos Aires. Together with the outcomes of the research in this proposal it will be included into M.S./Ph.D. classes on foundations of data science and algorithms for big data at Indiana University taught by the PI.<br/><br/>The PI will develop ultra-efficient algorithms for assisting humans in their understanding of analytic properties of high-dimensional functions and objectives used in deep learning. Three main goals and related challenges in the design of such tools are:(1) Performing algorithmic analysis of local properties of deep learning objectives in the absence of clear global structure (2) Enabling rigorous analysis of analytic properties of functions based on noisy data (3) Introducing tolerance to sampling errors in function evaluations arising in deep learning applications for performance reasons. The project will involve development of new mathematical methods for understanding how global properties of noisy functions such as monotonicity, convexity and Lipschitzness are affected by projections onto random low-dimensional linear subspaces. It will suggest choices of distributions for generation of such subspaces in order to best preserve the desired properties. A rigorous study of fundamental advantages of data-dependent methods will be conducted as a separate part of the project."
"1657471","CRII: CIF: Learning with Memory Constraints: Efficient Algorithms and Information Theoretic Lower Bounds","CCF","CRII CISE Research Initiation","02/15/2017","02/10/2017","Jayadev Acharya","NY","Cornell University","Standard Grant","Phillip Regalia","01/31/2020","$175,000.00","","acharya@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","026Y","7797, 7935, 8228","$0.00","The trade-offs between resources such as the amount of data, the amount of storage, computation time for statistical estimation tasks are at the core of modern data science. Depending on the setting, some of the resources might be more valuable than others. For example, in credit analysis and population genetics, the amount of data is vital. For applications involving mobile devices, sensor networks, or biomedical implants, the storage available is limited and is a precious resource. This project aims to advance our understanding of the trade-offs between the amount of storage and the amount of data required for statistical tasks by (i) designing efficient algorithms that require small space and (ii) establishing fundamental limits on the storage required for these tasks. The research is at the intersection of streaming algorithms, which is primarily concerned with storage requirements of algorithmic problems, and statistical learning, which studies data requirements for statistical tasks. <br/><br/>The investigators formulate basic statistical problems under storage constraints. The specific questions include entropy estimation of discrete distributions, a canonical problem that researchers from various fields including statistics, information theory, and computer science have studied. The paradigm of interest is the following: while the known sample-efficient entropy estimation algorithms require a lot of storage, it might be possible to reduce the storage requirements drastically by taking a little more than the optimal number of samples. The complementary side of the problem is purely information theoretic. In it, the researchers expect to develop general lower bounds that can be used to prove fundamental limits on the storage-sample trade-offs."
"1833511","NSF Student Travel Grant for 2018 GRC on Scientific Methods in Cultural Heritage Research (SMCHR 2018)","CCF","INFORMATION TECHNOLOGY RESEARC, COMM & INFORMATION FOUNDATIONS","05/01/2018","04/24/2018","Charles Johnson","RI","Gordon Research Conferences","Standard Grant","Phillip Regalia","04/30/2019","$20,000.00","","crj2@cornell.edu","512 Liberty Lane","West Kingston","RI","028921502","4017834011","CSE","1640, 7797","7556, 7935, 9150","$0.00","Technical art history is a new sub-discipline within art historical and conservation research that applies science and technology to the understanding and preservation of art objects. Only over the last decade has this community begun to accord image processing and computational science the status of a principal partner among the other scientific and technological disciplines,such as chemistry, materials science, physics, imaging technology, and dendrochronology, blended together in the growing specialization of technical art history. From July 22 to 27, 2018, the 4th Gordon Research Conference on Scientific Methods in Cultural Heritage Research with the theme ""Leading Edge Applications of Data Science, Degradation Science, and Conservation Strategies for Cultural Heritage"" will take place in Castelldefels, Spain. This award is to support graduate students at US universities with backgrounds in engineering and science that include applicable expertise in imaging science and technology, digital signal/image processing, or computer science - to attend the 2018 Gordon Research Conference on Scientific Methods in Cultural Heritage Research.<br/><br/>The inclusion of digital signal/image processing and related expertise from electrical engineering and computer science in the foundation of technical art history helps align art history with the burgeoning digital humanities. The expansion from words to images is an active frontier in the digital humanities. Advances in the computational component of technical art history, i.e. computational art history, are very likely to spawn ""technology transfer"" to other branches of the digital humanities dealing with images and objects. The potential for these newly opened cross-disciplinary applications of digital image processing to raise new technical challenges is heightened by aiding the inclusion of - particularly young - digital image processing specialists in the foundation-building period of this new field of technical art history. Another benefit arises with the public's exposure to science and technology brought on by the expansion of technical art history research evident in the recent upswing in the incorporation of technical information and their computer-based visualizations into museum exhibitions. This is an unparalleled opportunity for students in engineering, applied mathematics, computer science, and other sciences to be present at the beginning of this young field's effort to incorporate their image processing and computational science skills. Their presence at this conference will also add to the perception of viability of computational methods in the future of technical art history. The connections the students make with domain experts with the potential for future collaboration are priceless to both sides.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1563714","AF: Medium: Collaborative Research: Econometric Inference and Algorithmic Learning in Games","CCF","ALGORITHMIC FOUNDATIONS","04/01/2016","03/26/2019","Eva Tardos","NY","Cornell University","Continuing grant","Tracy Kimbrel","03/31/2020","$701,267.00","","eva@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7796","7924, 7932","$0.00","Classical work on economic analysis of the interactions of strategic agents starts with players that have valuations for outcomes, such as items or sets of items they may win in an auction, and analyzes equilibria of the resulting game, where players optimize their strategies to improve their outcomes. To empirically test the prediction of such a theory, one needs to recover valuations of the players. Most econometric methods used to recover valuations rely on the assumption that the game is at a stable equilibrium (known as a Nash equilibrium). It is not surprising that such a framework provides a poor fit to the data in changing or new markets. At the same time, there is a growing theoretical literature in algorithmic game theory that allows one to study games where the game is not at a stable equilibrium. The PIs' program focuses on developing a methodology for inference without relying on the standard notions of the stability of outcomes in dynamically changing environments, such as online auctions. The goal of this project is to develop a theory that allows the researchers to take advantage of new dynamic data sets from electronic markets available on the Internet, and using the findings from the data to further the underlying theory.<br/> <br/>The results of the project are intended to enable to application and development of Data Science tools for analysis and prediction in  non-stable and new market settings. This will affect a broad community of empirical researchers such as market analysts, by allowing them to study economic markets that have previously been considered hard or impossible to analyze.<br/><br/>The research program is based on using the theoretical results from algorithmic game theory on game outcomes when players use no-regret learning rules and combine these results with econometric techniques that allow one to estimate the best responses of players from the data using a set of non-parametric estimation techniques. The goal of the program, which PIs initiated in a paper in the ACM Conference on Economics and Computation in 2014, is to combine these approaches to develop a set of analytic tools for empirical analysis of games in non-equilibrium settings. Algorithmic game theory helps one to characterize the properties of outcomes in games (such as approximating factors for revenue and welfare in various cases), where the game is not at a stable equilibrium, assuming players use strategies that guarantee a certain no-regret property in place of the stronger equilibrium best response assumption. The project is aimed at combining the insights from algorithmic game theory with econometric methods to enable the analysis of dynamic markets. The intellectual merit of the project is twofold: (i) providing a methodology for inference in games (i.e., estimation of the payoff functions of players and the distribution of player types) in cases where the players use general classes of learning strategies; (ii) providing tools for the analysis of outcomes in non-equilibrium environments, including the analysis of statistical properties of the outcomes constructed using inferred preferences and types."
"1563708","AF: Medium: Collaborative Research: Econometric Inference and Algorithmic Learning in Games","CCF","ALGORITHMIC FOUNDATIONS","04/01/2016","09/19/2017","Denis Nekipelov","VA","University of Virginia Main Campus","Continuing grant","Tracy J. Kimbrel","03/31/2020","$328,932.00","","denis@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7796","7924, 7932","$0.00","Classical work on economic analysis of the interactions of strategic agents starts with players that have valuations for outcomes, such as items or sets of items they may win in an auction, and analyzes equilibria of the resulting game, where players optimize their strategies to improve their outcomes. To empirically test the prediction of such a theory, one needs to recover valuations of the players. Most econometric methods used to recover valuations rely on the assumption that the game is at a stable equilibrium (known as a Nash equilibrium). It is not surprising that such a framework provides a poor fit to the data in changing or new markets. At the same time, there is a growing theoretical literature in algorithmic game theory that allows one to study games where the game is not at a stable equilibrium. The PIs? program focuses on developing a methodology for inference without relying on the standard notions of the stability of outcomes in dynamically changing environments, such as online auctions. The goal of this project is to develop a theory that allows the researchers to take advantage of new dynamic data sets from electronic markets available on the Internet, and using the findings from the data to further the underlying theory.<br/> <br/>The results of the project are intended to enable to application and development of Data Science tools for analysis and prediction in  non-stable and new market settings. This will affect a broad community of empirical researchers such as market analysts, by allowing them to study economic markets that have previously been considered hard or impossible to analyze.<br/><br/>The research program is based on using the theoretical results from algorithmic game theory on game outcomes when players use no-regret learning rules and combine these results with econometric techniques that allow one to estimate the best responses of players from the data using a set of non-parametric estimation techniques. The goal of the program, which PIs initiated in a paper in the ACM Conference on Economics and Computation in 2014, is to combine these approaches to develop a set of analytic tools for empirical analysis of games in non-equilibrium settings. Algorithmic game theory helps one to characterize the properties of outcomes in games (such as approximating factors for revenue and welfare in various cases), where the game is not at a stable equilibrium, assuming players use strategies that guarantee a certain no-regret property in place of the stronger equilibrium best response assumption. The project is aimed at combining the insights from algorithmic game theory with econometric methods to enable the analysis of dynamic markets. The intellectual merit of the project is twofold: (i) providing a methodology for inference in games (i.e., estimation of the payoff functions of players and the distribution of player types) in cases where the players use general classes of learning strategies; (ii) providing tools for the analysis of outcomes in non-equilibrium environments, including the analysis of statistical properties of the outcomes constructed using inferred preferences and types."
"1763481","AF: Medium: Collaborative Research: Beyond Sparsity: Refined Measures of Complexity for Linear Algebra","CCF","COMM & INFORMATION FOUNDATIONS","03/15/2018","02/19/2019","Atri Rudra","NY","SUNY at Buffalo","Continuing grant","Phillip Regalia","02/28/2022","$258,810.00","","atri@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7797","7924, 7926, 7935, 9251","$0.00","Modern data science applications exploit structure in real life data using machine learning (including deep learning) algorithms. At the core of most of these systems are algorithms for a branch of mathematics called linear algebra. In particular, a large portion of these algorithms utilize the fact that real life data has properties that can be captured using certain parsimonious linear algebraic structures. This project studies new, more powerful linear algebraic structures and algorithms that exploit these new structures. Given the fundamental importance of these algorithms, ideas generated from this project are expected to be implemented in widely deployed machine learning systems. The outreach component of this project involves (1) a technical workshop for researchers from diverse areas and (2) outreach events for K-12 students.<br/><br/>A variety of problems in modern data science have been successfully characterized using a width. For example, one of the most common widths, the rank of a matrix, has a near-ubiquitous use across many applications. This project significantly expands the understanding of several recently proposed widths and extracts their full potential for positive practical outcomes. Furthermore, it contributes to the recently growing work on beyond worst-case analysis in linear algebra, machine learning and coding theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1525398","CIF: Small: Bypassing the L1 Norm: Non-Convex Regularization, Convex Optimization, and Sparse Signal Processing","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2015","07/08/2015","Ivan Selesnick","NY","New York University","Standard Grant","Phillip Regalia","07/31/2019","$315,175.00","","selesi@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7797","7923, 7936","$0.00","Numerous problems in nonlinear signal processing and data science are successfully tackled through their formulation as ill-conditioned inverse problems. Such problems arise in medical imaging, speech and audio processing, biomedical time-series analysis, manufacturing, and remote sensing. Many ongoing advances in these fields are based on sparse regularization (i.e., the modeling of data as highly compressible when appropriately transformed). This research program aims to advance mathematical and computational tools to pose and solve ill-conditioned inverse problems by developing new techniques for sparse regularization. <br/><br/>Convex formulations of problems arising in science and engineering are attractive because one may leverage a wealth of algorithms that are globally convergent and computationally efficient, even for large-scale non-smooth problems. While the L1 norm is a cornerstone of convex sparse regularization, it tends to under-estimate signal values. Non-convex sparse regularization is therefore a popular and valuable alternative; however, it is hampered by several complications: the optimal solution is generally a discontinuous function of the data and the objective function to be minimized generally possesses many sub-optimal local minima which can entrap optimization algorithms. <br/><br/>This research aims to exploit the effectiveness of non-convex sparse regularization without forgoing the principles of convex optimization, by applying the principle of convex relaxation to the objective function as a whole, rather than to the regularizer alone. In particular, this program undertakes the development of new non-separable non-convex penalty functions that ensure the convexity of the objective function (comprising data fidelity and sparse regularization terms) to be minimized."
"1763315","AF: Medium: Collaborative Research: Beyond Sparsity: Refined Measures of Complexity for Linear Algebra","CCF","COMM & INFORMATION FOUNDATIONS","03/15/2018","03/14/2018","Christopher Re","CA","Stanford University","Continuing grant","Phillip Regalia","02/28/2022","$134,482.00","","chrismre@cs.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7797","7924, 7926, 7935","$0.00","Modern data science applications exploit structure in real life data using machine learning (including deep learning) algorithms. At the core of most of these systems are algorithms for a branch of mathematics called linear algebra. In particular, a large portion of these algorithms utilize the fact that real life data has properties that can be captured using certain parsimonious linear algebraic structures. This project studies new, more powerful linear algebraic structures and algorithms that exploit these new structures. Given the fundamental importance of these algorithms, ideas generated from this project are expected to be implemented in widely deployed machine learning systems. The outreach component of this project involves (1) a technical workshop for researchers from diverse areas and (2) outreach events for K-12 students.<br/><br/>A variety of problems in modern data science have been successfully characterized using a width. For example, one of the most common widths, the rank of a matrix, has a near-ubiquitous use across many applications. This project significantly expands the understanding of several recently proposed widths and extracts their full potential for positive practical outcomes. Furthermore, it contributes to the recently growing work on beyond worst-case analysis in linear algebra, machine learning and coding theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1563732","SHF: Medium: Collaborative Research: An Inspector/Executor Compilation Framework for Irregular Applications","CCF","CI REUSE, SOFTWARE & HARDWARE FOUNDATION","09/01/2016","07/27/2016","Michelle Strout","AZ","University of Arizona","Standard Grant","Anindya Banerjee","08/31/2020","$396,771.00","","mstrout@cs.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","6892, 7798","026Z, 7433, 7924, 7943","$0.00","Computational science and engineering provides inexpensive exploration of physical phenomena and design spaces and helps direct experimentation and advise theory. Irregular applications such as molecular dynamics simulations, n-body simulations, finite element analysis, and big graph analytics constitute a critical and significant portion of scientific computing applications.  An irregular application is characterized by having indirect memory accesses such as A[B[i]] that cannot be determined when the application is being compiled, therefore severely limiting the applicability of the large body of work on parallelizing compiler technology. Consequently, irregular applications, which are so important in pushing forward the frontiers of science, place a very large burden on computational and domain scientists in developing high-performance implementations for the ever-changing landscape of parallel architectures. The intellectual merit of this project is to develop a compiler and runtime framework for irregular applications, particularly well suited for sparse matrix and graph computations that underlie critical problems in computational science and data science. The broader impact is to provide domain scientists a powerful tool for optimizing and porting performance-critical, irregular computations to current and future multi-core processors and many-core accelerators. The PIs will also continue  efforts in outreach and diversity to increase the participation in STEM careers, particularly among women and underrepresented minorities.<br/><br/><br/>The approach in this project is to extend the well-established inspector/executor paradigm where the computational dependence structure (based on the memory access pattern) is determined at runtime, and runtime information is passed to a compile-time generated executor. Specifically, an inspector can examine the memory access patterns early in the computation at runtime, and an executor leverages this information to perform data and computation reordering and scheduling to affect memory hierarchy and parallelism optimizations. The project is developing a compiler and runtime framework with new abstractions for expressing and manipulating inspectors; these inspectors may then be integrated nearly seamlessly with each other and with existing compiler optimizations (e.g., loop tiling) to optimize executors. The project is also extending prior work that supports non-affine input code and mixes compile-time and runtime optimization. The resulting system increases the productivity of expert programmers in achieving both high performance and portability on a wide variety of irregular applications."
"1564074","SHF: Medium: Collaborative Research: An Inspector/Executor Compilation Framework for Irregular Applications","CCF","CI REUSE, SOFTWARE & HARDWARE FOUNDATION","09/01/2016","07/27/2016","Mary Hall","UT","University of Utah","Standard Grant","Anindya Banerjee","08/31/2020","$399,993.00","","mhall@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","6892, 7798","026Z, 7433, 7924, 7943, 9150","$0.00","Computational science and engineering provides inexpensive exploration of physical phenomena and design spaces and helps direct experimentation and advise theory. Irregular applications such as molecular dynamics simulations, n-body simulations, finite element analysis, and big graph analytics constitute a critical and significant portion of scientific computing applications.  An irregular application is characterized by having indirect memory accesses that cannot be determined when the application is being compiled, therefore severely limiting the applicability of the large body of work on parallelizing compiler technology. Consequently, irregular applications, which are so important in pushing forward the frontiers of science, place a very large burden on computational and domain scientists in developing high-performance implementations for the ever-changing landscape of parallel architectures. The intellectual merit of this project is to develop a compiler and runtime framework for irregular applications, particularly well suited for sparse matrix and graph computations that underlie critical problems in computational science and data science. The broader impact is to provide domain scientists a powerful tool for optimizing and porting performance-critical, irregular computations to current and future multi-core processors and many-core accelerators. The PIs will also continue  efforts in outreach and diversity to increase the participation in STEM careers, particularly among women and underrepresented minorities.<br/><br/>The approach in this project is to extend the well-established inspector/executor paradigm where the computational dependence structure (based on the memory access pattern) is determined at runtime, and runtime information is passed to a compile-time generated executor. Specifically, an inspector can examine the memory access patterns early in the computation at runtime, and an executor leverages this information to perform data and computation reordering and scheduling to affect memory hierarchy and parallelism optimizations. The project is developing a compiler and runtime framework with new abstractions for expressing and manipulating inspectors; these inspectors may then be integrated nearly seamlessly with each other and with existing compiler optimizations (e.g., loop tiling) to optimize executors. The project is also extending prior work that supports non-affine input code and mixes compile-time and runtime optimization. The resulting system increases the productivity of expert programmers in achieving both high performance and portability on a wide variety of irregular applications."
"1563818","SHF: Medium: Collaborative Research: An Inspector/Executor Compilation Framework for Irregular Applications","CCF","CI REUSE, SOFTWARE & HARDWARE FOUNDATION","09/01/2016","07/27/2016","Catherine Olschanowsky","ID","Boise State University","Standard Grant","Anindya Banerjee","08/31/2020","$397,199.00","","cathie@cs.boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","6892, 7798","026Z, 7433, 7924, 7943","$0.00","Computational science and engineering provides inexpensive exploration of physical phenomena and design spaces and helps direct experimentation and advise theory. Irregular applications such as molecular dynamics simulations, n-body simulations, finite element analysis, and big graph analytics constitute a critical and significant portion of scientific computing applications.  An irregular application is characterized by having indirect memory accesses such as A[B[i]] that cannot be determined when the application is being compiled, therefore severely limiting the applicability of the large body of work on parallelizing compiler technology. Consequently, irregular applications, which are so important in pushing forward the frontiers of science, place a very large burden on computational and domain scientists in developing high-performance implementations for the ever-changing landscape of parallel architectures. The intellectual merit of this project is to develop a compiler and runtime framework for irregular applications, particularly well suited for sparse matrix and graph computations that underlie critical problems in computational science and data science. The broader impact is to provide domain scientists a powerful tool for optimizing and porting performance-critical, irregular computations to current and future multi-core processors and many-core accelerators. The PIs will also continue  efforts in outreach and diversity to increase the participation in STEM careers, particularly among women and underrepresented minorities.<br/><br/><br/>The approach in this project is to extend the well-established inspector/executor paradigm where the computational dependence structure (based on the memory access pattern) is determined at runtime, and runtime information is passed to a compile-time generated executor. Specifically, an inspector can examine the memory access patterns early in the computation at runtime, and an executor leverages this information to perform data and computation reordering and scheduling to affect memory hierarchy and parallelism optimizations. The project is developing a compiler and runtime framework with new abstractions for expressing and manipulating inspectors; these inspectors may then be integrated nearly seamlessly with each other and with existing compiler optimizations (e.g., loop tiling) to optimize executors. The project is also extending prior work that supports non-affine input code and mixes compile-time and runtime optimization. The resulting system increases the productivity of expert programmers in achieving both high performance and portability on a wide variety of irregular applications."
"1526900","AF: Small: Markov Chain Algorithms for Problems from Computer Science and Statistical Physics","CCF","ALGORITHMIC FOUNDATIONS","08/01/2015","07/21/2015","Dana Randall","GA","Georgia Tech Research Corporation","Standard Grant","Tracy J. Kimbrel","07/31/2019","$400,000.00","","randall@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","7923, 7926, 9102","$0.00","Sampling algorithms based on Markov chains are used across the sciences, primarily for approximate counting, combinatorial optimization and modeling.  These algorithms use random walks to explore a large state space, and determining their convergence time is typically the critical step for establishing the efficiency of many approximation algorithms using random sampling.  The primary goals of this project are identifying problems amenable to this approach, designing provably efficient algorithms, and developing probabilistic techniques to enable such analyses.  For each of these goals, computer science benefits from insights from related fields, especially statistical physics and discrete probability.<br/><br/>The project explores strong connections between phase transitions of physical systems and convergence rates of local Markov chains to explain the limitations of various natural approaches to sampling. This correspondence also guides our search for more efficient algorithms by allowing nonlocal moves or designing Markov chains on modified state spaces. This PI will explore both aspects, by designing methods from stronger analysis in the efficient and non-efficient regimes on both sides of the phase transition, and by searching for alternative non-local algorithms for sampling when local algorithms have been proven to be prohibitively slow.  Applications to be explored include the hard-core model from statistical physics, the Schelling model of segregation from economics, geometric sampling problems form planning and design, and sampling problems from data science where inputs are noisy or evolving over time.<br/><br/>The broader impacts of this interdisciplinary work have many facets, especially for bridging scientific fields by bringing insights from one field to another.  The PI regularly gives technical and survey talks to students and faculty across fields, directs an interdisciplinary research center and organizes workshops and conferences including participants from disparate disciplines.  The results disseminated by talks, publications and will be made accessible on websites.  The PI continues to be a strong advocate for women in academia, including serving as the ADVANCE Professor of Computing, participating on equity panels, presenting lectures to broad groups of women, and advising women Ph.D. students."
"1800317","AF: Small: New Directions in Learning Theory","CCF","ALGORITHMIC FOUNDATIONS","08/01/2017","10/18/2017","Avrim Blum","IL","Toyota Technological Institute at Chicago","Standard Grant","Tracy Kimbrel","05/31/2019","$98,041.00","","avrim@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7926","$0.00","This project is to develop core principles and technologies for systems that learn from observation and experience in order to better help their users.  While there is already a significant body of work in the area of machine learning, today's interconnected world provides both new challenges and new opportunities that classic methods are not able to address or take advantage of.  This project has three main thrusts.  The first involves development of methods that can extract useful information from auxiliary sources in addition to traditional labeled data.  This includes methods for quickly learning multiple related tasks by taking advantage of ways in which they relate to each other.  The second involves approaches for learning about what different users or agents want by observing the results of their interactions.  Finally, the third thrust involves development of new rigorous methods for quickly estimating the amount of resources that would be needed to solve a given learning task.  Broader impacts of the project include the training of a diverse set of graduate students, improving undergraduate curricula with respect to machine learning technology, and developing a new book for advanced undergraduates on algorithms and analysis for data science.<br/><br/>More specifically, the first main thrust of this work involves a combination of unsupervised, semi-supervised, and multi-task learning. This work will investigate problems of estimating error rates from unlabeled data, unifying co-training and topic models, learning multiple related tasks from limited supervision, and learning new representations of data using tools from high-dimensional geometry.  The second main thrust will focus on reconstructing estimates of agent utilities from observing the outcomes of economic mechanisms such as combinatorial auctions.  This thrust also includes problems of learning the rules of unknown mechanisms from experimentation.  Finally, the last thrust focuses on development of the theory of property testing for machine learning problems, with the goal of quickly estimating natural formal measures of complexity of a given learning task."
"1750555","CAREER: Stable Foundations for Reliable Machine Learning","CCF","ALGORITHMIC FOUNDATIONS","02/01/2018","03/02/2019","Moritz Hardt","CA","University of California-Berkeley","Continuing grant","Tracy Kimbrel","01/31/2023","$185,663.00","","hardt@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7796","1045, 7926","$0.00","Across all sciences, researchers hope to use algorithms and machine learning to derive reliable insights from data, but often research findings turn out to be false or hard to replicate. Indeed, assessing the validity of insights suggested by data is presently a difficult and error-prone task. Even in industry, where machine learning has fueled dramatic advances, more principled ways of benchmarking and improving the performance of a machine learning system would make a major difference. What often work best in practice are poorly understood heuristics, leading to much guesswork with varying results. This inscrutable behavior of machine learning also has repercussions on society at large as more and more people struggle with the implications of algorithmic decisions in their daily lives. Fairness, interpretability, and transparency have become major talking points as algorithms increasingly aid or replace human judgment.<br/><br/>The PI aims to build guiding theory alongside scalable algorithms that make the practice of machine learning more reliable, transparent, and aligned with societal values. Focusing on algorithmic stability as a unifying technical framework, this proposal targets several foundational challenges including the design of a robust methodology to address the reliability crisis in data science, a working theory for why and when large artificial neural networks train and generalize well, and a universal framework to reason about generalization in unsupervised learning as is presently lacking. A particular emphasis is on application domains of societal impact. The PI has long been invested in topics such as privacy, fairness, accountability and transparency in machine learning not only through academic publications, but also through workshops, mentorship, teaching, and interdisciplinary engagements."
"1717896","AF: Small: Efficiently Learning Neural Network Architectures with Applications","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","06/26/2017","Adam Klivans","TX","University of Texas at Austin","Standard Grant","Tracy Kimbrel","08/31/2020","$449,920.00","","klivans@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7796","7923, 7926","$0.00","In the last few years there have been several breakthroughs in machine learning and artificial intelligence due to the success of tools for learning ""deep neural networks"" including the best computer program for playing Go, the best programs for automatically playing Atari games, and the best tools for several fundamental object-recognition tasks.  These are considered some of the most exciting new results in all of computer science.<br/><br/>From a theoretical perspective, however, the mathematics underlying these neural networks is not as satisfying.  We have few rigorous results that explain how and why heuristics for learning deep neural networks perform so well in practice. The primary research goal of this proposal is to develop provably efficient algorithms for learning neural networks that have rigorous performance guarantees and give applications to related problems from machine learning.  Given the ubiquity of machine learning algorithms, this research will have direct impact on data science problems from a diverse set of fields including biology (protein interaction networks) and security (differential privacy).  The PI is also developing a new data mining course at UT-Austin that will incorporate the latest research from these areas.<br/><br/>A central technical question of this work is that of the most expressive class of neural networks that can be provably learned in polynomial time.  Furthermore, the algorithm should be robust to noisy data.  A neural network can be thought of as a type of directed circuit where the internal nodes compute some activation function of a linear combination of the inputs.  The classical example of an activation function is a sigmoid, but the ReLU (rectified linear unit) has become very popular.  In a recent work, the PI showed that a neural network consisting of a sum of one layer of sigmoids is learnable in fully-polynomial time, even in the presence of noise.  This is the most expressive class known to be efficiently learnable.  Can this result be extended to more sophisticated networks?  This question has interesting tie-ins to kernel methods and kernel approximations.<br/><br/>For the ReLU activiation, the PI has shown that this problem is most likely computationally intractable in the worst case.  The intriguing question then becomes that of the minimal assumptions needed to show that these networks are computationally tractable.  In a recent work, the PI has shown that there are distributional assumptions that imply fully-polynomial-time algorithms for learning sophisticated networks of ReLUs.  Can these assumptions be weakened?  This work has to do with proving that certain algorithms do not overfit by using compression schemes.  Another type of assumption that the weights of the unknown network are chosen in some random way (as opposed to succeeding in the worst-case).  This corresponds to the notion of random initialization from machine learning.  Can we prove a type of smoothed analysis for learning neural networks, where we can give fully-polynomial-time learning algorithms for almost all networks?<br/><br/>Finally, in this proposal we will explore what other tasks can be reduced to various types of simple neural network learning.  For example, the problem of one-bit compressed sensing can be viewed as learning a threshold activation using as few samples as possible.  Still, we lack a one-bit compressed sensing algorithm that has optimal tolerance for noise.  Another canonical example is matrix or tensor completion, where it is possible to reduce these challenges to learning with respect to polynomial activations.  Finding the proper regularization to ensure low sample complexity is an exciting area of research."
"1814041","CCF-BSF: AF: Small: Collaborative Research: Practice-Friendly Theory and Algorithms for Linear Regression Problems","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","08/27/2018","Petros Drineas","IN","Purdue University","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$249,892.00","","pdrineas@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7796","7923, 7926, 7933","$0.00","The project focuses on one of the most fundamental problems in the intersection of applied mathematics and computer science: solving systems of multiple linear equations in multiple variables. Such systems, also known as linear regression problems, have applications in various fields, from classical engineering to data science and machine learning. These applications yield systems with millions of equations and variables. The design of very efficient solver algorithms is thus a problem of paramount importance. Over the last twenty years there has been a tremendous focus and progress in the theory of algorithms for solving certain types of linear systems that are ubiquitous in applications, despite the fact that they are somewhat restricted (e.g. each equation has only two variables). Along with these algorithms, a wealth of new notions, techniques and tools has been acquired. The project will develop extensions of these techniques, targeting concrete applications in related fields. Towards this end, the project includes research problems that are appropriate for advanced undergraduate and graduate students with complementary interests and skills, ranging from applied to theoretical. Research will be disseminated through all standard channels, importantly including free software.<br/><br/>The project will pursue three main directions: (i) Bring the recent progress from the theoretical to the practical realm. Linear system solvers are useful in a variety of contexts, implying a need for implementations in disparate computational environments, including basic consumer computers, graphical processing units, or big parallel and distributed systems. This necessitates the development of new theory and algorithms that are practice-friendly, i.e. designed with the practical performance end-goal in mind. (ii) The impact of linear system solvers in the downstream applications in Data Science and Machine Learning can be accelerated and strengthened by pursuing their tighter integration with the target applications. A second major goal of the project is thus to pursue an exportation of techniques and notions from the theory of linear regression to specific problems in Machine Learning. This will require the development of adaptations and enhancements of these techniques. (iii) The study of specific algorithmic applications in Machine Learning also serves the third major goal of the project: the design of solvers for regression problems that go beyond the restricted types for which efficient solvers are currently known.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813374","CCF-BSF: AF: Small: Collaborative Research: Practice-Friendly Theory and Algorithms for Linear Regression Problems","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","08/27/2018","Ioannis Koutis","NJ","New Jersey Institute of Technology","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$249,866.00","","i.koutis@gmail.com","University Heights","Newark","NJ","071021982","9735965275","CSE","7796","7923, 7926, 7933","$0.00","The project focuses on one of the most fundamental problems in the intersection of applied mathematics and computer science: solving systems of multiple linear equations in multiple variables. Such systems, also known as linear regression problems, have applications in various fields, from classical engineering to data science and machine learning. These applications yield systems with millions of equations and variables. The design of very efficient solver algorithms is thus a problem of paramount importance. Over the last twenty years there has been a tremendous focus and progress in the theory of algorithms for solving certain types of linear systems that are ubiquitous in applications, despite the fact that they are somewhat restricted (e.g. each equation has only two variables). Along with these algorithms, a wealth of new notions, techniques and tools has been acquired. The project will develop extensions of these techniques, targeting concrete applications in related fields. Towards this end, the project includes research problems that are appropriate for advanced undergraduate and graduate students with complementary interests and skills, ranging from applied to theoretical. Research will be disseminated through all standard channels, importantly including free software.<br/><br/>The project will pursue three main directions: (i) Bring the recent progress from the theoretical to the practical realm. Linear system solvers are useful in a variety of contexts, implying a need for implementations in disparate computational environments, including basic consumer computers, graphical processing units, or big parallel and distributed systems. This necessitates the development of new theory and algorithms that are practice-friendly, i.e. designed with the practical performance end-goal in mind. (ii) The impact of linear system solvers in the downstream applications in Data Science and Machine Learning can be accelerated and strengthened by pursuing their tighter integration with the target applications. A second major goal of the project is thus to pursue an exportation of techniques and notions from the theory of linear regression to specific problems in Machine Learning. This will require the development of adaptations and enhancements of these techniques. (iii) The study of specific algorithmic applications in Machine Learning also serves the third major goal of the project: the design of solvers for regression problems that go beyond the restricted types for which efficient solvers are currently known.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814172","AF: Small: New Directions in Geometric Shortest Paths","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","10/01/2018","08/28/2018","Subhash Suri","CA","University of California-Santa Barbara","Standard Grant","Rahul Shah","09/30/2021","$299,876.00","","suri@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","2878, 7796","7923, 7929","$0.00","Shortest paths and reachability are fundamental problems with a long and distinguished history in computer science and mathematics. Due to growing integration of cyber and physical worlds through the Internet-of-Things (IoT), an increasing amount of data processing entails geometric models, and an ever growing set of smart mobile devices can interact with, and affect, the environment in which they operate - from self-driving cars to autonomous robots. Addressing the needs of these applications, this project explores new directions for shortest paths in geometric domains, organized around the following two broad themes: (1) improving shortest paths by removal of some path-blocking obstacles, and (2) searching for a randomized prize. The specific research topics of the project are novel, yet the general theme touches on many classical problems in geometry, combinatorial optimization, probability theory, decision science and search. The project significantly broadens the scope, applicability and relevance of geometric algorithms to search and planning in continuous spaces, by incorporating strategic intervention and planning under uncertainty. The fundamental nature of topics addressed in this project is likely to appeal to a broad set of students with diverse backgrounds, both graduate and undergraduate.  The material generated from this project will be integrated into multiple courses on algorithms and computational geometry, including the investigator's newly launched Foundations of Data Science course.<br/><br/>The project is centered around two research topics. The first topic deals with the following question of reachability: can one reach a target position from an initial position. When this is possible, the goal is to compute a feasible path, or perhaps the shortest one. The focus of the project is the ""if not"" side of this question, which is often left unattended.  Specifically, if no feasible path exists, or the shortest path is unacceptably long, how many and which obstacles should be removed? This form of reachability augmentation in geometric domains is both fundamental and intellectually exciting, with many surprising twists and turns. The solution quality and the algorithmic efficiency critically depend on the geometry of the workspace and model assumptions: are obstacles convex or non-convex?  Are obstacles disjoint or overlapping?  Is the removal cost the same for all obstacles or different?  The second topic proposes a novel framework for problems in which one tries a sequence of alternatives in search for a prize (favorable outcome), where each outcome is determined by a random trial. Computing combinatorial structures, such as shortest paths, in the presence of such random events poses significant intellectual challenges, and the project research contributes to an important body of knowledge.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1704417","AF:Medium:Collaborative Research:Estimation, Learning, and Memory: The Quest for Statistically Optimal Algorithms","CCF","ALGORITHMIC FOUNDATIONS","07/01/2017","08/06/2018","Gregory Valiant","CA","Stanford University","Continuing grant","Tracy J. Kimbrel","06/30/2021","$195,701.00","","gvaliant@cs.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","7924, 7926","$0.00","The goal of this project is to develop new, efficient algorithms that extract as much information as is possible from a given quantity of data.  In particular, this research aims to develop an understanding of how to leverage structure that is present in natural language settings, medical and genomic settings, and network- or graph-based settings. Many fundamental types of structure are encountered repeatedly in widely varying scientific and technological settings; our goal is to build on a recent body of work that focused on the simplest unstructured settings, and develop broadly applicable tools and insights to these diverse settings.   A central component of this project is a close interaction and transfer of ideas, problems, and techniques, between the theory community, the machine learning community, and the broader set of data-centric researchers and practitioners.<br/><br/>From a technical perspective, this research focuses on three fundamental types of structure: geometric structure, algebraic or low-rank structure, and the structure that is present in sequential<br/>data (such as natural language).   For the first two types of structure, the research focus is on understanding the possibilities and limitations in the sparse data regime where the amount of data is comparable to, or sublinear in, the dimensionality of the data.  In the third setting, the focus is on understanding the role of memory for learning and prediction tasks.<br/><br/>Beyond the direct research goals of the project, the PIs are extensively involved in teaching and outreach, including designing UW?s new data sciences curriculum, and developing new courses on algorithms and foundational aspects of data sciences at Stanford."
"1703574","AF: Medium: Collaborative Research: Estimation, Learning, and Memory: The Quest for Statistically Optimal Algorithms","CCF","ALGORITHMIC FOUNDATIONS","07/01/2017","08/06/2018","Sham Kakade","WA","University of Washington","Continuing grant","Tracy J. Kimbrel","06/30/2021","$351,001.00","Gregory Valiant","sham@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","7924, 7926","$0.00","The goal of this project is to develop new, efficient algorithms that extract as much information as is possible from a given quantity of data.  In particular, this research aims to develop an understanding of how to leverage structure that is present in natural language settings, medical and genomic settings, and network- or graph-based settings. Many fundamental types of structure are encountered repeatedly in widely varying scientific and technological settings; our goal is to build on a recent body of work that focused on the simplest unstructured settings, and develop broadly applicable tools and insights to these diverse settings.   A central component of this project is a close interaction and transfer of ideas, problems, and techniques, between the theory community, the machine learning community, and the broader set of data-centric researchers and practitioners.<br/><br/>From a technical perspective, this research focuses on three fundamental types of structure: geometric structure, algebraic or low-rank structure, and the structure that is present in sequential<br/>data (such as natural language).   For the first two types of structure, the research focus is on understanding the possibilities and limitations in the sparse data regime where the amount of data is comparable to, or sublinear in, the dimensionality of the data.  In the third setting, the focus is on understanding the role of memory for learning and prediction tasks.<br/><br/>Beyond the direct research goals of the project, the PIs are extensively involved in teaching and outreach, including designing UW?s new data sciences curriculum, and developing new courses on algorithms and foundational aspects of data sciences at Stanford."
"1617955","AF:Small: Nearest Neighbor Search in High Dimensional Spaces","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","05/17/2016","Alexandr Andoni","NY","Columbia University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$449,960.00","","andoni@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7923, 7926, 7929","$0.00","The goal of this project is to advance the state of the art of algorithms for the nearest neighbor search (NNS) problem. The NNS problem is one of the central computational problems arising when dealing with modern massive datasets.  For example, it underlies a classical classification rule in machine learning: to label a new object (such as an image), one can simply find the most similar objects (the nearest neighbors) in a preprocessed database, and use the label of the objects found. More generally, NNS is a key algorithmic tool in many areas including databases, data mining, information retrieval, computer vision, computational geometry, signal processing, bioinformatics, and others.  In such applications, the objects are usually represented in a high-dimensional space: e.g., a 20x20 image is naturally represented by a 400-dimensional vector, with one coordinate per pixel.<br/><br/>The PI intends to study the high-dimensional NNS problem, by addressing both foundational algorithmic questions, as well as applied aspects. The project aims for both scientific and educational impact. First, the study of the NNS problem is instrumental in developing fundamental concepts in areas such as high-dimensional computational geometry as well as sublinear space algorithms (including concepts such as dimension reduction, sketching, metric embeddings, etc). Second, due to the numerous applications of NNS, its efficient implementations are used widely in industry. Overall, the PI aims to foster a stronger connection between the theory and practice of NNS by code dissemination, public lectures, and student training. The PI's affiliation with Columbia's Data Science Institute puts the PI in a particularly good position to accomplish these goals.<br/><br/>To accomplish the project's algorithmic goals, the PI will leverage the recently developed approach of data-dependent hashing, where the hash function itself adapts to a given dataset. As a proof-of-concept, the PI and co-authors recently demonstrated that this new approach to NNS leads to algorithms outperforming the classical NNS algorithms (such as those based on the Locality-Sensitive Hashing). The project aims to develop this methodology further to its maturity, extend it to other relevant metrics (similarity measures) which have traditionally resisted efficient solutions, develop practical versions of the algorithms, and to understand the limits of these techniques."
"1841190","EAGER: Developing a Theory for Function Optimization on Graphs Using Local Information","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","07/13/2018","Varun Jog","WI","University of Wisconsin-Madison","Standard Grant","Phillip Regalia","09/30/2020","$175,326.00","Po-Ling Loh","vjog@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7797","7916, 7935","$0.00","This project seeks to advance the frontiers of knowledge concerning efficient search on networked data. The investigators will devise new algorithms for optimizing a function defined over a graph. Such algorithms will be easily implementable, broadly applicable, and backed by mathematical theory. Some example application areas in which the work will be relevant include faster web retrieval and cybersecurity, where it is important to identify key individuals in a large web of interconnected data. The project will also support the educational goals of the investigators by training multiple graduate students working at the interface of theoretical and applied data science research.<br/><br/>The work conducted in this project will establish new connections between continuous and discrete optimization. The investigators will explore notions of smoothness and convexity that may be used to characterize the convergence properties of their proposed optimization algorithms; unlike optimization on graphs, optimization on continuous domains is backed by a mature theory that has been developed over several decades. Developing an analogous theory for discrete domains such as graphs poses many challenges, however -- in particular, it requires developing new notions of derivatives, Hessians, smoothness, or convexity, which have no obvious analogs. This work is divided into the following sub-projects, each with a distinct set of research objectives: (1) Develop and analyze iterative and local algorithms on graphs; (2) Find suitable notions of smoothness and convexity on graphs, and analyze their consequences. In addition, the algorithms will be implemented and evaluated on various real-world networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740425","RCN: DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization","CCF","SPECIAL PROJECTS - CCF","09/15/2017","06/07/2018","Rebecca Wright","NJ","Rutgers University New Brunswick","Standard Grant","Tracy Kimbrel","08/31/2020","$500,000.00","Richard Karp, Shafrira Goldwasser, Tamra Carpenter","rwright@barnard.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","2878","7926","$0.00","Optimization tools and algorithms have transformed fields ranging from biology to finance, and they touch everyday lives through more efficient supply chains, better traffic management, and more secure power grids. New applications, particularly those stemming from machine learning and data science, are now challenging the field to solve larger and more complex problems on smaller devices in less time. The field is responding with innovative approaches leading to advances such as faster algorithms for maximum flow and near-real-time approximations, more efficient interior-point methods, and faster cutting-plane methods. Many of these breakthroughs bring together ideas from both continuous and discrete optimization. The DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization aims to accelerate progress by stimulating collaboration across the many communities of optimization. The planned activities bring together computer scientists, mathematicians, operations researchers, engineers, statisticians, and algorithm developers to advance both the foundations and applications of optimization. <br/><br/>The project begins with an intensive program at the Simons Institute during the fall semester of 2017 that launches the collaboration and builds momentum for activities conducted over the ensuing two years as part of the DIMACS Special Focus on Bridging Continuous and Discrete Optimization. The DIMACS special focus includes seven workshops that sustain the project through the end of 2019 and expand it to include more people and more topics. The project aims to improve the performance of optimization methods in challenging real settings with the potential to positively impact society by improving traditional applications in logistics, supply chains, engineering, infrastructure, and finance, as well as growing applications in machine learning and data science. The project will involve a large number of people in various scientific communities and expose them to new ideas, new problems, and new opportunities for collaboration. Participants will be diverse across a variety of dimensions, including women and other under-represented groups; a mix of junior and senior participants; people from multiple disciplines; and both industry and academic participants. There will also be international coordination with the Centre de Recherches Mathmatiques and Polytechnique Montreal."
"1651588","CAREER: Statistical Inference on Large Domains and Large Networks: Fundamental Limits and Efficient Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","03/01/2017","02/21/2017","Yihong Wu","CT","Yale University","Continuing grant","Phillip Regalia","02/28/2022","$209,342.00","","yihong.wu@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7797","1045, 7936","$0.00","The emergence of data science has brought about new perspectives on age-old statistical questions. Driven by contemporary applications such as neuroscience, functional genomics and social networks, an emerging research thread in high-dimensional statistics deals with new problems of a combinatorial nature, most importantly, inference on large domains and large graphs, such as entropy estimation on large alphabets, detecting community structures in networks, all of which rely on exploiting the intrinsic or extrinsic low-dimensionality of the problem. On the other hand, an important element absent from the classical statistical paradigm is the computational complexity of inference procedures, which is becoming increasingly relevant dealing with large-scale noisy datasets. The PI has detailed plans for mentoring both undergraduate and graduate students, targeting specifically at members of the under-represented communities.<br/><br/>Combining both statistical and computational perspectives, this research develops an interdisciplinary program aiming to advance the understanding of the fundamental inferential and algorithmic limits of statistical estimation on large domains and large networks, including functional estimation on large alphabets, learning graph properties with network sampling, extrapolating unseen species. These objectives come with a set of theoretical and practical challenges, which can be effectively addressed by a new combination of insights and techniques from information theory, high-dimensional statistics, approximation theory, random graphs and matrices, and optimization. In addition to determining the statistical limits and designing efficient and scalable procedures, which provide key enabling technologies for such high-impact applications of data science as neuroscience, gene association networks, and social network analysis, a major innovation lies in rigorously identifying the optimal statistical performance under complexity constraints and, complementarity, understanding the inferential power and limitations of important classes of algorithms such as spectral methods, relaxation hierarchies, and message-passing algorithms."
"1718533","AF: Small: New Algorithmic Primitives for Directed Graphs: Sparsification and Preconditioning","CCF","ALGORITHMIC FOUNDATIONS","07/01/2017","01/29/2019","Yang Peng","GA","Georgia Tech Research Corporation","Standard Grant","Rahul Shah","06/30/2020","$450,000.00","","rpeng@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","7923, 7926","$0.00","Many real-life problems arising from social networks, transportation, image processing, resource assignment and other domains use graphs as a fundamental underlying structure for modeling and solving problems. Graphs and networks are widely used formats for representing data via edges that describe pairs of related objects. There is a growing need for efficient graph algorithms due to the abundance of large graphs in many of these domains. This project will study two important tools in the development of efficient graph algorithms: sparsification, which removes edges while preserving the overall structure of the graph, and preconditioned iterative methods, which improve the qualities of solutions. It will also support the development of courses that incorporate experimental aspects motivated by data science, the training of research-oriented students, and outreach activities based on algorithmic problem solving.<br/> <br/>The goal of this project is to extend key primitives for efficient algorithms on undirected graphs to directed graphs. Recent works hinted that two important tools for designing provably efficient algorithms on undirected graphs, sparsification and preconditioning, can be generalized to directed graphs when used in conjunction with each other. Studying these routines together enables a greater range of algorithmic flexibility: the construction of approximate graphs now only need to preserve solutions relevant to the convergence of the other loop, instead of for all possible inputs. The project will focus on extending this interplay between sparsification and preconditioning through better understanding of the underlying tools, iterative methods and concentration bounds. Progress on these tools can in turn lead to improvements on fundamental problems in graph algorithms such as computing directed random walks, finding maximum matchings on dense bipartite graphs, and maintaining large matchings on dynamically changing graphs."
"1803362","EAGER: Semantics for Learning Functional Programming","CCF","SOFTWARE & HARDWARE FOUNDATION","01/01/2018","04/23/2018","Shriram Krishnamurthi","RI","Brown University","Standard Grant","Anindya Banerjee","06/30/2019","$158,000.00","Kathryn Fisler","sk+17@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7798","7798, 7916, 7943, 8206, 9150, 9251","$0.00","Programming is a rigorous and intellectually demanding activity. Programmers are expected to provide instructions to a black box device, whose behavior is very different from their own, to accomplish complex tasks. Even seasoned professionals can find this challenging, and beginners often struggle to do it. These problems are greatly amplified when a program has errors or produces incorrect output. A major obstacle is for the programmer to understand how the computer works at a level that is useful for expressing their needs and correcting their programs. The intellectual merits are to evaluate existing models of programming systems, and to define new ones that enable programmers to better understand the computer's execution. The project's broader significance and importance are to make effective programming more accessible to a much broader range of people, including those who intend to apply computing in other data-intensive domains.<br/><br/>Concretely, the research examines the use of programming language semantics as explanatory tools for non-technical users. Existing semantics provide rich explanations of behavior, but are expressed in highly technical terms that require significant expertise to understand and use. Furthermore, they have not been tested through application to actual debugging tasks. Therefore, this work intends to open up investigation into the human factors aspects of programming language semantics, understanding how well they perform in different settings, and potentially defining new semantics forms that are better suited to a broad range of programmers. The work will specifically focus on functional programming with an eye towards its role in data science curricula, which are of value across a broad spectrum of disciplines."
"1750162","CAREER: Automated Analysis and Design of Optimization Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","02/15/2018","02/08/2018","Laurent Lessard","WI","University of Wisconsin-Madison","Continuing grant","Phillip Regalia","01/31/2023","$188,483.00","","laurent.lessard@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7797","1045, 7935","$0.00","Iterative optimization algorithms lie at the heart of modern data-intensive applications such as machine learning, computer vision, and data science. Society has become increasingly reliant on such algorithms for commerce, transportation, healthcare, emergency response, and national security. Despite their critical role in society, algorithms are typically designed and tuned using insight from experts, extensive numerical simulations, and other heuristics. This research develops a more principled understanding and approach to algorithm design that automatically accounts for sensitivity to parameter choice, robustness to noise, and other sources of uncertainty. This approach enables algorithms to be engineered in a way that guarantees performance and safety, which is similar to how airplanes, skyscrapers, and computer hardware are built.<br/><br/>Iterative algorithms may be viewed as dynamical systems with feedback. In gradient-based descent methods, for example, gradients are evaluated at each step and used to compute subsequent iterates. By treating algorithms as control systems, this research leverages tools from robust control (specifically: integral quadratic constraints, graphical methods, and semidefinite representation) to analyze and ultimately synthesize a variety of algorithms under different assumptions in an efficient, scalable, and systematic manner. This research also involves collaborative efforts in the areas of graph structure learning of gene regulatory networks and interactive machine learning, which serve to test and validate new algorithm designs."
"1656951","CRII: CIF: Universal Analysis of Optimization Algorithms","CCF","CRII CISE Research Initiation","02/15/2017","02/10/2017","Laurent Lessard","WI","University of Wisconsin-Madison","Standard Grant","Phillip Regalia","01/31/2020","$175,000.00","","laurent.lessard@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","026Y","7797, 7936, 8228","$0.00","Iterative optimization algorithms lie at the heart of modern data-intensive applications such as machine learning, computer vision, and data science. The choice of algorithm, or even the choice of tuning parameters for a particular algorithm, has a dramatic effect on performance and viability. Even simple variants of well-known algorithms can be troublesome to analyze and must be considered on a case-by-case basis with the help of either deep insights by experts or extensive numerical simulations. Then, theoretical analyses of algorithms may fail to provide accurate or faithful performance guarantees because they do not account for sensitivity to parameter choice, robustness to noise either inherent to the algorithm or built in to how the iterations are computed, or other sources of uncertainty.<br/><br/>The starting point for this research is to view iterative algorithms as dynamical systems with feedback. In gradient-based descent methods, for example, gradients are evaluated at each step and used to compute subsequent iterates. This research leverages tools from robust control (specifically, integral quadratic constraints and semidefinite programming) to develop a versatile, scalable, and modular framework capable of analyzing a variety of algorithms under different assumptions in an efficient and systematic manner. Of particular interest will be large-scale algorithms such as stochastic gradient descent and its variants, as well as distributed or asynchronous implementations."
"1833276","NSF Workshop on Internet-of-Things (IoT) Hardware Systems","CCF","SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION, CYBER-PHYSICAL SYSTEMS (CPS)","08/01/2018","05/22/2018","Marilyn Wolf","GA","Georgia Tech Research Corporation","Standard Grant","Sankar Basu","07/31/2019","$30,000.00","","wolf@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1714, 2878, 7798, 7918","073Z, 7556, 7945, 9102","$0.00","This project supports a workshop to identify research directions and build a research community for Internet-of-Things (IoT) hardware systems. IoT hardware systems combine wireless, low-power sensor and actuator nodes in networks along with fog and cloud computing.  They provide rich data sets that can be used for real-time monitoring, control, and decision-making. IoT systems are widely used for industrial, transportation, health care, and other applications. As IoT systems increase in scope and complexity, new research challenges arise that require new results and novel interdisciplinary collaborations. The workshop will involve researchers with expertise in a wide range of topics relevant to IoT. The attendees will collaborate to develop a report identifying key challenges and opportunities in IoT hardware systems as well as important resources needed to carry out this research.  <br/><br/>The workshop convens researchers from related disciplines: sensor networks, VLSI, communication, signal processing, etc. The workshop will help to build a community of IoT researchers. The workshop will include graduate student attendees to broaden educational impact. The workshop organizers will strive to build an inclusive set of workshop attendees. The workshop will include a session on diversity issues in IoT research. Talks from the workshop will be broadcast for wider dissemination. The workshop report will be broadly distributed. Workshop attendees will be selected on the basis of white papers submitted to an open call. Workshop presentations will be broadcast on the Internet using video-conferencing tools. Topics of potential interest to the workshop include: VLSI IoT device architectures; ultra-low power design for IoT devices; IoT communication and networking; emerging technologies for IoT devices and systems; distributed algorithms under power and bandwidth constraints; IoT applications in manufacturing, transportation, health care, smart communities, etc.; safe and secure IoT systems; data science challenges in IoT systems; machine learning and AI using IoT.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816793","SHF: Small: Tools for Productive High-performance Computing with GPUs","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","12/21/2018","Ponnuswamy Sadayappan","OH","Ohio State University","Standard Grant","Almadena Chtchelkanova","06/30/2021","$499,718.00","Aravind Sukumaran Rajam","sadayappan.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7798","7923, 7942","$0.00","Graphical Processing Units (GPUs) are widely and cheaply available and have become increasingly powerful relative to general-purpose CPUs. Therefore, they are attractive targets for compute-intensive applications in computational science and data science. However, development of software to run on GPUs is time-consuming and requires expertise held by only a very small fraction of the application developer community. This project is developing a collection of tools to assist in the productive development of high-performance software for GPUs, so that the barrier to effective use of GPUs by the scientific community can be lowered.<br/><br/>A central idea being pursued in this research is the identification of primary hardware resource bottlenecks that limit performance of a GPU kernel, to guide the modification of the kernel in a manner that seeks to alleviate the identified bottleneck. Abstract kernel emulation along with sensitivity analysis with respect to hardware resource latency/throughput parameters are used for bottleneck identification. Three usage scenarios are targeted: (1) OpenMP offload, (2) domain-specific code generators, and (3) CUDA/OpenCL kernels.  The offload model introduced in OpenMP 4.0 is an attractive approach for transforming existing legacy codes as well as for newly developed codes, to facilitate productivity and portability. Domain-specific library generators exploit pattern-specific semantics in order to perform optimizing transformations that are beyond the scope of general-purpose optimizing compilers. Tensor contractions and stencils are two domains of particular emphasis. For all targeted usage scenarios, the collection of tools is intended to assist developers improve the performance of GPU code through a combination of model-driven search and auto-tuning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815434","AF: Small: Spectral and SDP Techniques: Average-Case Analysis and Subexponential Algorithms","CCF","ALGORITHMIC FOUNDATIONS","08/01/2018","05/18/2018","Luca Trevisan","CA","University of California-Berkeley","Standard Grant","Rahul Shah","07/31/2021","$500,000.00","","luca@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7796","7923, 7926","$0.00","This project involves the design and analysis of algorithms for combinatorial problems using techniques from linear algebra and convex optimization. The project bridges pure mathematics and data science, allowing new applications of mathematical ideas to the practice of computing, and from the activities on dissemination, exposition, outreach and mentoring. The project will create open-access lecture notes, surveys and blog posts, making highly technical results accessible to a broader audience. This project will play a key role in the training of graduate students, including two students belonging to underrepresented groups in computer science, and in the design of a new graduate course.<br/><br/>The project involves novel approaches to fundamental problems, such as certifying the unsatisfiability of random constraint satisfaction problems, efficiently certifying properties of sparse random graphs and sparse random matrices, understanding the power of sub-exponential size relaxations in the sum-of-squares hierarchies, developing new construction of graph sparsifiers and finding new ways to analyze certain probabilistic distributed processes. Some of the problems in the scope of this project are not believed to admit algorithms that perform correctly and efficiently on all inputs. For this reason, the project will focus on: (a) algorithms whose running time scale ""sub-exponentially"" and that outperform brute-force combinatorial search, and (b) algorithms that may perform poorly on a few inputs but that perform well on average on random inputs. The second goal depends on how the inputs are distributed, and a key focus of this project is to generalize past results that apply to certain specific distributions to broader classes of distributions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763657","CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning","CCF","SPECIAL PROJECTS - CCF","09/01/2018","06/28/2018","Viveck Cadambe","PA","Pennsylvania State Univ University Park","Continuing grant","Phillip Regalia","08/31/2022","$75,777.00","","VXC12@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","2878","075Z, 7924, 7935","$0.00","Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. <br/><br/>The specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes ""coded computing"", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763561","CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning","CCF","SPECIAL PROJECTS - CCF","09/01/2018","06/28/2018","Pulkit Grover","PA","Carnegie-Mellon University","Continuing grant","Phillip Regalia","08/31/2022","$97,696.00","","pgrover@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878","075Z, 7924, 7935","$0.00","Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. <br/><br/>The specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes ""coded computing"", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1563113","SHF: Medium: PRISM: Platform for Rapid Investigation of efficient Scientific-computing & Machine-learning","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/18/2017","Oyekunle Olukotun","CA","Stanford University","Standard Grant","Almadena Chtchelkanova","07/31/2019","$960,000.00","Jack Poulson","kunle@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7798","7924, 7941, 7942","$0.00","Today's systems demand acceleration in processing and learning using massive datasets. Unfortunately, because of poor energy scaling and power limits, performance and power improvements due to technology scaling and instruction level parallelism in general-purpose processors have ended. It is well known that full custom, application-specific hardware accelerators can provide orders-of-magnitude improvements in energy/op for a variety of application domains. Therefore, there is a special interest in systems that can optimize and accelerate the building blocks of machine learning and data science routines. Many of these building blocks share the same characteristics as building blocks of high performance computing kernels working on matrices. <br/><br/>Such application specific solutions rely on joint optimization of algorithms and the hardware, but cost hundreds of millions of dollars. PRISM (Platform for Rapid Investigation of efficient Scientific- computing and Machine-learning accelerators) is proposed to amortize these costs. PRISM enables application designers to get rapid feedback about both the available parallelism and locality of their algorithm, and the efficiency of the resulting application/hardware design. PRISM platform consists of two coupled tools that incorporate design knowledge at both the hardware and algorithm level. This knowledge enables the tool to give application designers the ability to quickly evaluate the performance of their applications on the proposed/existing hardware, without the application designer needing to be an expert at hardware or algorithms. This platform will leverage tools created from the team's prior research. <br/><br/>Initially, these tools will be used to create an efficient solution for each application, followed by a comparison of the resulting hardware designs. The possibility of creating platforms that span multiple classes of algorithms can then be explored. Finally, a comparison of these new architectures to existing heterogeneous architectures with GPUs and FPGAs will be made, to gain understanding about what modifications are necessary for these architectures to achieve higher levels of efficiency when supporting these classes of algorithms. The work on key applications will lead to better insight about the computation and communication intrinsic to these computations, and provide algorithms for these applications that will be effective on conventional and new architectures."
"1763702","CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning","CCF","SPECIAL PROJECTS - CCF","09/01/2018","06/28/2018","Georgios-Alex Dimakis","TX","University of Texas at Austin","Continuing grant","Phillip Regalia","08/31/2022","$71,994.00","","dimakis@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","2878","075Z, 7924, 7935","$0.00","Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. <br/><br/>The specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes ""coded computing"", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763673","CIF: Medium: Collaborative Research: Coded Computing for Large-Scale Machine Learning","CCF","SPECIAL PROJECTS - CCF","09/01/2018","06/28/2018","Amir Avestimehr","CA","University of Southern California","Continuing grant","Phillip Regalia","08/31/2022","$71,244.00","","avestime@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","2878","075Z, 7924, 7935","$0.00","Deep learning models are breaking new ground in data science tasks including image recognition, automatic translation and autonomous driving. This is achieved by neural networks that can be hundreds of layers deep and involve hundreds of millions of parameters. Training such large models requires distributed computations, very long training times and expensive hardware. This project studies coding theoretic techniques that can accelerate distributed machine learning and allow training with cheaper commodity hardware. Beyond the development of theoretical foundations, this project develops new algorithms for providing fault tolerance over unreliable cloud infrastructure that can significantly reduce the cost of large-scale machine learning. The research outcomes of the project will be broadly disseminated and integrated into education. <br/><br/>The specific focus of this research program is on mitigating the bottlenecks of distributed machine learning. Currently, scaling benefits are limited because of two reasons: first, communication is typically the bottleneck and second, straggler effects limit performance. Both problems can be mitigated using coding theoretic methods. This work proposes ""coded computing"", a transformative framework that combines coding theory with distributed computing to inject computational redundancy in a novel coded form. This framework is then used to develop three research thrusts: a) Coding for Linear Algebraic Computations b) Coding for Iterative Computations and c) Coding for General Distributed Computations. Each of the thrusts operates on a different layer of a machine learning pipeline but all rely on coding theoretic tools and distributed information processing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1518844","SHF: Large: Gradual Typing Across the Spectrum","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","03/16/2018","Matthias Felleisen","MA","Northeastern University","Standard Grant","Anindya Banerjee","06/30/2019","$2,300,195.00","Jan Vitek","matthias@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7798","7925, 7943, 9150, 9251","$0.00","Title: SHF: Large: Gradual Typing Across the Spectrum<br/><br/>The ``Gradual Typing Across the Spectrum'' project addresses an urgent, emergent need at the intersection of software maintenance and programming language research.  Over the past 20 years, working software engineers have embraced so-called scripting languages for a variety of tasks. They routinely use JavaScript for interactive web pages, Ruby on Rails for server-side software, Python for data science, and so on. Software engineers choose these languages because they make prototyping easy, and before the engineers realize it, these prototypes evolve into large, working systems and escape into the real world. Like all software, these systems need to be maintained---mistakes must be fixed, their performance requires improvement, security gaps call for fixes, their functionality needs to be enhanced---but scripting languages render maintenance difficult. The intellectual merits of this project are to address all aspects of this real-world software engineering problem. In turn, the project's broader significance and importance are the deployment of new technologies to assist the programmer who maintains code in scripting languages, the creation of novel technologies that preserve the advantages of these scripting frameworks, and the development of curricular materials that prepares the next generation of students for working within these frameworks. <br/><br/>A few years ago, the PIs launched programming language research efforts to address this problem. They diagnosed the lack of sound types in scripting languages as one of the major factors. With types in conventional programming languages, programmers concisely communicate design information to future maintenance workers; soundness ensures the types are consistent with the rest of the program. In response, the PIs explored the idea of gradual typing, that is, the creation of a typed sister language (one per scripting language) so that (maintenance) programmers can incrementally equip systems with type annotations. Unfortunately, these efforts have diverged over the years and would benefit from systematic cross-pollination.<br/><br/>With support from this grant, the PIs will systematically explore the spectrum of their gradual typing system with a three-pronged effort. First, they will investigate how to replicate results from one project in another. Second, they will jointly develop an evaluation framework for gradual typing projects with the goal of diagnosing gaps in the efforts and needs for additional research. Third, they will explore the creation of new scripting languages that benefit from the insights of gradual typing research."
"1714180","CIF: Small: Controlled Sensing with Social Learning","CCF","COMM & INFORMATION FOUNDATIONS","07/15/2017","07/07/2017","Vikram Krishnamurthy","NY","Cornell University","Standard Grant","Phillip Regalia","06/30/2020","$421,391.00","","vikramk@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","7923, 7936","$0.00","Abstract:  This research addresses the growing need for new techniques in controlled sensing and information fusion with social sensors. Social sensors learn from and interact with  each other  over a social network to estimate an underlying state  - the process is called social learning. The aim of this research is to develop mathematical models,  algorithms and analysis for controlled sensing and information  fusion  with social learning. Controlled sensing with social learning is a first step towards constructing generative feedback models and algorithms in the data science of social networks. Similar formulations arise in multi-agent signal processing problems  where automated decision makers interact to achieve a sensing goal. The research transcends classical statistical signal processing (which deals with extracting signals from noisy measurements) to address the deeper issue of how multi-agent decision systems and signal processing algorithms interact.<br/><br/>The objectives of this research  fall under two inter-related themes:  Bayesian social learning where quickest change detection and controlled fusion are considered; and interactive sensing in large scale networks where adaptive estimation of degree distribution dynamics, infection dynamics and posterior Cramer Rao bounds are considered. The research involves the interplay of Bayesian social learning, stochastic control and mean field dynamics.  The key unifying themes underpinning this research  are statistical signal processing and controlled sensing. Social learning involves myopic decision making by individual agents; while controlled sensing involves stochastic control over a time horizon. This interaction between myopic social sensors (local decision makers) and a non-myopic controller (global decision maker) results in unusual behavior. The scientific innovation of this research stems from advancing deep results in Bayesian estimation, stochastic optimization, weak convergence analysis, and lattice programming."
"1815254","AF: Small: Scalable Algorithms for Data and Network Analysis","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/30/2018","Shanghua Teng","CA","University of Southern California","Standard Grant","Tracy J. Kimbrel","05/31/2021","$500,000.00","","shanghua@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7796","7923, 7926","$0.00","Data-based decision-making involving big input data sets require a lot of time for algorithms to process them. In computer science, efficient algorithms are generally considered to be the ones whose running time does not increase ""too fast"" when input size grows. ""Scalable algorithms"" are among the most efficient algorithms, because their running time is required to be nearly linear or even sub-linear with respect to the problem size. In other words, their complexity scales gracefully when the input size scales up. In the age of Big Data, efficient algorithms are in higher demand now more than ever before. While big data takes us into the asymptotic world envisioned by the pioneers of computer science, the explosive growth of problem size has also significantly challenged the classical notion of efficient algorithms: Algorithms that used to be considered efficient, according to the traditional polynomial-time characterization, may no longer be adequate for solving today's problems. It is not just desirable, but essential, that efficient algorithms should be scalable. Thus, scalability, instead of polynomial-time computability, should be elevated to the central complexity notion for characterizing efficient computation, and this will be the focus of algorithm design for network sciences and big data. This project will focus on the design and analysis of scalable algorithms.  One of its primary objective is to build bridges between the area of algorithm design and the fields of network sciences and machine learning.  If successful, the project will help to provide a rigorous algorithmic framework for designing new scalable data and network analysis algorithms.  By focusing on notions of algorithmic efficiency --- such as scalability --- that respect the sensibilities of researchers in these disciplines as to what constitutes a practical algorithm in the age of big data, this project aims to increase the value of theoretical analyses to researchers in these fields.  As this project combines ideas from many disciplines within one coherent research effort, lectures and tutorials presented on the fruits of the project will help cross-fertilize the disciplines within its scope.  The development of theoretical algorithms that might have practical applicability should simplify education in algorithms for students beyond theoretical computer science, and allow discussion of practically important heuristics at early stages of computer science education. The interdisciplinary nature of this research will enable broader engagements with PhD students and researchers in network sciences, machine learning, numerical analysis, and social science and hence will enhance the chance to be successful in supporting and working with women and minority researchers.<br/><br/>The technical goal of this project is to systematically extend the family of algebraic, numerical, and combinatorial techniques from the recent breakthroughs in Laplacian linear solvers and max-flows/min-cuts, to a wide-range of problems that arise in network analysis, data mining, and machine learning.  This project aims to show that these techniques --- including advanced sampling, sparsification, and local exploration of networks --- will play increasing roles in improving algorithmic scalability.  It contains several open questions and conjectures ranging from network analysis to distribution sampling to social influences towards this goal.  By providing new algorithmic insights into various dynamic processes over graphs, the project also aims to improve the understanding of network facets beyond static graph structures in order to develop better algorithmic theory for network sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816442","AF: Small: Laplace-de Rham Operators in Scientific Computing and Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","05/30/2018","Amir Nayyeri","OR","Oregon State University","Standard Grant","Tracy J. Kimbrel","09/30/2021","$400,000.00","","nayyeria@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7796","7923, 7929","$0.00","Algorithmic problems about geometric spaces are ubiquitous in many fields of science and engineering.  For example, in data analysis, a central task is to extract important features of data, a high-dimensional point set in many cases.  The extraction of the significant features then leads to applications such as visualizing the data or clustering the data in a meaningful way.  Another example is in scientific computing where modeling the behavior of a physical phenomenon such as heat, sound or electricity within a geometric space is a central problem.  These problems can be specified by partial differential equations (PDEs), for which efficient solvers are necessary.  Many of these geometric problems can be cast as problems in linear algebra: computational problems on Laplace-de Rham operators, a sequence of matrices that succinctly present geometric spaces.  Thus, faster and simpler methods can be developed for these problems using powerful tools from linear algebra, which is the focus of this project.<br/><br/>Recent advances in the study of graph Laplacians (the first matrix in the Laplace-de Rham sequence) have resulted in a series of simple and elegant algorithms that helped important applications in computer science and data analysis. In contrast, higher dimensional variants of Laplace-de Rham operators, in spite of their key roles in science and engineering (e.g. in solving physical equations, in performing Helmholtz/Hodge decomposition for applications in data visualization and statistical ranking, or in revealing effective  geometric or topological properties of a hidden space) have remained largely under-explored. This project will fill this gap by addressing important open problems about Laplace-de Rham operators for two different types of applications under two high-level aims: (1) design efficient solvers for Laplace-de Rham matrices that appear in important problems raised from partial differential equations (PDEs), e.g. vector Laplacians in Navier-Stokes or Maxwell equations, in scientific computing, and (2) discover and harness effective properties of Laplace-de Rham matrices and their spectra for data analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1645514","SHF: EAGER: Developing General Techniques for Tightening Bounds of the Data-Movement Complexity of Large Scale Parallel Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","03/31/2017","Julien Langou","CO","University of Colorado at Denver-Downtown Campus","Standard Grant","Almadena Chtchelkanova","07/31/2019","$300,000.00","","julien.langou@ucdenver.edu","F428, AMC Bldg 500","Aurora","CO","800452571","3037240090","CSE","7798","7916, 7942","$0.00","Enabling faster and more energy-efficient numerical simulations is critical, for example, in basic science for enabling novel scientific discoveries, or in engineering for developing revolutionary new products.  In the last decades, technology trends in computer systems have resulted in widely differing rates of improvement in computational throughput as compared to data movement performance.  With future systems, the cost of data movement through the memory hierarchy is expected to become even more dominant relative to the cost of performing arithmetic operations, both in terms of time and energy. Consequently, the data movement and communication costs of a numerical simulation become determinant factors for the time to solution and the energy consumption.  <br/>This research is developing novel generic techniques for tightening bounds on the data movement complexity of numerical simulations. The<br/>outcomes of this research are methods to derive the communication needs of numerical simulations. The ability to assess the optimality of an algorithm enables understanding of the implications of various computing platform's parameters on the performance of a numerical simulation. The PIs are developing novel generic techniques for tightening bounds on the data movement complexity of a given algorithm on a given architecture.  This work engages in novel interdisciplinary perspectives and brings together applied mathematics, theoretical computer science, data analytics and high performance computing."
"1564298","SHF: Medium: A DSL for Data Visualization and Analysis in Imaging-Based Science and Scientific Computing","CCF","SOFTWARE & HARDWARE FOUNDATION, CDS&E","05/15/2016","09/16/2016","John Reppy","IL","University of Chicago","Continuing grant","Anindya Banerjee","04/30/2020","$1,182,415.00","Gordon Kindlmann","jhr@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7798, 8084","7433, 7924, 7943, 8084","$0.00","This research project is focused on the design, implementation, and application of Diderot, a high-level domain-specific programming language for analyzing and visualizing 3D and 4D (spatio-temporal) data, such as produced by imaging modalities and computational simulations on finite-element meshes.  Many areas of science now require computing with digital representations of the objects and systems being studied, but creating new research software to visualize and understand the data is a major bottleneck.  With new ways of volumetrically scanning specimens and increasingly nuanced models of dynamic systems, research software is becoming more complex, and the computational expense of running the software is significantly increasing.  Diderot addresses this urgent problem by simplifying the process of creating new parallel software to visualize and process complex scientific data.  The intellectual merits are how Diderot will accelerate expressing mathematical ideas in working code, expand the kinds of computational representations that Diderot understands, leverage the power of large supercomputers, and help programmers debug their programs.  The project's broader significance and importance are derived in part from the kinds of data that Diderot will handle, such as the high-resolution images produced by a microCT scanner at Argonne National Lab (used daily by scientists from all over the world), or the latest generation of light-sheet microscopes used in developmental biology to investigate fundamental questions about how cells organize themselves into tissues.  The project's significance also derives from the range of people who will use Diderot, from mathematicians writing specialized finite element method programs, to college students getting their first taste of scientific computation by working with real-world microscope data capturing the formation of the nervous system in a fish embryo.<br/><br/>Computation is an increasingly important tool for science, but the semantic gap between the available computational tools and scientific reasoning is often large.  One pressing research topic within computer science is how to create programming tools that can track the shift in parallel computing hardware from traditional general-purpose CPUs to heterogeneous processors with high-performance accelerators like GPUs.  This proposal seeks to accelerate research at the intersection of computation and science by exploiting domain-specific language (DSL) technology to bridge this semantic gap and to transform how scientists use software to understand data from measurement and simulation.  The PIs will build on the parallel DSL Diderot that they have designed.  Preliminary experience with Diderot demonstrates that it is possible to write visualization and analysis algorithms in a very mathematical programming notation that has performant parallel implementations on a variety of parallel hardware.  The project will build on these preliminary results in several ways: the PIs will extend Diderot to support a wider range of data models and to provide a richer set of computation tools for analyzing data; the PIs will work on scaling Diderot to handle larger data sets and larger parallel platforms; and the PIs will explore techniques and tools to better support domain-specific software development. These research thrusts will involve close collaboration between the areas of programming languages and image analysis. The design of language features will be driven by the needs of image analysis algorithms, as well as the foundational principles of programming languages. The design and implementation of Diderot will be evaluated using the latest image-analysis algorithms from the literature, as well as being used to prototype new algorithms."
"1553281","CAREER: Knowledge-driven Analytics, Model Uncertainty, and Experiment Design","CCF","ALGORITHMIC FOUNDATIONS, COMPUTATIONAL BIOLOGY","09/01/2016","07/16/2018","Xiaoning Qian","TX","Texas A&M Engineering Experiment Station","Continuing grant","Mitra Basu","08/31/2021","$298,372.00","","xqian@ece.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7796, 7931","1045, 7931, 9251","$0.00","The advancement of high-throughput high-content data acquisition techniques has pushed modern scientific research from traditional reductionism to constructionism for studying complex systems with a strong data-driven focus. However, there are still significant issues in big-data analytics, especially, regarding the reproducibility of data-driven research findings. To enable analytic methods from traditional reductionist scientific research to constructionist research with the help of rich data, an integrative Bayesian framework is proposed for systems prediction and intervention in high-dimensional network-based systems, which will provide ways to translate the unprecedented amount of heterogeneous data into reproducible scientific knowledge in a cumulative manner with novel concepts of objective-based uncertainty quantification and optimal experiment design based on that. <br/><br/>The proposed knowledge-driven analytics has strong potential of transforming available diverse large-scale data for reproducible knowledge to drive life and materials science research. If successful, it can eventually lead to computational tools for more efficient experiment design to maximize the use of existing big data and speed up the process for effective disease therapeutics and new materials discovery. The interdisciplinary nature of this proposal promises to foster cross-fertilization of ideas between engineering, life science, and materials science through research and education. The broader impact of this proposal involves the integration of the proposed research with an educational plan: (1) to strengthen interactions with the collaborators in life and materials sciences. In addition to making all the tools and research outcomes from this project publicly available, the developed algorithms and tools, including necessary technical help, will be distributed to the collaborators for more effective collaboration; (2) to develop new courses interfacing engineering, mathematics, life and materials sciences and incorporate them into the engineering curriculum for education and research training of students at all levels, which will help the new generation of researchers to establish a broad and solid foundation for interdisciplinary research and prepare them with required skills to address real-world challenges. The courses will be available across campus to attract female and minority students who are interested in research in science and engineering; (3) to involve both undergraduate and graduate students in the research in this interdisciplinary field with the efforts to increasing the participation of underrepresented groups in science and engineering through the collaborations with the REU programs and other scholarship programs developed to increase diversity at Texas A&M University (TAMU). <br/><br/>The scientific focus of this proposal is solving mathematical and computational problems that exist in high-dimensional network-based systems prediction and intervention with uncertain models. The following open problems will be addressed: (1) to develop a network-based Bayesian framework and methods for systematic analysis of heterogeneous data sets; (2) to define novel objective-based uncertainty quantification for assessing model uncertainty and data significance to enable cumulative analytics to improve systems understanding and optimize future experiment design; (3) to derive optimal Bayesian experiment design based on uncertainty quantification for different operational objectives, which can lead to the maximal use of existing data and effective future experiment design and systems intervention; and (4) to apply the developed methodologies for understanding and treating specific diseases, for example, cancer and type 1 diabetes; as well as designing experiments for new materials discovery in an efficient way, in collaboration with the collaborators in life and materials sciences, which will translate the knowledge to practical applications. This project will lay out the foundation for translating existing data into systems understanding of life, disease, and man-made systems to gain deeper insights and direct them to desirable systems behavior to benefit human society."
"1817635","CAREER:Information Theoretic Methods for Private Information Retrieval and Search in Distributed Storage Systems","CCF","COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace","09/01/2017","05/15/2018","Salim El Rouayheb","NJ","Rutgers University New Brunswick","Continuing grant","Phillip Regalia","02/28/2022","$255,023.00","","salim.elrouayheb@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7797, 8060","1045, 7434, 7935, 9251","$0.00","The recent data revolution is driving many aspects of modern societal and economic progress. Most of this massive data is now stored in the cloud  to enable easy access for a myriad of users who wish to share information including, for example, photos, videos, publications, opinions, and scientific data. Unfortunately, this has come at the expense of the user's privacy whose online activity can be used to profile him/her, making large parts of the population an easy target for discrimination and possible persecution.  This research aims at addressing the privacy challenge of data in the cloud by focusing on the problem of Private Information Retrieval (PIR) and Search in distributed storage systems (DSSs). PIR schemes enable users to query data without revealing information about the queries and hence their personal preferences, tendencies, health, or other traits.<br/><br/>Classical information theoretic PIR schemes require data to be replicated, which is not a scalable solution given the exponential growth of data. This research aims at creating a unified framework for studying coding schemes that, in addition to providing data reliability, cater to the need of private queries. The focus of the proposed research is on (i) explicit constructions of codes and PIR schemes that address practical and important aspects of distributed storage, such as storage cost, network communication cost, disk reads, latency and computations; (ii) explicit constructions of codes and schemes for private keyword search; (iii) characterization of the fundamental limits and tradeoffs between reliability, privacy and the different system overheads; (iv) testing software implementations of the schemes on real genomic and social science data. The project also incorporates several educational and outreach efforts, including the development of new publicly accessible online content on information theory, security, and privacy in distributed storage systems as well as pre-college outreach through the Global Leaders Program at the PI's institution."
"1539622","CyberSEES: Type 2: Collaborative Research: Cyber-infrastructure and Technologies to Support Large-Scale Wildlife Monitoring and Research for Wildlife and Ecology Sustainability","CCF","CyberSEES","11/01/2015","05/22/2018","Roland Kays","NC","Friends of the North Carolina State Museum of Natural Sciences","Standard Grant","Rahul Shah","10/31/2019","$309,982.00","","rwkays@ncsu.edu","11 West Jones Stree","Raleigh","NC","276011029","9197079847","CSE","8211","8208, 8231","$0.00","Minimizing the impact of human actions on wildlife is a priority for conservation biology.  Distributed motion-sensitive cameras, or camera traps, are popular tools for monitoring wildlife populations.  Recent work has shown that camera trap surveys can be expanded to large scales by crowdsourcing through citizen science, producing big data sets needed to evaluate the effect of sustainability strategies on wildlife populations.  However, these large-scale surveys create millions of photographs that create new challenges for data processing and quality control. <br/><br/>This project seeks to develop advanced computing technologies for cloud-based large-scale data sensing, analysis, annotation, management, and preservation for wildlife and ecological sustainability to inform effective resource management, decision-making, and polices on human actions to protect wildlife and natural resources. Specifically, the project aims to: (1) explore a citizen scientist-based approach and system for large-scale sustainable data collection; (2) develop deep-learning based fine-grain animal species recognition from large data sets and automated content annotation; (3) study cloud-based computing with thin-client access and resource allocation for scalable deployment and easy access by citizen scientists; and (4) develop a comprehensive data annotation quality monitoring and control framework with tightly coupled computer annotation, crowd-sourcing, and expert review to ensure a high scientific standard of data quality. These tools will be integrated into the eMammal infrastructure to study three questions on wildlife sustainability: energy development, housing development, and wildlife harvest.  These tools will also be available to other wildlife researchers using the eMammal system, enabling an improved understanding how humans can live sustainably with wildlife. <br/><br/>Collaborative wildlife monitoring and tracking at large geographical and time scales will contribute to the understanding of complex dynamics of wildlife systems, and provide important scientific evidence for informed decisions and effective solutions to sustainability issues in wildlife environments. This project will provide unique, exciting, and interdisciplinary opportunities for mentoring graduate students and involving K-12 and undergraduate students into professionally guided research.  The citizen science approach used in this project should accommodate hundreds of students in research."
"1725743","SPX: CISIT: Computing In Situ and In Memory for Hierarchical Numerical Algorithms","CCF","SPX: Scalable Parallelism in t","10/01/2017","09/11/2017","George Biros","TX","University of Texas at Austin","Standard Grant","Marilyn McClure","09/30/2020","$800,000.00","Lizy John, Andreas Gerstlauer","gbiros@gmail.com","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","042Y","026Z, 9229","$0.00","High performance computing holds an enormous promise for revolutionizing science, technology, and everyday life through modeling and simulation, statistical inference, and artificial intelligence.  Despite the numerous successes in software and hardware technologies, energy efficiency barriers have become a major hurdle towards more powerful computers -- from mobile devices all the way to supercomputers. The originally natural separation between the memory subsystem and the central processing unit (CPU) of a computer has emerged as one the main reasons for energy inefficiency. Data movement between the memory and the CPU requires orders of magnitude more energy than the computations themselves. To address these challenges, this project will consider novel architectural design paradigms and algorithms that are aimed at blurring these traditional boundaries between separated memory and computation subsystems and, by distributing computations to be performed directly in the memory or as part of the memory data transfers, achieve order of magnitude gains inenergy efficiency and performance. This project will investigate such novel approaches in the context of a class of methods in computational mathematics, which appear at the core of many problems in computational science, large-scale data analytics, and machine learning.<br/><br/>Specifically, this project will focus on data-driven rather than compute-driven co-design of algorithms and architectures for the construction, approximation, and factorization of hierarchical matrices. The end-goal of the project is the design of a novel architecture, CISIT (for ``Computing In Situ and In Transit''), that specifically aims to address acceleration of both computation and data movement in the context of hierarchical matrices. CISIT will uniquely combine traditional general-purpose CPU and GPU cores with: (1) acceleration of core algorithmic primitives using custom hardware; (2) in-situ computing capabilities that will comprise both processing in or near main memory as well as computing within on-chip caches and memory close to the cores; (3) novel in-transit compute capabilities that will enable cutting down on and in many cases completely eliminating unnecessary roundtrip data transfers by processing of data transparently as it is transferred between main memory and local compute cores across the cache hierarchies. Upon success, CISIT will influence future architectural implementations.  Along with the research activities, an educational and dissemination program will be designed to communicate the results of this work to both students and researchers, as well as a more general audience of computational and application scientists."
"1539389","CyberSEES: Type 2: Collaborative Research: Cyber-infrastructure and Technologies to Support Large-Scale Wildlife Monitoring and Research for Wildlife and Ecology Sustainability","CCF","CyberSEES","11/01/2015","08/25/2015","Zhihai He","MO","University of Missouri-Columbia","Standard Grant","Rahul Shah","10/31/2019","$699,489.00","Tony Han, Joshua Millspaugh","hezhi@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","8211","8208, 8231, 9150","$0.00","Minimizing the impact of human actions on wildlife is a priority for conservation biology.  Distributed motion-sensitive cameras, or camera traps, are popular tools for monitoring wildlife populations.  Recent work has shown that camera trap surveys can be expanded to large scales by crowdsourcing through citizen science, producing big data sets needed to evaluate the effect of sustainability strategies on wildlife populations.  However, these large-scale surveys create millions of photographs that create new challenges for data processing and quality control. <br/><br/>This project seeks to develop advanced computing technologies for cloud-based large-scale data sensing, analysis, annotation, management, and preservation for wildlife and ecological sustainability to inform effective resource management, decision-making, and polices on human actions to protect wildlife and natural resources. Specifically, the project aims to: (1) explore a citizen scientist-based approach and system for large-scale sustainable data collection; (2) develop deep-learning based fine-grain animal species recognition from large data sets and automated content annotation; (3) study cloud-based computing with thin-client access and resource allocation for scalable deployment and easy access by citizen scientists; and (4) develop a comprehensive data annotation quality monitoring and control framework with tightly coupled computer annotation, crowd-sourcing, and expert review to ensure a high scientific standard of data quality. These tools will be integrated into the eMammal infrastructure to study three questions on wildlife sustainability: energy development, housing development, and wildlife harvest.  These tools will also be available to other wildlife researchers using the eMammal system, enabling an improved understanding how humans can live sustainably with wildlife. <br/><br/>Collaborative wildlife monitoring and tracking at large geographical and time scales will contribute to the understanding of complex dynamics of wildlife systems, and provide important scientific evidence for informed decisions and effective solutions to sustainability issues in wildlife environments. This project will provide unique, exciting, and interdisciplinary opportunities for mentoring graduate students and involving K-12 and undergraduate students into professionally guided research.  The citizen science approach used in this project should accommodate hundreds of students in research."
"1764077","SHF: Medium: Interactive Debegging for Big Data Analytics","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","05/18/2018","Miryung Kim","CA","University of California-Los Angeles","Continuing grant","Sol J. Greenspan","06/30/2022","$436,197.00","Tyson Condie","miryung@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7798","7798, 7944, 9102","$0.00","An abundance of data in science, engineering, national security, and health care has led to the emerging field of big data analytics.  To process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. While DISC systems help to address the scalability challenges of big data analytics, they also introduce an enormous challenge for data scientists in understanding and resolving errors. This project addresses the severe lack of debugging support in DISC systems today, which makes it difficult for data scientists to understand their applications, determine the causes of identified errors, and ensure that such errors are properly repaired. <br/> <br/>The research provides two kinds of debugging support for big data processing programs in modern DISC systems like Apache Spark: new interactive, real-time debugging primitives for large-scale distributed processing and tool-assisted fault-localization services for big data. Technical approaches include a new data provenance technique for providing fine-grained visibility into large-scale distributed data processing and runtime optimizations for iterative development and debugging workloads. Tool-assisted fault localization services leverage these underlying provenance and optimization techniques to pinpoint and characterize the root causes of errors efficiently. Big data analytics is increasingly important in the 21st century, where daily lives leave behind a detailed digital record and decision-makers of all kinds, from companies to government agencies, would like to base their actions on data. The research contributes to improving productivity and correctness of big data applications, which is crucial for many disciplines that distill terabytes of low-value data into high-value insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813081","SHF:Small: Collaborative Research: Understanding, Modeling, and System Support for HPC Data Reduction","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/22/2018","Xubin He","PA","Temple University","Standard Grant","Almadena Y. Chtchelkanova","06/30/2021","$199,898.00","","xubin.he@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7798","7923, 7942","$0.00","High-performance computing (HPC) enables high fidelity science missions that capture microscopic phenomena that were impossible to study in the past. In order to allow science missions to be accomplished in a timely manner, it is critical to manage the massive datasets that HPC generates in efficient way so that the time to knowledge can be shortened. This project aims to understand the role and usage of data reduction in large computational applications. Research and educational opportunities are provided to train a new generation of computer scientists and engineers, particularly those under-represented groups, to ensure the U.S. competitiveness in high-performance computing. <br/> <br/>The goal of this project is to address a number of critical gaps in using data reduction for HPC-based science missions. In particular, 1) the impact of reduction error on scientific discovery is mathematically and experimentally studied; 2) analytical models are formulated to estimate the reduction performance, without forcing users to compress the full data. For data-intensive applications, having this capability is important so that domain scientists do not have to go through the cumbersome trial-and-error process to figure out what reduction can offer; 3) the project provides potentially more efficient data analysis and reduction capabilities for exascale computing. The integrated research activities in this NSF project will significantly improve the understanding and usage of data reduction on future systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812861","SHF:Small: Collaborative Research: Understanding, Modeling, and System Support for HPC Data Reduction","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/22/2018","Qing Liu","NJ","New Jersey Institute of Technology","Standard Grant","Almadena Y. Chtchelkanova","06/30/2021","$259,024.00","","qliu@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","7798","7923, 7942","$0.00","High-performance computing (HPC) enables high fidelity science missions that capture microscopic phenomena that were impossible to study in the past. In order to allow science missions to be accomplished in a timely manner, it is critical to manage the massive datasets that HPC generates in efficient way so that the time to knowledge can be shortened. This project aims to understand the role and usage of data reduction in large computational applications. Research and educational opportunities are provided to train a new generation of computer scientists and engineers, particularly those under-represented groups, to ensure the U.S. competitiveness in high-performance computing. <br/> <br/>The goal of this project is to address a number of critical gaps in using data reduction for HPC-based science missions. In particular, 1) the impact of reduction error on scientific discovery is mathematically and experimentally studied; 2) analytical models are formulated to estimate the reduction performance, without forcing users to compress the full data. For data-intensive applications, having this capability is important so that domain scientists do not have to go through the cumbersome trial-and-error process to figure out what reduction can offer; 3) the project provides potentially more efficient data analysis and reduction capabilities for exascale computing. The integrated research activities in this NSF project will significantly improve the understanding and usage of data reduction on future systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757017","REU Site: EXERCISE - Explore Emerging Computing in Science and Engineering","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2018","01/26/2018","Enyue Lu","MD","Salisbury University","Standard Grant","Rahul Shah","01/31/2021","$359,995.00","","ealu@salisbury.edu","1101 Camden Avenue","Salisbury","MD","218016837","4105462399","CSE","1139","9250","$0.00","The project is a renewal of the Research Experiences for Undergraduates (REU) EXERCISE (Explore Emerging Computing in Science and Engineering) site at Salisbury University (SU) for the next three years. EXERCISE is an interdisciplinary project that explores emerging paradigms in parallel computing with data and compute intensive applications in science and engineering. The goal of the project is to offer student participants, particularly from primarily undergraduate institutions (PUIs), a valuable research experience in parallel computing. The project will promote ""parallel thinking"", an important computational thinking skill guiding current generation students into the twenty-first century computing era. The site will prioritize recruiting under-represented students and females, and attract students from local historically black college and universities (HBCUs), PUIs, and community colleges on Maryland's Eastern Shore into computational science and engineering majors and the general Science, Technology, Engineering, and Mathematics (STEM) fields. The Principal Investigator, together with faculty mentors, will supervise a 10-week REU program that gives a diverse cohort of students a taste of computational thinking in the domain of parallel computing and also an understanding of the graduate school experience. The host institution SU will collaborate with the University of Maryland Eastern Shore, an HBCU, and the University of Maryland College Park for multi-disciplinary faculty expertise and diverse summer activities including field trips, social activities, high school outreach, and graduate school application information sessions.<br/><br/>Processing complex information and large data in conventional von Neumann computer architectures is becoming increasingly difficult. Computers are undertaking a fundamental turn toward concurrency architectures such as hyperthreading, multi-core, and many-core architectures. Emerging parallel and distributed computing paradigms adapted to these concurrent architectures have begun to demonstrate the power of solving problems with large datasets and high computational complexity in a wide range of applications. However, there are fundamental difficulties in program semantics related to process interleaving: a parallel program can yield inconsistent answers, or even crash, due to unpredictable interactions between simultaneous tasks. Secondly, communication, memory access, and I/O overhead may result in run-time delays. Finally, it is difficult to ensure that programs consume resources in a manner that simultaneously achieves efficiency and meets performance goals. The REU Site will focus on four aspects of parallel computing, namely: algorithms, software, architecture and applications to address these parallel computing challenges. Students will work with faculty mentors in completing cutting-edge research projects to tackle data and compute intensive applications that emphasize the above four aspects. By the end of program, students will acquire valuable skills, gain a broader and deeper understanding of research, and develop greater confidence in their abilities. In particular, they will be exposed to emerging paradigms in parallel computing such as Map-Reduce and Graphical Processing Unit computing, and will have opportunities to explore concurrent software and multiprocessor architectures, and design efficient parallel algorithms, and to tackle data and compute intensive problems in computer and social networks, image and natural language processing, pattern recognition and machine learning."
"1657333","CRII: SHF: Improving Programmability of GPGPU/NVRAM Integrated Systems with Holistic Architectural Support","CCF","CRII CISE Research Initiation","02/01/2017","01/31/2017","Xuehai Qian","CA","University of Southern California","Standard Grant","Yuanyuan Yang","01/31/2020","$175,000.00","","xuehai.qian@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","026Y","7941, 8228","$0.00","In the era of big data, the industry faces growing demand for higher computing power and large-capacity high performance storage. GPGPU and NVRAM are two prominent technologies that will play the key role in the ""Big Data revolution"". This project, which holistically improves the programmability of GPGPU/NVRAM integrated systems, tackles the ""programmability bottleneck"" faced in GPGPU and NVRAM. It will make it easier to develop correct applications in GPGPU and NVRAM with high performance. As a result, the project will enforce the desire of applying GPGPUs and NVRAM into a wide-range of HPC and big data applications which could then gain hundreds times speedup while ensuring recoverability. Overall, the outcomes of this project will help ensure the sustainable performance to support the supercomputing/big data processing in science and engineering (e.g. finance, medical, biology, petroleum, aerospace, and geology). This project will also contribute to society through engaging high-school and undergraduate students from minority-serving institutions into research, attracting women and under-represented groups into graduate education, expanding the computer engineering curriculum with GPGPU/NVRAM architectures, disseminating research infrastructure for education and training, and collaborating with the industry.<br/><br/>This research investigates synergetic approaches and techniques to holistically improve the programmability of GPGPU/NVRAM integrated systems with the following techniques: (1) Timestamp-Based GPU Coherence Protocol. It avoids storage overhead by not storing sharing states (e.g. Shared, Modified, Exclusive, etc.) and the list of sharers. It reduces the traffic overhead by not sending explicit invalidation messages.  (2) Integration of Persistency and the Scoped-Synchronization. This research aims to study the new notion of Persistent Scope (PS) , which incorporates the necessary persistency semantics into the existing scoped-synchronization in GPGPU programming models.  Efficient architecture design that fully decouples consistency and persistency will be explored. (3) Data Sharing-Aware CTA Scheduler and Cache Management. This research plans to investigate  a sharing-aware CTA scheduler that attempts to assign CTAs with data sharing to the same SM to improve temporal and spatial locality."
"1522054","Collaborative Research: CompSustNet: Expanding the Horizons of Computational Sustainability","CCF","INFO INTEGRATION & INFORMATICS, EXPERIMENTAL EXPEDITIONS","12/15/2015","02/20/2019","Carla Gomes","NY","Cornell University","Continuing grant","Sylvia Spengler","11/30/2020","$6,474,276.00","Jon Conrad, John Hopcroft, David Shmoys, Bart Selman","gomes@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7364, 7723","7364, 7556, 7723, 9102","$0.00","Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.<br/><br/>Computational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.<br/><br/>CompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. <br/><br/>Advancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."
"1521675","Collaborative Research: CompSustNet: Expanding the Horizons of Computational Sustainability","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","02/12/2019","Warren Powell","NJ","Princeton University","Continuing grant","Sylvia Spengler","11/30/2020","$280,000.00","","powell@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7723","7723","$0.00","Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.<br/><br/>Computational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.<br/><br/>CompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. <br/><br/>Advancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."
"1521672","Collaborative Research: CompSustNet: Expanding the Horizons of Computational Sustainability","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","02/12/2019","Gerald Roth","TN","Vanderbilt University","Continuing grant","Sylvia Spengler","11/30/2020","$156,476.00","","j.roth@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","7723","7723, 9150","$0.00","Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.<br/><br/>Computational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.<br/><br/>CompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. <br/><br/>Advancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."
"1533762","XPS: EXPL: DSD: A Memristive Hardware Platform for Large Scale Combinatorial Optimization","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/25/2015","Engin Ipek","NY","University of Rochester","Standard Grant","Yuanyuan Yang","08/31/2019","$298,283.00","","ipek@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","8283","","$0.00","Mathematical optimization plays a vital role in virtually every scientific discipline. A 2013 report by the U.S. National Academy of Sciences (NAS) identifies optimization as one of the seven giants of statistical data analysis on massive data, while the U.S. Department of Energy (DOE) Office of Science reports that mathematical optimization will increasingly be used in the exascale era. Solving large-scale optimization problems on massive datasets can require weeks or months of computation time on modern supercomputers, which regrettably deliver only a small fraction of the peak performance due to significant data movement overheads. Hardware and software innovations that can improve energy efficiency by orders of magnitude are needed before exascale platforms for mathematical optimization can become practical.<br/><br/><br/>This project embodies a radically different vision of the future, one where large scale combinatorial optimization problems are mapped onto a memory-centric, non-Von Neuman compute substrate and solved in situ within the memory cells, with orders of magnitude greater performance and energy efficiency than contemporary supercomputers. Recent developments in the resistive random access memory (RRAM) technology are leveraged to build an extremely low power, fast memory substrate for accelerating combinatorial optimization algorithms. RRAM is a memristive, non-volatile memory technology that provides FLASH-like density and DRAM-like read speeds. The project exploits the electrical properties of RRAM in combination with CMOS transistors to enable in situ optimization within the memory cells, thereby eliminating data movement between the memory arrays and the computational units, reducing the energy, and significantly increasing the performance. Novel algorithms will be developed to map problems from different scientific and engineering domains onto the proposed memristive hardware substrate. Software modules and libraries for memory allocation and partitioning; dynamic resource management; and hardware-software co-design will be developed to give the user control of the optimization process at runtime. At the hardware level, the accelerator employs a versatile organization of data arrays constructed from novel RRAM cells, capable not only of storing the data, but also of performing in situ computation on that data. The proposed research holds the promise of bringing about a transformative change in the performance and energy efficiency of exascale optimization frameworks, with tremendous positive fallout to science, technology, and society as a whole. Architecture and software innovations will be disseminated to the broader research community through published papers, as well as tutorials on the proposed framework and mapping algorithms. The educational component of the project involves training both graduate and undergraduate students in computer architecture, as well as a memory systems course that integrates the RRAM technology into the syllabus. The PI is also personally involved in local programs promoting the participation of women and underrepresented minorities in computer science and engineering, and has an ongoing effort to increase the enrollment of local minorities in University of Rochester's CS and ECE programs."
"1521687","Collaborative Research: CompSustNet: Expanding the Horizons of Computational Sustainability","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","01/31/2018","Thomas Dietterich","OR","Oregon State University","Continuing grant","Sylvia J. Spengler","11/30/2020","$822,375.00","John Selker, Alan Fern, Xiaoli Fern, Weng-Keen Wong","tgd@cs.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7723","7723","$0.00","Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.<br/><br/>Computational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.<br/><br/>CompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. <br/><br/>Advancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."
"1564000","AF: Medium: Dropping Convexity: New Algorithms, Statistical Guarantees and Scalable Software for Non-convex Matrix Estimation","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","07/25/2017","Sujay Sanghavi","TX","University of Texas at Austin","Continuing grant","Balasubramanian Kalyanasundaram","08/31/2020","$654,723.00","Inderjit Dhillon","sanghavi@mail.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7796","7924, 7933","$0.00","An image from your camera is a matrix of numbers, but most matrices of numbers would not look like an image -- the matrix of numbers in an image reflect structure from the scene.  Many applications of data analysis across science, engineering, and business can be viewed as taking a matrix of observations and fitting low-rank or otherwise structured matrices to explain their relationships. Image and video analysis is not the only example; the problem arises in structural analysis of social networks, divining user preferences for new products and services, and many other analysis tasks. <br/><br/>As the scale and dimensionality of these problems increases, the data analyst is faced with a gap between rigor and scale: theoretically sound algorithms often have requirements (e.g. repeated/random access to data) that are feasible only on medium-scale datasets, and even then may not provide answers in ""interactive time"" (i.e. smallish time scales required for a human interactively analyzing data). Thus practice has turned towards methods that lack rigorous guarantees, but that are scalable and have been observed to provide decent approximation.<br/><br/> This project aims to narrow this gap by two technical observations: <br/>(a) Recognizing that fast matrix inference necessitates non-convex algorithms, it focuses on developing a rigorous analysis of the same, and  <br/>(b) by explicitly incorporating big-data architectures (out of core, and distributed multicore) in the algorithm design and statistical analysis stage itself. it focuses on several specific tasks, including pass-efficient low-rank approximation, minimizing general convex functions over the non-convex set of low-rank matrices, robust matrix estimation, and non-linear and kernel matrix settings. <br/><br/><br/>The project trains graduate students in the mathematical and computational development important for data analysis. The promise of big data can only be realized by scaling infrastructure with data to continue to provide statistically meaningful insights; this project aims to realize this promise for a large suite of matrix estimation problems."
"1421231","AF:  Small:  Towards better geometric algorithms: Summarizing, partitioning and shrinking data","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","06/11/2014","Sariel Har-Peled","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rahul Shah","08/31/2019","$490,364.00","","sariel@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7923, 7929","$0.00","New (and not so new) technologies like GPS, LIDAR, smart-phones and apps are now used to collect vast amounts of geometric and geographic data.  In many such cases the data is too large to be handled by a single computer, and it needs to be broken up into small chunks, so that it can be handled by a cluster/cloud of computers.  This award funds research to develop efficient algorithms for manipulating and summarizing such data. In particular, this award concentrates on the issue of how to partition such data efficiently in a balanced way, so that it can be efficiently processed.<br/><br/>The algorithms and insights obtained from the technical work will benefit Computer Science and related disciplines where geometric data and algorithms are widely used. The PI hopes to broaden the scope of Computer Science (and Computational Geometry) by introducing new techniques, that would lead to faster and better algorithms, and potentially new applications of geometric data. <br/><br/>The project will support and train at least two new PhD students in Computer Science at UIUC. The PI is committed to popularizing ideas and techniques that would be investigated by this proposal, by giving courses, publishing the research, potentially writing a new book on the topic, and use less convectional new tools to disseminate the research using blogs, social media, and online videos."
"1659144","REU Site: Scientific Computing for Structure in Big or Complex Datasets","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/01/2017","02/13/2017","Burton Rosenberg","FL","University of Miami","Standard Grant","Rahul Shah","02/29/2020","$360,000.00","","burt@cs.miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","1139","9250","$0.00","This project provides for our future generation of effectively trained research scientists. The project anticipates future directions in science by training young scientists in the theoretical and practical use of computation to find, understand, and exploit structure in large or complex data sets. Large and complex data sets are ubiquitous in many areas of science, business and humanities. The project aims to broaden participation in scientific research by locating and encouraging talented students at institutions with fewer research offerings than has the REU site. Along with encouraging a career in science, the project aims to further engage each participant in a lifelong dialog with science and scientific research.<br/><br/>Training, mentorship, presentation, and peer engagement are all key elements addressed by this project to encourage our talented undergraduates to become research scientists. The project provides full scholarships to undergraduates to work with mentors in a ten week summer session on a project from the site's portfolio of projects. The program begins with a science boot camp. Throughout the experience the participants attend lecture series and engage with their cohort of REU participants, further facilitating growth as career scientists. At the conclusion, the participants present their research to their peers and the research community at the site, with some projects selected to be presented at a national conference. <br/><br/>The portfolio of mentored projects share the theme of computation to find structure in large or complex data sets. Projects feature computation either for the exploration or manipulation of data, such as for visualization or high performance GPU computation, or for the theoretical framework of computation, such as for understanding brain processes, deep learning, and neural nets. Because of the cross-cutting nature of computational science, the projects come from diverse fields with mentors from diverse departments, including Computer Science, Chemistry and Psychology, the Center for Computational Sciences, and the medical school."
"1652862","CAREER: Efficient Algorithms for Learning and Testing Structured Probabilistic Models","CCF","ALGORITHMIC FOUNDATIONS","02/01/2017","03/26/2019","Ilias Diakonikolas","CA","University of Southern California","Continuing grant","Tracy Kimbrel","01/31/2022","$315,290.00","","diakonik@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7796","1045, 7926","$0.00","In recent years, the amount of available data in science and technology has exploded and is currently expanding at an unprecedented rate. The general task of making accurate inferences on large and complex datasets has become a major bottleneck across various disciplines. A natural formalization of such inference tasks involves  viewing the data as random samples drawn from a probabilistic model -- a model that we believe describes the process generating the data. The overarching goal of this project is to obtain a refined understanding of these inference tasks from both statistical and computational perspectives. The questions addressed in this project arise from pressing challenges faced in modern data analysis. A crucial component of the project involves fostering collaboration between different communities. Furthermore, the PI will mentor high-school and undergraduate students, and design several new theory courses integrating research and teaching at the undergraduate and graduate levels.<br/><br/>The PI will investigate several fundamental algorithmic questions in unsupervised learning and testing for which there is an alarming gap in our current understanding. These include designing efficient algorithms that are stable in the presence of deviations from the assumed model, circumventing the curse of dimensionality in distribution learning, and testing high-dimensional probabilistic models. This set of directions could lead to new algorithmic and probabilistic techniques, and offer insights into the interplay between structure and efficiency in unsupervised estimation. This research ties into a broader range of work across computer science, probability, statistics, and information theory."
"0939370","Emerging Frontiers of Science of Information","CCF","SCI & TECH  CTRS (INTEG PTRS), COMM & INFORMATION FOUNDATIONS, STCs - 2010 Class","08/01/2010","09/15/2018","Wojciech Szpankowski","IN","Purdue University","Cooperative Agreement","Phillip Regalia","08/31/2019","$43,801,079.00","Sergio Verdu, Bin Yu, Andrea Goldsmith, Madhu Sudan, Peter Shor","spa@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1297, 7797, 8005","019Z, 7936, 9171, 9218, 9251, HPCC","$0.00","Center Name: Center for Science of Information <br/>Center Director: W. Szpankowski, <br/>Lead Institution: Purdue University <br/><br/>The foundations of modern communications and ancillary trillion plus dollar economic windfall were laid in 1948 by Claude Shannon who introduced a general mathematical theory of the inherent information content in data and its reliable communication in the presence of noise. While Shannon?s Theory has had a profound impact, its application beyond storage and point-to-point communication, e.g., to the Internet, poses fundamental challenges, among the most vexing facing today?s scientists and engineers. The overarching vision of the proposed Center for Science of Information is to develop a new science of information that incorporates common features generally associated with data/information, such as space, time, structure, semantics and context that are not addressed by Shannon?s Theory. The realization of this vision requires a center-level environment that can focus the efforts of a sizeable (and diverse) group of researchers, for a protracted period of time, on these critically important challenges, which could have far reaching societal impact and enormous economic ramifications. Under the umbrella of this overarching vision, the proposed center will explore the following fundamental issues: (i) modeling complex systems and development of analytical techniques for information flow (e.g., understanding Darwinian selection); (ii) quantification and extraction of informative substructures in complex systems (e.g., discovering functionally relevant structures in gene regulatory networks or modular entities in social networks); (iii) understanding of spatio-temporal coding used to exchange information through timing and localization in complex systems (e.g., building more efficient ad hoc networks and understanding neuronal activity); (iv) data-driven knowledge discovery based on formal information-theoretic measures (e.g., finding semantically relevant information in unstructured repositories); (v) steganography, data obfuscation and hiding as mechanisms for robustness (e.g., developing secure systems for monitoring and surveillance); and (vi) discovering principles of redundancy and fault tolerance in diverse natural systems (e.g., understanding the interplay between erasure coding and distributed system design). <br/><br/>The intellectual merits of the proposed center include the community of students and academic and industrial scholars it seeks to sustain, the theoretical advances it hopes to achieve, and the novel insights and tools it hopes to provide to explicate a myriad of diverse systems, ranging from the life sciences through business applications. The broader impacts of this Center extend beyond the potential scientific, societal and economic ramifications and include the creation of an ?active and thriving community of students and scholars? who will train the next generation of scientists and engineers, enlighten the public, and ultimately pave the way for the next information revolution. The Center team is composed of over 40 investigators, many having already made significant accomplishments in multiple research areas relevant to the Science of Information. The Center team is a very diverse group: it has a mix of junior and senior researchers, including several members of underrepresented groups. They bring expertise in all essential areas of research, including Computer Science, Chemistry, Economics, Statistics, Environmental Science, Information Theory, Life Sciences, and Physics. The institutional partners include nine premier institutions (Purdue, Bryn Mawr, Howard, MIT, Princeton, Stanford, UC Berkeley, UCSD, and UIUC), two of which have significant underrepresented student populations. The academic institutions are complemented by the Center?s industrial partners (Amgen, Bell Labs, Configuersoft, Google, HP, Lilly, NEC, Qualcomm, and Yahoo) and by world-renowned researchers at international institutions."
"1845146","CAREER:  Pushing the Theoretical Limits of Scalable Distributed Algorithms","CCF","ALGORITHMIC FOUNDATIONS","07/01/2019","02/05/2019","Benjamin Moseley","PA","Carnegie-Mellon University","Continuing grant","Rahul Shah","06/30/2024","$103,499.00","","moseleyb85@gmail.com","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","1045, 7934","$0.00","Science and engineering are becoming more reliant on analyzing data sets that have massive size. Processing these data sets typically requires using many machines, such as on the cloud, meaning that algorithms and software for data processing need to be redesigned to efficiently use a large number of machines. This project will give algorithmic techniques for distributed computing models (aka frameworks) such as Spark. Such a framework enables programmers to easily deploy algorithms on tens to thousands of machines as long as the algorithm fits into the computational restrictions of the framework. The algorithmic primitives developed will be tools that algorithm designers and programmers can leverage to analyze large amounts of data on many machines. This will impact industry, science and the economy that is increasingly reliant on large data analysis. Research outcomes will be integrated with education by including the latest research on data analytics in the undergraduate and master of science in business analytics programs. <br/><br/>Massively distributed frameworks such as Spark and MapReduce are a key technology for processing large data sets. These systems have traditionally been used to solve relatively simple problems. Recent investigation has shown they are potentially useful for a richer class of applications. With this potential as a proof-of-concept, this project will discover algorithmic techniques tailored to these frameworks to unlock their underlying power and broaden their applicability. A recently developed theoretical model of computation will be used to drive the development of algorithmic techniques designed to leverage the unique features of the frameworks. The new algorithms and techniques will be used to offer scalable solutions for key problems arising in graph processing, data mining, and bioinformatics.  Specifically, the project will develop algorithms to compute shortest paths on massive graphs, algorithms for local alignment of biological sequences and some of the first provably scalable algorithms for hierarchical clustering. Achieving these goals can influence practice and theoretical research similarly to successes in other areas such as streaming algorithms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1539582","CyberSEES: Type2: Collaborative Research: SmartFarm - Research and Education for Sustainable Agriculture Practices","CCF","CyberSEES","10/01/2015","08/24/2015","Balaji Sethuramasamyraja","CA","California State University-Fresno Foundation","Standard Grant","Rahul Shah","09/30/2019","$149,898.00","Bruce Roberts","balajis@csufresno.edu","5241 N. Maple Ave","Fresno","CA","937400001","5592780840","CSE","8211","8208, 8231","$0.00","Ecological sustainability depends critically on the ability of world food production to manage increasingly limited natural resources (such as arable land and water) with new techniques that both enhance environmental stewardship and increase farm productivity.  To make agriculture more productive and the productivity gains sustainable, growers are increasingly turning to environmental sensor measurement, data acquisition, and data analysis. To date, however, these tools have failed to achieve widespread use by smallholder agricultural concerns.  Importantly, individual growers and ranchers are underserved by many recent advances in the commercial and research sectors that make data analytics consumable as simple ""black box"" end-user products. Current decision support offerings for these constituencies are variously limited, proprietary, complex, costly, require that growers relinquish control over their data,  or are not widely available.  This project therefore investigates a comprehensive research, educational, and outreach program called SmartFarm, which couples new scientific research in computer science, agronomics, and precision agriculture with novel outreach and educational pathways that enable our youth and communities to transform and ensure agriculture sustainability. The research will bring new computing technologies to growers that are easy to use,  facilitate data privacy and control, and enable farm-focused, data-driven analysis and decision support that helps growers increase their yields sustainably.  In addition, the educational and outreach plans will introduce  technically adept youth, who are increasingly ecologically conscious, to the challenges and rewards associated with computer science and precision agricultural science so that they are adequately prepared and inspired to participate in the global challenge of increasing sustainable food production.<br/><br/>To address the problem of sustainable food security and food safety, this project investigates unifying cyberinfrastructure and agriculture analytics to enable precision, agronomics-driven farming by individual growers unlike what is available today.  The proposed system, called SmartFarm, integrates disparate environmental sensor technologies into a customized, open-source, cloud-based data appliance with new analytics that provide growers with a secure, easy to use, low-cost data analysis and decision support system.  Using open-source private cloud platforms, this data appliance can be hosted at a range of scales including personal, private clouds on-farm, large-scale public clouds, or in some combination of the two.  The research program will facilitate new knowledge in: (i) multi-analytic agricultural applications for farm control, dynamic decision support, and emergency response; (ii) self-managing, extensible private cloud systems; (iii) robust sensing and data acquisition techniques, application programming interfaces, and processing engines tailored to the needs of farmers and ranchers; and (iv) private and hybrid cloud software architectures for precision farming that are code and data compatible with public cloud industry standards.  The outreach and education efforts will expose students to cross-disciplinary research and educational activities that train them as new agronomists in new technologists in precision agricultural."
"1839267","TRIPODS+X:RES:Collaborative Research: Improving Templated Microstructures via Topological Data Analysis","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, SPECIAL INITIATIVES, DMR SHORT TERM SUPPORT","10/01/2018","09/10/2018","Dunbar Birnie","NJ","Rutgers University New Brunswick","Standard Grant","Christopher W. Stark","09/30/2021","$300,000.00","Deborah Silver","dbirnie@rci.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","041Y, 1253, 1642, 1712","047Z, 062Z, 8037","$0.00","The importance of microstructures in Material Science is well  recognized. Their local and global geometry influence the functional  behaviors of the materials being designed in a major way.  Therefore,  methodologies for their controlled manufacturing have always been a  focus of intense research. Now, with continuing advancements in  characterization of materials at higher resolution and faster time  scales there is intensified need for data driven digital simulation  and analysis of structure. This project focuses on leveraging the new  area of topological data analysis in advancing the design of  templated microstructure designs through a collaboration between  material and data scientists at Rutgers University and data  scientists at the TRIPODS center at Ohio State University. Templating  is the ideal topical area for this collaboration because it so  definitively directs shape development during processing and can  benefit greatly from deeper topological and statistical analytics.  The researchers will develop a topology-related synergy between Materials  Science, Computer Science, and Statistics that will enable improved  processing of materials using templating. The geometrical and  topological advances developed in this program are expected to also  be extensible to other areas of materials processing, each of which  has unique shape novelty, alignment effects, or texture development.  The project's work could also benefit a range of similar application fields  such as medical image analysis, computational neuroanatomy,  geographic information systems, and engineering designs. Indeed,  collaborations to apply geometric/topological methods to some of  these other application fields are already underway at the TRIPODS  center at OSU and could benefit from close collaboration with this  Materials-focused program as it develops.<br/><br/>The proposed research involves concepts from mathematical areas  of algebraic topology and geometry, applied statistics, and  computational areas of algorithms and graph theory. These will be  applied to materials microstructures created by templating to help  understand topological interconnections, shapes, and dynamics, which  would be of benefit to functional improvements in device operation.  Research in topological data analysis has brought forth the need to  investigate topological concepts in the presence of finite data,  approximations, and noise, constraints that are always encountered in  real materials characterization. Geometric and topological  computation with intentionally structured materials will yield big  and diverse data that can influence and improve future material and  device fabrication efforts. These new data methods will be of  interest to the topological and statistical communities as well as  open up new avenues for predicting and intentionally creating  structures with enhanced functionality.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1539570","CyberSEES:Type2:Collaborative Research: SmartFarm - Research and Education for Sustainable Agriculture Practices","CCF","CyberSEES","10/01/2015","08/24/2015","Bo Liu","CA","California Polytechnic State University Foundation","Standard Grant","Rahul Shah","09/30/2019","$150,000.00","","bliu17@calpoly.edu","One Grand Ave","San Luis Obispo","CA","934070830","8057562982","CSE","8211","8208, 8231","$0.00","Ecological sustainability depends critically on the ability of world food production to manage increasingly limited natural resources (such as arable land and water) with new techniques that both enhance environmental stewardship and increase farm productivity.  To make agriculture more productive and the productivity gains sustainable, growers are increasingly turning to environmental sensor measurement, data acquisition, and data analysis. To date, however, these tools have failed to achieve widespread use by smallholder agricultural concerns.  Importantly, individual growers and ranchers are underserved by many recent advances in the commercial and research sectors that make data analytics consumable as simple ""black box"" end-user products. Current decision support offerings for these constituencies are variously limited, proprietary, complex, costly, require that growers relinquish control over their data,  or are not widely available.  This project therefore investigates a comprehensive research, educational, and outreach program called SmartFarm, which couples new scientific research in computer science, agronomics, and precision agriculture with novel outreach and educational pathways that enable our youth and communities to transform and ensure agriculture sustainability. The research will bring new computing technologies to growers that are easy to use,  facilitate data privacy and control, and enable farm-focused, data-driven analysis and decision support that helps growers increase their yields sustainably.  In addition, the educational and outreach plans will introduce  technically adept youth, who are increasingly ecologically conscious, to the challenges and rewards associated with computer science and precision agricultural science so that they are adequately prepared and inspired to participate in the global challenge of increasing sustainable food production.<br/><br/>To address the problem of sustainable food security and food safety, this project investigates unifying cyberinfrastructure and agriculture analytics to enable precision, agronomics-driven farming by individual growers unlike what is available today.  The proposed system, called SmartFarm, integrates disparate environmental sensor technologies into a customized, open-source, cloud-based data appliance with new analytics that provide growers with a secure, easy to use, low-cost data analysis and decision support system.  Using open-source private cloud platforms, this data appliance can be hosted at a range of scales including personal, private clouds on-farm, large-scale public clouds, or in some combination of the two.  The research program will facilitate new knowledge in: (i) multi-analytic agricultural applications for farm control, dynamic decision support, and emergency response; (ii) self-managing, extensible private cloud systems; (iii) robust sensing and data acquisition techniques, application programming interfaces, and processing engines tailored to the needs of farmers and ranchers; and (iv) private and hybrid cloud software architectures for precision farming that are code and data compatible with public cloud industry standards.  The outreach and education efforts will expose students to cross-disciplinary research and educational activities that train them as new agronomists in new technologists in precision agricultural."
"1659833","REU Site:  Summer Undergraduate Program in Engineering Research at Berkeley (SUPERB):  Collecting and Using Big Data for the Public Good","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2017","01/17/2017","James Demmel","CA","University of California-Berkeley","Standard Grant","Rahul Shah","01/31/2020","$259,200.00","","demmel@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","1139","9250","$0.00","It is evident to all of us that the widespread and growing availability of vast amounts of real-time data presents both a tremendous opportunity to improve our understanding of the world and make better automated decisions, as well as a great technical challenge to collect, communicate and process this data efficiently and reliably. Leading researchers in Electrical Engineering and Computer Science will mentor undergraduate students on their proposed REU projects. Potential impacts range from more efficient sensor networks, to improved wireless access to the data they produce, to better human robot interaction, to better analysis of medical data from microscopes, and even helping prevent nuclear war. The project seeks to address a multitude of societal problems that can be examined by collecting and using big data while inspiring students to dedicate themselves to this work. The project is committed to exposing a diverse group of undergraduate researchers that will expand the impact of this project and the engineering research pipeline. At the conclusion of the program, participants will be proficient in using big data to solve societal problems that have direct impacts on their communities. <br/><br/>The goal of the Summer Undergraduate Engineering Research project in the Electrical Engineering and Computer Sciences Department is to prepare and motivate a group of diverse competitive candidates for graduate student.  The focus of the REU site is electrical engineering and computer science to support collecting and using big date for the public good.  Students spend nine weeks during the summer working on high caliber research projects addressing technical challenges arising in collecting, communicating and processing the vast amounts of data becoming available both efficiently and reliably. This project covers the entire range of challenges and opportunities, from better sensor networks to collect this data more efficiently; to improved wireless resource management to move the data; to machine learning techniques for processing the images, including from microscopes, that make up much of the new data; to designing robots that can learn to interact better with humans based on the data they collect; to better enforcement of the Comprehensive Nuclear Test Ban Treaty by analyzing seismic data used to detect underground nuclear tests. The project will have research contributions in areas such as communications and networking, human computer interaction, machine learning, robotics, scientific computing and visualization."
"1839252","TRIPODS+X:RES:Collaborative Research: Improving Templated  Microstructures via Topological Data Analysis","CCF","TRIPODS Transdisciplinary Rese, SPECIAL INITIATIVES, DMR SHORT TERM SUPPORT","10/01/2018","09/10/2018","Tamal Dey","OH","Ohio State University","Standard Grant","Christopher W. Stark","09/30/2021","$300,000.00","Sebastian Kurtek","tamaldey@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","041Y, 1642, 1712","047Z, 062Z","$0.00","The importance of microstructures in Material Science is well  recognized. Their local and global geometry influence the functional  behaviors of the materials being designed in a major way.  Therefore,  methodologies for their controlled manufacturing have always been a  focus of intense research. Now, with continuing advancements in  characterization of materials at higher resolution and faster time  scales there is intensified need for data driven digital simulation  and analysis of structure. This project focuses on leveraging the new  area of topological data analysis in advancing the design of  templated microstructure designs through a collaboration between  material and data scientists at Rutgers University and data  scientists at the TRIPODS center at Ohio State University. Templating  is the ideal topical area for this collaboration because it so  definitively directs shape development during processing and can  benefit greatly from deeper topological and statistical analytics.  The researchers will develop a topology-related synergy between Materials  Science, Computer Science, and Statistics that will enable improved  processing of materials using templating. The geometrical and  topological advances developed in this program are expected to also  be extensible to other areas of materials processing, each of which  has unique shape novelty, alignment effects, or texture development.  The project's work could also benefit a range of similar application fields  such as medical image analysis, computational neuroanatomy,  geographic information systems, and engineering designs. Indeed,  collaborations to apply geometric/topological methods to some of  these other application fields are already underway at the TRIPODS  center at OSU and could benefit from close collaboration with this  Materials-focused program as it develops.<br/><br/>The proposed research involves concepts from mathematical areas  of algebraic topology and geometry, applied statistics, and  computational areas of algorithms and graph theory. These will be  applied to materials microstructures created by templating to help  understand topological interconnections, shapes, and dynamics, which  would be of benefit to functional improvements in device operation.  Research in topological data analysis has brought forth the need to  investigate topological concepts in the presence of finite data,  approximations, and noise, constraints that are always encountered in  real materials characterization. Geometric and topological  computation with intentionally structured materials will yield big  and diverse data that can influence and improve future material and  device fabrication efforts. These new data methods will be of  interest to the topological and statistical communities as well as  open up new avenues for predicting and intentionally creating  structures with enhanced functionality.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1539586","CyberSEES: Type2: Collaborative Research: SmartFarm - Research and Education for Sustainable Agriculture Practices","CCF","COMM & INFORMATION FOUNDATIONS, CyberSEES","10/01/2015","04/16/2018","Chandra Krintz","CA","University of California-Santa Barbara","Standard Grant","Rahul Shah","09/30/2019","$907,996.00","Richard Wolski","ckrintz@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7797, 8211","7935, 8208, 8231, 9251","$0.00","Ecological sustainability depends critically on the ability of world food production to manage increasingly limited natural resources (such as arable land and water) with new techniques that both enhance environmental stewardship and increase farm productivity.  To make agriculture more productive and the productivity gains sustainable, growers are increasingly turning to environmental sensor measurement, data acquisition, and data analysis. To date, however, these tools have failed to achieve widespread use by smallholder agricultural concerns.  Importantly, individual growers and ranchers are underserved by many recent advances in the commercial and research sectors that make data analytics consumable as simple ""black box"" end-user products. Current decision support offerings for these constituencies are variously limited, proprietary, complex, costly, require that growers relinquish control over their data,  or are not widely available.  This project therefore investigates a comprehensive research, educational, and outreach program called SmartFarm, which couples new scientific research in computer science, agronomics, and precision agriculture with novel outreach and educational pathways that enable our youth and communities to transform and ensure agriculture sustainability. The research will bring new computing technologies to growers that are easy to use,  facilitate data privacy and control, and enable farm-focused, data-driven analysis and decision support that helps growers increase their yields sustainably.  In addition, the educational and outreach plans will introduce  technically adept youth, who are increasingly ecologically conscious, to the challenges and rewards associated with computer science and precision agricultural science so that they are adequately prepared and inspired to participate in the global challenge of increasing sustainable food production.<br/><br/>To address the problem of sustainable food security and food safety, this project investigates unifying cyberinfrastructure and agriculture analytics to enable precision, agronomics-driven farming by individual growers unlike what is available today.  The proposed system, called SmartFarm, integrates disparate environmental sensor technologies into a customized, open-source, cloud-based data appliance with new analytics that provide growers with a secure, easy to use, low-cost data analysis and decision support system.  Using open-source private cloud platforms, this data appliance can be hosted at a range of scales including personal, private clouds on-farm, large-scale public clouds, or in some combination of the two.  The research program will facilitate new knowledge in: (i) multi-analytic agricultural applications for farm control, dynamic decision support, and emergency response; (ii) self-managing, extensible private cloud systems; (iii) robust sensing and data acquisition techniques, application programming interfaces, and processing engines tailored to the needs of farmers and ranchers; and (iv) private and hybrid cloud software architectures for precision farming that are code and data compatible with public cloud industry standards.  The outreach and education efforts will expose students to cross-disciplinary research and educational activities that train them as new agronomists in new technologists in precision agricultural."
"1750428","CAREER: Inferring Graph Structure via Spectral Representations of Network Processes","CCF","COMM & INFORMATION FOUNDATIONS","04/01/2018","01/29/2018","Gonzalo Mateos Buckstein","NY","University of Rochester","Continuing grant","Phillip Regalia","03/31/2023","$158,048.00","","gmateosb@ece.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7797","1045, 7935","$0.00","Coping with the challenges found at the intersection of Network Science and Big Data necessitates fundamental breakthroughs in modeling, identification, and controllability of distributed network processes -- often conceptualized as signals defined on graphs. There is an evident mismatch between the scientific understanding of signals defined over regular domains (time or space) and graph-valued signals. Knowledge about time series was developed over the course of decades and boosted by real needs in areas such as communications, speech, or control. On the contrary, the prevalence of network-related signal processing problems and the access to quality network data are recent events. In this context, research in this project aims to push the frontiers of knowledge in network-analytic information processing, and thus make progress towards understanding the inherent complexities of strongly coupled systems such as the brain. Students will also be trained to tackle the problems at the intersection of Big Data and Network Science, thereby contributing to workforce development as well.<br/><br/>Under the assumption that the signals are related to the topology of the graph where they are supported, the goal of graph signal processing is to develop algorithms that fruitfully leverage this relational structure, and can make inferences about these relationships when they are only partially observed. Most graph signal processing efforts to date assume that the underlying network is known, and then analyze how the graph's algebraic and spectral characteristics impact the properties of the graph signals of interest. However, such assumption is often untenable in practice and arguably most graph construction schemes are largely informal, distinctly lacking an element of validation.  The intellectual merit of this research project is to investigate how to use information available from graph signals to learn the underlying graph topology, through innovative approaches that operate in the graph spectral domain. The idea is to consider the graph Fourier transform of the snapshot signals associated with an arbitrary graph and, among all the feasible networks, search for one that endows the resulting transforms with target spectral properties and the sought graph with appealing physical characteristics. Aligned with current trends in data-driven scientific inquiry into complex networked systems, the aim is to shift from: (i) descriptive accounts to inferential graph signal processing techniques that can explain as well as predict network behavior; and from (ii) ad hoc graph constructions to rigorous formulations rooted in well-defined models and principles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1527636","CIF: Small: Collaborative Research: Ordinal Data Compression","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","08/11/2015","Olgica Milenkovic","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","08/31/2019","$250,000.00","","milenkov@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7935","$0.00","With the emergence of Big Data platforms in social and life sciences, it is becoming of paramount importance to develop efficient lossless and lossy data compression methods catering to the need of such information systems. Although many near-optimal compression methods exist for classical text, image and video data, they tend to perform poorly on data which naturally appears in fragmented or ordered form. This is especially the case for so called ordinal data, arising in crowd-voting, recommender systems, and genome rearrangement studies. There, information is represented with respect to a ?relative,? rather than ?absolute? scale, and the particular constraints of the ordering cannot be properly captured via simple dictionary constructions. This project seeks to improve the operational performance of a number of data management, cloud computing and communication systems by developing theoretical, algorithmic and software solutions for ordinal data compaction.<br/><br/>The main goal of the project is to develop the first general and comprehensive theoretical framework for ordinal compression. In particular, the investigators propose to investigate new distortion measures for ordinal data and rate-distortion functions for lossy ordinal compression; rank aggregation and learning methods for probabilistic ordinal models, used for ordinal clustering and quantization; and smooth compression and compressive computing in the ordinal domain. The proposed analytical framework will also allow for addressing algorithmic challenges arising in the context of compressing complete, partial and weak rankings. The accompanying software solutions are expected to find broad applications in areas as diverse as theoretical computer science (sorting, searching and selection), machine learning (clustering and learning to rank), and gene prioritization and phylogeny (reconstruction of lists of influential genes and ancestral genomes, respectively)."
"1642550","CIF: Small: Collaborative Research: Ordinal Data Compression","CCF","COMM & INFORMATION FOUNDATIONS","03/01/2016","05/20/2016","Arya Mazumdar","MA","University of Massachusetts Amherst","Standard Grant","Phillip Regalia","08/31/2019","$246,331.00","","arya@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7797","7923, 7935","$0.00","With the emergence of Big Data platforms in social and life sciences, it is becoming of paramount importance to develop efficient lossless and lossy data compression methods catering to the need of such information systems. Although many near-optimal compression methods exist for classical text, image and video data, they tend to perform poorly on data which naturally appears in fragmented or ordered form. This is especially the case for so called ordinal data, arising in crowd-voting, recommender systems, and genome rearrangement studies. There, information is represented with respect to a ?relative,? rather than ?absolute? scale, and the particular constraints of the ordering cannot be properly captured via simple dictionary constructions. This project seeks to improve the operational performance of a number of data management, cloud computing and communication systems by developing theoretical, algorithmic and software solutions for ordinal data compaction.<br/><br/>The main goal of the project is to develop the first general and comprehensive theoretical framework for ordinal compression. In particular, the investigators propose to investigate new distortion measures for ordinal data and rate-distortion functions for lossy ordinal compression; rank aggregation and learning methods for probabilistic ordinal models, used for ordinal clustering and quantization; and smooth compression and compressive computing in the ordinal domain. The proposed analytical framework will also allow for addressing algorithmic challenges arising in the context of compressing complete, partial and weak rankings. The accompanying software solutions are expected to find broad applications in areas as diverse as theoretical computer science (sorting, searching and selection), machine learning (clustering and learning to rank), and gene prioritization and phylogeny (reconstruction of lists of influential genes and ancestral genomes, respectively)."
"1733878","AitF: Collaborative Research: Fast, Accurate, and Practical: Adaptive Sublinear Algorithms for Scalable Visualization","CCF","Algorithms in the Field","09/15/2017","09/08/2017","Aditya Parameswaran","IL","University of Illinois at Urbana-Champaign","Standard Grant","Tracy Kimbrel","08/31/2020","$234,000.00","","adityagp@berkeley.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7239","","$0.00","With the wealth of data being generated in every sphere of human endeavor, data exploration--analyzing, understanding, and extracting value from data--has become absolutely vital. Data visualization is by far the most common data exploration mechanism, used by novice and expert data analysts alike. Yet data visualization on increasingly larger datasets remains difficult: even simple visualizations of a large dataset can be slow and non-interactive,  while visualizations of a sampled fraction of a dataset can mislead an analyst. <br/><br/>The project aims to develop FastViz, a scalable visualization engine, that will not only enable visualization on datasets that are orders of magnitude larger in the same time, but also ensure the resulting visualizations satisfy key properties essential for correct analysis by end-users. To ensure immediate utilization, FastViz will be applied to three real-world application domains: battery science, advertising analysis, and genomic data analysis, and implemented in Zenvisage, an open-source visual exploration platform developed by the PIs.  Students in the project gain invaluable experience in combining the algorithmic and systems considerations that enable data exploration. <br/><br/>FastViz's development is driven by simultaneous investigation of systems considerations, such as indexing and storage techniques that enable various forms of online sampling, and algorithmic considerations for <br/>(a) visualization generation, where the goal is to produce incrementally improving visualizations in which the important features are displayed first, and <br/>(b) visualization selection, where the goal is to select, from a collection of as yet not generated visualizations, those that satisfy desired criteria. <br/>On the systems front, FastViz will leverage and contribute back to recent developments on online sampling systems that enable the use of more powerful sampling modalities.  <br/>On the algorithms front, FastViz will draw ideas from testing, distribution learning, and sublinear algorithms literature that, to the best knowledge of the PIs, have not been adapted in practice.  The algorithms developed will obey optimality guarantees, and wherever possible, instance-optimality guarantees, ensuring that they will adapt to data characteristics in the most efficient way possible.  <br/><br/>The project will lead to a better understanding of the interplay between sampling algorithms development and systems design, facilitating the adoption of more realistic models and algorithms on the one hand, and the development of more powerful sampling engines that enable the models required within the algorithms."
"1733808","AitF: Collaborative Research: Fast, Accurate, and Practical: Adaptive Sublinear Algorithms for Scalable Visualization","CCF","Algorithms in the Field","09/15/2017","09/08/2017","Ronitt Rubinfeld","MA","Massachusetts Institute of Technology","Standard Grant","Tracy Kimbrel","08/31/2020","$232,999.00","","ronitt@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7239","9102","$0.00","With the wealth of data being generated in every sphere of human endeavor, data exploration--analyzing, understanding, and extracting value from data--has become absolutely vital. Data visualization is by far the most common data exploration mechanism, used by novice and expert data analysts alike. Yet data visualization on increasingly larger datasets remains difficult: even simple visualizations of a large dataset can be slow and non-interactive,  while visualizations of a sampled fraction of a dataset can mislead an analyst. <br/><br/>The project aims to develop FastViz, a scalable visualization engine, that will not only enable visualization on datasets that are orders of magnitude larger in the same time, but also ensure the resulting visualizations satisfy key properties essential for correct analysis by end-users. To ensure immediate utilization, FastViz will be applied to three real-world application domains: battery science, advertising analysis, and genomic data analysis, and implemented in Zenvisage, an open-source visual exploration platform developed by the PIs.  Students in the project gain invaluable experience in combining the algorithmic and systems considerations that enable data exploration. <br/><br/>FastViz's development is driven by simultaneous investigation of systems considerations, such as indexing and storage techniques that enable various forms of online sampling, and algorithmic considerations for <br/>(a) visualization generation, where the goal is to produce incrementally improving visualizations in which the important features are displayed first, and <br/>(b) visualization selection, where the goal is to select, from a collection of as yet not generated visualizations, those that that satisfy desired criteria. <br/>On the systems front, FastViz will leverage and contribute back to recent developments on online sampling systems that enable the use of more powerful sampling modalities.  <br/>On the algorithms front, FastViz will draw ideas from testing, distribution learning, and sublinear algorithms literature that, to the best knowledge of the PIs, have not been adapted in practice.  The algorithms developed will obey optimality guarantees, and wherever possible, instance-optimality guarantees, ensuring that they will adapt to data characteristics in the most efficient way possible.  <br/><br/>The project will lead to a better understanding of the interplay between sampling algorithms development and systems design, facilitating the adoption of more realistic models and algorithms on the one hand, and the development of more powerful sampling engines that enable the models required within the algorithms."
"1733796","AitF: Collaborative Research: Fast, Accurate, and Practical: Adaptive Sublinear Algorithms for Scalable Visualization","CCF","Algorithms in the Field","09/15/2017","09/08/2017","Ilias Diakonikolas","CA","University of Southern California","Standard Grant","Tracy J. Kimbrel","08/31/2020","$233,000.00","","diakonik@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7239","","$0.00","With the wealth of data being generated in every sphere of human endeavor, data exploration--analyzing, understanding, and extracting value from data--has become absolutely vital. Data visualization is by far the most common data exploration mechanism, used by novice and expert data analysts alike. Yet data visualization on increasingly larger datasets remains difficult: even simple visualizations of a large dataset can be slow and non-interactive,  while visualizations of a sampled fraction of a dataset can mislead an analyst. <br/><br/>The project aims to develop FastViz, a scalable visualization engine, that will not only enable visualization on datasets that are orders of magnitude larger in the same time, but also ensure the resulting visualizations satisfy key properties essential for correct analysis by end-users. To ensure immediate utilization, FastViz will be applied to three real-world application domains: battery science, advertising analysis, and genomic data analysis, and implemented in Zenvisage, an open-source visual exploration platform developed by the PIs.  Students in the project gain invaluable experience in combining the algorithmic and systems considerations that enable data exploration. <br/><br/>FastViz's development is driven by simultaneous investigation of systems considerations, such as indexing and storage techniques that enable various forms of online sampling, and algorithmic considerations for <br/>(a) visualization generation, where the goal is to produce incrementally improving visualizations in which the important features are displayed first, and <br/>(b) visualization selection, where the goal is to select, from a collection of as yet not generated visualizations, those that that satisfy desired criteria. <br/>On the systems front, FastViz will leverage and contribute back to recent developments on online sampling systems that enable the use of more powerful sampling modalities.  <br/>On the algorithms front, FastViz will draw ideas from testing, distribution learning, and sublinear algorithms literature that, to the best knowledge of the PIs, have not been adapted in practice.  The algorithms developed will obey optimality guarantees, and wherever possible, instance-optimality guarantees, ensuring that they will adapt to data characteristics in the most efficient way possible.  <br/><br/>The project will lead to a better understanding of the interplay between sampling algorithms development and systems design, facilitating the adoption of more realistic models and algorithms on the one hand, and the development of more powerful sampling engines that enable the models required within the algorithms."
"1565431","SHF: Large: Collaborative Research: Next Generation Communication Mechanisms exploiting Heterogeneity, Hierarchy and Concurrency for Emerging HPC Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2016","08/22/2018","William Barth","TX","University of Texas at Austin","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$422,457.00","Carlos Rosales-Fernandez, Jerome Vienne","bbarth@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7925, 7942","$0.00","This award was partially supported by the CIF21 Software Reuse Venture whose goals are to support pathways towards sustainable software elements through their reuse, and to emphasize the critical role of reusable software elements in a sustainable software cyberinfrastructure to support computational and data-enabled science and engineering.<br/><br/>Parallel programming based on MPI (Message Passing Interface) is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics. The emergence of Dense Many-Core (DMC) architectures like Intel's Knights Landing (KNL) and accelerator/co-processor architectures like NVIDIA GPGPUs are enabling the design of systems with high compute density. This, coupled with the availability of Remote Direct Memory Access (RDMA)-enabled commodity networking technologies like InfiniBand, RoCE, and 10/40GigE with iWARP, is fueling the growth of multi-petaflop and ExaFlop systems. These DMC architectures have the following unique characteristics: deeper levels of hierarchical memory; revolutionary network interconnects; and heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level). <br/>For these emerging systems, a combination of MPI  and other programming models, known as MPI+X (where X can be PGAS, Tasks, OpenMP, OpenACC, or CUDA), are being targeted.  The current generation communication protocols and mechanisms for MPI+X programming models cannot efficiently support the emerging DMC architectures.  This leads to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next generation applications be designed/co-designed with the proposed communication mechanisms?<br/><br/>A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions.  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on Stampede and Comet and other systems at OSC and OSU.  The proposed designs will be integrated into the widely-used MVAPICH2 library and made available for public use.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials will be organized at XSEDE, SC and other conferences to share the research results and experience with the community."
"1565414","SHF: Large: Collaborative Research: Next Generation Communication Mechanisms exploiting Heterogeneity, Hierarchy and Concurrency for Emerging HPC Systems","CCF","CI REUSE, Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","08/15/2016","08/25/2017","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$1,171,893.00","Karen Tomko, Hari Subramoni, Khaled Hamidouche","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","6892, 7354, 7798","7354, 7433, 7925, 7942","$0.00","This award was partially supported by the CIF21 Software Reuse Venture whose goals are to support pathways towards sustainable software elements through their reuse, and to emphasize the critical role of reusable software elements in a sustainable software cyberinfrastructure to support computational and data-enabled science and engineering.<br/><br/>Parallel programming based on MPI (Message Passing Interface) is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics. The emergence of Dense Many-Core (DMC) architectures like Intel's Knights Landing (KNL) and accelerator/co-processor architectures like NVIDIA GPGPUs are enabling the design of systems with high compute density. This, coupled with the availability of Remote Direct Memory Access (RDMA)-enabled commodity networking technologies like InfiniBand, RoCE, and 10/40GigE with iWARP, is fueling the growth of multi-petaflop and ExaFlop systems. These DMC architectures have the following unique characteristics: deeper levels of hierarchical memory; revolutionary network interconnects; and heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level). <br/>For these emerging systems, a combination of MPI  and other programming models, known as MPI+X (where X can be PGAS, Tasks, OpenMP, OpenACC, or CUDA), are being targeted.  The current generation communication protocols and mechanisms for MPI+X programming models cannot efficiently support the emerging DMC architectures.  This leads to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next generation applications be designed/co-designed with the proposed communication mechanisms?<br/><br/>A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions.  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on Stampede and Comet and other systems at OSC and OSU.  The proposed designs will be integrated into the widely-used MVAPICH2 library and made available for public use.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials will be organized at XSEDE, SC and other conferences to share the research results and experience with the community."
"1565336","SHF: Large: Collaborative Research: Next Generation Communication Mechanisms exploiting Heterogeneity, Hierarchy and Concurrency for Emerging HPC Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2016","08/04/2016","Amitava Majumdar","CA","University of California-San Diego","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$405,651.00","Mahidhar Tatineni","majumdar@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7798","7925, 7942","$0.00","This award was partially supported by the CIF21 Software Reuse Venture whose goals are to support pathways towards sustainable software elements through their reuse, and to emphasize the critical role of reusable software elements in a sustainable software cyberinfrastructure to support computational and data-enabled science and engineering.<br/><br/>Parallel programming based on MPI (Message Passing Interface) is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics. The emergence of Dense Many-Core (DMC) architectures like Intel's Knights Landing (KNL) and accelerator/co-processor architectures like NVIDIA GPGPUs are enabling the design of systems with high compute density. This, coupled with the availability of Remote Direct Memory Access (RDMA)-enabled commodity networking technologies like InfiniBand, RoCE, and 10/40GigE with iWARP, is fueling the growth of multi-petaflop and ExaFlop systems. These DMC architectures have the following unique characteristics: deeper levels of hierarchical memory; revolutionary network interconnects; and heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level). <br/>For these emerging systems, a combination of MPI  and other programming models, known as MPI+X (where X can be PGAS, Tasks, OpenMP, OpenACC, or CUDA), are being targeted.  The current generation communication protocols and mechanisms for MPI+X programming models cannot efficiently support the emerging DMC architectures.  This leads to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next generation applications be designed/co-designed with the proposed communication mechanisms?<br/><br/>A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions.  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on Stampede and Comet and other systems at OSC and OSU.  The proposed designs will be integrated into the widely-used MVAPICH2 library and made available for public use.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials will be organized at XSEDE, SC and other conferences to share the research results and experience with the community."
"1553385","CAREER: A Theory of Mechanisms with Unstructured Beliefs","CCF","ALGORITHMIC FOUNDATIONS","02/15/2016","02/14/2019","Jing Chen","NY","SUNY at Stony Brook","Continuing grant","Tracy Kimbrel","01/31/2021","$380,520.00","","jingchen@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7796","1045, 7932","$0.00","This project expands the theory of mechanisms to settings with unstructured beliefs. More specifically, computer science studies optimization problems when the data is at hand, and mechanism design studies optimization problems when the data is in the hands of rational players, who may strategically lie about the data in their possession if this is in their interest. The need for mechanism design arises in many contexts in both computer science and economics, such as distributed computation, auctions, social networks, healthcare, and multiagent systems at large. A player's belief about other players in a mechanism is the information he has about them: it's called ""belief"" rather than ""knowledge"" to reflect that the information may not always be correct or consistent. Players' beliefs form the basis of their reasoning, and largely affect the mechanism's structure and performance. Beliefs can be null, purely possibilistic, or purely probabilistic; but, very often, they are unstructured: a player's belief can be anywhere between a set of possibilities and a fully-fledged probability distribution. Despite the tremendous progress made in mechanism design, the vast space where players have unstructured beliefs remains highly unexplored. This project studies fundamental questions about such settings and aims at developing a theory of mechanisms for analyzing and leveraging unstructured beliefs. If successful, it will make profound contributions to both computer science and economics, and is expected to have a long-lasting impact on logic, multiagent systems, and game theory.<br/> <br/>The technical part of this project consists of two key components. On the one hand, it will establish the logical foundation for different information structures and characterize corresponding notions of rationality. On the other hand, it will develop new tools and criteria for mechanism design, and provide new mechanisms for strategic settings such as auctions. This project will establish a solid scientific foundation for mechanisms with unstructured beliefs, bring mechanism design closer to logic and epistemic game theory, and further strengthen the connections between computer science and economics. Results from this project will be broadly disseminated in computer science, economics, and operations research. The PI will encourage the participation of women and members of underrepresented minority groups in this project. Moreover, new graduate and undergraduate curriculums will be developed based on this project."
"1657939","CAREER: Transforming data analysis via new algorithms for feature extraction","CCF","ALGORITHMIC FOUNDATIONS","06/01/2016","06/29/2018","Luis Rademacher","CA","University of California-Davis","Continuing grant","Rahul Shah","06/30/2020","$329,094.00","","lrademac@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7796","1045, 7926","$0.00","Analysis and exploration of data, including classification, inference, and retrieval, are ubiquitous tasks in science and applied fields. Given any such task, a fundamental paradigm is the extraction of features that are relevant. In the design of algorithms for the analysis and exploration of data, feature extraction techniques act as basic building blocks or primitives that can be combined to model complex behavior. Some of the fundamental feature extraction tools include Principal Component Analysis (PCA), Independent Component Analysis (ICA), and half-space-based learning and classification. Data rarely satisfy the precise assumptions of these models and feature extraction tools, and combining these tools amplifies errors. This motivates the challenging task of designing new algorithms that are robust against noise and that can be combined as building blocks while keeping the error propagation under control.<br/><br/>The proposed work will:<br/>(1) Raise ICA from a very successful practical tool to an algorithmic primitive with strong theoretical guarantees and applicability to a rich family of problems beyond independence.<br/>(2) Find reasonable assumptions and algorithms that allow efficient learning of intersections of half-spaces.<br/>(3) Systematically study the following well-motivated refinement of PCA known as the subset selection problem. This refinement aims to select relevant features among the given features of the input data, unlike PCA, which creates new and possibly artificial features.<br/><br/>New feature extraction algorithms enhance the toolbox available to researchers in data-intensive fields such as biology, signal processing and computer vision. They also enable improved data analysis by practitioners in security, marketing, business and government processes, and essentially any field that involves the analysis of feature-rich data. The proposed work includes the implementation of the more practical algorithms.<br/><br/>Education and outreach aspects of this project include the mentoring of young researchers, the design of a new course for graduate and undergraduate students incorporating some of the PI's research, and the involvement of pre-college students and local communities into science and research."
"1833332","CAREER: Programming the Existing and Emerging Memory Systems for Extreme-scale Parallel Performance","CCF","SOFTWARE & HARDWARE FOUNDATION","03/01/2018","02/25/2019","Yonghong Yan","SC","University of South Carolina at Columbia","Continuing grant","Almadena Chtchelkanova","01/31/2022","$348,209.00","","yanyh@cse.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","7798","1045, 7942","$0.00","High performance computing (HPC) focuses on using numerical model to simulate complex science and engineering phenomena, such as galaxies, weather and climate, molecular interactions, electric power grids, and aircraft in flight.  Over the next decade the goal is to build HPC parallel system capable of extreme-scale performance (one exaflop (1018)operations per second) and processing exabyte (1018) of data. However, one of the biggest challenges of achieving extreme-scale performance is what is known as the hardware memory wall, which is about the growing gap between the speed of computation performed by CPU and the speed of supplying data to the CPU from memory systems (about x100 time slower). The low performance efficiency of modern HPC system (average <60% and could be as low as 5%) manifests the memory wall impact since a huge amount of computation cycles are wasted for waiting for the arrival of input data.  It becomes very critical to create effective software solutions for achieving the computation potential of hardware and for improving the efficiency and usability of the existing and future computing system. Such solutions will significantly benefit a broad range of disciplines that use parallel computers to solve scientific and engineering problems, and accelerate scientific discovery and problem solving to improve quality of life of the society. <br/>This CAREER project develops innovative software techniques to address the programming and performance challenges of the existing and emerging memory systems: 1) a portable abstract machine model for programming, compiling and executing parallel applications, 2) new programming interface and model for data mapping, movement, and consistency, and 3) machine-aware compilation and data-aware scheduling techniques to realize an asynchronous task flow execution model to hide the latency of data movement. It addresses the memory wall challenge by developing a memory-centric programming paradigm for helping achieve extreme-scale performance of parallel applications with minimum impairment to programmability. For education, the project involves a broader community starting from high school in the area of HPC and computer science."
"1750472","CAREER: A Comprehensive and Lightweight Framework for Transcriptome Analysis","CCF","COMPUTATIONAL BIOLOGY","02/01/2018","02/11/2019","Robert Patro","NY","SUNY at Stony Brook","Continuing grant","Mitra Basu","01/31/2023","$293,353.00","","rob.patro@gmail.com","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7931","1045, 7931","$0.00","Over the past decade, sequencing technologies have been developed that enable the profiling of gene expression across a wide variety of organisms and tissue types.  These technologies allow the investigation, on a transcriptome-wide scale, of how gene expression changes in different conditions, under various stimuli, and in different disease states. These technologies are transformative in progressing basic science (e.g., understanding cell biology) and applied science (e.g., approaches to drug development).  However, the deluge of data produced by these technologies brings with it a host of computational challenges, such as discovering if samples contain genes previously not annotated, accurately determining the sequence of these genes, and quantifying the abundance of all the genes expressed in a sample. Much effort has been dedicated to developing reliable computational methods for processing this data. Yet, even the best existing solutions are sometimes unsatisfactory in terms of their accuracy, and are becoming computationally burdensome given the rapid rate at which new data is being produced. <br/><br/>The goal of this project is to develop a new generation of accurate and lightweight methods for analyzing gene and transcript expression using sequencing data. These tools will apply new data structures and algorithmic ideas to the problems of mapping sequencing reads, discovering and assembling new transcripts, and accurately and robustly quantifying gene expression. Further, these methods will work in the context of both established technologies and the newly-emerging protocols that allow measuring cell-specific gene expression across thousands of individual cells.  The methods and software produced as a result of this project will help enable new discoveries by being more sensitive and accurate than existing approaches, will reduce costs by decreasing computational demands, and will speed up analyses by producing results more quickly than existing approaches. The outreach goals of this project include the creation of educational media including videos and a podcast series that will help convey key insights and benefits of new computational genomics methods to both practicing biologists as well as to the scientifically-interested public at large.<br/><br/>Lightweight quantification methods streamline many common transcriptomic analyses, like differential expression testing in well-annotated organisms and common tissue types. Yet, substantial challenges remain that prevent the use of lightweight methods in many analysis tasks, e.g., when novel transcripts should be considered, or when events such as intron retention may play an important role. This work will advance the accuracy and fundamental capabilities of lightweight transcriptome analysis methods. Specifically, a new graph-based data structure will be developed for indexing a collection of reference sequences. A lightweight alignment tool will be built around this index that will incorporate a statistical model that allows sharing of splicing contexts across large collections of samples to guide and inform difficult alignment problems. A multi-sample methodology for joint transcript discovery and quantification will also be developed, based on new approaches to modeling the joint likelihood of transcript sequences and their abundances. Efficient likelihood factorizations will allow this approach to remain computationally convenient. Finally, a suite of tools for processing and quantifying high-throughput, single-cell RNA-seq data will be developed.  These tools will adopt a novel approach for solving the cell barcoding, UMI deduplication, and gene expression estimation problems jointly, and in a unified statistical framework.  The underlying model will share statistical information between cells to improve clustering and quantification, and to analyze expression at the resolution supported by the data, i.e., as groups of distinguishable isoforms. All of these tools will be released as high-quality, open-source software."
"1758578","CAREER: Geometric Frontiers in Algorithm Design","CCF","ALGORITHMIC FOUNDATIONS","08/16/2017","06/30/2018","Anastasios Sidiropoulos","IL","University of Illinois at Chicago","Continuing grant","Rahul Shah","01/31/2020","$275,694.00","","Sidiropo@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","7796","1045, 7929, 9251","$0.00","Complex data sets arise in a plethora of application domains from measurements of various physical processes, history of financial transactions, logs of user activity in a network, and so on. The analysis of such data sets is therefore a task of increasing importance for science and engineering. Even though in many applications there is an abundance of such raw inputs, extracting meaningful information can often be a major computational challenge. In most cases, this difficulty is due to the lack of a useful representation of the data.<br/><br/>Over the recent years, geometric methods have become an indispensable tool for overcoming this difficulty. The reason behind this development is the fact that a data set endowed with pairwise similarities can be naturally interpreted as a geometric space. Such data sets include DNA sequences, statistical distributions, collections of news articles, and so on. Under this interpretation, several important data analytic questions can be understood as geometric computational problems. For example, the problem of classification can be expressed as geometric partitioning. Similarly, the problem of fitting a model to set of measurements can be thought of as interpolation in some appropriate geometric space.<br/><br/>In these contexts, the main algorithmic challenges occur in high-dimensional, or more generally, complex metric spaces. This project aims at resolving some of the main problems inherent in the analysis of such geometric data sets, and thus enabling improved solutions for a variety of computational tasks. The project also seeks to use diverse mathematical tools in the setting of geometric data analysis, forging new connections between mathematics and computer science.<br/><br/>The proposed research will be part of the PI's curriculum development, undergraduate and graduate teaching, and educational and outreach activities. It will also provide the set of research problems that will be used for mentoring undergraduate and graduate students."
"1618603","CCF-BSF: CIF: Small: Collaborative Research: Coding and Information - Theoretic Aspects of Local Data Recovery","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/27/2016","Alexander Barg","MD","University of Maryland College Park","Standard Grant","Phillip Regalia","06/30/2019","$249,823.00","","abarg@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7797","7923, 7935","$0.00","This project studies fundamental problems in data coding that can improve the efficiency of distributed storage systems by increasing data reliability and availability while reducing storage overhead compared to existing industry standards. The results of this research can benefit storage applications ranging from financial, scientific monitoring, and signal processing to social networks and sharing platforms. The new combinatorial, coding, and information theoretic tools developed in this project will be incorporated in course curricula in the respective institutions of the principal investigators.<br/><br/>Data coding with locality, the focus of this project, is a rapidly developing area of coding theory that was initially motivated by applications in distributed storage, and has links to many areas of network science (e.g., index coding and network coding) as well as to computer science. This project advances the theory and practice of data coding with local recovery by investigating broad implications of the locality constraint in coding problems. These include studying new error-correcting code families and their decoding, fundamental limitations on the code parameters and capacity bounds under the requirements of local data recovery. The newly designed coding schemes developed in this project will be validated through implementation and evaluation in simulated computer environment, aiming at enhanced performance of data coding in current industry solutions."
"1618512","CCF-BSF: CIF: Small: Collaborative Research: Coding and Information - Theoretic Aspects of Local Data Recovery","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/27/2016","Arya Mazumdar","MA","University of Massachusetts Amherst","Standard Grant","Phillip Regalia","06/30/2019","$249,996.00","","arya@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7797","7923, 7935","$0.00","This project studies fundamental problems in data coding that can improve the efficiency of distributed storage systems by increasing data reliability and availability while reducing storage overhead compared to existing industry standards. The results of this research can benefit storage applications ranging from financial, scientific monitoring, and signal processing to social networks and sharing platforms. The new combinatorial, coding, and information theoretic tools developed in this project will be incorporated in course curricula in the respective institutions of the principal investigators.<br/><br/>Data coding with locality, the focus of this project, is a rapidly developing area of coding theory that was initially motivated by applications in distributed storage, and has links to many areas of network science (e.g., index coding and network coding) as well as to computer science. This project advances the theory and practice of data coding with local recovery by investigating broad implications of the locality constraint in coding problems. These include studying new error-correcting code families and their decoding, fundamental limitations on the code parameters and capacity bounds under the requirements of local data recovery. The newly designed coding schemes developed in this project will be validated through implementation and evaluation in simulated computer environment, aiming at enhanced performance of data coding in current industry solutions."
"1629129","XPS: FULL: A Fresh Look at Near Data Computing: Coordinated Data and Computation Government","CCF","Exploiting Parallel&Scalabilty","07/01/2016","06/29/2016","Mahmut Kandemir","PA","Pennsylvania State Univ University Park","Standard Grant","Yuanyuan Yang","06/30/2019","$875,000.00","Chitaranjan Das, Anand Sivasubramaniam","kandemir@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8283","","$0.00","Many important computer applications in our daily lives depend on processing large amounts of data. While moving data from storage devices to processing components can be very time consuming, with increasing core counts and emerging applications, moving data within a computer system can also incur significant latencies, thereby hurting application performance and energy efficiency. Unfortunately, existing solutions to minimize this data movement overhead have limited potential. Thus, it has become essential to explore a holistic approach for minimizing data movements, and shifting from the compute-centric model being used today to a data-centric or near-data computing (NDC) model for effectively handling the data processing needs of different classes of applications. The PIs aim to integrate their research on NDC with the educational activities and student training at Penn State for nurturing the future workforce in science and engineering. The outreach activities include engaging undergraduates in the NDC research, and working with the CSATS (Center for Science and the Schools) and VIEW (Visit In Engineering Weekend) programs at Penn State to get involved with the ongoing STEM-oriented K-12 activities.<br/><br/>This project aims to revisit the near-data computing concept from a fresh perspective by undertaking a cross-layer approach for exploring the potential benefits of moving computation closer to data. Thus, instead of considering only the Boolean extremes of near-data computing in the hardware of processor core vs. the DRAM (as in the case of past attempts), this project explores a rich spectrum of possibilities between these two. Specifically, focusing on emerging multicore systems and multithreaded applications from three important application domains (high performance computing, embedded/mobile computing, and datacenter computing), this research tries to address the ""where"", ""when"", ""what"", and ""how"" questions of near-data computing in the context of deep memory hierarchies. This comprehensive approach to moving computation closer to the data aims to break the memory wall, which is the biggest barrier to the scalability of emerging chip multiprocessors."
"1559593","REU Site: Science of Software","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2016","01/29/2019","Christopher Parnin","NC","North Carolina State University","Standard Grant","Rahul Shah","01/31/2020","$355,365.00","Timothy Menzies","cjparnin@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","1139","9250","$0.00","This project is for a new REU site at North Carolina State University. It is focused on training undergraduates in software engineering, particularly in skills related to data analytics. Data analytics has become increasingly important in how companies and researchers make decisions about improving the reliability and usability of software.<br/><br/>The intention of the project is to give research exposure and training to a diverse group of undergraduate students. The project aims at building students' analytical skills as well as research skills with the goal of leading them toward a research career in future. The project will primarily recruit women and minorities through the STARs alliance and from non-PhD granting universities.<br/> <br/>The project covers various areas of software engineering including visualization and data manipulation in virtual reality, model-based reasoning, human aspects (e.g., eye tracking data), bug fixing, and software analytics which is a cross between software and data mining methods. The overall research theme will investigate how to take advantage of qualitative organizational knowledge which gives data its context and meaning integrated with quantitative data mining methods that support exploration of large and complex datasets.<br/> <br/>The project will encourage the students to disseminate their research findings through presentations as well as publishing in research venues."
"1331133","BSF:201229:Efficient Algorithms for Geometric Optimization","CCF","SPECIAL PROJECTS - CCF","09/01/2013","09/11/2013","Pankaj Agarwal","NC","Duke University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$32,843.00","","pankaj@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","2878","2878, 7796","$0.00","This project is funded as part of the United States-Israel Collaboration in Computer Science (USICCS) program.  Through this program, NSF and the United States - Israel Binational Science Foundation (BSF) jointly support collaborations among US-based researchers and Israel-based researchers.<br/><br/>This collaborative reserach project between Duke University and Tel Aviv University aims to study several topics in geometric optimization and related problems that arise in the processing of geometric data in a variety of application areas, such as sensor networks, imaging, surveillance, navigation, geographic information systems, modeling and animation, meshing, computer graphics and vision, bioinformatics, robotics and manufacturing.  Processing geometric data in these applications is challenging, because this data is typically huge, measured with uncertainty, obtained in a distributed manner (e.g., in sensor networks), or in an online manner (e.g., in streaming applications), and may involve moving points (e.g., in imaging, animation and modeling).  All these traits make the processing a rather demanding task, and call for the design of novel algorithmic techniques for handling it efficiently.  For many of these problems, exact algorithms, even polynomial in the input size, are impractical, and approximation algorithms are needed. Even then, making these algorithms depend efficiently on the error parameter is often a difficult and challenging task.<br/><br/>The project addresses the above challenges by developing general algorithmic techniques such as computing geometric summaries (including coresets and random samples), handling noisy geometric data under various probabilistic models of uncertainty, handling online data, and handling kinetic data (involving moving objects). Some of the specific problems that are targeted include shape matching, clustering, and geometric searching. Each of these topics raises interesting algorithmic questions, both theoretical and practical, and the project will address as many of them as possible."
"1815145","AF: Small: Approximation Algorithms for Learning Metric Spaces","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/24/2018","Anastasios Sidiropoulos","IL","University of Illinois at Chicago","Standard Grant","Rahul Shah","05/31/2021","$399,899.00","","Sidiropo@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","7796","7923, 7926, 7929","$0.00","The analysis of large and complicated data sets is a task of increasing importance in several brunches of science and engineering. In many application scenarios, the data consists of a set of objects endowed with some information that quantifies similarity or dissimilarity of certain pairs. Examples of such inputs include statistical distributions, user preferences in social networks, DNA sequences, and so on. A natural way to interpret such data sets is as metric spaces. That is, each object is treated as a point, and the dissimilarity between two objects is encoded by their distance. The successful application of this data-analytic framework requires finding a distance function that faithfully encodes the ground truth. The project seeks to develop new algorithms for computing such faithful metrical representations of data sets. The underlying research problems lead to new connections between the areas of computational geometry, algorithm design, and machine learning. The project aims at transferring ideas between these scientific communities, and developing new methods for data analysis in practice. The project will be part of the investigator's curriculum development, teaching, and educational and outreach activities. The main research problems that the project seeks to study will form the basis for the training of both undergraduate and graduate students. The investigator is committed to working with minorities and underrepresented groups.<br/><br/><br/>This metrical interpretation of data sets has been successful in practice and has lead to a plethora of algorithmic methods and ideas, such as clustering, dimensionality reduction, nearest-neighbor search, and so on. The area of metric learning is concerned mainly with methods for discovering an underlying metric space that agrees with a given set of observations. Specifically, the problem of learning the distance function is cast as an optimization problem, where the objective function quantifies the extend to which the solution satisfies the input constraints. A common thread in many works on metric learning is the use of methods and ideas from computational geometry and the theory of metric embeddings. Over the past few decades, these ideas have also been the subject of study within the theoretical computer science community. However, there are several well-studied problems in metric learning that have not received much attention in the algorithms and computational geometry communities. Moreover, there are many metric embedding tools and techniques, that where developed within the algorithms community, that have not yet been used in the context of metric learning. The project aims at bridging this gap by a systematic study of algorithmic metric learning problems from the point of view of computational geometry and approximation algorithms. The major goals of this effort are transferring algorithmic tools to the metric learning framework, as well as providing theoretical justification for metric learning methods that are successful in practice but currently lack provable guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845360","CAREER: CIF: Theory and Applications of Geometric Deep Learning","CCF","COMM & INFORMATION FOUNDATIONS","05/01/2019","03/05/2019","Joan Bruna Estrach","NY","New York University","Continuing grant","Phillip Regalia","04/30/2024","$91,665.00","","jb4496@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7797","1045, 7935","$0.00","Deep Learning has quickly become the gold standard for solving a multitude of tasks in computer vision, speech recognition, and natural language processing. In essence, appropriately designed artificial neural networks are shown large quantities of labeled data-points, allowing their internal parameters to be continually adjusted or 'learned'. Despite its apparent simplicity, such neural networks require certain regularities in the input data domain in order to become effective, which limits its application to areas beyond those described above. For example, in natural images, pixels are arranged into regular squared grids, whereas text and speech contains samples sequentially aligned. This project aims at overcoming this important limitation, by allowing neural networks to operate on far more general data domains, such as chemical compounds or social networks. Its research outcomes have the potential to impact a broad range of technological areas currently limited by lack of appropriate end-to-end learning systems and/or computational scalability. Besides the prospect of significantly increasing the efficiency of data-driven discoveries in physics and chemistry domains, the methods studied in this project directly apply to related disciplines such as weather forecasting or network science. This project will foster cross-disciplinary collaborations between physicists, chemists, computer scientists and applied mathematicians, and will support education and diversity by creating novel courses and outreach activities integrating these disciplines.<br/><br/>The goal of this project is to develop the mathematical and statistical foundations of geometric deep learning---a family of neural network models and algorithms that leverage geometric properties of the data in order to solve complex tasks---and to demonstrate its effectiveness in practical settings such as the physical sciences. This will be enabled by addressing two important limitations of current deep learning methods. The first is their ability to learn how to perform algorithmic or statistical inference tasks with optimum computational complexity, which the second is their application to domains that lack the regular sampling structure of images, video, text or speech. Both objectives share a fundamental interplay between geometry and learning that this project aims to elucidate. This project pursues the notion of geometric stability, the mathematical foundation that underpins the efficiency of deep learning architectures and facilitates its extension to more general domains, modeled as graphs. This will be used to study the optimization landscape and generalization error of geometric deep learning models, where current learning theory struggles to explain its empirical performance. Finally, the project will demonstrate the effectiveness of geometric deep learning with applications to physical sciences. Most physical systems---from atoms to galaxies---are governed by complex dynamical systems and are defined over irregular, non-Euclidean domains, presenting a serious challenge for existing deep learning architectures. This project seeks to overcome these limitations by building 'physics-aware' geometric deep learning models with adaptive computational complexity, applied to particle physics, chemistry and cosmology, by incorporating prior knowledge of the physical dynamics into the graph structure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718924","CIF:Small:Minimum Mean Square Error Estimation and Control of Partially-Observed Boolean Dynamical Systems with Applications in Metagenomics","CCF","COMPUTATIONAL BIOLOGY","08/01/2017","07/17/2017","Ulisses Braga Neto","TX","Texas A&M Engineering Experiment Station","Standard Grant","Mitra Basu","07/31/2020","$350,000.00","","ulisses@ece.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7931","7931, 9102","$0.00","This research concerns the development and application of innovative signal processing techniques to dynamical systems associated with the complex interactions among microbes, human cells, and their metabolic products. This project investigates innovative methods for estimation and control of processes that consist of the complex interactions of many switching elements, such as ""presence"" and ""absence"" of a particular microbial species in the human gut, enabling the characterization of microbial communities and their interactions with host cells. This project provides life scientists with computational tools for biochemical pathway discovery as well as rational intervention design, as in optimal drug scheduling and diet modifications to treat human disease.<br/><br/>This project develops computational methods for systems identification and optimal control of Boolean dynamical systems partially observable through noisy time series data, with application in the modeling of metabolic interactions in the gut microbiome and their evolution. This project facilitates the integrative analysis of multimodal microbiota data, including transcriptomic, metatranscriptomic, metagenomic, and metabolomic data, enabling the characterization of environmental exposures, microbial communities, and their interactions with host gut epithelial cells. While previous work on Boolean dynamical systems has been based on ad-hoc binarization of measurement data and ignore the presence of unobservable variables, the methodology developed here allows the state process to be hidden and relies directly on indirect or incomplete noisy time series measurements of the states. A novel aspect of this work is that it is based on a minimum mean-square error criterion for optimal estimation and control, as opposed to maximum-a-posteriori methods typically used in Boolean and other discrete spaces. This project includes the construction of likelihood functions for various modalities of metagenomic data for use with exact and approximate optimal estimation and systems identification methods using the noisy measurement data.  The methodology developed in this research is validated using novel time series metagenomic data provided by the project's life science collaborators."
"1639995","E2CDA: Type II: Self-Adaptive Reservoir Computing with Spiking Neurons: Learning Algorithms and Processor Architectures","CCF","Energy Efficient Computing: fr","09/01/2016","07/05/2018","Peng Li","TX","Texas A&M Engineering Experiment Station","Continuing grant","Sankar Basu","08/31/2019","$332,742.00","","pli@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","015Y","7945","$0.00","While computing has become increasingly data centric across many disciplines, conventional computer architectures have limited potential in meeting the escalating performance and energy efficiency needs in this era of data-driven science and engineering. This project aims to develop brain-inspired neural models of computation and adaptive processor architectures to enable intelligent data processing and learning in a wide range of applications. While being strongly interdisciplinary, this work will bridge neuroscience, artificial neural networks, computer architecture, and hardware engineering. The planned research will provide rich training and educational opportunities to students, and produce new curriculum. Research participation from undergraduate and underrepresented students will be promoted. The outcomes of this project will be broadly disseminated. Research collaboration with the US industry will be actively pursued via interaction with the Semiconductor Research Corporation. <br/><br/>This work is aimed at attaining brain-like learning performance by imitating how the brain represents, processes, and learns from information, and more specifically, by developing models of computation based on the third-generation spiking neural networks, and efficient adaptive processor architectures.  Within the framework of so called reservoir computing, the proposed neural models mimic key characteristics of the brain such as information processing based on spike timing. Furthermore, this project will develop brain-inspired learning mechanisms to allow training of complex recurrent spiking neural networks. Self-adaptive processor architectures with integrated on-chip learning, light-weight runtime learning performance prediction, and energy management will be developed to maximize system energy efficiency while providing a guarantee of performance."
"1409836","CIF: Medium: Collaborative Research: Estimating simultaneously structured models: from phase retrieval to network coding","CCF","COMM & INFORMATION FOUNDATIONS","08/15/2014","06/05/2017","Maryam Fazel","WA","University of Washington","Continuing grant","Phillip Regalia","07/31/2019","$500,005.00","","mfazel@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7797","7924, 7935","$0.00","In modern data-intensive science and engineering, researchers are faced with estimating models where available observations are far fewer than the dimension of the model to be estimated. The key to the success of compressed sensing, matrix completion, and other problems of this type, is to properly exploit knowledge about the ""structure"" of the model. While structures such as sparsity have been separately studied, the problem of ""simultaneous structures"" has been neglected, since it is implicitly assumed by practitioners that simply combining known results for each structure would solve the joint problem. Interestingly, the PIs recently proved that this approach can result in a significant gap.<br/><br/>This proposal will develop theory and computationally tractable methods for estimating simultaneously structured models with minimal observations. It combines (1) a top-down approach to understand the fundamental limitations based on the geometry of how structures interact, and (2) a problem-specific, bottom-up approach to exploit domain knowledge in constructing appropriate penalties. This work addresses a variety of applications including (1) sparse principal component analysis, a central problem in statistics seeking approximate but sparse eigenvectors, (2) sparse phase retrieval and quadratic compressed sensing in signal processing, and (3) code design for communications and network coding.<br/><br/>The ability to systematically derive structured models from data will have far-reaching impact on engineering challenges in the era of Big Data and ubiquitous computing. Handling models with multiple structures poses deep theoretical and computational challenges that this proposal focuses on. Applications in machine learning, signal processing, and network coding are discussed. The PIs will incorporate research results in their teaching, organize technical workshops to bring together mathematicians and engineers, and seek the involvement of undergraduate students in this work through summer research programs."
"1748585","CIF: Medium: Collaborative Research: Frontiers in coding for cloud storage systems","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2017","09/12/2018","Onur Ozan Koyluoglu","CA","University of California-Berkeley","Continuing grant","Phillip Regalia","02/29/2020","$298,202.00","","ozan.koyluoglu@gmail.com","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7797","7924, 7935","$0.00","Cloud storage systems increasingly form the backbone of software services that underwrite everyday lives, serving a large array of businesses and forming an essential pillar of the economy. Given the sheer volume of interactions within these distributed systems, there arise a multitude of issues in building, maintaining and enhancing them. The most salient of these challenges are in the reliability, availability, consistency, confidentiality and privacy of data stored in these vast systems. <br/><br/>The proposed research is expected to have a significant impact on the manner in which cloud storage systems are designed and deployed. In these systems, storage node failures can have a significant impact on the efficiency of the overall system. This project enhances fault tolerant mechanisms to enable efficient recovery from failures, while augmenting the overall data availability and privacy offered by such systems. The research effort will advance the science of cloud computing by developing a new family of algorithms for distributed storage , and connect the advances to the significant industry needs in this topic. The research agenda will also be tightly integrated with education and outreach activities with direct involvement of underrepresented minorities, graduate, and undergraduate students.<br/><br/>Focusing on efficient maintenance of data with a range of desirable qualities, including mechanisms that ease data encoding, accessibility, updates, as well as privacy, the main objectives of this effort include (i) to develop coding schemes where a failed element can be regenerated with higher repair efficiencies from its local neighbors, (ii) to bring together the advantages of both local decodability and local repairability into one coding solution, (iii) to design mechanisms that provide low cost data updates in addition to efficient repair, (iv) to develop codes that can be resilient against failures with different scales/modalities, (v) to develop coding mechanisms taking advantage of implementation aspects of existing systems, and (vi) to develop coding schemes that enable users to access their data in a private manner. This effort addresses these challenges using a combination of tools from  disciplines spanning coding theory, information theory, communications, as well as combinatorial and discrete mathematics."
"1524312","CIF: Small: Towards Structural Information","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","08/18/2015","Wojciech Szpankowski","IN","Purdue University","Standard Grant","Phillip Regalia","08/31/2020","$499,594.00","","spa@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7923, 7935","$0.00","With the ability to acquire data at high velocity, variety, and volume on diverse natural and engineered processes, comes the need to derive novel insights and translate raw data into context-specific knowledge as structural information, of crucial importance for further advances in engineering and science.  This project pursues an information theoretic foundation, inspired by the great success of information theory in establishing fundamental limits for problems related to relatively simple classes of random processes (such as Markov processes or ergodic sequences). Many real-world domains exhibit complexities that violate assumptions used to derive these fundamental results.  For example, data bases often have strong structural correlations, and these underlying data structures often do not lend themselves naturally to formulation in the classical information-theoretic framework. In yet other cases, data interpretability is itself an issue: as an example, the absence of a product recommendation is distinct from a negative recommendation. The outcome of many analytics tasks, including inference and recommendation, is not easily modeled by traditional information theory formalisms. These challenges notwithstanding, this project posits that formalisms inspired by information theory are critical when dealing with data at scale, speed, and complexity in today's applications, where reliability of solutions from ad-hoc analytics can be questioned. Tools developed as part of the project will be used in areas such as the characterization of biological and social networks, and development of robust pervasive communications infrastructure.<br/><br/><br/>Data is increasingly available in various forms and it appears in exponentially increasing amounts. Most of such data is multidimensional and context dependent; thus it necessitates novel structural theory and efficient algorithms to extract meaningful information. Typically, a database for these new types of data is in the form of a ""data structure,"" which in turn conveys a ""shape"" of the data. The data itself consist of labels implanted in the structure, often locally correlated. This project aims to quantify the information conveyed by such multimodal data structures, via the following specific goals: (1) Discover fundamental limits of information content for a wide range of multimodal data structures with correlated labels. Once this goal is met, the project will devise asymptotically optimal lossless and lossy compression algorithms achieving these limits. (2) Develop Lempel-Ziv like algorithms for graph compression (with correlated labels) and graph data mining. (3) Understand structural properties of large systems with local mutual dependencies, constraints, and interactions often described by Markov fields. Finally, (4) Analyze flow of structural information over a noisy channel."
"1845852","CAREER: Information-Theoretic Foundations of Fairness in Machine Learning","CCF","COMM & INFORMATION FOUNDATIONS","02/01/2019","12/07/2018","Flavio Calmon","MA","Harvard University","Continuing grant","Phillip Regalia","01/31/2024","$106,915.00","","flavio@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7797","1045, 7936, 9102","$0.00","Machine learning algorithms can identify complex patterns in very large datasets. These algorithms are increasingly used in applications of significant social consequence, such as loan approval, hiring, and bail and sentencing decisions. However, real-world data may reflect discrimination patterns that exist in society at large. Consequently, decisions based on algorithms that learn from data are at risk of inheriting and, ultimately, reinforcing discriminatory and unfair social biases. This project aims to precisely characterize the operational limits of discrimination discovery and control in machine learning by combining legal and social science definitions of fairness with powerful mathematical tools from information theory, statistics, and optimization. This cross-disciplinary effort aims to provide fundamental theory and design guidelines for data scientists and engineers who will create the next generation of fair data-driven algorithms and applications. The technical results of this project will also inform the debate surrounding the social impact of machine learning. Moreover, this research will be used as a vessel for engaging students and researchers from diverse backgrounds in the applicability of information theory, machine learning, optimization, and, more broadly, math and engineering to social challenges.<br/><br/>Automated methods for discovering and controlling discrimination in machine learning inherently face a trade-off between fairness and accuracy, and are limited by the dimensionality of the underlying data. This project creates a comprehensive information-theoretic framework that captures the limits of discrimination control by determining (i) how to systematically identify data features that may lead to discrimination; (ii) how to ensure fairness by producing new, information-theoretically grounded data representations; (iii) the fundamental information-theoretic trade-offs between fairness, distortion, and accuracy; and (iv) the impact of finite samples in discrimination detection and mitigation. The key advantage of the information-theoretic methodology adopted in this project is that it captures fundamental, algorithm-independent properties of discrimination, while being fertile ground for the development of novel mathematical tools and models relevant to both data scientists and information theorists. The theoretical component of this research weaves new connections between information theory and robust statistics by analyzing the impact of local perturbations of probability distributions on discrimination metrics, and creates new information-theoretic models useful in discrimination control, privacy, and representation learning. The applied component of this research develops robust, data-driven methods for measuring and mitigating discrimination that are immediately relevant for fair algorithmic decision-making in applications of consequence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815840","AF: Small: Data Stream Algorithms with Application to Linear Algebra","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/21/2018","David Woodruff","PA","Carnegie-Mellon University","Standard Grant","Rahul Shah","05/31/2021","$433,978.00","","dwoodruf@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7923, 7926","$0.00","Many important problems in machine learning, scientific computing, and statistics benefit from fast procedures for solving numerical linear algebra problems. At the same time, many large-scale datasets, such as internet search logs, network traffic, and sensor network data, have been studied in data-stream literature. Surprisingly, a number of fast procedures used in numerical linear algebra have been made possible by exploiting tools that were originally developed in the data-stream literature. These developments are based on a technique called sketching, which is a tool for quickly compressing a problem to a smaller version of itself, for which one can then afford to run a much slower procedure on the smaller problem. A major goal of this project is to study foundational problems in the data-stream domain, and develop their connections to problems in numerical linear algebra. The algorithms developed here will be accessible to graduate and undergraduate students from computer science, machine learning, and mathematics, and the investigator plans to integrate the results of the project into a graduate course on algorithms for big data, as well as undergraduate algorithms courses. The investigator is actively working with underrepresented minority and undergraduate researchers on topics directly related to this project.<br/><br/>The first main thrust of this project is to develop new techniques for fundamental problems in data streams where the goal is to use minimal amount of memory while running algorithms over one pass on a data stream. Such problems include statistical problems - such as estimating the variance, moments, most frequent items, heavy hitters - for many of which optimal memory bounds are still unknown. Another challenge in this domain is that of processing massive streams for real-world graphs, such as geometric intersection graphs, which arise in cellular networks and scheduling theory. By studying a breadth of problems in different corners of data streams, the investigator plans to develop new techniques and build unexpected applications. The second main thrust of the project is to develop new algorithms and hardness results for fundamental problems in numerical linear algebra, connecting such problems to the study of data streams. CountSketch, which is a low-memory heavy hitters algorithm, paved the way for obtaining time-optimal algorithms for regression. This project will continue to push forward such connections, for example, by studying CountSketch in the context of tensors, which is a new application domain. The project will also investigate many deterministic (instead of randomized) data stream algorithms, and understanding their role in linear algebra. A major goal is to understand the limitations of speedups to linear algebra problems. One particular problem of interest here is to obtain low rank approximation with spectral norms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1830711","AF: Small: Collaborative Research: Algorithmic and Computational Frontiers of MapReduce for Big Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","01/01/2018","03/22/2018","Benjamin Moseley","PA","Carnegie-Mellon University","Standard Grant","Rahul Shah","06/30/2019","$114,469.00","","moseleyb85@gmail.com","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7923, 7926, 7934","$0.00","Modern science and engineering heavily relies on processing massive data sets and the size of the data requires applications to run using distributed computing frameworks.  However, many existing methods essential to the applications are not easily adapted to work in distributed settings. This project aims to develop new efficient ways of processing large data sets in widely used distributed computing platforms. The project will reveal new methods for processing diverse and complex data sets of massive size and allow for various applications scale to large inputs. The work has the potential to fundamentally change algorithmic techniques used in distributed computing, helping to shape big data research, the computing industry, and the growing economy reliant on big data analysis. Research outcomes will be integrated with education by writing an extensive survey/tutorial on the core algorithmic ideas used in the new discoveries to make the ideas transparent to the algorithmic developers and practitioners. The PIs will make some of the discovered algorithmic ideas accessible even to undergraduate students, helping them get prepared to cope with algorithmic challenges in distributed computing for large data sets. Special efforts will be made to include women and minorities in advising and mentoring plans.<br/><br/>The main goal of the project is to find new ways of unlocking the underlying power of MapReduce, a popular distributed platform, through the development of new algorithmics. The developed algorithms should have provably strong guarantees and demonstrate the effectiveness via empirical experiments. Considering the increasing demand for large data analysis, establishing a solid theoretical MapReduce model and developing new algorithmic ideas will have the potential to establish faster and memory efficient algorithms for distributed computing. The PIs will consider a collection of carefully chosen problems to understand in the MapReduce setting that not only have strong connections to theoretical work but also have the potential for high impact in real world Big Data applications: Clustering, Distributed Dynamic Programming, and Limitations of MapReduce. This will be done in parallel with the attempt to better understand the currently accepted MapReduce models that have been developed and to perhaps further refine them to better connect models with practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717388","SHF: Small: Developing a Highly Efficient and Accurate Approximation System for Warehouse-Scale Computers with the Sub-dataset Distribution Aware Approach","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2017","06/06/2017","Jun Wang","FL","University of Central Florida","Standard Grant","Yuanyuan Yang","05/31/2020","$450,000.00","","Jun.Wang@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7798","7923, 7941","$0.00","Despite the fact that today's warehouse-scale computers supply enormous data processing capacity, getting an ad-hoc query answer from a big dataset remains challenging. To attack the problem, recent years have seen one trend to exploit approximate computing to achieve faster execution on a much smaller sample of the original data by sacrificing result accuracy to a reasonable extent. Both offline based sampling approaches and online cluster sampling solutions have been gradually deployed in a real world to accelerate big data query. Educational benefits arise from broadening the experience of students from a top ranked Hispanic Ph.D. degree awarding institution and enhanced computer science/engineering curriculum activities. The online cross-institution undergraduate elective course about warehouse-scale computer and big data will be helpful in providing a re-imagined learning experience that makes optimum use of today's technologies supplemented by a broad range of media-rich study materials that students from three different universities.<br/><br/>There are major difficulties in developing an integrated hardware and software, scalable approximation system. The main challenge is to minimize the total size of accessed data and its associative I/O overhead subject to a given error bound. Existing popular cluster sampling with equal probability solutions do not deal well with many real-world applications following a non-uniform distribution. This research aims to tackle those challenges by investigating new sub-dataset distribution aware methods to capture sub-dataset distributions especially for non-uniform types, applying cluster sampling with unequal probability to address the inefficient sampling and large variance problem caused by non-uniform sub-dataset distribution, and taking into account the unique properties of sampling process to match with the computer hardware features, such as SSD arrays to unleash their full potential. The research will ensure future big data approximation system enables high velocity of big-data analytics to revolutionize the way that people interact with the world; and high productivity improvement of the economic impact through the efficient and effective data processing."
"1637360","AitF: Spectral Methods in the Field: New Tools for Discovering Latent Structure in Societal-Scale Data","CCF","Algorithms in the Field","09/01/2016","01/18/2017","Sham Kakade","WA","University of Washington","Standard Grant","Maria Zemankova","08/31/2019","$600,000.00","Joshua Blumenstock","sham@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7239","","$0.00","The rapid rise in the use of mobile phones, social media, and digital sensors has created opportunities to observe and understand the rapidly changing structure of populations around the world. In particular, the data captured on these population-scale digital networks can inform policy-relevant questions about the evolving nature of societies around the world. For example, policymakers would like to know how idiosyncratic violence impacts the resilience of local communities, how the presence of foreign troops changes local patterns of interaction, and how draughts and natural disasters affect feelings of national solidarity. Unfortunately, appropriate models and algorithms do not exist to make sense of evolving, societal-scale data. This project will develop scalable algorithms to help make sense of real-world networked sensor data, with the potential for significant impact in the increasingly connected global society.<br/><br/> The technical focus of this project is on adapting recent algorithmic advances in the theoretical computer science and machine learning literatures to real-world, societal-scale network data. This approach will leverage recent advances in spectral methods, which provide provably efficient algorithms for estimating hidden structure in data and improve upon the state of the art in three important ways. The first objective is to adapt and scale current spectral models to real-world datasets with millions of interconnected actors, which have weighted and directed edges with heavy-tailed degree distributions. The second goal is to translate existing methods to dynamic regime, to address the non-stationary nature of real-world data. The third goal is to characterize the computational and statistical limits of what can be achieved with these models. The algorithms and tools developed through this research will be made available to the broader academic community via open source code repositories such as GitHub and BitBucket."
"1350590","CAREER: Computational and Statistical Tradeoffs in Massive Data Analysis","CCF","COMM & INFORMATION FOUNDATIONS","02/01/2014","06/17/2018","Venkat Chandrasekaran","CA","California Institute of Technology","Continuing grant","Phillip Regalia","01/31/2020","$475,000.00","","venkatc@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7797","1045, 7936","$0.00","In modern signal processing, one is frequently faced with statistical inference problems involving massive datasets. For example, the experiments at the Large Hadron Collider at CERN generate hundreds of petabytes of data each year, which must be stored and processed efficiently in order to further our understanding of particle physics. Similar challenges also arise in seismic monitoring where massive amounts of data are acquired over large areas via cellphone accelerometers. Analyzing such large datasets is usually viewed as a substantial computational challenge. However, if data are a signal processor?s main resource then access to more data should be viewed as an asset rather than as a burden, and larger datasets should lead to a reduction in the runtime of data analysis algorithms.<br/><br/>This project blends concepts from computer science and from statistical signal processing to address the challenge with massive datasets by developing ?algorithm weakening? frameworks in which a data analysis procedure backs off to simpler methods as the data scale in size, leveraging the growing inferential strength of the data to ensure that a desired level of statistical accuracy is achieved with reduced runtime.  The approach is concretely illustrated across a range of statistical estimation tasks, with convex relaxation techniques playing a prominent role as an algorithm weakening mechanism.  In seeking a precise characterization of the computational and statistical tradeoffs obtained via convex relaxation, the investigator formalizes and studies new measures for characterizing the quality of approximation of one convex set by another.  An interesting feature of this research is that convex relaxations which provide poor performance in combinatorial optimization problems may nonetheless yield useful solutions when employed in problems with inferential objectives."
"1714417","AF:  Small:  Fast and accurate computational tools for large-scale evolutionary inference:  a phylogenetic network approach","CCF","COMPUTATIONAL BIOLOGY","08/15/2017","06/15/2017","Kevin Liu","MI","Michigan State University","Standard Grant","Mitra Basu","07/31/2020","$404,748.00","","kjl@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7931","7923, 7931","$0.00","A grand challenge in science is reconstructing the ""Tree of Life"", which is the phylogeny, or evolutionary history, of all species on Earth. The notion of a Tree of Life reflects Darwin's view of evolution as ""tree-like"", where bifurcating speciation results in an ancestral species giving rise to two genetically isolated descendant species. However, recent studies have challenged this view. ""Non-tree-like"" evolution due to inter-species gene flow - where DNA is shared between species existing at the same time - has significantly shaped the evolution of a far greater diversity of species than was ever thought possible, including humans and Neandertal, mice, and butterflies. In these cases, the phylogeny cannot be described by a tree, but is instead a more general structure known as a phylogenetic network. Our understanding of evolution and biology is at a crossroads. How frequently is the traditional assumption of tree-like evolution violated in the Tree of Life? What is the evolutionary role of gene flow? Applications include understanding the spread of antibiotic resistance among bacteria, which costs the U.S. over $35 billion and a loss of 23,000 lives annually, and pesticide resistance in weeds, mice, and other pests, which costs the U.S. billions of dollars annually.<br/> <br/>Phylogenetics, or the discipline which seeks to reconstruct evolutionary histories using biomolecular sequences and other biological data, can shed new light into these questions. Two ingredients are necessary for phylogenetic reconstruction and analysis: suitable biological data for the organisms under study, and computational methods capable of efficiently and accurately analyzing the data. Today, biological data abounds due to recent biotechnological advances, and large-scale datasets are common. However, computational methods have not kept pace. New computational frameworks are needed for fast and accurate phylogenetic network inference and analysis in the era of ""big data"".<br/> <br/>To address these challenges, this project will create new computational frameworks for fast and accurate network-based phylogenetic inference using large-scale genomic sequence datasets and evolutionary analysis of continuous biological data. The new methodologies will be validated using a comprehensive performance study. More broadly, this project will enable student training, scientific outreach, open-source software development, and scientific research that may yield new biological and biomedical discoveries.<br/><br/><br/>Phylogenies are typically inferred using computational analysis of biomolecular sequence data, and phylogenetic comparative methods are used for evolutionary analysis of continuous biological data (e.g., trait data). Today, ""big data"" challenges abound due to rapid advances in sequencing and related biotechnologies. In particular, large-scale datasets with hundreds of genomes are now common. The state of the art of phylogenetic inference therefore faces two critical scalability challenges: (1) the number of organisms in a study, and (2) greater evolutionary divergence reflecting the complex interplay of tree-like and non-tree-like evolution. For discrete sequence data, state-of-the-art methods address the second challenge, but are not scalable beyond inputs with a few dozen genomes; for continuous data, scalable approaches are needed to address the second challenge in the context of phylogenetic uncertainty and adaptive evolution. <br/><br/>The proposed research creates new computational approaches that address both challenges for discrete sequence data and continuous data. The first objective is to create a novel computational framework for scalable phylogenetic network inference using large-scale genomic sequence data. The framework makes use of the multi-species network coalescent model to account for genetic drift, incomplete lineage sorting, and gene flow as well as traditional substitution-based models of sequence evolution. The framework builds on the PI's work on large-scale phylogenetic tree inference by adapting divide-and-conquer algorithms to the more general case of networks, resulting in accurate and efficient inference. The second objective is to develop novel stochastic models and methods for analyzing continuous character evolution on phylogenetic networks. The new models will generalize widely-used non-neutral models of continuous character evolution that assume tree-like evolution, and will be used to create new methods for phylogenetic inference using heterogeneous large-scale inputs. The third objective is to validate the new computational methodologies using new empirical and synthetic benchmarks. The empirical benchmarks include mouse, plant, and fungal datasets that have been produced through ongoing collaborations."
"1525978","AF: Small: Homological Methods for Big Enough Data","CCF","ALGORITHMIC FOUNDATIONS","08/01/2015","07/21/2015","Donald Sheehy","CT","University of Connecticut","Standard Grant","Rahul Shah","07/31/2019","$340,954.00","","donald@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7796","7923, 7929","$0.00","How do we know if big data is big enough? As algorithms make more and more decisions from data, we also need these algorithms to assure us that the decisions were well-informed, i.e. that enough data went into them. The theory of homological sensor networks, a branch of topological data analysis, was originally created to test if a collection of sensors covers a domain of interest, but the same theory can test if a data set ""covers"" an underlying decision space. Homological methods can complement, extend, and even replace statistical methods to give confidence in the completeness of a data set. Because they are topological, they can give robust signatures or summaries of data that are invariant to a wide range of implicit or explicit transformations. This project aims to extend the theoretical and algorithmic foundations of the homological sensor networks to be applicable in data analysis. Broader impacts include strengthening connections between theoretical computer science (TCS) and applied algebraic topology, and widening the range of data analyses to which topological methods and tools apply.<br/><br/>The PI will train both undergraduate and graduate researchers by incorporating advanced concepts in combinatorial topology in undergraduate and graduate curricula. The PI will also educate the larger TCS and data analysis communities through expository videos and open source software.<br/><br/>The specific aim of the proposal is to extend guarantees on homological sensor networks to apply to non-smooth sets, k-coverage, and dynamic coverage.  A second specific aim is to push these algorithmic results back into the theoretical foundations of the sampling theories that underlie data analysis problems, by extending the so-called Persistent Nerve Theorem and defining new classes of near-homeomorphisms to capture the realities of unknown transformations in data while still providing theoretical guarantees.  The third specific aim is to develop algorithms that extract information from what was traditionally called ""topological noise"" as simple experiments reveal that although it doesn't carry topological information, it does carry useful geometric information that may be used for classification and inference."
"1763540","SHF: Medium: Collaborative Research: ECC: Ephemeral Coherence Cohort for I/O Containerization and Disaggregation","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","05/21/2018","Marc Snir","IL","University of Illinois at Urbana-Champaign","Continuing grant","Almadena Y. Chtchelkanova","05/31/2021","$328,846.00","Franck Cappello","snir@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7924, 7942","$0.00","Leadership computing facilities for high-performance computing (HPC) have a huge investment in the file and storage systems. The reason is that the HPC storage system often is the Achilles Heel of HPC systems, as it is fraught with numerous scenarios for contention, congestion and performance variability. This problem is getting worse due to: (a) the increased importance of data-driven HPC and the growth in the amount of data generated by large-scale simulation; and (b) the slower growth of disk speed, as compared to CPU speed. The addition of high-bandwidth persistent memory devices as burst-buffers brings in new opportunities for fast caching of application data while still allowing data persistence. However, the conventional approach of exploiting burst-buffers as yet another caching layer cannot reduce the lengthy and costly data processing steps in the deep I/O stack or reconcile occasional contentions inside the complex storage system. This project, therefore, seeks to exploit burst-buffers as repositories of persistent application-specific parallel file systems, with a lifetime commensurate to the lifetime of an application or an application campaign on a HPC system. This is a collaborative project between University of Illinois at Urbana-Champaign and Florida State University. <br/><br/>This project formulates a research framework called Ephemeral Coherence Cohort (ECC) that offers an abstraction to represent the active collection of application data through containerization, insulate I/O activities across different applications, and enable storage disaggregation for ephemeral allocation and dynamic utilization of burst buffers. The proposed ECC framework aims to enhance a variety of mission-critical applications running on the Department of Energy and the National Science Foundation leadership computing facilities. The project strengthens the collaboration between University of Illinois Urbana-Champaign and the Florida State University. The project has plans to organize panels and birds-of-feather sessions on burst buffer research in the upcoming HPC conferences and collaborate with leaders of super-computing centers for wider community penetration with techniques from this research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618866","AF: Small: Approximation Algorithms and Data Structures for Geometric Retrieval","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","05/31/2016","David Mount","MD","University of Maryland College Park","Standard Grant","Tracy Kimbrel","08/31/2019","$407,537.00","","mount@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7796","7923, 7926, 7929","$0.00","Many data sets can profitably be viewed as points in a<br/>high-dimensional space: think of employee salary and seniority,<br/>weather stations' hourly reports of temperature, humidity, wind speed,<br/>and direction, or even the images from your cellphone camera as color<br/>values per pixel.  Geometric retrieval problems seek to preprocess<br/>multi-dimensional geometric data for rapid access; for two examples,<br/>""nearest neighbor queries"" could find images similar to a query image,<br/>and ""range queries"" could find all times with temperature and humidity<br/>above given thresholds.  These queries are of fundamental importance throughout<br/>engineering and science, and have applications in knowledge discovery<br/>and data mining, pattern recognition and classification, machine<br/>learning, data compression, multimedia databases, document retrieval,<br/>and statistics.<br/><br/><br/>The high computational complexity of nearest neighbor and range queries in high dimensions has<br/>inspired research into approximate solutions. The work of this project<br/>deepens and broadens our understanding of the computational<br/>complexity of these two problems. It studies new, more efficient solutions to<br/>key special cases, including<br/>low-complexity approximations to convex bodies, faster algorithms for<br/>polytope membership queries, applications to approximate nearest<br/>neighbor searching, efficient approximation algorithms for Euclidean<br/>minimum spanning trees, and range searching with structural queries.<br/>These improved algorithms will lead to more efficient solutions in the<br/>applications described above.<br/><br/><br/>Software systems and libraries developed as part of this project will be<br/>made freely available over the Web to help scientists and engineers in<br/>other disciplines. The algorithms developed as a part of this project<br/>will be incorporated into graduate and undergraduate courses at the<br/>University of Maryland. Course materials will be made available over the<br/>Web as a resource for researchers interested in learning more about this<br/>area."
"1617653","AF: Small: Collaborative Research: Algorithmic and Computational Frontiers of MapReduce for Big Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","05/25/2016","Sungjin Im","CA","University of California - Merced","Standard Grant","Rahul Shah","06/30/2020","$247,230.00","","sim3@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092598670","CSE","7796","7923, 7934","$0.00","Modern science and engineering heavily relies on processing massive data sets and the size of the data requires applications to run using distributed computing frameworks.  However, many existing methods essential to the applications are not easily adapted to work in distributed settings. This project aims to develop new efficient ways of processing large data sets in widely used distributed computing platforms. The project will reveal new methods for processing diverse and complex data sets of massive size and allow for various applications scale to large inputs. The work has the potential to fundamentally change algorithmic techniques used in distributed computing, helping to shape big data research, the computing industry, and the growing economy reliant on big data analysis. Research outcomes will be integrated with education by writing an extensive survey/tutorial on the core algorithmic ideas used in the new discoveries to make the ideas transparent to the algorithmic developers and practitioners. The PIs will make some of the discovered algorithmic ideas accessible even to undergraduate students, helping them get prepared to cope with algorithmic challenges in distributed computing for large data sets. Special efforts will be made to include women and minorities in advising and mentoring plans.<br/><br/>The main goal of the project is to find new ways of unlocking the underlying power of MapReduce, a popular distributed platform, through the development of new algorithmics. The developed algorithms should have provably strong guarantees and demonstrate the effectiveness via empirical experiments. Considering the increasing demand for large data analysis, establishing a solid theoretical MapReduce model and developing new algorithmic ideas will have the potential to establish faster and memory efficient algorithms for distributed computing. The PIs will consider a collection of carefully chosen problems to understand in the MapReduce setting that not only have strong connections to theoretical work but also have the potential for high impact in real world Big Data applications: Clustering, Distributed Dynamic Programming, and Limitations of MapReduce. This will be done in parallel with the attempt to better understand the currently accepted MapReduce models that have been developed and to perhaps further refine them to better connect models with practice."
"1563742","CIF: Medium: Collaborative Research: Frontiers in coding for cloud storage systems","CCF","COMM & INFORMATION FOUNDATIONS","03/01/2016","03/14/2018","Venkatesan Guruswami","PA","Carnegie-Mellon University","Continuing grant","Phillip Regalia","02/29/2020","$417,260.00","","guruswami@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7924, 7935, 9251","$0.00","Cloud storage systems increasingly form the backbone of software services that underwrite everyday lives, serving a large array of businesses and forming an essential pillar of the economy. Given the sheer volume of interactions within these distributed systems, there arise a multitude of issues in building, maintaining and enhancing them. The most salient of these challenges are in the reliability, availability, consistency, confidentiality and privacy of data stored in these vast systems. <br/><br/>The proposed research is expected to have a significant impact on the manner in which cloud storage systems are designed and deployed. In these systems, storage node failures can have a significant impact on the efficiency of the overall system. This project enhances fault tolerant mechanisms to enable efficient recovery from failures, while augmenting the overall data availability and privacy offered by such systems. The research effort will advance the science of cloud computing by developing a new family of algorithms for distributed storage , and connect the advances to the significant industry needs in this topic. The research agenda will also be tightly integrated with education and outreach activities with direct involvement of underrepresented minorities, graduate, and undergraduate students.<br/><br/>Focusing on efficient maintenance of data with a range of desirable qualities, including mechanisms that ease data encoding, accessibility, updates, as well as privacy, the main objectives of this effort include (i) to develop coding schemes where a failed element can be regenerated with higher repair efficiencies from its local neighbors, (ii) to bring together the advantages of both local decodability and local repairability into one coding solution, (iii) to design mechanisms that provide low cost data updates in addition to efficient repair, (iv) to develop codes that can be resilient against failures with different scales/modalities, (v) to develop coding mechanisms taking advantage of implementation aspects of existing systems, and (vi) to develop coding schemes that enable users to access their data in a private manner. This effort addresses these challenges using a combination of tools from  disciplines spanning coding theory, information theory, communications, as well as combinatorial and discrete mathematics."
"1755791","CRII: AF: RUI: Faster and Cache-Efficient Similarity Filters and Searches for Big Data","CCF","ALGORITHMIC FOUNDATIONS","02/15/2018","02/15/2018","Mayank Goswami","NY","CUNY Queens College","Standard Grant","Rahul Shah","01/31/2020","$175,000.00","","Mayank.Goswami@qc.cuny.edu","65 30 Kissena Blvd","Flushing","NY","113671575","7189975400","CSE","7796","7796, 7926, 8228","$0.00","Analysing and organising the massive amount of data collected everyday is one of the biggest challenges faced by computer scientists. This impacts almost every field of modern human life, such as health care, military applications, smart cities, transportation, networks, etc. To quickly analyse and organise such huge amounts of data, one needs to develop space and time efficient algorithms, that accelerate the search for information while minimising memory requirements. This project aims to develop space-efficient similarity filters, which quickly filter out queries that have very little similarity from the records existing in the database. This has applications in security, databases, machine learning, file systems, vision, pattern recognition, compression and networks. Apart from the targeted impact of these applications to society, this project also aims at producing the next generation of researchers and promoting under-represented groups in science and technology. Minority undergraduate and high school students will be involved in this project. Queens College, CUNY is a diverse institution, representing a wide range of ethnic minorities and has 28% Hispanic students. In an effort to engage more undergraduate students and women in research (Queens College has roughly 56% female students), the PI plans to offer a new course that develops an understanding of approximate membership and similarity search data structures in the setting of big data.<br/><br/><br/>Similarity searching is a fundamental problem in this context, where a massive data of records needs to be organised so as to answer quickly queries of the form: given a record, is it similar to some record in the data? Similarity search, modeled as near(est) neighbor search (NNS) is an important and well-studied problem with numerous applications: Given a set P of n points lying in some d-dimensional metric space, the goal is to preprocess P so as to return for a given query point q the point p in P that is closest to q. Exact versions of this problem are hard to solve, and all solutions to the approximate version of the problem require superlinear (more than nd) space. This space usage is prohibitive for many big data settings, where not just n, but d could be huge (e.g., the number of pixels in an image). This project is aimed at developing the theory and applications of Similarity Filters, which are decision (yes/no) based data structures. Given a query q, a (c, r)-similarity filter always returns yes if q is within a distance r of some input point, and returns no with a small error probability (thus some false positives are allowed) if all input points are at least cr away from the query. Recent results from the researcher shows that such filters can be built with sublinear (asymptotically less than nd) space. The project takes on two tasks: 1) to understand the approximation factor-space-query tradeoff for similarity filters, developing tight upper and lower bounds, especially in the sublinear space regime, and 2) to develop I/O efficient Similarity Filters and nearest neighbor data structures, akin to quotient or cascade filters for the approximate membership problem (where the in-RAM Bloom Filter is the standard data structure, used in many networking applications). The overarching goal is to use similarity filters on RAM in conjunction with a disk-based data structure: faraway queries will be filtered quickly, and nearby queries will have their neighbor(s) returned following a slower but cache friendly search. The availability of such a two-layered space-and-time-efficient data structure will greatly boost the scope of applications when the amount of data is massive, especially in databases, networks, security and machine learning."
"1652218","CAREER: Algorithmic Challenges and Opportunities in Spatial Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","02/15/2017","02/27/2019","Donald Sheehy","CT","University of Connecticut","Continuing grant","Rahul Shah","01/31/2022","$341,853.00","","donald@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7796","1045, 7929","$0.00","Spatial data takes many forms including configuration spaces of robots or proteins, collections of shapes or measures, and physical models and measurements from new sensing technologies. These data sets often contain intrinsic, nonlinear, low-dimensional structure hidden in complex high-dimensional input representations.<br/>  To uncover such structure one needs to adapt to local changes in scale, recognize multiscale structure, represent the intrinsic space underlying the data, compute with coarse approximate distances, and integrate heterogeneous data into meaningful distance functions. There is a need for algorithms and data structures that can search, represent, and summarize such data sets efficiently.  The PI will develop new data structures, models of computation, sampling theories, sampling algorithms, and metrics for addressing these challenges.<br/>  <br/>  The specific aim of the project is to adapt hierarchical metric data structures to work with locally adaptive distances using new models of computation that only use approximate distance comparisons.  These models acknowledge the reality that with sufficiently complex data, even a single distance computation can be expensive.  A second specific aim is to develop new multiscale sampling theories as well as new algorithms for computing such samples.  These samples and sampling algorithms will be applicable to a wide range of problems and will extend and generalize greedy and farthest-point strategies.  A third specific aim is to develop algorithms for new metrics and distance functions for heterogeneous data to more accurately represent intrinsic structure in data.  These algorithms will generalize methods used in both Voronoi refinement mesh generation and topological data analysis.<br/><br/>  The project will make geometric methods applicable to a much wider range of problems and with that comes the need for wider understanding of advanced geometry and topology.  The PI will integrate research and education, introducing computer science students at both the undergraduate and graduate level to foundational ideas in spatial data analysis, from geometry to topology.  The PI will also work one-on-one to mentor undergraduates from traditionally underrepresented groups and help train high school teachers."
"1409130","AF: Medium: Collaborative Research: Multi-dimensional Scheduling and Resource Allocation in Data Centers","CCF","ALGORITHMIC FOUNDATIONS","08/01/2014","06/12/2017","Sungjin Im","CA","University of California - Merced","Continuing grant","Tracy Kimbrel","07/31/2019","$397,007.00","","sim3@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092598670","CSE","7796","7924, 7926, 7932, 9251","$0.00","Data centers and cluster computing platforms have become the dominant computational paradigms of the past decade, becoming the de facto method of executing Big Data workloads. Typically, the entire cluster is treated as a set of resources shared by multiple clients who submit jobs requiring different types of resources. To add to this heterogeneity (or dimensionality) in resource requirements, machines in a cluster can be heterogeneous in terms of resources they provide. In such settings, resources need to be allocated and priced appropriately so as to balance performance with demand.<br/><br/>In this project, the PIs seek to study job scheduling in data centers to optimize temporal Quality of Service metrics such as response time along with fairness, when jobs can have resource requirements that are multi-dimensional.  The main question studied can be phrased as: How does the temporal nature of job scheduling interplay with the dimensionality of resource requirements, and how do these two in turn interact with the classical economic desiderata of incentives and fairness?<br/><br/>This project is differentiated from previous work in aiming to develop appropriate models and algorithms through the lens of theoretical computer science, particularly by a fusion of the disparate fields of approximation and online algorithms, algorithmic game theory, and stochastic optimization. The resulting insights will be used to also develop new techniques to address classical scheduling and game theoretic problems that have defied successful solutions.  The project is interdisciplinary, and the theoretical models and techniques developed will be motivated by the application domain of new hardware architectures stemming from emerging technologies, and the heterogeneity arising from provisioning them within a data center. Further, empirical validations will be performed, both via simulation on traces from data center executions, as well as deployment and experiments on clusters.  This will ultimately influence the design and deployment of internet systems that use and generate massive data.<br/><br/>The interdisciplinary nature of the project points to not only the need for training a pipeline of students from high school students to graduates and imparting to them the power of algorithmic thinking and its broader relevance, but also the necessity for bringing scientists, mathematicians, and system builders to the same platform for active exchange of ideas. Towards this end, the PIs seek to equip the next generation of students, including women and minorities, with the relevant algorithmic skills by an education plan that includes effective teaching and mentorship, as well as to broadly disseminate the proposed work by organizing workshops and by writing books and surveys."
"1718970","CCF-BSF:  AF:  Small:  Convex and Non-Convex Distributed Learning","CCF","ALGORITHMIC FOUNDATIONS","01/01/2018","07/10/2017","Nathan Srebro","IL","Toyota Technological Institute at Chicago","Standard Grant","Tracy Kimbrel","12/31/2020","$249,978.00","","nati@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7926, 7934","$0.00","Machine learning is an increasingly important approach in tackling many difficult scientific, engineering and artificial intelligence tasks, ranging from machine translation and speech recognition, through control of self driving cars, to protein structure prediction and drug design. The core idea of machine learning is to use examples and data to automatically train a system to perform some task. Accordingly, the success of machine learning is tied to availability of large amounts of training data and our ability to process it. Much of the recent success of machine learning is fueled by the large amounts of data (text, images, videos, etc) that can now be collected.  But all this data also needs to be processed and learned from---indeed this data flood has shifted the bottleneck, to a large extent, from availability of data to our ability to process it.  In particular, the amounts of data involved can no longer be stored and handled on single computers. Consequently, distributed machine learning, where data is processed and learned from on many computers that communicate with each other, is a crucial element of modern large scale machine learning.<br/><br/>The goal of this project is to provide a rigorous framework for studying distributed machine learning, and through it develop efficient methods for distributed learning and a theoretical understanding of the benefits of these methods, as well as the inherent limitations of distributed learning. A central component in the PIs' approach is to model distributed learning as a stochastic optimization problem, where different machines receive samples drawn from the same source distribution, thus allowing methods and analysis that specifically leverage the relatedness between data on different machines. This is crucial for studying how availability of multiple computers can aid in reducing the computational cost of learning.  Furthermore, the project also encompasses the more challenging case where there are significant differences between the nature of the data on different machines (for instance, when different machines serve different geographical regions, or when each machine is a personal device, collecting data from a single user). In such a situation, the proposed approach to be studied is to integrate distributed learning with personalization or adaptation, which the PIs argue can not only improve learning performance, but also better leverage distributed computation.<br/><br/>This is an international collaboration, made possible through joint funding with the US-Israel Binational Science Foundation (BSF).  The project brings together two PIs that have worked together extensively on related topics in machine learning and optimization."
"1750640","CAREER: A Stable Foundation for Trustworthy Data Analysis","CCF","ALGORITHMIC FOUNDATIONS","02/01/2018","02/11/2019","Jonathan Ullman","MA","Northeastern University","Continuing grant","Tracy Kimbrel","01/31/2023","$208,514.00","","jullman@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7796","1045, 7926","$0.00","Every day, massive amounts of data are collected, analyzed, and used to make high-stakes decisions, raising many questions about how to use this data in a trustworthy manner.  This project is about two such questions: (1) How can researchers prevent false discovery, and use data to learn meaningful facts about a population without overfitting to that data?  Despite decades of research into methods for preventing false discovery, it remains a vexing problem for the scientific community.  (2) How can researchers use valuable but sensitive data to learn about a population without compromising the privacy of individuals in that data?  This task has proven to be quite delicate, and there have been several high profile attacks on supposedly anonymous datasets, causing a lack of confidence in the most commonly used approaches.  <br/><br/>Although they may seem unrelated, surprisingly, both of these questions can be addressed using stable algorithms---algorithms that are insensitive to small changes in their inputs.  In the past decade, differential privacy emerged as a strong form of algorithmic stability that guarantees a high degree of individual privacy, yet admits highly accurate data analysis.  More recently, differential privacy has been shown to prevent false discovery in interactive data analysis---the common scenario where the same dataset is analyzed repeatedly, which has been implicated in a ""statistical crisis in science.""<br/><br/>This project will take a unified approach to advancing the state-of-the-art in privacy and false discovery via algorithmic stability.  The main outcomes of this project will be building the theoretical foundations of interactive data analysis, developing new computationally efficient stable algorithms for central problems in these areas, understanding the limits of privacy and interactive data analysis both in theory and in practice, and broadening the reach of algorithmic stability to address other challenges in trustworthy data analysis."
"1452795","CAREER: Novel Algorithms for Dynamic Network Analysis in Computational Biology","CCF","ALGORITHMIC FOUNDATIONS, COMPUTATIONAL BIOLOGY","03/01/2015","02/11/2019","Tijana Milenkovic","IN","University of Notre Dame","Continuing grant","Mitra Basu","02/29/2020","$540,000.00","","tmilenko@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7796, 7931","1045, 7796, 7931, 9102, 9251","$0.00","Broader significance and importance. Proteins are major macromolecules of life. Thus, understanding how proteins function in the cell is critical. Genomic sequence research has revolutionized understanding of cellular functioning. However, as recognized in the post-genomic era, genes (proteins) do not function in isolation. Instead, they carry out cellular processes by interacting with each other. This is exactly what biological networks model. Unlike genomic sequence data, biological network data enable the study of complex cellular processes that emerge from the collective behavior of the proteins. Thus, biological network research is promising to give new insights into principles of life, evolution, disease, and therapeutics. However, current network research deals with static representations of biological data, even though cellular functioning is dynamic. This is in part due to unavailability of experimentally-derived dynamic biological network data, owing to limitations of biotechnologies for data collection. Efficient computational strategies for both inference and analysis of dynamic biological networks are needed to advance understanding of cellular functioning compared to static biological network research. This is exactly the focus of this project. Dynamic biological network research has biological applications of societal importance, such as studying cellular changes with disease progression, drug treatment, or age, which will be explored as a part of this project. Thus, the project could contribute to global health. It may impact other domains as well, e.g., social networks. Also, this project will result in educational activities that are intertwined with its research, such as forming interdisciplinary scientists via novel curriculum development activities, or strengthening the computer science population via research supervision, career mentoring, and community outreach to K-12 and (under)graduate students, focusing on women.<br/><br/>Technical description. This proposal will result in new computational directions for dynamic biological network research. New algorithms will be developed for inference of systems-level biological networks underlying a dynamic biological process, by combining the static network topology with other data types, such as measurements of gene expression or protein abundance at different times. Then, novel methods for analyzing the dynamic network data will be developed to gain insights into the underlying cellular changes. For example, the idea of graphlets (small subgraphs), which has been well established in static biological network research, will be taken to the next level to allow for graphlet-based analyses of dynamic biological networks. Also, novel computational strategies will be designed to allow for dynamic network clustering. The proposed methods will be used in collaborative applications that encompass representative dynamic biological processes: early cancer detection and chemotherapy resistance, both in the context of pancreatic cancer, as well as studying human aging. These interdisciplinary applications will be used as concrete model systems to innovate fundamental computational research. Because network research spans many domains, open-source software implementing the new methods will be offered to researchers from diverse disciplines. The software will also serve as an educational tool. Integration of research and education will be promoted even further. Interdisciplinary student training will be offered via novel courses on network research. A literate approach to education will aim to advance students' communication skills. Proven pedagogical strategies will be used to improve student learning. Research supervision and career mentoring will be offered to K-12 and (under)graduate students, with focus on minorities and women, thus integrating diversity into the project. Interdisciplinary research and educational collaborations will allow for wide distribution of the proposed ideas and results. The results will also be disseminated through tutorial and workshop organization at renowned international conferences."
"1849136","NSF Student Travel Grant for 2018 International Workshop on String Algorithms in Bioinformatics (StringBio)","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","09/07/2018","Sharma Thankachan","FL","University of Central Florida","Standard Grant","Mitra Basu","09/30/2019","$20,000.00","","sharma.thankachan@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7798","7556, 7931, 7942","$0.00","The emerging string analytics problems from bioinformatics introduce new theoretical challenges and necessitate researchers to re-engineer already well-established solutions to cope with big data issues. Consequently, string matching algorithms and related data structures are gaining an ever-increasing interest as they form the backbone of many current genomic data processing tools. This project supports primarily undergraduate student attendance at a workshop focused solely on string matching algorithms in bioinformatics (StringBio). The workshop primarily aims to gather researchers, practitioners, and students with a different format that is composed of short tutorials and research talks. The travel award will provide partial support to 15-20 students, with preference to students from under-represented groups in order to inspire these participants to pursue higher studies not only in Bioinformatics and related interdisciplinary areas, but also in algorithms, data structures, and their applications in the newly emerging trends of big data. <br/><br/>The concise tutorials to be given by experienced researchers on foundations of string processing will cover related algorithms and data structures from standard to advanced levels. These tutorials aim to help students and computer science researchers who would like to make a quick start in bioinformatics challenges by addressing the basic building blocks and current state-of-the-art. Different from standard conference presentations of already achieved progress, the research talks by experts in StringBio will describe current and ongoing research studies to which, the attendees can join by expressing their interests and potentials. The talks are expected to provide an excellent opportunity, particularly for the beginners in the path to their research experience.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1409946","SHF: Medium: Compute on Data Path: Combating Data Movement in High Performance Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2014","04/10/2017","Yong Chen","TX","Texas Tech University","Standard Grant","Almadena Chtchelkanova","05/31/2019","$1,016,000.00","Barbara Chapman, Robert Ross, Yonghong Yan, Dries Kimpe","yong.chen@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","7798","7924, 7942, 9251","$0.00","High performance computing enabled simulation has been widely considered a third pillar of science along with theory and experimentation, and is a strategic tool in many aspects of scientific discovery and innovation. High performance computing simulations, however, have become highly data intensive in recent years due to data acquisition and generation becoming much cheaper, newer high-resolution multi-model scientific discovery producing and requiring more data, and the insight that useful data can be mined out of large amounts of data being substantially increased. <br/><br/>This project combats the increasingly critical data movement challenge in high performance computing. This project studies the feasibility of a new Compute on Data Path methodology that expects to improve the performance and energy efficiency for high performance computing. This new methodology models both computations and data as objects with a data model that encapsulates and binds them. It fuses data motion and computation leveraging programming model and compiler. It develops an object-based store and runtime to enable computations along data path pipeline. In recent years, a proliferation of advanced high performance computing architectures including multi- and many-core systems, co-processors and accelerators, and heterogeneous computing platforms have been observed. The software solution that addresses the critical data movement challenge, however, has significantly lagged behind. This project has the potential of advancing the understandings and the software solution and further unleashing the power of high performance computing enabled simulation."
"1350888","CAREER: Foundations for Geometric Analysis of Noisy Data","CCF","ALGORITHMIC FOUNDATIONS","05/15/2014","05/07/2018","Jeff Phillips","UT","University of Utah","Continuing grant","Rahul Shah","04/30/2020","$522,135.00","","jeffp@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7796","1045, 7929, 9150","$0.00","An important role of computational geometry is to understand and formalize the structure of data.  And as data is becoming a central currency of modern science, this role is growing in importance.  However, much of classical computational geometry inherently assumes that all aspects of data are known and precise.  This is rarely the case in practice.  This project focuses on building the foundations for two extensions to classic geometric settings pertinent to noisy data.  <br/><br/>1. The PI will study locational uncertainty in point sets, where the location of each data point is described by a probability distribution.  Given such an input, the goal is to formalize how to construct, approximate, and concisely represent the distribution of geometric queries on this uncertain data.  <br/><br/>2. The PI will study the geometric consequences of applying a statistical kernel (e.g. a Gaussian kernel) to a data set.  He will investigate how this process can smooth data, remove degeneracies, and implicitly simplify and regularize algorithms.  Moreover, he will explore the geometric structure of the resulting kernel density estimate, and how it relates to algorithms for the data and approximate representations of the data.  <br/><br/>The PI will lead the development of a data-focused educational program around the themes of data analysis, algorithmics, and visualization.  The PI is developing a model course for this program on data mining; it focuses on the geometric, statistical, and algorithmic properties of data.  An extensive set of course notes is being compiled, accompanied with videotaped lectures freely available online.  This class and program attract many interdisciplinary and diverse students and observers.  This program is part of a larger effort to make relevant data analysis techniques from computational geometry available to a broader data-rich audience."
"1844887","CAREER: Information Theoretic Methods in Data Structures","CCF","ALGORITHMIC FOUNDATIONS","02/01/2019","01/30/2019","Omri Weinstein","NY","Columbia University","Continuing grant","Balasubramanian Kalyanasundaram","01/31/2024","$48,310.00","","omri@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","1045, 7927","$0.00","Data structures constitute the backbone of algorithm design and information retrieval, and underlie the performance of countless industrial-scale applications, from internet routing and road navigation to cloud storage, search and compression. As such, understanding what data structures can, and cannot, compute efficiently is a fundamental question in both theory and practice. Motivated by this question and the growing amounts of data to be processed in modern databases, this project is focused on two primary themes: (1) developing new mathematical tools for proving unconditional lower bounds on the operational time and memory consumption of static and dynamic data structures, thereby reducing the (huge) existing gaps from known upper bounds; (2) exploring the interplay between information theory and data structures in large-scale storage applications. In particular, the second part of the project will focus on designing novel data structures for information retrieval (mainly compression and search) over correlated datasets. The project reflects a scientific agenda for conducting interdisciplinary research in the intersection of theoretical computer science and information theory, bridging and impacting research and education in both communities.<br/><br/>In recent years, information theory and communication complexity have emerged as powerful mathematical tools for proving unconditional lower bounds on the operational time of data structures and for analyzing the power of pre-processing information. Proving time-space lower bounds in the ""cell-probe"" model has long been recognized as one of the major challenges of complexity theory, with many implications on theory and practice. The first part of this project will develop new information-theoretic tools for proving such trade-offs, and will explore connections between data-structure lower bounds and other areas in computational complexity and mathematics, such as locally-decodable codes, circuit complexity, streaming, functional analysis, rigidity and algebraic geometry. Apart from its role as an analytic tool for computation, the interplay between information theory and data structures is particularly vital in large-scale storage applications. The increasing size and complexity of data sets being uploaded and processed on remote data centers, pose a real problem of scalability for traditional data compression and database architectures. The second part of this project establishes a new research program for storage and information-retrieval problems that arise over massive correlated data sets, focusing on the (unexplored) problem of locally decodable source coding for correlated files in static and dynamic scenarios.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1535948","AitF: FULL: Collaborative Research: Compact Data Structures for Traffic Measurement in Software-Defined Networks","CCF","Algorithms in the Field","09/01/2015","08/18/2015","Jennifer Rexford","NJ","Princeton University","Standard Grant","Tracy Kimbrel","08/31/2019","$360,000.00","","jrex@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7239","012Z","$0.00","Software-Defined Networking (SDN) is changing the way networks are designed and managed, by separating the ""control plane"" (which decides how to handle the traffic) from the ""data plane"" (which actually forwards each packet).  Many large companies---like Google, Microsoft, and Facebook---have already deployed SDN technology, and many equipment vendors support open interfaces for programming their switches.  While most work on SDN focuses on how to control the network, measuring the traffic in the network is equally important.  Traffic measurement is useful to identify congested links, denial-of-service attacks, performance problems, and configuration mistakes, and also drives decisions of how the network should forward traffic in the future.  However, the support for traffic measurement in today's commodity switches is quite primitive.  In this proposal, the PIs bring algorithmic research on so-called ""compact data structures"" to bear on the problem of programmable traffic measurement in SDNs.  Compact data structures can give approximate answers to measurement questions with limited overhead in terms of switch memory and processing resources.  <br/><br/>The project is interdisciplinary, bringing together researchers in computer networking and theoretical computer science to match practical problems with novel solutions.  The proposed research starts with designing new query abstractions for collecting traffic statistics on existing SDN switches, and then progresses to identifying new compact data structures so that future switches can support much richer traffic measurement at reasonable overhead.  The researchers have close ties with network administrators and switch vendors, allowing them to ground the project in a strong understanding of both operational requirements and hardware constraints, and also influence future SDN technology.<br/><br/>This project aims to identify a switch data-plane architecture for collecting diverse traffic statistics, as well as a small set of programmable sketches and samples for variety of analyses to trade-off accuracy and resources.  The architecture will include a measurement control API between the controller and the switch, and this needs a communication-efficient interface, along with a high-level language for specifying traffic queries, and with that, a run-time system on the controller that compiles these queries into commands to the switches with suitable CDSs.  These challenges will be addressed using OpenFlow API that is widely popular for SDNs and in new redesigns.  This is a conversation between the networking and algorithmic communities, mutually informing each other on what is possible, what is required, and ultimately what is effective and useful."
"1822949","SPX: Collaborative Research: Rethinking Data Center Abstractions Utilizing Warehouse-Scale Shared Memory","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/30/2018","David Wentzlaff","NJ","Princeton University","Standard Grant","Marilyn McClure","09/30/2022","$500,000.00","","wentzlaf@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","042Y","026Z","$0.00","Warehouse-scale computers (or data centers) are essential for the technology that people rely on every day: from making purchases, to hailing rides, to sharing experiences with friends.  Programming warehouse-scale computers requires specialized software that coordinates the many individual computers that make up the data center, while ensuring that the system will continue to operate if any machine fails.  Unfortunately, this data center software has fundamental differences from that developed for single computers (i.e., that is taught to most computer science students) resulting in long development times and poor performance.  The proposed work will bridge the gap between the software systems used in current data centers and what is available to most programmers (and computing students). The proposed work will allow individual computers within a data center to communicate through ""shared memory""---the same mechanism used within small-scale computers from phones to laptops to individual servers. The project has the potential to make warehouse scale computing much more accessible to everyone. In particular, it will allow an easy transition for software that runs on common machines (laptops, desktops) to the datacenter. Additionally, the project will create many educational opportunities through enhanced classroom projects and creation of research opportunities for undergraduates.<br/><br/>The project's distinguishing feature is a holistic design of new computing hardware and operating systems to allow this ""shared memory"" abstraction to provide both the scale and failure tolerance of specialized data center software.  While prior hardware takes an approach of ""share all"" or ""share nothing"", the proposed hardware will allow subsets of data to be shared across subsets of hardware.  Then, the operating system will be extended to automatically manage the access to this shared memory, so that programmers do not need to be aware of the difference.  By coordinating software and hardware management, the project will overcome prior scalability and failure tolerance challenges of sharing memory. It will also allow easy sharing of datacenter resources, preventing fragmentation and reducing the cost of using datacenters and cloud computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1564167","CIF: Medium: Collaborative Research: Frontiers in coding for cloud storage systems","CCF","COMM & INFORMATION FOUNDATIONS","03/01/2016","05/08/2018","Sriram Vishwanath","TX","University of Texas at Austin","Continuing grant","Phillip Regalia","02/29/2020","$408,000.00","","sriram@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7797","7924, 7935, 9251","$0.00","Cloud storage systems increasingly form the backbone of software services that underwrite everyday lives, serving a large array of businesses and forming an essential pillar of the economy. Given the sheer volume of interactions within these distributed systems, there arise a multitude of issues in building, maintaining and enhancing them. The most salient of these challenges are in the reliability, availability, consistency, confidentiality and privacy of data stored in these vast systems. <br/><br/>The proposed research is expected to have a significant impact on the manner in which cloud storage systems are designed and deployed. In these systems, storage node failures can have a significant impact on the efficiency of the overall system. This project enhances fault tolerant mechanisms to enable efficient recovery from failures, while augmenting the overall data availability and privacy offered by such systems. The research effort will advance the science of cloud computing by developing a new family of algorithms for distributed storage , and connect the advances to the significant industry needs in this topic. The research agenda will also be tightly integrated with education and outreach activities with direct involvement of underrepresented minorities, graduate, and undergraduate students.<br/><br/>Focusing on efficient maintenance of data with a range of desirable qualities, including mechanisms that ease data encoding, accessibility, updates, as well as privacy, the main objectives of this effort include (i) to develop coding schemes where a failed element can be regenerated with higher repair efficiencies from its local neighbors, (ii) to bring together the advantages of both local decodability and local repairability into one coding solution, (iii) to design mechanisms that provide low cost data updates in addition to efficient repair, (iv) to develop codes that can be resilient against failures with different scales/modalities, (v) to develop coding mechanisms taking advantage of implementation aspects of existing systems, and (vi) to develop coding schemes that enable users to access their data in a private manner. This effort addresses these challenges using a combination of tools from  disciplines spanning coding theory, information theory, communications, as well as combinatorial and discrete mathematics."
"1846369","CAREER: Guaranteed Nonconvex Optimization for High-Dimensional Learning","CCF","COMM & INFORMATION FOUNDATIONS","02/01/2019","12/07/2018","Mahdi Soltanolkotabi","CA","University of Southern California","Continuing grant","Phillip Regalia","01/31/2024","$102,562.00","","soltanol@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","1045, 7797, 7936","$0.00","Contemporary nonconvex learning approaches in signal processing and machine learning are revolutionizing our ability to process data in their natural form, bringing transformative changes to modern life ranging from web searches and social networks to healthcare, commerce, and imaging. Despite wide empirical success in applications, these learning schemes lack a clear mathematical foundation that can enable not only rigorous performance analysis but can guide system designs based on an understanding of what would work, when and why. This project aims to develop a unified framework to design and analyze efficient nonconvex optimization algorithms. The resulting signal estimation and learning algorithms will be deployed in novel applications aimed at learning understandable models from data, which in turn will allow for better systems that can acquire data faster and at higher resolution and quality. Components from this project are integrated into an advanced graduate class and select results will serve to motivate K-12 students to pursue careers in STEM (Science, Technology, Engineering and Math).<br/><br/>In this project, the investigator studies a family of iterative algorithms for nonconvex data fitting problems that arise in modern signal processing and artificial intelligence, such as phaseless imaging and neural network training. The overarching goal of the project is to understand when these algorithms converge to globally optimal solutions and to characterize their behavior and convergence rate in terms of key quantities such as the number of data samples/observations, prior knowledge about the model, initialization accuracy, etc. The theoretical investigations utilize techniques from high-dimensional probability, statistics, optimization and nonlinear dynamics in control. The theoretical analysis guides the design of more reliable learning algorithms that can seamlessly scale to massive data sizes and are robust to node failures that arise in modern distributed computing environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1737230","BSF: 2016257:  Building Models for Reading Comprehension in Specialized Domains from Scratch","CCF","SPECIAL PROJECTS - CCF","09/01/2017","08/31/2017","Vivek Srikumar","UT","University of Utah","Standard Grant","Tracy Kimbrel","08/31/2019","$35,039.00","","svivek@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","2878","014Z","$0.00","Machine learning algorithms are increasingly allowing people to search, structure, and access the textual information created daily in every possible domain. In areas with abundant annotated data, where supervised learning algorithms can be applied, algorithms for text understanding have had success in structuring text and providing natural language interfaces. When building a system in a new domain for which there is little to no data, however, data collection and annotation data can be prohibitively expensive.  This project explores a protocol for developing text understanding systems that read text and provide a natural language interface in a particular domain (such as biology or history) -- this can allow specialized communities to have digital access to data that is otherwise locked in text.  The project also trains students as part of an international collaboration -- this award supports travel of the US-based researchers to collaborate in a project funded by the US-Israel Binational Science Foundation.   <br/><br/>The project encompasses both data collection and model training, and considers the interaction between the two. To replace expert annotations it uses crowdsourcing workers in an iterative procedure that starts training a structured predictor from almost no data. It creates an interactive framework in which users ask questions and verify candidate answers that are later used to retrain the system. It aims to jointly train over multiple domains, and use domain adaptation methods to transfer knowledge from one domain to another. <br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1565719","CRII: AF: Novel evolutionary models and algorithms to connect genomic sequence and phenotypic data","CCF","","08/01/2016","02/29/2016","Kevin Liu","MI","Michigan State University","Standard Grant","Mitra Basu","07/31/2019","$174,968.00","","kjl@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","026y","7796, 7931, 8228","$0.00","An organism's genome is the collection of all of its DNA, which can be written as a single string. All of the biological complexity of an organism is encoded within its genome. One of the greatest challenges in science today is understanding how the ""syntax"" of the genome gives rise to the ""semantics"" of biological function and, indeed, all life on Earth. The theory of evolution offers a way forward. These seemingly heterogeneous biological features are merely facets of the common evolutionary process by which they arose. By querying their phylogeny, or evolutionary history, we can begin to decipher the living language in which genomes are written and, ultimately, master it for our own purposes.<br/><br/>Today, phylogenies are primarily reconstructed by computational analysis of biomolecular sequence data. Crucially, state-of-the-art algorithms treat genomes as an unordered bag of observations, not as an ordered sequence of observations. This assumption is made for pure mathematical convenience. In contrast, DNA is a linear molecule, the information encoded in the genome is understood to be sequential, and its order matters greatly. Recombination is one of the major evolutionary processes that rearranges genomes over time, ultimately shaping the sequential ordering of information. Sequence dependence due to recombination (or the lack thereof) is an essential aspect of the computational problem of phylogenetic inference, and yet the common assumption that loci (positions in the genome) are independent and identically distributed remains a major methodological gap.<br/><br/>To address this critical need, this project will create new evolutionary models and algorithms for inferring species phylogenies from genomes while accounting for point mutations, genetic drift, and recombination. A connection is then forged to systems biology by building the new evolutionary models into a new computational method for mapping the genomic architecture of complex phenotypes (observable traits). The new methods will be validated using an extensive performance study incorporating empirical and synthetic data. Analyses of the empirical data are anticipated to result in new biological discoveries such as understanding the genetic basis of adaptive traits in house mouse, the most widely used laboratory organism.<br/><br/>This project incorporates significant educational and outreach components. The new mathematical models, algorithms, and tools proposed in the research objectives will be the basis for two workshop series: one targeted to evolutionary computation researchers and the other to evolutionary biologists. Interdisciplinary training at the undergraduate and graduate level includes underrepresented minority students. Open implementations of all methods and data will be publicly available through a collaborative online community. <br/><br/>This project entails three integrated research objectives. First, algorithms for inferring species phylogenies under a new combined model of point mutations, genetic drift, and recombination will be developed. The combined model unites the coalescent model of population genetics with a hidden Markov model to capture varying degrees of sequence dependence among neighboring loci due to recombination. A key challenge is scalability, which is addressed using new approximation algorithms. Second, the new evolutionary models will be fused with a linear mixed model to capture dependence between genomic loci and a trait encoded by causal loci within the genome. The new models will be the basis for new algorithms that address several related problems in functional genomics. One application is association mapping, which seeks to infer causal loci based upon significant correlation between allele frequencies and observed trait values. Third, a performance study will be conducted to validate the new computational methodologies."
"1823032","SPX: Collaborative Research: Rethinking Data Center Abstractions Utilizing Warehouse-Scale Shared Memory","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/30/2018","Henry Hoffmann","IL","University of Chicago","Standard Grant","M. Mimi McClure","09/30/2022","$444,491.00","","hankhoffmann@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","042Y","026Z","$0.00","Warehouse-scale computers (or data centers) are essential for the technology that people rely on every day: from making purchases, to hailing rides, to sharing experiences with friends.  Programming warehouse-scale computers requires specialized software that coordinates the many individual computers that make up the data center, while ensuring that the system will continue to operate if any machine fails.  Unfortunately, this data center software has fundamental differences from that developed for single computers (i.e., that is taught to most computer science students) resulting in long development times and poor performance.  The proposed work will bridge the gap between the software systems used in current data centers and what is available to most programmers (and computing students). The proposed work will allow individual computers within a data center to communicate through ""shared memory""---the same mechanism used within small-scale computers from phones to laptops to individual servers. The project has the potential to make warehouse scale computing much more accessible to everyone. In particular, it will allow an easy transition for software that runs on common machines (laptops, desktops) to the datacenter. Additionally, the project will create many educational opportunities through enhanced classroom projects and creation of research opportunities for undergraduates.<br/><br/>The project's distinguishing feature is a holistic design of new computing hardware and operating systems to allow this ""shared memory"" abstraction to provide both the scale and failure tolerance of specialized data center software.  While prior hardware takes an approach of ""share all"" or ""share nothing"", the proposed hardware will allow subsets of data to be shared across subsets of hardware.  Then, the operating system will be extended to automatically manage the access to this shared memory, so that programmers do not need to be aware of the difference.  By coordinating software and hardware management, the project will overcome prior scalability and failure tolerance challenges of sharing memory. It will also allow easy sharing of datacenter resources, preventing fragmentation and reducing the cost of using datacenters and cloud computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1535878","AitF: FULL: Collaborative Research: Compact Data Structures for Traffic Measurement in Software-Defined Networks","CCF","Algorithms in the Field","09/01/2015","08/18/2015","Shanmugavelayu Muthukrishnan","NJ","Rutgers University New Brunswick","Standard Grant","Tracy J. Kimbrel","08/31/2019","$360,000.00","","muthu@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7239","012Z","$0.00","Software-Defined Networking (SDN) is changing the way networks are designed and managed, by separating the ""control plane"" (which decides how to handle the traffic) from the ""data plane"" (which actually forwards each packet).  Many large companies---like Google, Microsoft, and Facebook---have already deployed SDN technology, and many equipment vendors support open interfaces for programming their switches.  While most work on SDN focuses on how to control the network, measuring the traffic in the network is equally important.  Traffic measurement is useful to identify congested links, denial-of-service attacks, performance problems, and configuration mistakes, and also drives decisions of how the network should forward traffic in the future.  However, the support for traffic measurement in today's commodity switches is quite primitive.  In this proposal, the PIs bring algorithmic research on so-called ""compact data structures"" to bear on the problem of programmable traffic measurement in SDNs.  Compact data structures can give approximate answers to measurement questions with limited overhead in terms of switch memory and processing resources.  <br/><br/>The project is interdisciplinary, bringing together researchers in computer networking and theoretical computer science to match practical problems with novel solutions.  The proposed research starts with designing new query abstractions for collecting traffic statistics on existing SDN switches, and then progresses to identifying new compact data structures so that future switches can support much richer traffic measurement at reasonable overhead.  The researchers have close ties with network administrators and switch vendors, allowing them to ground the project in a strong understanding of both operational requirements and hardware constraints, and also influence future SDN technology.<br/><br/>This project aims to identify a switch data-plane architecture for collecting diverse traffic statistics, as well as a small set of programmable sketches and samples for variety of analyses to trade-off accuracy and resources.  The architecture will include a measurement control API between the controller and the switch, and this needs a communication-efficient interface, along with a high-level language for specifying traffic queries, and with that, a run-time system on the controller that compiles these queries into commands to the switches with suitable CDSs.  These challenges will be addressed using OpenFlow API that is widely popular for SDNs and in new redesigns.  This is a conversation between the networking and algorithmic communities, mutually informing each other on what is possible, what is required, and ultimately what is effective and useful."
"1628401","XPS: EXPL: Cache Management for Data Parallel Architecture","CCF","Exploiting Parallel&Scalabilty","09/01/2016","08/22/2016","Zheng Zhang","NJ","Rutgers University New Brunswick","Standard Grant","M. Mimi McClure","08/31/2019","$300,000.00","","eddy.zhengzhang@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8283","","$0.00","Current advances in computer science and other disciplines rely on the massive computation horsepower of data parallel architectures, such as GPUs. Programming data parallel architecture is not easy, as it requires the efficient handling of data movements across the memory hierarchy of thousands of processing cores. <br/><br/>To date, data movement problems have been primarily studied in uni-core and multi-core programming systems.  Thus, shifting to a many-core programming paradigm presents the new challenges of 1) scalability, 2) software and hardware interface, and 3) addressing the trade-off between performance and energy. First, the data movement models in uni-core and multi-core processors do not scale well, thus, this project develops scalable analytical models and yet provides powerful heuristics in practice. Second, it is important to redefine the responsibilities of software and hardware. Given the complexity of many-core architecture, it is impossible to solve data movement problems using software-only or hardware-only approaches. This project optimizes data movements with a cross-stack design principle that aims to combine the strengths of software and hardware. Third, previous studies have focused on performance without much consideration to issues of power and energy efficiency. This project targets both performance and energy, models the energy cost of data movement and integrates this information into the power/energy model for the entire system. Overall, this project can help shape future software-hardware cache interfaces and lay the foundation for the design of next-generation cache systems."
"1650596","EAGER: Concurrent Data Structures","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","08/31/2016","James Aspnes","CT","Yale University","Standard Grant","Rahul Shah","08/31/2019","$265,044.00","","aspnes@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7796","7916, 7926, 7934","$0.00","Most computer programming describes a sequence of steps to be carried out one by one by a single processor core.  As the rate of increase in processor speed has slowed, CPU manufacturers have responded by building systems with many processor cores that together carry out multiple tasks simultaneously.  These processors share a common memory that they use both for their own individual computations and for communication with other processors.  Organizing this shared memory so that the processors can make progress without being delayed by other processors requires careful coordination and the design of specialized data structures and communication protocols to allow efficient cooperation without conflicts.  The project will study how letting processors make random choices between different ways to accomplish the same task to improve the efficiency and amount of memory used by these data structures, an approach that appears to be necessary given known impossibility results for non-randomized methods.  This may significantly improve our ability to exploit the power of multicore machines, while simplifying the work of programmers of these machines.  In addition to this impact on computer science and the practice of programming, the project will directly impact both undergraduate and graduate research.  Because concurrent data structures are well-suited to undergraduate implementation projects, which avoid difficulties that often arise with involving undergraduates in more theoretical research, the project will serve as a bridge for recruiting students into cutting-edge, high-stakes research, including students from under-represented groups.  At the graduate level, results from the project will feed directly into the PI's teaching, including updates to the PI's publicly-available lecture notes already in use by many students at other institutions.<br/><br/>The main question considered by the project is: Can we remove bottlenecks in concurrent executions of programs in shared-memory system using data structures that avoid traditional concurrency control tools like locking?  Two techniques that appear promising are the use of randomization, which avoids problems where bad timing consistently produces bad executions in which different processes interfere with each others' operations, and limited-use assumptions, where shared data structures are designed under the assumption that they will only be used for a limited number of operations, allowing for significant reductions in cost and complexity.  In addition to applying these techniques individually to the design of concurrent shared-memory data structures, the project will also consider how these methods can complement each other, for example by the use of randomized helping techniques to transform short-lived limited-use data structures into long-lived unlimited-use data structures.  A key element of this work will be the development of improved cost measures for shared-memory data structures, including dynamic measures of space complexity that more accurately reflect practical memory usage than the worst-case measures of maximum memory consumption found in previous work."
"1527354","CIF: Small: Secure and Private Function Computation by Interactive Communication","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","08/07/2015","Prakash Narayan","MD","University of Maryland College Park","Standard Grant","Phillip Regalia","08/31/2019","$499,998.00","","prakash@eng.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7797","7923, 7935","$0.00","This research takes an information theoretic approach to develop principles that govern secure or private function computation by multiple terminals that host user data. The goal of the terminals is to compute locally and reliably, a given function of all the possibly correlated user data, using an interactive communication protocol. The protocol is required to satisfy separate security and privacy conditions. The former stipulates for each terminal that a coalition of the remaining terminals should glean no more information about the data at the terminal from their own data and the communication -- than can be obtained from the function value. The latter protects each individual user's data at a terminal from a similar coalition. A common framework is developed for analyzing the distinct concepts of security and privacy, and new information theoretic formulations and approaches are proposed with the objective of understanding basic underlying principles. Potential applications arise, for instance, in: hospital databases that store clinical drug trial results or university databases with student performance records; private information retrieval from user data stored in private clouds; and security and privacy certifications for the identities/locations of communities and individuals participating in crowd-sourced traffic and navigation services. <br/><br/>The investigators' technical approach involves the development of a theory with three main distinguishing features. It (i) establishes a key role for interactive communication in reducing communication complexity, and in enhancing security and privacy; and formulates computable measures of security and privacy in terms of conditional Renyi entropy; (ii) provides a common framework for formulating and analyzing problems of secure and private function computation with prominent roles for classical Shannon theory as well as zero-error combinatorial information theory; and introduces the concept of a multiuser privacy region for quantifying privacy tradeoffs among users; and (iii) develops a new method for obtaining converse bounds for communication complexity, upon analyzing the common randomness or shared information generated in function computation with an interactive communication protocol. Rooted in information theory, estimation theory and theoretical computer science, a central objective of the research is to elucidate tradeoffs among computation accuracy, terminal security and user privacy; key to these tradeoffs is the essential role of interactive communication. Furthermore, it aims at creating advances in information theory through the introduction of new models and concepts. Expected outcomes are precise characterizations of the mentioned fundamental tradeoffs, and associated algorithms for secure and private computing."
"1816219","AF: Small: Data-Driven Model Reduction for Optimal Control of Large-Scale Systems","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","08/23/2018","Athanasios Antoulas","TX","William Marsh Rice University","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$499,373.00","Matthias Heinkenschloss","aca@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7796","7923, 7933","$0.00","Dynamical systems are a principal tool in the modeling, prediction, and control of physical phenomena ranging from heat dissipation in complex microelectronic devices,  to vibration suppression in large wind turbines, to flow simulation. Optimal control of dynamical systems plays an important role in the many science and engineering applications where one wants to generate inputs to improve the performance of a system. However, ever-increasing need to include more detail at the modeling stage inevitably leads to larger-scale, more complex dynamical systems. Many of these large-scale systems are obtained from spatial discretizations of time-dependent coupled systems of partial differential equations. The simulation of such complex dynamical systems creates huge demands on computational resources and such high fidelity simulations may become unmanageable when the system needs to be queried at many different inputs. This project develops and applies a new class of model reduction methods for the efficient simulation and optimal control of dynamical systems. The model reduction approaches developed in this project approximate large, complex models of time-dependent processes using smaller, computationally efficient models that are nonetheless capable of representing accurately the outputs of the original process under a variety of operating conditions. Thus the new model reduction methods allow simulation and control of systems that would otherwise not be practical with high fidelity computational models.<br/><br/>The new model reduction methods developed in this project are data-driven. Unlike existing methods, in many cases, the new methods allow the generation of efficient reduced order computational models directly from (experimental) data and do not require access to high fidelity computational simulations. Overall, this new approach offers several advancements. First, it generates a reduced order model of the original nonlinear dynamical system that is valid for all inputs. Second, the new approach is data-driven and avoids the use of projections. This means less detailed access to the components of the original system are needed, which makes the new method particularly attractive when details of system simulation is inaccessible. This project will develop mathematical algorithms for the creation of such data-driven reduced order models and provide theoretical analyses of their performance. In addition, this project integrates the new model reduction approach into the solution of optimal control problems. For this integration, crucial additional input-output relations will be identified and the reduced order model generation will be augmented to ensure that these additional input-output relations are well approximated. This new approach will be applied to simulation and control of important biological phenomena and of fluid flows.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1527435","AF: Small: Compact Data Structures for String Matching and Retrieval","CCF","ALGORITHMIC FOUNDATIONS","06/01/2015","05/29/2015","Rahul Shah","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Maria Zemankova","05/31/2019","$225,787.00","","rahul@csc.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7796","7923, 7926, 9150","$0.00","In the era of big-data, one needs to organize massive amounts of data <br/>so that it can be searched quickly. This requires the building of an <br/>index over the raw data. In many cases of big-data, like world-wide web<br/>or DNA sequencing, the actual information content is low. This data is <br/>highly compressible. On the other hand, the indexes require space which <br/>is several times the raw data. Thus, indexing and compression are often <br/>conflicting goals. The field of compact (or succinct) data structures <br/>attempts to achieve both these goals -compression and indexability- <br/>simultaneously. This project will address some of the most fundamental <br/>open problems in this field. This will have impact on next generation <br/>biological sequence mining databases which could simply work within the <br/>memory of a PC instead of requiring high-performance clusters. The <br/>foundations built by this project will also impact image matching and <br/>music retrieval. Since data structures is one of the most fundamental <br/>areas in computer science education, research from this project will <br/>also impact data structures curriculum.<br/><br/>Suffix trees are central to string indexing and have myriads of <br/>applications. However, suffix trees are known to take 15 to 50 times <br/>the size of the text they index. This actually stems from a complexity <br/>gap in the size of data which is n log s bits compared to the size of <br/>the index which is O( n log n) bits, for the text of n characters drawn <br/>from alphabet size of s. The techniques of Burrows-Wheeler Transform(BWT)<br/>and Phi-function were introduced in the last decade to address this gap. <br/>Most subsequent research in this field has treated BWT as a black box, <br/>compressing augmenting structures around it to address various <br/>applications. However, many problems (like parameterized pattern matching<br/>and 2D pattern matching) have remained open in this field. This project <br/>will attempt to go deeper and beyond the philosophy of BWT to solve such <br/>issues. It will also try to build foundations for deriving lower bounds <br/>for problems where compact index would be impossible. To create better <br/>understanding of data structure space and query complexity, the project <br/>will explore the recent theoretical model called ""encoding  model"". The <br/>project will also explore the applied case of compressed indexing for <br/>highly repetitive sequences.<br/><br/>For further information see the project web site at:  <br/>http://csc.lsu.edu/~rahul/succinct"
"1618795","AF: Small: Algorithms and Information Theory for Causal Inference","CCF","ALGORITHMIC FOUNDATIONS","08/01/2016","06/03/2016","Leonard Schulman","CA","California Institute of Technology","Standard Grant","Rahul Shah","07/31/2019","$450,000.00","","schulman@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7796","7923, 7926","$0.00","This project is concerned, firstly, with algorithmic and information-theoretic aspects of Causal Inference. With the exception of some scientific data that is gathered purely for knowledge, most data is gathered for the purpose of potential intervention: this holds for medicine, public health, environmental regulations, market research, legal remedies for discrimination, and in many other domains. A decision-maker cannot take advantage of correlations and other structural characterizations that are discovered in data without knowing about causal relationships between variables. Historically, causality has been teased apart from correlation through controlled experiments. However there are several good reasons that one must often make do with passive observation: ethical reasons; governance constraints; and uniqueness of the system and the inability to re-run history. Absent experiments, we are without the principal arsenal of the scientific method.<br/><br/>Yet there is a special class of systems in which it is possible to perform causality inference purely from passive observation of the statistics. For a system to fall in this class one must be able to establish on physical grounds that certain observable variables are statistically independent of certain others, conditional on a third set being held fixed; the formalism for this is ``semi-Markovian graphical models"". It is known which semi-Markovian models fall in this class, subject to the assumption of perfect statistics. From this starting point there remain significant theoretical challenges before these ideas can have the greatest possible impact on practice. Some of the challenges to be addressed include:<br/><br/>(1) The PI will aim to quantify how the stability (condition number) of causal identification depends on the various sources of uncertainty (statistical error; numerical error; model error) and as a function of the structure of the graphical model. The purpose is both to understand what inference is justifiable from existing data, and to impact study design so that data with the greatest leverage is collected. For the former objective, in particular, the PI seeks an efficient algorithm to compute the condition number of a given semi-Markovian model at the specific observed statistics. For the last objective the PI seeks an efficient algorithm to compute the worst-case condition number of a given semi-Markovian model.<br/><br/>(2) Existing causal identification algorithms, applied to data inconsistent with the model (which is unavoidable due to statistical error, and normally also due to model error), will yield an inference inconsistent with the model. The project will help to understand if projection onto the model may improve stability.<br/><br/>(3) One of the obstacles to use of existing methods is that they require sample size exponential in the size of the graphical model. The project aims to determine when it is possible to infer causality using only the marginal distributions over small subsets of the observable variables; this will reduce sample size and likely improve condition number.<br/><br/>(4) In the majority of semi-Markovian models, causality is not identifiable. This leaves open however the possibility of determining (or giving a nontrivial outer bound for) the feasible interval of causal effects. No effective algorithm is currently known for this problem, and we wish to provide one. Such an algorithm could be used to show that an intervention is favorable despite the effect not being fully identifiable.<br/><br/>(5) The project aims to lift the causal-inference algorithm to time series, as well as study the connections with the distinct techniques (Granger causality and Massey's directed information) normally used in this setting.<br/><br/>Secondary emphases of the project include broader research in theoretical computer science. In particular, studying connections between ``boosting"" or ``multiplicative weights"" methods used in algorithms and machine learning, and their variants which arise out of selection or self-interest in the system dynamics of ecosystems (``weak selection"") and economic marketplaces (``tatonnement"").<br/><br/>Inseparably from the research effort, the PI will train students and postdocs in these and related areas of the theory of computation."
"1524250","CIF: Small: Inference over Asymmetric Network and Data Structures","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","06/09/2016","Ali Sayed","CA","University of California-Los Angeles","Standard Grant","Phillip Regalia","08/31/2019","$514,000.00","","sayed@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7797","7923, 7935, 9251","$0.00","In an age when the word ""network"" may refer to social networks, power networks, transportation networks, or data networks, interest in information processing over graphs has met with resurgence. The efforts under this proposal are relevant to applications involving large and distributed amounts of data, as is the case with health informatics, surveillance applications, data networks, or cloud computing. The research results will enable engineering systems to benefit from a bottom-up design paradigm involving coordination among less powerful agents to achieve higher levels of cognition and performance by an interconnected network of cooperating agents. The research will develop techniques that enable agents to work collectively for a common objective and to counter the degrading effect of imbalances that may exist in their interactions such as the fact that some agents in the network may be more informed than other agents; some agents may have a domineering tendency; some agents may be willing to share only partial information due to privacy and secrecy considerations; and some agents may be subject to corrupted data; or have different objectives than the group. Results developed under this work will benefit the training of a diverse body of students in the strategic area of network science. The results will also be disseminated broadly to the research community online and in archival publications and meetings.<br/><br/>Performance indicators in networked applications include cooperation among agents to bestow resilience to failure; privacy and secrecy considerations where agents may not be comfortable sharing data with remote centers for processing; and the fact that large amounts of data may already be available in dispersed form and aggregation of the data at a central location can be costly. These considerations have motivated the development of powerful distributed mechanisms that enable agents to cooperate to attain superior inference capabilities. Many distributed techniques ignore critical asymmetries that exist in both network and data structures. Robotic swarms are one example of a notable application where agents can benefit from adjusting their exploration space in response to asymmetry conditions, malfunctioning of neighbors, or suspicious behavior by intruders. A second example is the use of networked learners to mine information from big databases, such as those related to health informatics, power grids, social networks, or surveillance applications. If asymmetries are ignored, they can lead to erroneous inference conclusions and degraded performance. This research effort exploits cognitive abilities to enable networks to counter asymmetries and their deleterious effects. In particular, this project seeks adaptation and learning mechanisms that endow multi-agent systems with the ability to implement decision-making processes that mimic quorum responses by animal groups, the ability to identify and react to domineering or intrusive behavior, the ability to implement divide-and-conquer, clustering, and labor division strategies, and the ability to counter the effect of corrupted data."
"1617986","CIF: Small: Advancing Adaptive Importance Sampling for Signal Processing","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/24/2016","Monica Fernandez-Bugallo","NY","SUNY at Stony Brook","Standard Grant","Phillip Regalia","05/31/2019","$498,493.00","","monica.bugallo@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7797","7923, 7936","$0.00","There are many applications where the interest is in learning about unknowns from observed data. The goals range from predicting future data to learning about scientific or societal truths. Bayesian signal processing allows for explicit incorporation of all available information about an addressed task. It amounts to optimally combining common-sense knowledge and observational evidence. Due to its strength and appeal, Bayesian modeling and analysis has been embraced by all of science and engineering. However, the main current problems are those with large numbers of unknowns (complex systems) and/or large amounts of data (big data). This raises the concern that Bayesian inference may become computationally incapable of handling them because of the sheer size and complexity of the studied systems. The goal of this project is to advance the theory and practice of a class of Bayesian methods, adaptive importance sampling (AIS), for dealing with problems where the numbers of unknowns and/or data are large.<br/><br/>This project focuses on building a novel framework for AIS that will extend its use for Bayesian inference to problems with large amounts of unknowns and/or data. The research involves investigating in greatest detail the intricacies of AIS on several areas including (a) novel schemes for AIS with emphasis on new strategies for adaptive learning and for stable weight computation, and on advanced approaches for dealing with high dimensional models and big data, (b) model selection and machine learning, (c) global optimization, and (d) application to a case-study where understanding the progression of cancer from cancer stem cells is of interest."
"1813877","CIF: Small: Precise Computational and Statistical Tradeoffs for Iterative Signal Estimation and Supervised Learning","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","07/01/2018","06/18/2018","Mahdi Soltanolkotabi","CA","University of Southern California","Standard Grant","Phillip Regalia","06/30/2021","$490,691.00","","soltanol@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","2878, 7797","075Z, 7923, 7935","$0.00","Due to the proliferation of new sensors and acquisition devices, massive data sets are gathered in many modern applications ranging from medical imaging to online advertisement. Conventional statistical signal estimation and machine learning theory aims to understand how the predictive ability or accuracy of data analysis algorithms scales with data sizes. However, given the sheer size of modern data sets, these classical statistical theories are insufficient as algorithms have to operate effectively under a variety of new constraints such as, limited processing time, fixed computational budget and communication constraints. This project aims to develop the fundamental limits of how statistical accuracy tradeoffs with these resource constraints together with algorithms that nearly achieve such performance limits. The resulting signal estimation and learning algorithms are deployed in novel applications aimed at decreasing the acquisition time in medical imaging devices and speeding up parallelized algorithms in other data processing domains. Parts of this project are integrated into an advanced graduate class and select results will serve to motivate K-12 students to pursue careers in STEM (Science, Technology, Engineering and Math).<br/> <br/>In this project, the team of researchers study a family of convex optimization algorithms used for signal estimation and unsupervised learning. The main goal of this project is to understand how the statistical performance of iterative convex optimization algorithms tradeoffs with various statistical and computational resources such as run time, data size, communication, etc. The theoretical investigations utilizes techniques from convex analysis, probability, and information theory. The theoretical analysis can be used to establish fundamental performance bounds for popular convex optimization problems and guide the design of new algorithms that achieve optimal trade-offs between competing resource constraints.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1647432","EAGER: Dryads - Next Generation Tree Algorithms","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/28/2016","Robert Brunner","IL","University of Illinois at Urbana-Champaign","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$299,815.00","Vincent Reverdy","bigdog@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7916, 7942","$0.00","Many data sets can be represented via a natural hierarchical ordering, which can be easily represented programmatically by using tree data structures. For example, two-dimensional spatial data can be organized by using quad-trees, while three-dimensional data can be organized by using oct-trees. As data volumes continue to increase, compact representations of the extremely large data become increasingly important since the representations can enable much more efficient data selection, transportation, and processing. Yet the development of standardized, generic and efficient tree data structures that both scale to massive data and leverage the capabilities of modern computer architectures remains an unmet need. This research effort addresses this need by designing and implementing a library of generic implicit tree abstractions that will provide the foundation for next generation analysis codes in data driven sciences. By working with the C++ standardization committee, this research will potentially impact millions of software developers, worldwide, since this low level language is implicitly used by many high-level language analysis tools and libraries.<br/><br/>This research will investigate generic and high performance tree building blocks by exploring two key elements. First, low-level bit manipulation techniques will be created that can be optimized for specific computer architectures (such as the Intel Haswell). These techniques will be developed in conjunction with the international C++ standardization committee as an open source library and will impact a wide range of applications areas including arbitrary precision arithmetic, cryptography, and tree indexing strategies. Second, a generic library of implicit tree structures will be developed, by using the previously developed bit manipulation techniques, and submitted as a new, open-source library to the Boost community for broader dissemination. Finally, to demonstrate the efficacy of these new software libraries, two example tree applications will be developed and published: an oct-tree used for numerical simulations and a decision trees used for machine learning."
"1408784","AF: Medium: Collaborative Research: Multi-dimensional Scheduling and Resource Allocation in Data Centers","CCF","ALGORITHMIC FOUNDATIONS","08/01/2014","05/30/2017","Kameshwar Munagala","NC","Duke University","Continuing grant","Tracy J. Kimbrel","07/31/2019","$568,486.00","Benjamin Lee","kamesh@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796","7924, 7926, 7932","$0.00","Data centers and cluster computing platforms have become the dominant computational paradigms of the past decade, becoming the de facto method of executing Big Data workloads. Typically, the entire cluster is treated as a set of resources shared by multiple clients who submit jobs requiring different types of resources. To add to this heterogeneity (or dimensionality) in resource requirements, machines in a cluster can be heterogeneous in terms of resources they provide. In such settings, resources need to be allocated and priced appropriately so as to balance performance with demand.<br/><br/>In this project, the PIs seek to study job scheduling in data centers to optimize temporal Quality of Service metrics such as response time along with fairness, when jobs can have resource requirements that are multi-dimensional.  The main question studied can be phrased as: How does the temporal nature of job scheduling interplay with the dimensionality of resource requirements, and how do these two in turn interact with the classical economic desiderata of incentives and fairness?<br/><br/>This project is differentiated from previous work in aiming to develop appropriate models and algorithms through the lens of theoretical computer science, particularly by a fusion of the disparate fields of approximation and online algorithms, algorithmic game theory, and stochastic optimization. The resulting insights will be used to also develop new techniques to address classical scheduling and game theoretic problems that have defied successful solutions.  The project is interdisciplinary, and the theoretical models and techniques developed will be motivated by the application domain of new hardware architectures stemming from emerging technologies, and the heterogeneity arising from provisioning them within a data center. Further, empirical validations will be performed, both via simulation on traces from data center executions, as well as deployment and experiments on clusters.  This will ultimately influence the design and deployment of internet systems that use and generate massive data.<br/><br/>The interdisciplinary nature of the project points to not only the need for training a pipeline of students from high school students to graduates and imparting to them the power of algorithmic thinking and its broader relevance, but also the necessity for bringing scientists, mathematicians, and system builders to the same platform for active exchange of ideas. Towards this end, the PIs seek to equip the next generation of students, including women and minorities, with the relevant algorithmic skills by an education plan that includes effective teaching and mentorship, as well as to broadly disseminate the proposed work by organizing workshops and by writing books and surveys."
"1832228","AF: Small: Sublinear Algorithms for Real Data","CCF","ALGORITHMIC FOUNDATIONS","08/24/2017","03/13/2018","Sofya Raskhodnikova","MA","Trustees of Boston University","Standard Grant","Tracy J. Kimbrel","05/31/2019","$135,941.00","","sofya@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7796","7923, 7926","$0.00","The area of sublinear algorithms aims to establish algorithmic foundations for processing big data. What global properties of the data can we understand while reading only a small portion of it? What if the data is noisy? Can we quickly restore a corrupted data point while ensuring some global property of the data? This project focuses specifically on sublinear algorithms for real-valued numeric data and on real-world challenges associated with it.<br/><br/>Effectively exploiting big data can provide significant societal benefits. This project has the potential to change how real-valued data is processed and analyzed, and how privacy of sensitive datasets is handled. In addition, this project includes educational activities designed to ensure the work's broader impact and to improve workforce training. They include advising and mentoring graduate students; continuing to build a strong theoretical foundations group at Penn State; including the results of the proposed research in the PI's graduate course on sublinear algorithms; widely disseminating the results of this research and, more broadly, algorithmic and computational ideas via talks and publications; and fostering diversity by providing mentoring and educational activities targeted at women in computer science and mathematics.<br/><br/>While there are established and successful lines of research in sublinear algorithms and property testing on Boolean functions, codes (and, more generally, algebraic properties), graphs, and discrete distributions, several recent applications of sublinear algorithms require working with real data. These include the study of Lipschitz functions with applications to data privacy and the study of submodular functions with applications to economics. To achieve their full potential, sublinear algorithms need to be able to handle real-valued data. The aim of this project is to lay the foundations for this area of study. This will require the development of new tools and will open up new connections and new areas of application.  The research activities are grouped into three parts: 1. testing and local reconstruction of Lipschitz functions with applications to data privacy; 2. new measures for accuracy guarantees, with the focus on L_p metrics; and 3. new models for data access."
"1717884","CIF: Small: Collaborative Research: Error Correction with Natural Redundancy","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2017","07/17/2017","Jehoshua Bruck","CA","California Institute of Technology","Standard Grant","Phillip Regalia","07/31/2020","$166,666.00","","bruck@paradise.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7797","7923, 7935","$0.00","Part 1: Nontechnical description of the project<br/><br/>This project studies the fundamental problem of removing errors from data by using internal structures of data. It shows that the vast amount of data stored in current data-storage systems possess very rich structures; therefore, by fully exploiting them for error correction, the reliability of data-storage systems can be improved significantly. The project studies several fundamental aspects of this technology, including how to discover and characterize the highly complex structures of various types of data, how to use them to correct errors in data efficiently to improve the reliability of data-storage systems, how to combine the technology with existing error-correction techniques that are based on adding external redundancy to data, and how to implement the technology in practical data-storage systems. <br/><br/>This project addresses a critical issue of the modern society: how to ensure that data can be stored reliably at large scale and over a long time. The new technology has the potential to substantially improve the dependability of information infrastructure, which accesses vast amounts of data frequently for scientific and industrial computing. The project is interdisciplinary in nature: it combines multiple scientific fields including information theory, machine learning, big data analysis and algorithm design, and aims to educate students and contribute to workforce development for next-generation storage systems. The project conjugates rigorous theoretical analysis and significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts.<br/><br/>Part 2: Technical description of the project<br/><br/>This project studies how to use the inherent redundancy in big data for error correction. Examples of big data include languages, images, databases, and others. The inherent redundancy is integrated with error-correcting codes (ECC) for effective error correction. The objective is to elevate data reliability in storage systems to the next level. To achieve this goal, new techniques will be developed to discover various types of inherent redundancy in both compressed and uncompressed data. New approaches will be explored to combine inherent-redundancy decoders and ECC decoders for effective error correction. Fundamental limits of both capacity and computational complexity will be studied for error correction using inherent redundancy.<br/><br/>This project combines error correction with machine learning and is interdisciplinary in nature. It will expand the current knowledge on error correction in multiple ways. First, it uses techniques in natural language processing and deep learning to discover new types of redundancy in big data that are suitable for error correction, and which extend beyond current knowledge in joint source-channel coding. This includes redundancy discovery techniques for data already compressed by various compression algorithms. Second, it explores decoding algorithms for ECCs with not only regular ECC-imposed redundancy, but also irregular inherent redundancy. It extends existing error correction schemes to cast the fundamental limits of inherent redundancy for error correction, in terms of both capacity and computational complexity. Third, by integrating a theoretical study with practical systems, a foundation can be laid for next-generation systems that store and transmit big data.<br/><br/>Modern society relies increasingly heavily on digital data. With the explosive amount of data generated each day, it is essential to make advances in error correction that can catch the speed of data explosion. This project aims at improving data reliability significantly to the next level, and improvements in this direction can be highly beneficial to the daily work and life of the modern society. This project, being interdisciplinary between coding theory and machine learning, can foster collaboration between the information theory and computer science communities. The project combines rigorous theoretical analysis with significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts. The proposed research will be integrated with engineering education by developing new courses for graduate and undergraduate students, and involving under-represented, domestic and international students in advanced research. The results will be actively publicized in national/international conferences and journals."
"1718886","CIF: Small: Collaborative Research: Error Correction with Natural Redundancy","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2017","07/17/2017","Anxiao Jiang","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","07/31/2020","$299,999.00","Krishna Narayanan","ajiang@cs.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7797","7923, 7935","$0.00","Part 1: Nontechnical description of the project<br/><br/>This project studies the fundamental problem of removing errors from data by using internal structures of data. It shows that the vast amount of data stored in current data-storage systems possess very rich structures; therefore, by fully exploiting them for error correction, the reliability of data-storage systems can be improved significantly. The project studies several fundamental aspects of this technology, including how to discover and characterize the highly complex structures of various types of data, how to use them to correct errors in data efficiently to improve the reliability of data-storage systems, how to combine the technology with existing error-correction techniques that are based on adding external redundancy to data, and how to implement the technology in practical data-storage systems. <br/><br/>This project addresses a critical issue of the modern society: how to ensure that data can be stored reliably at large scale and over a long time. The new technology has the potential to substantially improve the dependability of information infrastructure, which accesses vast amounts of data frequently for scientific and industrial computing. The project is interdisciplinary in nature: it combines multiple scientific fields including information theory, machine learning, big data analysis and algorithm design, and aims to educate students and contribute to workforce development for next-generation storage systems. The project conjugates rigorous theoretical analysis and significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts.<br/><br/>Part 2: Technical description of the project<br/><br/>This project studies how to use the inherent redundancy in big data for error correction. Examples of big data include languages, images, databases, and others. The inherent redundancy is integrated with error-correcting codes (ECC) for effective error correction. The objective is to elevate data reliability in storage systems to the next level. To achieve this goal, new techniques will be developed to discover various types of inherent redundancy in both compressed and uncompressed data. New approaches will be explored to combine inherent-redundancy decoders and ECC decoders for effective error correction. Fundamental limits of both capacity and computational complexity will be studied for error correction using inherent redundancy.<br/><br/>This project combines error correction with machine learning and is interdisciplinary in nature. It will expand the current knowledge on error correction in multiple ways. First, it uses techniques in natural language processing and deep learning to discover new types of redundancy in big data that are suitable for error correction, and which extend beyond current knowledge in joint source-channel coding. This includes redundancy discovery techniques for data already compressed by various compression algorithms. Second, it explores decoding algorithms for ECCs with not only regular ECC-imposed redundancy, but also irregular inherent redundancy. It extends existing error correction schemes to cast the fundamental limits of inherent redundancy for error correction, in terms of both capacity and computational complexity. Third, by integrating a theoretical study with practical systems, a foundation can be laid for next-generation systems that store and transmit big data.<br/><br/>Modern society relies increasingly heavily on digital data. With the explosive amount of data generated each day, it is essential to make advances in error correction that can catch the speed of data explosion. This project aims at improving data reliability significantly to the next level, and improvements in this direction can be highly beneficial to the daily work and life of the modern society. This project, being interdisciplinary between coding theory and machine learning, can foster collaboration between the information theory and computer science communities. The project combines rigorous theoretical analysis with significant practical applications, to foster collaboration between academia and industry, and create new scientific advances with combined efforts. The proposed research will be integrated with engineering education by developing new courses for graduate and undergraduate students, and involving under-represented, domestic and international students in advanced research. The results will be actively publicized in national/international conferences and journals."
"1719139","CCF-BSF: CIF: Small: Distributed Information Retrieval: Private, Reliable, and Efficient","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","08/31/2017","Alexander Vardy","CA","University of California-San Diego","Standard Grant","Phillip Regalia","08/31/2020","$450,000.00","","vardy@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7923, 7935, 7937","$0.00","The digital age is predicated on information being ubiquitous. The ability to access relevant data stored on remote servers ""in the cloud"" has become an indispensable resource in everyday lives. Numerous online services let users query public datasets for data items such as map directions, stock quotes, and flight prices, to name a few. Digital content providers also rely on user queries to identify the content desired by the user. Unfortunately, such queries have the potential to reveal highly-sensitive information *about the users*, thereby compromising their privacy. For example, institutional investors querying a stock-market database for the value of certain stocks may prefer not to reveal their interest in these stocks since it could influence their price. As another example, most people are deeply uncomfortable with exposing their media consumption diet to a centralized server that can be targeted by hacking or subpoena. It can be convincingly argued that access to such media consumption profiles can reveal the person's sexual orientation, political leanings, and cultural affiliations.<br/><br/>A great deal of research has been devoted to methods that guarantee the security and integrity of *the data*. Much less work, however, has been devoted to protecting the privacy of *the user*. Efficient and reliable retrieval of information from distributed databases, with information-theoretic guarantees of user privacy, is the focus of this project. The relevant area of research is known as private information retrieval. While most of the existing results in this area are theoretical, a major goal in this project is to bridge the gap between the theory of private information retrieval and the practice of distributed storage. As such, potential outcomes of this investigation may extend beyond the scope of academic research, contributing to new technologies and products.<br/><br/>Private information retrieval (PIR), conceived in the seminal papers of Chor, Goldreich, Kushilevitz, and Sudan over 20 years ago, has been traditionally studied in theoretical computer science and cryptography, with emphasis on the complexity of the communication between the user and the servers that store the database. While major breakthroughs have been achieved in this area over the years, the prevailing paradigm has always been that of replicating the database on several non-communicating servers. Such replication leads to a significant storage overhead, which is undesirable. Moreover, motivated by advances in coding for distributed storage, it was recently recognized that if database replication is replaced by *database coding*, the full power of coding-theoretic methods can be brought to bear on the problem. While extremely promising, this line of research is still in its infancy. The goal of this project is to follow-up on the database coding idea, and follow it through to its ultimate potential. In pursuit of this goal, the following questions are addressed:<br/><br/>(1) What is the information-theoretic capacity of PIR? That is, what is the maximum amount of information that can be privately retrieved per downloaded bit, under various scenarios?<br/><br/>(2) What is the optimal storage overhead of PIR? Can we achieve both privacy and efficient communication (on the download and upload) without replicating the stored data even once?<br/><br/>(3) How can both privacy and download efficiency be maintained in the presence of impediments such as malicious or colluding servers, unsynchronized data, and/or communication errors?<br/><br/>(4) Codes for distributed storage systems tolerate and repair node failures while making the data available to several users at once. How can we combine PIR protocols with such coding?<br/><br/>(5) What is the best possible tradeoff between the various desirable PIR features, such as download efficiency, storage overhead, and resilience to errors/collusions/node-failures?<br/><br/>This proposal is a natural outgrowth of the research recently carried out by the PIs and others in this area. Prior related work will provide a springboard for rapid progress toward the ambitious research objectives of this project. Knowledge, techniques, and qualitative insights gained through this investigation are expected to contribute to the foundations of the field, and to help bridge the gap between the theory of PIR and the practice of distributed storage."
"1763547","SHF: Medium: Collaborative Research: ECC: Ephemeral Coherence Cohort for I/O Containerization and Disaggregation","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","05/21/2018","Weikuan Yu","FL","Florida State University","Continuing grant","Almadena Y. Chtchelkanova","05/31/2021","$328,837.00","Sarp Oral","yuw@cs.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7798","7924, 7942","$0.00","Leadership computing facilities for high-performance computing (HPC) have a huge investment in the file and storage systems. The reason is that the HPC storage system often is the Achilles Heel of HPC systems, as it is fraught with numerous scenarios for contention, congestion and performance variability. This problem is getting worse due to: (a) the increased importance of data-driven HPC and the growth in the amount of data generated by large-scale simulation; and (b) the slower growth of disk speed, as compared to CPU speed. The addition of high-bandwidth persistent memory devices as burst-buffers brings in new opportunities for fast caching of application data while still allowing data persistence. However, the conventional approach of exploiting burst-buffers as yet another caching layer cannot reduce the lengthy and costly data processing steps in the deep I/O stack or reconcile occasional contentions inside the complex storage system. This project, therefore, seeks to exploit burst-buffers as repositories of persistent application-specific parallel file systems, with a lifetime commensurate to the lifetime of an application or an application campaign on a HPC system. This is a collaborative project between University of Illinois at Urbana-Champaign and Florida State University. <br/><br/>This project formulates a research framework called Ephemeral Coherence Cohort (ECC) that offers an abstraction to represent the active collection of application data through containerization, insulate I/O activities across different applications, and enable storage disaggregation for ephemeral allocation and dynamic utilization of burst buffers. The proposed ECC framework aims to enhance a variety of mission-critical applications running on the Department of Energy and the National Science Foundation leadership computing facilities. The project strengthens the collaboration between University of Illinois Urbana-Champaign and the Florida State University. The project has plans to organize panels and birds-of-feather sessions on burst buffer research in the upcoming HPC conferences and collaborate with leaders of super-computing centers for wider community penetration with techniques from this research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750780","CAREER:  AF: Giving Form to Data with a Geometric Scaffold","CCF","ALGORITHMIC FOUNDATIONS","09/01/2018","02/05/2018","Benjamin Raichel","TX","University of Texas at Dallas","Continuing grant","Rahul Shah","08/31/2023","$71,962.00","","Benjamin.Raichel@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7796","1045, 7929","$0.00","Using geometry to find structure in data is an old idea (Plato is quoted as saying, ""God ever geometrizes"") that gains fresh application as our data changes. Today's data sets, from areas such as machine learning, are often massive and high-dimensional: for example, when trying to classify news articles, each article may be represented as a point where the frequency of each word is a different dimension. This leads to geometries that are hard to grasp intuitively and hard to work with computationally (the ""curse of dimensionality""). Finding a smaller and lower dimensional subset of the data points that approximately preserves geometric structure not only reduces computation time but also can improve results by suppressing extraneous features. Representing inter-point distances of a general space in a more structured space can support new operations, such as data visualization by mapping the 2-D plane of the screen, or more efficient computation by mapping to the hierarchical structure of a tree. This project takes the age-old practice of teasing out geometric structure and applies it to the large- and high-dimensional data sets of the modern world. By taking a geometric approach to foundational problems in areas such as big data and machine learning, this project seeks to more closely connect computational geometry and these other areas, in turn both modernizing the classical field of computational geometry and advancing these other areas. The educational goals of this project will be achieved by directly supporting student research on the outlined topics, incorporating topics into developing new courses, and organizing regular seminars in order to grow the visibility and interdisciplinary nature of algorithms and theoretical computer science at the awardee institution.<br/><br/>Given a data set, the goal is to specify its geometric structure, use this structure to summarize and embed into simpler spaces where computations can be done efficiently, and when this is not possible, identify how to minimally fix the data to facilitate these tasks.  The project's focus is on three interrelated topics concerning geometric structure that lie at the intersection of big data, geometry, and machine learning: 1) data factorization and sparsification, 2) metric embeddings for structured spaces, and 3) metric violation distance.  The ultimate purpose is to develop better algorithms for handling data, ranging from better clustering algorithms to better classification algorithms. The ubiquity of such algorithms implies that any progress has the potential for significant real world impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1719118","CCF: AF: Small: Quantum Data Structures and Algorithms","CCF","QUANTUM COMPUTING","09/01/2017","08/31/2017","Willem van Dam","CA","University of California-Santa Barbara","Standard Grant","Dmitry Maslov","08/31/2020","$450,000.00","","vandam@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7928","7923, 7928","$0.00","Quantum computing looks at the potential benefits of processing information in a quantum mechanical manner. Due to the superposition principle of quantum physics in combination with the interference phenomenon, quantum algorithms are capable of performing certain tasks more efficiently than is possible with traditional, classical computers. As it uses ideas from various fields, research in quantum computing spans the spectrum from experimental physicists, through computer science, to pure mathematics. Conversely, this research often affects these various different fields.  The principle investigator, Dr Wim van Dam, will develop and analyze new data structures to be used by quantum algorithms. As is well known from standard classical computation theory, the way in which information is stored can play a crucial role in increasing the efficiency of the algorithms that act on this data. While the theory of quantum algorithms is fairly well sophisticated by now, much less is known about the role that data structures might have in increasing the benefits of processing information quantum mechanically. This proposal aims to remedy this lack of understanding. <br/><br/>Van Dam will investigate how the specific architecture of a quantum computer affects the optimal storage of data. He will also look at the possibility of data structures to encode trees, graphs, analog data, and hash functions in a quantum mechanical manner. Lastly Van Dam and his students will investigate the properties of quantum software where one stores quantum transformations as quantum states.<br/><br/>As part of the project Van Dam will host the Annual Conference on the Theory of Quantum Computation, Communication and Cryptography on the campus of UC Santa Barbara. Van Dam will also participate in UCSB's Research Mentorship Program during which gifted high-school students get to work in the research groups of the principle investigator."
"1651492","CAREER: Communication- Efficient Distributed Computation: Information- Theoretic Foundations and Algorithms","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","02/15/2017","07/17/2018","Ravi Tandon","AZ","University of Arizona","Continuing grant","Phillip Regalia","01/31/2022","$393,061.00","","tandonr@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","2878, 7797","1045, 7797, 7935","$0.00","This CAREER project is motivated by the exponential rise in the volume of data being processed for knowledge discovery applications, such as business process optimization, healthcare analytics, cybersecurity, and scientific computing. To speed up computation, there is an increasing interest in distributed computation; however, distributing a computational task over multiple heterogeneous machines involves data movement between machines leading to communication bottlenecks, significantly impacting the time for computation. By reducing the cost and time for distributed computation, the research outcomes can have a significant and immediate economic benefit as well as a lasting societal impact. The research objectives of this project are strongly interconnected with educational components and outreach activities, which include undergraduate research involvement, interdisciplinary curriculum development, outreach to K-12 in collaboration with SARSEF (Southern Arizona Research, Science and Engineering Foundation), and raising STEM awareness through the STEM learning center and the Arizona Science, Engineering, and Mathematics (ASEM) scholars mentoring program.<br/><br/>To enable communication-efficient distributed computation, this CAREER project will pursue the following intertwined research objectives. The first research objective is to make distributed data shuffling communication-efficient through novel coded data delivery mechanisms, that are adaptable to the underlying topology and can cope with the temporal nature of computational tasks. The second research objective is to reduce the impact of computational heterogeneity by novel work exchange algorithms through intermediate communication while minimizing the time for computation. The third research objective complements the first two in characterizing fundamental information theoretic limits by developing lower bounds on the communication necessary for: a) data shuffling as a function of storage and topology; and b) the minimum time for computation as a function of intermediate communication and storage."
"1717660","SHF:Small: Collaborative Research: Tailoring Memory Systems for Data-Intensive HPC Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/04/2017","Xubin He","PA","Temple University","Standard Grant","Almadena Y. Chtchelkanova","07/31/2020","$310,000.00","","xubin.he@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7798","7923, 7942","$0.00","High-performance computing is of strategic importance for computational science and engineering in the United States, and is on an accelerated path to sustaining scientific discovery at much increased flops. To advance the scientific discovery to the next level and allow new science missions to be accomplished in a timely manner, it is critical to address the memory performance holistically in high-performance computing platforms. This project provides architectural and system support and optimization for building memory systems tailored for data-intensive applications, e.g., big data analytics. This project offers research and educational opportunities for both undergraduate and graduate students, and trains a new generation of computer scientists and engineers in the area of high-performance computing. <br/>The objective of this project is to address the research challenges in building an efficient memory system by designing new techniques from several aspects. It develops a novel centralized memory refresh scheme at the cluster-level to manage memory refresh overhead, which has been increasingly performance-impacting and energy-consuming. It designs a new memory scheduling policy, taking advantages of new memory characteristics. It makes memory characteristics/peculiarities be available to the processor and operating system, so that they can make well-informed decisions to fully exploit memory performance potentials. It leverages in-memory computing to enable efficient in-situ processing. The integration of all these techniques provides a holistic solution to building an efficient memory system tailored for data-intensive applications."
"1452915","CAREER: Social Networks - Processes, Structures, and Algorithms","CCF","ALGORITHMIC FOUNDATIONS","07/01/2015","08/15/2018","Grant Schoenebeck","MI","University of Michigan Ann Arbor","Continuing grant","Tracy J. Kimbrel","06/30/2020","$414,105.00","","schoeneb@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7796","1045, 7926, 7932","$0.00","This project seeks to develop a rigorous theoretical understanding of complex and strategic network processes, network structure, and algorithms for network properties. Social networks are an abstraction used to study social structure via pair-wise social interactions, and have proven useful in analyzing how local actions affect global trends. Better understanding of social networks promises a better understanding of and the ability to influence a wide range of phenomena, including: what technologies/practices people and firms adopt, how information is transmitted and aggregated, and how network structure relates to the agents' ability to search within the network. In all of these instances individuals' activities can have a global-scale impact, which is mediated by a network. The increasing presence of computer-accessible data (e.g., websites, user-generated content, usage data from telecommunications, apps, web-browsing, etc.) has rekindled an interest in this field because of the new ability to gather data to test theories on a large scale. This project seeks to develop new algorithms and theoretical frameworks to help fully make use of these data.<br/><br/>This project will develop and apply traditional tools, insights, and approaches from theoretical computer science including functional analysis, graph theory, combinatorics, linear algebra, probabilistic analysis, linear and semidefinite program hierarchies, complexity theory, and game theory to the study of network processes and structure. This project will transform the way we use social network data by: 1) developing the technical tools required to achieve a better understanding of specific complex and strategic processes (including those mentioned above), 2) identifying network structures that are efficiently verifiable and are useful for understanding nuances in the network processes, and 3) improving our understanding of certain network processes by explicitly accounting for agents' strategic reasoning. The technical content of this project will have direct applications to related fields such as probability, economics, sociology, and statistical physics. Additionally, a key goal of this project is to move beyond worse-case analysis; if successful, this will pave the way to exporting theoretical computer ideas to many disciplines where their current application is limited due to its fixation on worst-case hardness---in particular fields that feature networks such as biology and epidemiology. The PI plans to develop curriculum to introduce computer science topics to high school students and involve undergraduates in his research."
"1539256","CyberSEES: Type 2: Collaborative Research: A Computational and Analytic Laboratory for Modeling and Predicting Marine Biodiversity and Indicators of Sustainable Ecosystems","CCF","CyberSEES","09/01/2015","08/27/2015","Heidi Sosik","MA","Woods Hole Oceanographic Institution","Standard Grant","Sylvia J. Spengler","08/31/2019","$629,174.00","Stace Beaulieu","hsosik@whoi.edu","183 OYSTER POND ROAD","WOODS HOLE","MA","025431041","5082893542","CSE","8211","8208, 8231","$0.00","Marine ecosystems are complex and variable networks of interacting organisms that support roughly half of global productivity and play important roles in sustaining global processes. The lower trophic levels that are the focus of this project include primary producers at the base of the food web, as well as bacteria and a variety of small grazers that turn over rapidly and provide links that transfer energy to higher trophic levels, such as fish, seabirds, and marine mammals, helping to maintain healthy ecosystems. Mounting recognition points to the societal value of not just timely information about the status and change of marine ecosystems but how well we can model and predict them. While existing and emerging observation systems are poised to contribute, there is a large need for computationally-enabled marine biodiversity data and information systems. Despite recognition as system-level science, the way specialists and non-specialists think about ecosystems must move from reducing the problem spaces into specialized and finer partitions to a significantly more integrative and predictive one. This research effort brings together computational and information scientists, oceanographers and microbiologists to develop a Marine Biodiversity Virtual Laboratory (MBVL). In addition to research investigations of marine ecosystems, the Virtual Laboratory provides a platform for education via student diversity programs at the three institutions. The important learning opportunities will be two-fold for students: (1) to learn about, model, and make predictions for biodiversity in natural systems, and (2) to be exposed to working in an interdisciplinary team that includes both natural scientists and computer scientists. The project aims to foster a new generation of data scientists with skills that can cross disciplines. There will be capabilities to trace, verify and trust the original data and model sources that comprise key indicators of diversity as part of ecosystem health; this has become a requirement for technical (and societal) implementations.  MBVL will contain a knowledge base for such indicators. While this is essential for research, the MBVL will also enhance education as students will truly probe, question, explore and experiment with the health, risks, and possible changes in specific marine environments. <br/><br/>MBVL will address multi-scale, heterogeneous data challenges with informatics solutions that enable the cyber-generation and documentation of biodiversity indicators, providing the traceability between data and information to be used as a basis for sustainable ecosystem-based management and needed policy decisions. Thus researchers and students will be further engaged to think and act at the system-level. The research will have impacts beyond ocean sciences, i.e. more broadly in management and policymaking regarding marine resources and ecosystem services via key links with fisheries managers enabled by collaborations external to this project and with industry partners to evaluate and disseminate solutions to a much wider user base."
"1539291","CyberSEES: Type 2: Collaborative Research: A Computational and Analytic Laboratory for Modeling and Predicting Marine Biodiversity and Indicators of Sustainable Ecosystems","CCF","CyberSEES","09/01/2015","08/27/2015","David Mark Welch","MA","Marine Biological Laboratory","Standard Grant","Sylvia J. Spengler","08/31/2019","$151,236.00","","dmarkwelch@mbl.edu","7 M B L ST","WOODS HOLE","MA","025431015","5082897243","CSE","8211","8208, 8231","$0.00","Marine ecosystems are complex and variable networks of interacting organisms that support roughly half of global productivity and play important roles in sustaining global processes. The lower trophic levels that are the focus of this project include primary producers at the base of the food web, as well as bacteria and a variety of small grazers that turn over rapidly and provide links that transfer energy to higher trophic levels, such as fish, seabirds, and marine mammals, helping to maintain healthy ecosystems. Mounting recognition points to the societal value of not just timely information about the status and change of marine ecosystems but how well we can model and predict them. While existing and emerging observation systems are poised to contribute, there is a large need for computationally-enabled marine biodiversity data and information systems. Despite recognition as system-level science, the way specialists and non-specialists think about ecosystems must move from reducing the problem spaces into specialized and finer partitions to a significantly more integrative and predictive one. This research effort brings together computational and information scientists, oceanographers and microbiologists to develop a Marine Biodiversity Virtual Laboratory (MBVL). In addition to research investigations of marine ecosystems, the Virtual Laboratory provides a platform for education via student diversity programs at the three institutions. The important learning opportunities will be two-fold for students: (1) to learn about, model, and make predictions for biodiversity in natural systems, and (2) to be exposed to working in an interdisciplinary team that includes both natural scientists and computer scientists. The project aims to foster a new generation of data scientists with skills that can cross disciplines. There will be capabilities to trace, verify and trust the original data and model sources that comprise key indicators of diversity as part of ecosystem health; this has become a requirement for technical (and societal) implementations.  MBVL will contain a knowledge base for such indicators. While this is essential for research, the MBVL will also enhance education as students will truly probe, question, explore and experiment with the health, risks, and possible changes in specific marine environments. <br/><br/>MBVL will address multi-scale, heterogeneous data challenges with informatics solutions that enable the cyber-generation and documentation of biodiversity indicators, providing the traceability between data and information to be used as a basis for sustainable ecosystem-based management and needed policy decisions. Thus researchers and students will be further engaged to think and act at the system-level. The research will have impacts beyond ocean sciences, i.e. more broadly in management and policymaking regarding marine resources and ecosystem services via key links with fisheries managers enabled by collaborations external to this project and with industry partners to evaluate and disseminate solutions to a much wider user base."
"1718297","SHF:Small: Collaborative Research: Tailoring Memory Systems for Data-Intensive HPC Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/04/2017","Qing Liu","NJ","New Jersey Institute of Technology","Standard Grant","Almadena Y. Chtchelkanova","07/31/2020","$147,998.00","","qliu@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","7798","7923, 7942, 9215, 9251","$0.00","High-performance computing is of strategic importance for computational science and engineering in the United States, and is on an accelerated path to sustaining scientific discovery at much increased flops. To advance the scientific discovery to the next level and allow new science missions to be accomplished in a timely manner, it is critical to address the memory performance holistically in high-performance computing platforms. This project provides architectural and system support and optimization for building memory systems tailored for data-intensive applications, e.g., big data analytics. This project offers research and educational opportunities for both undergraduate and graduate students, and trains a new generation of computer scientists and engineers in the area of high-performance computing. <br/>The objective of this project is to address the research challenges in building an efficient memory system by designing new techniques from several aspects. It develops a novel centralized memory refresh scheme at the cluster-level to manage memory refresh overhead, which has been increasingly performance-impacting and energy-consuming. It designs a new memory scheduling policy, taking advantages of new memory characteristics. It makes memory characteristics/peculiarities be available to the processor and operating system, so that they can make well-informed decisions to fully exploit memory performance potentials. It leverages in-memory computing to enable efficient in-situ processing. The integration of all these techniques provides a holistic solution to building an efficient memory system tailored for data-intensive applications."
"1618679","AF: Small: Harmonic Analysis for Quantum Complexity","CCF","ALGORITHMIC FOUNDATIONS","06/01/2016","05/17/2016","Ryan O'Donnell","PA","Carnegie-Mellon University","Standard Grant","Dmitri Maslov","05/31/2019","$450,000.00","","odonnell@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7923, 7927, 7928","$0.00","The PI will conduct new research to advance the field of harmonic analysis of Boolean functions.  At a high level, harmonic analysis can be described as the theory of using a particular mathematical tool, the Fourier transform, for detecting, analyzing, and representing patterns in large data sets.  Applying it in the context of Boolean functions refers to the case where the data comes from 0s and 1s, the basic building blocks in computer science.  Harmonic analysis of Boolean functions has proven to be a powerful tool in computer science, with application to theories of machine learning, error-correcting codes, cryptography, privacy, optimization algorithms, and distributed computing.  Part of the aim of this project is to further develop the mathematical and computer science foundations of harmonic analysis of Boolean functions.  Another aim of the project is to expand its scope to include further applications in the field of quantum computation.  If and when quantum computers are built, it is known that they will be able to break cryptographic codes that are secure today; this is fundamentally due to their ability to efficiently perform Fourier transforms on huge data sets.  However the full potential power of quantum computation is not well understood, and building quantum computers remains a major engineering challenge.  The project will investigate these issues, analyzing the advantages of quantum computers over classical ones, and understanding how to more efficiently test components of quantum computers. A final key outcome of the project will be scientific and educational training for computer science graduate students at Carnegie Mellon University, as well as wide dissemination of the research produced.<br/><br/>            At a more technical level, the project has several major-stretch-goals that will be used to guide the research in harmonic analysis of Boolean functions.  Included among these are conjectures of Aaronson and Ambainis concerning the influence of variables on low-degree Boolean functions, and on the decision tree complexity of checking correlation of a function with its Fourier transform.  Proofs of these conjectures would yield new complexity results delimiting the power of quantum computation versus classical computation.  Another major technical goal of the project is to develop the theory of learning and testing an unknown quantum state."
"1525817","AF: Small: Geometric Methods for Network Science","CCF","ALGORITHMIC FOUNDATIONS","07/01/2015","08/09/2016","Subhash Suri","CA","University of California-Santa Barbara","Standard Grant","Rahul Shah","06/30/2019","$599,875.00","","suri@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7796","7923, 7929, 8091","$0.00","Analyzing the spread of an epidemic, evaluating the vulnerability of Internet-based infrastructures, modeling dynamics of opinion in online social networks, understanding interconnections in the brain, and interactions between genes -- all these tasks are, in an abstract setting, struggling to understand large-scale, complex networks of interlinked nodes.  These networks are complex not only because of their scale---social networks and the Internet-of-Things span billions of nodes and even coarse information maps of the brain involve many millions of voxels and pathways---but also because they entail complex, noisy, and time-varying behavior---the structure of links in social networks or connections in brain is poorly understood, and virtually all real networks are subject to non-linear dynamics. The abstraction of graph theory, in which any nodes can be linked, leads to algorithms that become computational bottlenecks on the graphs for these large and messy networks. This project advocates that embedding complex networks in a geometric space gives a framework to apply the use of geometric methods for fast, scalable and approximate analysis of complex networks.<br/><br/>The project has three research thrusts: <br/>(1) theoretical investigation of geometric embeddings for 'scalable network analysis:' to better understand theoretically why certain graphs appear (empirically) to embed nicely in low-dimensional spaces, discover natural graph properties that cause large distortions, and design efficient and scalable algorithms for constructing low-distortion embeddings.<br/>(2) probabilistic embeddings for 'uncertain graphs:' to evaluate how the spatial richness of geometric embeddings capture the link uncertainties inherent in virtually all practical graph models, and<br/>(3) embedding of graphs in two-dimensional plane for 'information cartography:' to embed complex graphs in the two-dimensional plane to reveal important structural themes.<br/>These thrusts complement each other, since network analysis invariably requires both an initial quantitative part---estimating various network statistics by highly efficient algorithms that are enabled by the embedding of the entire network---followed by a qualitative presentation---displaying those network aggregates and substructrures in a visual context to create an informational cartogram.<br/><br/>The results of this project will significantly broaden the applicability of geometric algorithms to network science, and to modern data sets in general. The project touches upon many topics in theoretical computer science and mathematics including discrete and computational geometry, non-Euclidean geometry, probability theory, graph drawing, and information cartography."
"1617801","CIF: Small: Collaborative Research: Scalable Nonconvex Optimization with Statistical Guarantees for Information Computing in High Dimensions","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/29/2016","Yiyuan She","FL","Florida State University","Standard Grant","Phillip Regalia","06/30/2020","$285,000.00","","yshe@stat.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7797","7923, 7936","$0.00","The data explosion in all fields of science recently creates an urgent need for methodologies for analyzing high dimensional data with low-dimensional structure. The research, devoted to developing transformative theory and methods for scalable nonconvex optimization with statistical guarantees in noisy settings, has applications in a wide range of disciplines such as signal processing, machine learning, operations research, and computer vision. The investigators propose methodologies of statistics-guided nonconvex optimization and optimization-assisted statistical analysis to study convergence rate, acceleration, and statistical accuracy of iterative nonconvex algorithms for signals with mixed types of structural parsimony. The project cross-fertilizes ideas from statistics, operations research, engineering, and computer science and has education tightly coupled with research.  The integrated research and education help students develop critical thinking through cross-disciplinary training, and assist students in becoming life-long learners. The investigators use the rich topics in this project to inspire the learning and discovery interest of the public and students of all ages; in addition, the outreach activities help attract minority and female students to careers in science.<br/><br/>The research performs statistical-accuracy guided algorithmic analysis of general majorization-minorization algorithms for fast and stable signal recovery. Scalable and randomized acceleration schemes are proposed and studied in big-data applications. The investigators develop an innovative optimization-based statistical methodology for analyzing multi-regularized sparse estimators in high dimensions. The project applies the techniques to robust principle component estimation, hierarchical modeling, network learning, among others.  The research creates a fusion of optimization and statistics for information computing in high dimensions, and deepens and broadens existing compressed sensing and nonconvex optimization theories and methods."
"1617815","CIF: Small: Collaborative Research: Scalable Nonconvex Optimization with Statistical Guarantees for Information Computing in High Dimensions","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/29/2016","Dapeng Wu","FL","University of Florida","Standard Grant","Phillip Regalia","06/30/2020","$215,000.00","","wu@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7797","7923, 7936","$0.00","The data explosion in all fields of science recently creates an urgent need for methodologies for analyzing high dimensional data with low-dimensional structure. The research, devoted to developing transformative theory and methods for scalable nonconvex optimization with statistical guarantees in noisy settings, has applications in a wide range of disciplines such as signal processing, machine learning, operations research, and computer vision. The investigators propose methodologies of statistics-guided nonconvex optimization and optimization-assisted statistical analysis to study convergence rate, acceleration, and statistical accuracy of iterative nonconvex algorithms for signals with mixed types of structural parsimony. The project cross-fertilizes ideas from statistics, operations research, engineering, and computer science and has education tightly coupled with research.  The integrated research and education help students develop critical thinking through cross-disciplinary training, and assist students in becoming life-long learners. The investigators use the rich topics in this project to inspire the learning and discovery interest of the public and students of all ages; in addition, the outreach activities help attract minority and female students to careers in science.<br/><br/>The research performs statistical-accuracy guided algorithmic analysis of general majorization-minorization algorithms for fast and stable signal recovery. Scalable and randomized acceleration schemes are proposed and studied in big-data applications. The investigators develop an innovative optimization-based statistical methodology for analyzing multi-regularized sparse estimators in high dimensions. The project applies the techniques to robust principle component estimation, hierarchical modeling, network learning, among others.  The research creates a fusion of optimization and statistics for information computing in high dimensions, and deepens and broadens existing compressed sensing and nonconvex optimization theories and methods."
"1747669","EAGER: Cyberinfrastructure Reproducibility Project: Computational Science and Engineering","CCF","INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE, SOFTWARE & HARDWARE FOUNDATION","09/01/2017","08/31/2017","Lorena Barba","DC","George Washington University","Standard Grant","Almadena Chtchelkanova","08/31/2019","$299,847.00","","labarba@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","1640, 7231, 7798","040Z, 7916, 7942","$0.00","The progress of science today relies heavily on the use of computers, from lab workstations to national supercomputers. This project confronts the issue of ensuring that scientific knowledge stemming from computational research complies to high standards of reproducibility and rigor. The NSF CISE Directorate ""Dear Colleague Letter: Encouraging Reproducibility in Computing and Communications Research"" (NSF 17-022, 2016) encourages researchers to embrace completeness and transparency in developing rigorous protocols. Reproducibility concerns often focus on transparency via open data, code, and other research objects. But providing the data and code to run all analyses again is, however, a minimum standard. In the context of high-performance computing where research uses multi-million-dollar facilities the understanding of where lie the sources of non-reproducibility is still lacking. This project uses methodical replication of previous studies in computational fluid dynamics as the model for reaching that understanding and for developing the guiding principles of study design that can guarantee, as much as possible, reproducible findings. The results of this project will serve NSF's mission to promote the progress of science; to advance the national health, prosperity and welfare; to secure the national defense by bringing new and necessary understanding about how to achieve rigorous, reproducible computational research. <br/>This project conducts methodical replication of published studies in computational fluid dynamics, as a model for the broad field of computational science and engineering. Its aims are to 1) identify and characterize sources of non-reproducibility, and 2) develop guidelines on Design for Reproducibility, i.e., study design guaranteeing reproducible findings. The project addresses the role of scientific software libraries (like linear algebra solvers), the influence of hardware architectures , the role of new technologies for reproducible research (e.g., containers, cloud services). The project also provides guidelines for making assessments about the success of replication studies in the context of high-performance computing, including when it is reasonable or not to expect bit-by-bit numerical reproducibility. The research tackles three phases of computational research: Research methods; Research communication; Research assessment. The project also develops openly licensed training materials on reproducible computational research, including a graduate seminar course and a Responsible Conduct of Research (RCR) module."
"1562041","AF: Medium: Generalized Algebraic Graph Theory: Algorithms and Analysis","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","07/25/2017","Daniel Spielman","CT","Yale University","Continuing grant","Rahul Shah","08/31/2020","$572,933.00","","spielman@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7796","7924, 7926","$0.00","The PI will develop new methods for analyzing, reasoning about, and making predictions about graphs and networks.  Graphs and networks appear throughout society and science.  They include road and transportation networks, communication networks, power networks and social networks.  They are one of the dominant abstractions of data in Computer Science, and are used to model abstract interactions in fields ranging from Genomics to Image Processing and Machine Learning.<br/><br/>The project has two major research thrusts.  The first is the development of faster algorithms for performing existing analyses.  The second is the development of new approaches to understanding the structure of graphs and networks.  During the project, the PI will also develop and distribute course materials to teach recent developments in the field, will give public lectures on related material, will train graduate and undergraduate students in research, and will develop software that others can use to perform these analyses.<br/><br/>The fundamental object to be studied in this project are graph structured block matrices (GSBMs)---block matrices whose nonzero structure corresponds to the edges of a graph.  The first part of the project will involve the development of fast algorithms for the solution of systems of linear equations in GSBMs that can be written as a sum of positive semindefinte matrices with each matrix corresponding to one edge of the graph.  These GSBMs are generalizations of Laplacian matrices and arise in many application areas, including Optimization, Computational Science, and Image Processing.  The second part of the project will involve the generalization of spectral graph theory to the study of the expected characteristic polynomials of GSBMs with randomly chosen block matrices.  Spectral graph theory has been one of the most useful tools for analyzing graphs and networks.  The extension of the theory to random GSBMs should enable analyses that are not possible with the standard approach."
"1439079","XPS:FULL:DSD: A novel framework for developing highly scalable and energy efficient guaranteed quality mesh generation for 3D and 4D finite element analysis","CCF","Exploiting Parallel&Scalabilty","08/01/2014","07/18/2014","Nikos Chrisochoides","VA","Old Dominion University Research Foundation","Standard Grant","Micah Beck","07/31/2019","$850,000.00","Dimitrios Nikolopoulos, Andrey Chernikov","nikos@cs.odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","CSE","8283","","$0.00","Computational science is one of the ""three pillars,"" complementing traditional theoretical and physical experimental studies in science and engineering. Parallel finite element mesh generation is a critical building block for this pillar and is becoming even more relevant for a growing number of engineering and life science applications. This project will set up a trajectory to deliver the &#64257;rst exascale-era unstructured finite element (FE) guaranteed quality Delaunay mesh generation.<br/><br/>Why is FE mesh generation an important computational science tool? Many partial differential equations (PDEs) that are used to model complex multi-scale phenomena such as blood flow in the human body can only be solved by numerical approximation techniques. These techniques require the approximation of the domain by tessellating it into simpler geometric shapes such as triangles and tetrahedra in two and three dimensions, respectively. This project's focus is in 3D and 4D tessellation methods for life science applications such as blood flow simulations for Cerebro-Vascular Disease (CVD, or stroke), one of the leading natural causes of death in the US. <br/><br/>In order to deliver exascale-era mesh generation, this project is set to achieve billion-way concurrency using substantially less electric power than today's state-of-the-art methods. This goal will be achieved by focusing on the following three objectives: (1) Integration of multiple parallel Delaunay mesh generation methods into a telescopic framework. (2) Development of application-speci&#64257;c models that describe the inherent concurrency and data access patterns of this framework. (3) Development of domain-speci&#64257;c energy-efficient and component-level (core and memory) power scaling for massively parallel mesh generation methods. <br/><br/>This project will have broader impact in many other life science applications such as those related to the President's BRAIN Initiative. For example, the mesh generation techniques from this project can be customized to perform (by 2020) computer simulations to understand the circuitry of the human brain - an important milestone to help understand diseases like Parkinson's and Alzheimer's, which are expected to increase with the aging of the U.S. population. <br/><br/>Finally, this project will contribute via the PI's MERIT outreach program for STEM education in K-12 and college students. The PI's goal is to ""mentor, excite, and retain"" students and help them to identify STEM areas of study that will transform them into responsible college graduates. The PI's MERIT Freshman Seminars (as opposed to traditional Freshman Seminars) re-connect students in the context of Research Experiences for Undergraduates (REU) activities (based on students interests) with highly visible national priorities such as the President's BRAIN Initiative. The MERIT program covers a variety of topics to reach as many students as possible with diverse interests and background. The goal is to prepare computational scientists to be entrepreneurs capable of understanding the ethical, economic, and research challenges in health care we face today."
"1841455","Second Workshop for Women in Computational Topology (WinCompTop 2)","CCF","TOPOLOGY, ALGORITHMIC FOUNDATIONS","02/01/2019","01/31/2019","Ellen Gasparovic","NY","Union College","Standard Grant","Rahul Shah","01/31/2020","$15,000.00","Erin Chambers","gasparoe@union.edu","807 Union Street","Schenectady","NY","123083103","5183886101","CSE","1267, 7796","7556, 7929","$0.00","This award will support the participation of 10 US-based participants at the Second Workshop for Women in Computational Topology (WinCompTop 2), during July 1-5, 2019. WinCompTop2 is a workshop that facilitates the formation of new and lasting research collaborations between junior and senior women working in the field of computational topology, a field that seeks to identify underlying patterns and mathematical structure in data from areas ranging from medical imaging to networks of sensor arrays.  WinCompTop 2 is designed to grow the network of women active in this field by providing both research opportunity and a unique environment for mentoring, collaboration, and cohort building. This type of event is critical, since despite numerous initiatives and undeniable progress over the past few decades, gender imbalance in mathematics and computer science remains significant. In particular, mathematics and computer science are two of three disciplines with the lowest percentage of women attaining PhDs, according to recent data collected by the National Science Foundation. There is considerable evidence that conferences and workshops such as WinCompTop2, geared toward women can and do make a significant impact, and participants report that they see strong benefits from these workshops, which support collaborations, broaden contacts, and raise visibility in the greater mathematical community.<br/><br/>The WinCompTop2 workshop fits under the broad umbrella of Research Collaboration Conferences for Women, which the Association for Women in Mathematics encourages and supports. The majority of the time at the workshop will be spent tackling open problems in small working groups headed by established leaders in the field. The schedule consists of long blocks of time for the working groups to do research. At two or more points during the week, all participants will convene and one member from each group will provide a brief summary of their team's progress. In addition, there will be a poster session for graduate students and early-career participants as well as other crucial networking and mentoring opportunities between women at different points in their career as well as between peer colleagues. These relationships will foster a supportive and community-oriented atmosphere that is ideal for generating professional connections and collaborations. The conference website may be found here: https://maths.anu.edu.au/news-events/events/women-computational-topology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617851","AF: Small: Sublinear Algorithms for Graph Optimization Problems","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","05/20/2016","Sanjeev Khanna","PA","University of Pennsylvania","Standard Grant","Rahul Shah","08/31/2019","$450,000.00","","sanjeev@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7796","7923, 7926","$0.00","Very-large scale graphs routinely arise in applications where the data describes pairwise relationships among a set of objects. Some standard examples include the Web graph, social networks, and biological networks. The prevalence of such large data sets has led to a rapidly growing interest in the design of sublinear algorithms, that is, algorithms whose computational resource requirements are substantially smaller than the input size. As vast amounts of networked data is being collected and processed in diverse application domains, sublinear algorithms that accurately compute and describe relevant properties of the data will increasingly play an important role in computing on such data. The goal of this project is to develop sublinear algorithms for several fundamental graph optimization problems. The specific graph problems studied in this project are of both theoretical and practical interest, and are among the most well-studied problems in combinatorial optimization. Additionally, a study of these problems through the lens of sublinear algorithms is likely to yield new insights into computational aspects of these fundamental problems. The research proposed here will go hand-in-hand with educational and student-training initiatives, including mentoring and training of undergraduate and graduate students, and teaching in programs that introduce high-school students to exciting ideas in theoretical computer science.<br/><br/>The research focus of this project is broadly divided into three parts. In the first part, the PIs consider streaming algorithms for graph problems where an input graph is revealed as a sequence of edge insertions and deletions. Some representative problems studied in this part include matching and cut problems in graphs. While both cuts and matchings have received considerable attention in the streaming literature, several important questions concerning their computability in the streaming model remain unresolved. In the second part, they consider communication-efficient protocols in a distributed setting when the input graph is partitioned across multiple sites. This model offers a natural abstraction for distributed computation and is closely related to the streaming model. A representative problem here is to understand the communication complexity of the maximum matching problem. The third part of this project investigates a new model for sketching graphs that consist of a (large) static part and a (small) dynamic part. The goal here is to understand if there exist compact sketches for several fundamental graph problems whose size is proportional to the size of the dynamic part of the input graph such that any updates to the dynamic part can be applied directly to the sketch."
"1849632","CRII: SHF: Towards a Cognizant Virtual Software Modeling Assistant using Model Clones","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2019","02/05/2019","Matthew Stephan","OH","Miami University","Standard Grant","Sol Greenspan","07/31/2021","$161,861.00","","matthew.stephan@miamioh.edu","500 E High Street","Oxford","OH","450563653","5135293600","CSE","7798","7798, 7944, 8228","$0.00","Software is growing increasingly omnipresent in society. Correspondingly, software quality, which includes both security and reliability, is more important than ever. Software failures can cause significant problems to both individuals and national economies. Security is also a paramount societal concern. This research project helps address quality by improving software modeling, a critical stage in software development where engineers specify what the software does and how it works.  It takes the first step in allowing engineers to leverage knowledge from data in the form of others' experiences and best practices to incorporate established models for use in their software. This project tackles the fundamental issues of how to 1) gather this data properly, 2) derive useful insights about that data, and 3) best present these insights and suggestions to engineers. By providing this assistance and information, engineers can make better-informed decisions, thus yielding higher quality software for all society.  In addition to helping engineers, software modeling is an important aspect of the STEM curriculum, including computer science, software engineering, and other engineering disciplines. Instructors can utilize the approaches and tools derived from this award as a teaching tool by helping students think critically about design decisions.  This will yield better computer scientists and engineers who are more comfortable and versed in formal software modeling.<br/><br/>Model-driven engineering (MDE) is an established formal methodology for building large-scale secure quality software systems.  This award will improve that quality by realizing a cognizant virtual software-modeling assistant to improve software design and MDE.  This project uses model-clone detection to analyze models during development, finds similar models from the same domain and/or best practices, and treats those similar models as training data to reason about in order to suggest model additions and modifications to users. Such assistance will yield similar benefits to those afforded by analogous source-code approaches based on past usage statistics. This involves an exploratory investigation to develop a new approach and prototype virtual software-modeling assistant using an established model clone detector.  In the first phase of this research, the investigator will build a prototype with the capability to analyze incomplete models being constructed/extended by engineers to suggest completed models for insertion based on similarity to those from the same domain and/or best practices.  In the second phase, the investigator will create an assistant that produces granular suggestions based on analyzing similar models, and presents options to engineers of operations they may want to do next based on those operations' prevalence in the knowledge base formed by those similar models.  This research's insights and data will provide the foundation necessary to build more advanced modeling assistants, conduct user studies and educational assessments of the approach, and help lay a foundation for the cognification of modeling.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1539608","CyberSEES: Type 1: A Novel Machine Learning Framework for Urban Heat Island Causal Analysis: a Fusion of Observations and Physical Models","CCF","CyberSEES","09/01/2015","08/27/2015","Yan Liu","CA","University of Southern California","Standard Grant","Phillip Regalia","08/31/2019","$400,000.00","George Ban-Weiss","yanliu.cs@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8211","8207, 8231","$0.00","The urban heat island is a phenomenon whereby urban areas have higher surface and atmospheric temperatures than surrounding suburban and rural regions. These higher temperatures lead to increases in building cooling energy use during summer, and can exacerbate urban air pollution, human thermal comfort, and heat waves. This project unites expertise in three fields---machine learning, civil and environmental engineering, and earth science---to develop a novel framework that integrates data-driven approaches and physics-based climate simulation models to better understand the physical process drivers of urban heat islands around the world. Better understanding the main drivers of urban heat islands in cities is critical for identifying the appropriate engineering solutions for mitigating urban warming from heat islands. This research will have broad societal impact by changing the ways that people participate in scientific data analysis tasks, and will build a stronger body of research in computational sustainability. <br/><br/>The proposed framework will integrate data-driven approaches and physically based models in one discovery process. It consists of three important steps: (i) latent feature discovery, which aims to automatically infer high-level feature representations from large scale observational data via deep networks. These latent features capture the complex nonlinear transformation of observed variables as a metaphor of latent physical processes; (ii) latent feature interpretation, which generates candidates of urban heat island drivers from the latent features via a compiled dataset. It provides insights into how these latent features are associated with physical processes; and (iii) urban heat island driver identification, which designs effective experiments to identify the causes of heat islands by varying observed variables from simulation models. The project is expected to advance the knowledge of key physical process drivers causing urban heat islands in cities. In addition, the developed framework has the potential to advance machine learning, including feature learning, causal analysis with confounders, and causal experiment design. The source code for computational tools and the data sets collected through the project will be freely disseminated to the broader research and educational community."
"1813173","SHF: Small: GPU-dedicated Graph Transformations for Accelerating Iterative Graph Analytics","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/27/2018","Zhijia Zhao","CA","University of California-Riverside","Standard Grant","Almadena Y. Chtchelkanova","09/30/2021","$499,987.00","Rajiv Gupta","zhijia@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","2878","7923, 7942","$0.00","Graph analytics yields deeper knowledge in many scientific domains by mining large volumes of highly connected data, such as social networks, airline networks, biological networks, and internet topology. Due to its compute and data-intensive nature, GPUs with massive parallelism hold great potential in accelerating graph analytics. However, existing solutions exhibit low utilization of GPU resources caused by the mismatch between GPU's design for regular computations and the highly irregular nature of real-world graphs. Moreover, GPUs often fail to handle relatively large graphs due to their limited on-device memory. The goal of this research is to dramatically improve the GPU resource utilization and boost the scalability of graph analytics by transforming the graphs in ways that make the data and workloads better fit in the GPU computing. The results of this research include software products that can be readily deployed on existing large-scale high-performance systems equipped with GPUs for executing real-world graph applications. More broadly, this research helps accelerate new discoveries in scientific fields like bioinformatics, social science, and public security. <br/><br/>Specifically, this research develops a series of GPU-oriented graph transformations that together address the challenges of irregularity, scalability, and load imbalance at the input graph level. These include: (1) graph transformations for regularity which transform the irregular graph structures into more regular ones to address the low GPU efficiency; (2) graph transformations for scalability which transform a large graph into a mix of acyclic and cyclic small graphs, with each of them fitting into the GPU global memory. By maximally migrating computation from the acyclic graphs to the cyclic ones, the transformations can greatly reduce the data movement between GPU memory and host memory; and (3) graph transformations for multi-GPU systems which address the GPU load imbalance caused by the variation of active nodes in iterative graph analytics. This is achieved by generating small yet overlapped graphs and selectively processing the overlapped regions. Finally, this research integrates the above transformations to maximize the overall benefits by tailoring the design of the transformations to the properties of input graphs and GPU platforms. The evaluation includes large graph data sets from KONNECT and SNAP repositories. The implementations of graph analysis algorithms are packaged into easy-to-use programming interfaces and released over the course of this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1525024","AF:  Small: Efficient Algorithms for Querying Noisy Distributed/Streaming Datasets","CCF","ALGORITHMIC FOUNDATIONS","06/15/2015","06/11/2015","Qin Zhang","IN","Indiana University","Standard Grant","Tracy J. Kimbrel","05/31/2019","$444,320.00","","qzhangcs@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7796","7923, 7934","$0.00","This project aims to study the design of efficient query algorithms for noisy datasets in distributed and streaming applications.  Noisy data is universal in today's world. Imprecise and varying references to the same real-world entities are ubiquitous in scientific and commercial databases.  This noise poses significant obstructions to accurate data analytics.  As an example of ""noisy data,"" consider YouTube videos. YouTube tracks the views of individual videos. However, there are frequently many similar versions of the same event and answering a basic question such as ""How many people viewed this event?"" is challenging using current techniques.  This project will provide new techniques and insights to combat the noisy nature of large datasets, and hence will enhance our ability to process the ever-increasing quantity of business and scientific data. The products of this project will be integrated into a trilogy of graduate and undergraduate courses on algorithms, databases, and data mining. The PI will disseminate research outcomes by giving talks at conferences/workshops, universities, industrial labs, as well as online media.<br/><br/>More technically, this project tries to answer the following question: can we run distributed and streaming algorithms directly on the noisy datasets, resolve the noise ""on the fly"", and retain communication and space efficiency compared with the noise-free setting?  The PI plans to study statistical, relational and graph problems.  This project has the potential to impact a wide range of active research areas in theoretical computer science, including distributed and streaming algorithms, group testing, compressed sensing, communication complexity, clustering, and locality sensitive hashing."
"1814409","AF: Small: New Frontiers in Local Error-Correction","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","10/01/2018","08/31/2018","Swastik Kopparty","NJ","Rutgers University New Brunswick","Standard Grant","Tracy J. Kimbrel","09/30/2021","$397,000.00","","swastik.kopparty@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","2878, 7796","7923, 7926, 7927","$0.00","Modern communication and data storage technologies use mathematical error-correcting codes to cope with noise and unreliability of physical devices. This project will develop new error-correcting codes which support ""local"" algorithms for error-detection and error-correction: these give strong error-correction guarantees at ultra-fast speeds. The error-correcting codes and algorithms developed in this project will have the potential be applied to real-world data storage applications, which is very relevant to current cloud computing technology. The educational component of this project will involve the mentoring and education of junior researchers who intend to pursue their own careers in research, including women and minorities. This project will also develop courses about the important advances in the relevant topics, and make the course materials publicly available.<br/><br/>At the technical level, this project investigates the main problems on the existence and construction of, and algorithms and fundamental limits for local error-correcting codes. Local error-correcting codes are modern versions of error-correcting codes that support sublinear-time error detection and/or correction. They have gained increasing importance in theoretical computer science over the last few decades, both because of potential applications to communication and data storage, as well as connections to complexity theory, pseudorandomness, and cryptography. This project will develop new algebraic, probabilistic and algorithmic tools to design and reason about such codes, and local algorithms more generally. This project is motivated by several recent advances made by the investigator, such as the construction of new high rate error-correcting codes allowing, for the first time, subpolynomial-time error-correction and error-detection, and the first constructions of probabilistically checkable proofs of constant rate, checkable in sublinear time. These advances have significantly altered what is believed to be possible in this domain, and have the potential to dramatically change the way data is stored in data centers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1514245","CIF: Medium: Collaborative Research: Quickest Change Detection Techniques with Signal Processing Applications","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2015","07/12/2017","Venugopal Veeravalli","IL","University of Illinois at Urbana-Champaign","Continuing grant","Phillip Regalia","07/31/2019","$653,975.00","Georgios Fellouris","vvv@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7924, 7936","$0.00","The problem of detecting changes in stochastic systems, often referred to as sequential change detection or quickest change detection, arises in various branches of science and engineering. In all these applications, an anomaly in the environment changes in some way the distribution of the sequentially acquired observations. The goal is to detect the change and raise an alarm as soon as possible, so that any necessary action can be taken in time, while controlling the rate of false alarms below an acceptable level. While the quickest change detection problem has been actively studied since early 1950s, there are many open challenges in this field that are of theoretical as well as practical interest. <br/><br/>This research addresses long-standing open problems in quickest change detection, as well as challenging problems that are motivated by modern applications, such as the following: 1) Optimum quickest change detection for Markov data; 2) Optimum quickest detection for transient changes; 3) (Asymptotically) optimum quickest change detection schemes for multistream data when changes are sparse; 4) Joint quickest change detection and isolation (localization of the change) in multistream data; 5) Controlled sensing for quickest change detection and isolation with composite post change hypothesis; 6) Data-driven quickest outlier detection and isolation. Furthermore, the investigators study the applications of their results in the following areas: 1) Line outage detection in power systems; 2) Epidemic detection; 3) Change detection in financial applications; 4) Surveillance using sensor networks; 5) Dynamic spectrum sensing; 6) Intrusion detection in power grids/networks; 6) Anomaly and fraud detection in big data."
"1527618","CIF: Small: Learning Mixed Membership Models with a Separable Latent Structure: theory, provably efficient algorithms, and applications","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","07/09/2015","Prakash Ishwar","MA","Trustees of Boston University","Standard Grant","Phillip Regalia","08/31/2019","$500,000.00","Venkatesh Saligrama","pi@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7797","7923, 7936","$0.00","In a wide spectrum of problems in science and engineering that includes hyperspectral imaging, gene expression analysis, and metabolic networks, the observed data is high-dimensional and arises from an unknown random mixture of a small set of unknown shared latent (hidden) causes. Being able to successfully and efficiently identify the latent causes from the observed data is important not only for scientific understanding, but also for efficient data representation and decision making. Popular algorithms for such problems make use of approximations and heuristics for computational tractability.  As a consequence, consistency or efficiency guarantees for such algorithms are either very weak or nonexistent. This research involves the development of algorithms for learning latent-cause models from high-dimensional data with provable statistical and computational efficiency guarantees.<br/><br/>The linchpin of this research is a natural separability property of the shared latent factors ? the presence of a signature component for each latent factor ? that is approximately satisfied by the estimates produced by popular approaches. This research aims to establish that approximate separability is not only a natural and convenient structural property of mixed-membership latent-factor models, but is, in fact, an inevitable consequence of high-dimensionality. This research also involves the development of a suite of provably consistent and statistically and computationally efficient algorithms for a diverse set of mixed-membership latent-factor problems by suitably leveraging the geometry induced by the signature components. The key insight is to identify the signature parts of each latent factor as extreme points in a suitable space. This can be done efficiently through appropriately defined random projections. The random-projections-based algorithm is naturally amenable to a low-communication-cost distributed implementation that is attractive for web-scale distributed data-mining applications."
"1513373","CIF: Medium: Collaborative Research: Quickest Change Detection Techniques with Signal Processing Applications","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2015","09/19/2016","George Moustakides","NJ","Rutgers University New Brunswick","Continuing grant","Phillip Regalia","07/31/2019","$467,186.00","","gm463@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7797","7797, 7924, 7936","$0.00","CIF: Medium: Collaborative Research: Quickest Change Detection Techniques with Signal Processing Applications<br/><br/>Project abstract<br/><br/>The problem of detecting changes in stochastic systems, often referred to as sequential change detection or quickest change detection, arises in various branches of science and engineering. In all these applications, an anomaly in the environment changes in some way the distribution of the sequentially acquired observations. The goal is to detect the change and raise an alarm as soon as possible, so that any necessary action can be taken in time, while controlling the rate of false alarms below an acceptable level. While the quickest change detection problem has been actively studied since early 1950s, there are many open challenges in this field that are of theoretical as well as practical interest. <br/><br/>This research addresses long-standing open problems in quickest change detection, as well as challenging problems that are motivated by modern applications, such as the following: 1) Optimum quickest change detection for Markov data; 2) Optimum quickest detection for transient changes; 3) (Asymptotically) optimum quickest change detection schemes for multistream data when changes are sparse; 4) Joint quickest change detection and isolation (localization of the change) in multistream data; 5) Controlled sensing for quickest change detection and isolation with composite post change hypothesis; 6) Data-driven quickest outlier detection and isolation. Furthermore, the investigators study the applications of their results in the following areas: 1) Line outage detection in power systems; 2) Epidemic detection; 3) Change detection in financial applications; 4) Surveillance using sensor networks; 5) Dynamic spectrum sensing; 6) Intrusion detection in power grids/networks; 6) Anomaly and fraud detection in big data."
"1740391","RAISE: Deep Gravitational Wave Exploration, Instrumental Insights and Noise Removal Through Machine Learning","CCF","LIGO RESEARCH SUPPORT, OFFICE OF MULTIDISCIPLINARY AC, COMM & INFORMATION FOUNDATIONS, INSPIRE","07/01/2017","06/14/2017","Szabolcs Marka","NY","Columbia University","Standard Grant","Phillip Regalia","06/30/2020","$1,000,000.00","Imre Bartos, John Wright, Zsuzsanna Marka","smarka@phys.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1252, 1253, 7797, 8078","049Z, 7936","$0.00","This is a RAISE Award supported by the Office of Integrative Activities, the Signal Processing Systems program of the Division of Computing and Communications (CCF) of the Computer & Information Science & Engineering Directorate (CISE), the Office of Multidisciplinary Activities of the Mathematical and Physical Sciences Directorate (MPS) and the Gravitational Physics program of Physics Division in MPS. The recent discovery of gravitational waves from colliding black unveiled a new era of broad opportunities for studying the Cosmos. The coming years will bring about the proliferation of detections of black hole mergers as well as other sources of gravitational waves, including stellar explosions. For every discovery, there will be numerous weak gravitational wave signals buried in the detector noise that will be difficult to unearth. Gravitational-wave detectors are incredibly complex systems, where there are myriads of independent ways noise sources can interfere with the recorded data, occasionally producing curious data artifacts that are difficult to distinguish from gravitational waves. Machine learning is uniquely suited to make sense of this complexity, and disentangle data from the noise to broaden our horizon to detecting gravitational waves.<br/><br/>The PIs will design machine-learning techniques to make sense of LIGO's 400,000 auxiliary data channels and identify patterns in detector behavior to enable the identification of cosmic signals in the midst of highly non-linear and non-Gaussian background noise. The PIs will research and use optimal strategies, including sparse regression and robust principal component analysis, to distinguish detector or environmental artifacts from astrophysical signals to discover gravitational waves that otherwise could have remained invisible."
"1652442","CAREER:  Machine and Structure Oblivious Graph Analytics","CCF","CAREER: FACULTY EARLY CAR DEV, Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","04/15/2017","04/16/2017","Erik Saule","NC","University of North Carolina at Charlotte","Continuing grant","Almadena Y. Chtchelkanova","03/31/2022","$281,305.00","","esaule@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","1045, 7354, 7798","1045, 7942","$0.00","Graphs are fundamental mathematical tools used to represent entities and their interactions, such as intersections and roads that connect them, proteins and the genes that regulate them, or people and the social relation that binds them. In the last two decades, graphs have been applied to virtually all parts of human activity such as health, literature, national defense, and urban planning.  The Internet and the information age in general increased significantly the amount of data that can be leveraged, and this has increased the size of the graphs being studied as well as the complexity of the analyses performed on them. Data analysts can not easily look into this kind of data as the current software and simple machines can not easily process the analysis and utilizing more powerful systems is often out of their skill set. Technically, the problem is that there is a wide variety of graphs to analyze (meshes of 3D objects, social networks, road networks to name a few) that have different properties in term of size, diameter, and connectivity. Even for a single problem, these differences cause differences in the algorithm that will solve the problem best; but the issue is magnified by the variety of analysis to perform. To make the matter worse, powerful workstations, accelerators, and clusters are different computing systems that are hard to leverage and could be relevant factors depending on which graph and which analysis is performed.<br/><br/>This project answers the question posed by application scientists `How to best solve MY computational graph problem?'. The purpose of the project is to gain a clear understanding of the performance of graph algorithms on different hardware architectures, to understand which modes of operation are preferable to use, to design new algorithms for the cases where no good solutions exists, and to design better algorithms for common use cases.  The project is based around a model-develop-experiment cycle to construct better algorithms geared at particular use cases. In particular it develops new algorithmic techniques to perform graph analysis by shortening critical paths, by leveraging vectorization, and by replicating data to improve load balance. Accurate modeling of the analyses is used to give insight on how to design better algorithms and to enable picking the best way to perform an analysis.  Software is designed to confirm the soundness of the performed work and to provide application experts with an efficient tool that does not require high performance computing expertise. <br/>The project provides software, algorithms, and models which increase productivity of data analysts by reducing the development burden on the analyst and by efficiently using computing systems to analyze graphs in a timely fashion. The project also contributes to the education of undergraduate students by designing educational modules to train them in understanding and solving computing performance issues, and to the broadening of participation in STEM by preparing related activities and presenting them in diverse high schools and science fairs."
"1813053","AF: Small : Collaborative Research : A Theory of High Dimensional Property Testing","CCF","ALGORITHMIC FOUNDATIONS, EPSCoR Co-Funding","07/01/2018","06/21/2018","Deeparnab Chakrabarty","NH","Dartmouth College","Standard Grant","Rahul Shah","06/30/2020","$265,017.00","","deeparnab@dartmouth.edu","OFFICE OF SPONSORED PROJECTS","HANOVER","NH","037551404","6036463007","CSE","7796, 9150","7923, 7926, 9150","$0.00","The advent of massive data sets requires the design and analysis of algorithms accessing only a tiny portion of input data. This proposal aims to further the mathematical study of these algorithms in the context of sublinear algorithms and property testing within theoretical computer science. Specifically, the focus is an in-depth understanding of data represented as high dimensional functions, a paradigm prevalent in many optimization problems, and how useful properties of these can be quickly ascertained using small samples. An understanding of these issues foreseeably will lead to better, faster, and more robust algorithms for data analysis. The proposal involves training and mentoring graduate and undergraduate students at the investigators' respective institutions with special attention given to women and minority students. The findings of this proposal will be made accessible to public not only via technical reports but also via blogs and videos accessible to everyone. The investigators have a track record of converting theoretical understandings to practical algorithms, and this proposal will continue this effort.<br/> <br/>Discrete, high dimensional functions are ubiquitous in science, and it is imperative to understand and exploit properties of these functions. Many fundamental properties such as monotonicity, Lipschitz continuity, convexity and submodularity are defined by bounds on the first, second, or higher derivatives of these functions. This proposal aims to understand the theory behind derivative-bounded property testing, and in particular discover the fastest algorithms that determine whether a function (approximately) satisfies a property from this class. In particular, the proposal aims to achieve the following goals: (1) Obtain a fast tester of submodularity and discrete convexity, building on previous work of the investigators on first derivative testers. (2) Transfer results from discrete settings to continuous settings, and obtain testers for properties like convexity. (3) Uncover more connections between geometric concepts like duality and isoperimetry with derivative testing algorithms, as has been suggested by previous work of the investigators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813165","AF: Small : Collaborative Research : A Theory of High Dimensional Property Testing","CCF","ALGORITHMIC FOUNDATIONS","07/01/2018","06/21/2018","C. Seshadhri","CA","University of California-Santa Cruz","Standard Grant","Rahul Shah","06/30/2020","$229,999.00","","sesh@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7796","7923, 7926","$0.00","The advent of massive data sets requires the design and analysis of algorithms accessing only a tiny portion of input data. This proposal aims to further the mathematical study of these algorithms in the context of sublinear algorithms and property testing within theoretical computer science. Specifically, the focus is an in-depth understanding of data represented as high dimensional functions, a paradigm prevalent in many optimization problems, and how useful properties of these can be quickly ascertained using small samples. An understanding of these issues foreseeably will lead to better, faster, and more robust algorithms for data analysis. The proposal involves training and mentoring graduate and undergraduate students at the investigators' respective institutions with special attention given to women and minority students. The findings of this proposal will be made accessible to public not only via technical reports but also via blogs and videos accessible to everyone. The investigators have a track record of converting theoretical understandings to practical algorithms, and this proposal will continue this effort.<br/> <br/>Discrete, high dimensional functions are ubiquitous in science, and it is imperative to understand and exploit properties of these functions. Many fundamental properties such as monotonicity, Lipschitz continuity, convexity and submodularity are defined by bounds on the first, second, or higher derivatives of these functions. This proposal aims to understand the theory behind derivative-bounded property testing, and in particular discover the fastest algorithms that determine whether a function (approximately) satisfies a property from this class. In particular, the proposal aims to achieve the following goals: (1) Obtain a fast tester of submodularity and discrete convexity, building on previous work of the investigators on first derivative testers. (2) Transfer results from discrete settings to continuous settings, and obtain testers for properties like convexity. (3) Uncover more connections between geometric concepts like duality and isoperimetry with derivative testing algorithms, as has been suggested by previous work of the investigators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1619110","CCF-BSF: AF: Small: Collaborative Research: Algorithmic Techniques for Inferring Transmission Networks from Noisy Sequencing Data","CCF","ALGORITHMIC FOUNDATIONS","08/01/2016","07/20/2016","Aleksandr Zelikovskiy","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Mitra Basu","07/31/2020","$200,000.00","","alexz@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","7796","7923, 7931","$0.00","Many viruses encode their genome in RNA and exhibit high genomic diversity within their hosts. Advances in sequencing technologies have made it feasible to track viral transmissions and timely detect outbreaks on a global scale.  The goal of this project is to develop a comprehensive set of predictive mathematical models and accurate computational methods for integrated analysis of the massive epidemiological and sequencing datasets generated by emerging molecular surveillance programs.<br/> <br/>Research results will be broadly disseminated via journal publications and presentations at international conferences, including the Workshop on Computational Advances in Molecular Epidemiology organized by the PIs. Prototype implementations of developed algorithms will be distributed as open-source packages and incorporated in the cloud-based Global Hepatitis Outbreak and Surveillance Toolkit (GHOST) developed at CDC.  The project will provide ample opportunities for promoting participation of women and underrepresented groups in bioinformatics and molecular epidemiology research at GSU, UCONN, Georgia Tech and Tel Aviv University.  An important aspect of the project is to disseminate core concepts and ideas from Computer Science and Computational Biology to wide target audiences including: (1) teaching Computer Science, in an informal setting, to  middle and high school students, and (2) incorporating computational thinking into Life Science curriculum at the undergraduate university level.<br/> <br/>The proposed research and education activities will leverage the extensive expertise of an interdisciplinary team comprised of computer scientists, mathematicians, and molecular epidemiologists to develop accurate mathematical models and computational methods for key problems in molecular epidemiology including deconvolution and inference of viral variants from error-prone pooled sequencing data, inference of relatedness between viral samples and transmission networks, inference of transmission event times and network parameters, as well as predictive modeling  of transmission network dynamics. The team will carry out extensive algorithm validation on massive molecular surveillance datasets generated at CDC and develop robust prototype software implementations."
"1619253","SHF: Small: Collaborative Research: ALETHEIA: A Framework for Automatic Detection/Correction of Corruptions in Extreme Scale Scientific Executions","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","03/07/2019","Tom Peterka","IL","Northwestern University","Standard Grant","Almadena Chtchelkanova","05/31/2019","$244,197.00","Sheng Di","tpeterka@mcs.anl.gov","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7798","7923, 7942","$0.00","Trusting scientific applications requires guaranteeing the validity of computed results. Unfortunately, many examples of scientific computations have led to incorrect results, sometimes with catastrophic consequences. Currently known validation techniques cover only a fraction of the possible corruptions that numerical simulation and data analytics applications may suffer during execution. As science processes grow in size and complexity, the reliability and validity of their constituent steps is increasingly difficult to ascertain. Assessing validity in the presence of potential data corruptions is a serious and insufficiently recognized problem. Corruption may occur at all levels of computing, from the hardware to the application. An important aspect of these corruptions is that until they are discovered, all executions are at risk of being corrupted silently. In some documented cases, months have elapsed between the discovery of a corruption and notification to users. In the meantime, a potentially large number of executions may be corrupted, and incorrect conclusions may result. It may be difficult, after the fact, to check whether executions have actually been corrupted or not, so that even if corruptions do not lead to mistakes, they may lead to significant productivity losses. Virtually all simulations producing very large results need to reduce their data volume in some way before saving it --one technique is called lossy compression. <br/>This project strives to validate the end result of the simulation coupled with lossy compression. This approach is useful for scientific simulations in such diverse areas as climate, cosmology, fluid dynamics, weather, and astrophysics --the drivers of this project. <br/>This collaborative project applies the principle of an external algorithmic observer (EAO), where the product of a scientific application is compared with that of a surrogate function of much lower complexity. Corruptions are corrected using a variation of triple modular redundancy: if a corruption is detected, a second surrogate function is executed, and the correct value is chosen from the two results that are most in agreement. This new online detection/correction approach involves approximate comparison of the lossy compressed results of the scientific application and the surrogate function. The project explores the detection performance of surrogate functions, lossy compressors, and approximate comparison techniques. The project also explores how to select the surrogate, lossy compression, and approximate functions to optimize objectives and constraints set by the users. The evaluation considers a set of five applications spanning different computational methods, producing large datasets with I/O bottlenecks, and covering a variety of science problem domains relevant to the NSF. <br/>In addition to serving the needs of scientists working in the fields listed above, this project will enhance the research experience of undergraduate students. A summer school focused on resilience is planned for summer 2016, and corruption detection/correction will be a major topic. The project is also organizing tutorials in major science conferences that include online detection/correction of numerical simulations."
"1750443","CAREER: Communication, Information, and Interactive Compression","CCF","ALGORITHMIC FOUNDATIONS","02/01/2018","02/12/2019","Gillat Kol","NJ","Princeton University","Continuing grant","Tracy Kimbrel","01/31/2023","$166,717.00","","gillat.kol@gmail.com","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7796","1045, 7926, 7927","$0.00","Information theory had a profound impact on both the practical and theoretical communities, finding application in almost every branch of science and beyond. Information theory is especially relevant today with the rise of the internet and data-intensive applications. Lately, many of these applications are interactive, involving several communicating parties, and the trend for more interaction is growing.<br/><br/>Interactive information theory is a new field of study at the interface between information theory and computational complexity theory. The key ingredient that sets it apart from classical information theory is that it studies problems where parties are interacting and information flows in more than one direction. The goal of this project is to deepen our understanding of interactive information theory. Specifically, it considers the interactive compression problem, which is the interactive analogue of the data compression problem. Informally, the celebrated data compression theorems imply that every message can be compressed to its information content, measured using the Entropy function. The interactive compression problem asks whether the transcript of every communication protocol between several parties can be compressed to (roughly) its ""information content.""<br/><br/>An affirmative answer to this problem would yield a powerful method for proving nearly tight communication complexity lower bounds, and may shed light on other central problems in theoretical computer science that are closely related to it, such as the direct sum and parallel repetition problems for different models, and on the log-rank conjecture. Good compression protocols also suggest a new paradigm in protocol design, where one optimizes over the information revealed by the protocol and then applies a compression scheme to obtain a protocol with low information. The study of interactive compression can also be of interest to the privacy and information theory communities, which have considered similar notions.The PI will mentor students, give tutorials and create a new course on interactive information for Computer Science and Electrical Engineering majors.<br/>"
"1615407","CCF-BSF: AF: Small: Collaborative Research: Algorithmic Techniques for Inferring Transmission Networks from Noisy Sequencing Data","CCF","ALGORITHMIC FOUNDATIONS","08/01/2016","07/20/2016","Leonid Bunimovich","GA","Georgia Tech Research Corporation","Standard Grant","Mitra Basu","07/31/2020","$100,000.00","","bunimovh@math.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","7923, 7931","$0.00","Many viruses encode their genome in RNA and exhibit high genomic diversity within their hosts. Advances in sequencing technologies have made it feasible to track viral transmissions and timely detect outbreaks on a global scale.  The goal of this project is to develop a comprehensive set of predictive mathematical models and accurate computational methods for integrated analysis of the massive epidemiological and sequencing datasets generated by emerging molecular surveillance programs.<br/> <br/>Research results will be broadly disseminated via journal publications and presentations at international conferences, including the Workshop on Computational Advances in Molecular Epidemiology organized by the PIs. Prototype implementations of developed algorithms will be distributed as open-source packages and incorporated in the cloud-based Global Hepatitis Outbreak and Surveillance Toolkit (GHOST) developed at CDC.  The project will provide ample opportunities for promoting participation of women and underrepresented groups in bioinformatics and molecular epidemiology research at GSU, UCONN, Georgia Tech and Tel Aviv University.  An important aspect of the project is to disseminate core concepts and ideas from Computer Science and Computational Biology to wide target audiences including: (1) teaching Computer Science, in an informal setting, to  middle and high school students, and (2) incorporating computational thinking into Life Science curriculum at the undergraduate university level.<br/> <br/>The proposed research and education activities will leverage the extensive expertise of an interdisciplinary team comprised of computer scientists, mathematicians, and molecular epidemiologists to develop accurate mathematical models and computational methods for key problems in molecular epidemiology including deconvolution and inference of viral variants from error-prone pooled sequencing data, inference of relatedness between viral samples and transmission networks, inference of transmission event times and network parameters, as well as predictive modeling  of transmission network dynamics. The team will carry out extensive algorithm validation on massive molecular surveillance datasets generated at CDC and develop robust prototype software implementations."
"1856359","CyberSEES: Type 2: A Coastal Resilience Collaboratory: Cyber-enabled Discoveries for Sustainable Deltaic Coasts","CCF","CyberSEES","04/19/2018","12/13/2018","Qin Chen","MA","Northeastern University","Standard Grant","Phillip Regalia","09/30/2019","$866,033.00","","q.chen@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","8211","8208, 8231, 9150","$0.00","Communities on modern river deltas with total populations greater than 500 million people face threats from global reductions in river sediment, land subsidence and rising sea level. Risk mitigation efforts may require intensive computer simulations that are integrated with data collection and engineering analytics for guidance. This project establishes a Coastal Resilience Collaboratory with a three-fold mission: 1) enhance the collaboration among earth scientists, computer scientists, cyberinfrastructure specialists and coastal engineers tasked with solving the sustainability issues of deltaic coasts; 2) identify risk mitigation for coastal communities subject to flooding hazards using approaches that integrate restoration and protection; and 3) leverage NSF investments in cyberinfrastructure to address problems of major national importance involving engineering design guided by coastal system responses to specific hazard mitigation projects. Effective linkages of cyberinfrastructure that enables rapid sharing and integration of available data resources and computational tools will be evaluated. The project will also evaluate how effectively these cyberinfrastructure products promote the wider use of high-performance computing and data analytics in the coastal engineering and science research community. The proposed project has a wide range of broader impacts, ranging from education and workforce development, to dissemination of research results to the general public, K-12 students, and coastal managers and decision makers.<br/><br/>The Coastal Resilience Collaboratory core research program builds on a recently funded Coastal SEES project (EAR-1427389), which serves as the science driver for the cyberinfrastructre development and its enabled simulation experiments. One of the grand challenges for earth system science is to characterize dynamic environmental processes at appropriate space and time scales with integrated observation networks and models. The project advances four elements: 1) A simulation management system for a high-level web-based interface, improving multiphysics model usability for coastal scientists/engineers not familiar with advanced computing resources; 2) Application packaging for cloud-computing using Docker container technology to facilitate prototype simulation experiments in two large river deltas to test a range of hypotheses; 3) Accelerator technology to achieve high performance levels aimed at making a GPU- accelerated Boussinesq code base available to coastal engineers for the design of sustainable infrastructure; and 4)Aapplications for visualization and access to toolkits on mobile devices to support decision-making and educational activities. The three simulation experiments that test system interactions in the modeling framework proposed is expected to produce foundational knowledge that can evaluate potential impacts of deltaic landscape change on coasts around the world and suggest mitigation solutions."
"1618347","CCF-BSF: AF: Small: Collaborative Research: Algorithmic Techniques for Inferring Transmission Networks from Noisy Sequencing Data","CCF","ALGORITHMIC FOUNDATIONS","08/01/2016","07/20/2016","Ion Mandoiu","CT","University of Connecticut","Standard Grant","Mitra Basu","07/31/2019","$199,999.00","Mukul Bansal","ion@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7796","7923, 7931","$0.00","Many viruses encode their genome in RNA and exhibit high genomic diversity within their hosts. Advances in sequencing technologies have made it feasible to track viral transmissions and timely detect outbreaks on a global scale.  The goal of this project is to develop a comprehensive set of predictive mathematical models and accurate computational methods for integrated analysis of the massive epidemiological and sequencing datasets generated by emerging molecular surveillance programs.<br/> <br/>Research results will be broadly disseminated via journal publications and presentations at international conferences, including the Workshop on Computational Advances in Molecular Epidemiology organized by the PIs. Prototype implementations of developed algorithms will be distributed as open-source packages and incorporated in the cloud-based Global Hepatitis Outbreak and Surveillance Toolkit (GHOST) developed at CDC.  The project will provide ample opportunities for promoting participation of women and underrepresented groups in bioinformatics and molecular epidemiology research at GSU, UCONN, Georgia Tech and Tel Aviv University.  An important aspect of the project is to disseminate core concepts and ideas from Computer Science and Computational Biology to wide target audiences including: (1) teaching Computer Science, in an informal setting, to  middle and high school students, and (2) incorporating computational thinking into Life Science curriculum at the undergraduate university level.<br/> <br/>The proposed research and education activities will leverage the extensive expertise of an interdisciplinary team comprised of computer scientists, mathematicians, and molecular epidemiologists to develop accurate mathematical models and computational methods for key problems in molecular epidemiology including deconvolution and inference of viral variants from error-prone pooled sequencing data, inference of relatedness between viral samples and transmission networks, inference of transmission event times and network parameters, as well as predictive modeling  of transmission network dynamics. The team will carry out extensive algorithm validation on massive molecular surveillance datasets generated at CDC and develop robust prototype software implementations."
"1439084","XPS: FULL: CCA: Collaborative Research: Cache-Adaptive Algorithms: How to Share Core among Many Cores","CCF","Exploiting Parallel&Scalabilty","08/01/2014","08/01/2014","Rob Johnson","NY","SUNY at Stony Brook","Standard Grant","Tracy J. Kimbrel","07/31/2019","$799,999.00","Michael Bender, Rezaul Chowdhury","rob@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","8283","","$0.00","This project will develop theoretical and algorithmic foundations for writing programs that efficiently share limited memory on multi-core computers. On a multi-core computer, each process is allocated some share of the memory, but its share can fluctuate over time as other processes start, stop, and change their demands for memory. Most of today's programs do not cope well with memory fluctuations -- they have difficulty taking full advantage of additional memory freed up by other processes and they can slow to a crawl when their memory allocation decreases.<br/><br/>By enabling programmers to write software that can adapt to memory fluctuations, this research will provide new levels of flexibility, performance, and resource utilization for scientific and commercial applications running on shared-memory multi-core infrastructure. Cloud services will respond more rapidly to changes in workload. High-performance-computing applications will achieve higher memory utilization, enabling scientists to do more with less hardware. By creating a more efficient and flexible computing infrastructure, this project has the potential to accelerate the pace of discovery in other scientific fields.  For example, biological applications such as protein docking are likely to benefit from this research because the performance of current software is limited by contention for memory.<br/><br/>This project will build upon the PIs' recently proposed notion of cache-adaptive algorithms, i.e., algorithms that automatically adapt to memory fluctuations.  This project will develop cache-adaptive theory and applications in four ways:<br/><br/>1. The PIs will extend cache-adaptive analytical techniques to apply to more algorithms, such as cache-oblivious FFT and cache-oblivious serial and parallel dynamic programs.<br/>2. The PIs will develop the foundations of cache-adaptive data structures, such as cache-adaptive priority queues.<br/>3. The PIs will measure the impact of adaptivity on actual performance, focusing on cache-adaptive sorting, serial and parallel dynamic programs, and stencil computations.<br/>4. The PIs will implement cache-adaptive parallel software for computational biology applications, such as protein-protein docking, dynamic programs, and other HPC simulations.<br/><br/>The PIs will offer courses on parallel algorithms, parallel programming, cache-efficient and external-memory algorithms as part of a new degree program in computational sciences that is being launched at Stony Brook through its recently established Institute for Advanced Computational Sciences (IACS).  These courses are designed to disseminate high-performance computing research results to students and faculty in other fields, such as physics, chemistry, biology and math.  The PIs will also design a course, targeted at computer science students, on theoretical and systems aspects of external memory computing in the context of big data, databases, and file systems. The PIs will use super-computing resources from the XSEDE program, giving students access to some of the world?s fastest supercomputing clusters for their programming assignments and course projects.<br/><br/>The PIs will engage in outreach and dissemination by organizing parallel programming workshops as part of the IACS and in collaboration for Brookhaven National Labs.  PIs will also give tutorials on parallel computing, memory-efficient computing, and big data and at conferences and at other universities."
"1617488","SHF: Small: Collaborative Research:   ALETHEIA: A Framework for Automatic Detection/Correction of Corruptions in Extreme Scale Scientific Executions","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/03/2016","Marc Snir","IL","University of Illinois at Urbana-Champaign","Standard Grant","Almadena Y. Chtchelkanova","05/31/2019","$250,000.00","Franck Cappello","snir@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7923, 7942","$0.00","Trusting scientific applications requires guaranteeing the validity of computed results. Unfortunately, many examples of scientific computations have led to incorrect results, sometimes with catastrophic consequences. Currently known validation techniques cover only a fraction of the possible corruptions that numerical simulation and data analytics applications may suffer during execution. As science processes grow in size and complexity, the reliability and validity of their constituent steps is increasingly difficult to ascertain. Assessing validity in the presence of potential data corruptions is a serious and insufficiently recognized problem. Corruption may occur at all levels of computing, from the hardware to the application. An important aspect of these corruptions is that until they are discovered, all executions are at risk of being corrupted silently. In some documented cases, months have elapsed between the discovery of a corruption and notification to users. In the meantime, a potentially large number of executions may be corrupted, and incorrect conclusions may result. It may be difficult, after the fact, to check whether executions have actually been corrupted or not, so that even if corruptions do not lead to mistakes, they may lead to significant productivity losses. Virtually all simulations producing very large results need to reduce their data volume in some way before saving it --one technique is called lossy compression. <br/>This project strives to validate the end result of the simulation coupled with lossy compression. This approach is useful for scientific simulations in such diverse areas as climate, cosmology, fluid dynamics, weather, and astrophysics --the drivers of this project. <br/>This collaborative project applies the principle of an external algorithmic observer (EAO), where the product of a scientific application is compared with that of a surrogate function of much lower complexity. Corruptions are corrected using a variation of triple modular redundancy: if a corruption is detected, a second surrogate function is executed, and the correct value is chosen from the two results that are most in agreement. This new online detection/correction approach involves approximate comparison of the lossy compressed results of the scientific application and the surrogate function. The project explores the detection performance of surrogate functions, lossy compressors, and approximate comparison techniques. The project also explores how to select the surrogate, lossy compression, and approximate functions to optimize objectives and constraints set by the users. The evaluation considers a set of five applications spanning different computational methods, producing large datasets with I/O bottlenecks, and covering a variety of science problem domains relevant to the NSF. <br/>In addition to serving the needs of scientists working in the fields listed above, this project will enhance the research experience of undergraduate students. A summer school focused on resilience is planned for summer 2016, and corruption detection/correction will be a major topic. The project is also organizing tutorials in major science conferences that include online detection/correction of numerical simulations."
"1544267","EAGER: Computational Teichmuller Theory","CCF","ALGORITHMIC FOUNDATIONS","06/15/2015","06/05/2015","Wei Zeng","FL","Florida International University","Standard Grant","Tracy J. Kimbrel","05/31/2019","$249,989.00","","wzeng@cs.fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7796","7916, 7929","$0.00","Shape transformation and matching plays a fundamental role in engineering and biomedicine. Shape metric with powerfully discriminative ability is critical and highly desired in practice for machine learning in big geometric data. This project aims to develop the rigorous computational framework for finding the intrinsic mapping between shapes and the intrinsic shape metric among general surfaces based on Teichmller theory. This proposed research will open a new paradigm for geometric analysis, lay down the theoretical foundation and develop a new class of algorithms and software tools for shape transformation and matching. It is expected to greatly increase the applicability of mining and learning technologies for the emerging ubiquitous large-scale geometric data. Its success will significantly advance computational/digital geometry and will enhance the abilities of geometry and topology theories to solve real-world shape analysis problems. The resulting algorithms will have a broad range of applications in the fields of science, engineering and biomedicine. Potential applications include morphometry analysis, cancer detection and abnormality prediction in medical imaging, motion/deformation tracking and dynamics analysis in graphics and vision, and facial recognition, expression analysis and information. <br/><br/>This research aims to develop the rigorous computational framework of Teichmller theory for the Teichmller Map, which is unique and has the minimal angle distortion in its homotopy class, and the derived Teichmller Distance among general surfaces. The computation approach is based on the insight from the quasiconformal Teichmller theory and the variational principle. It first solves the computation of the holomorphic quadratic differentials and then computes the Teichmller maps. The discrete Teichmller theory will be established, and the existence and uniqueness of the solution is guaranteed. The results are applied for shape analysis and deep learning in big data, including facial recognition, expression analysis, brain study, etc. The new paradigm of discrete Teichmller theory will make major impacts on computer science, including geometric modeling, computer graphics, visualization, vision, networking, and medical imaging. The methodology will also have impacts on pure sciences, engineering and biomedicine, and potential benefits for homeland security and national defense. The algorithms developed during the research will be made freely available on the world wide web. The PI will encourage minority groups at FIU to pursue research in geometry, and make the research accessible to more audience through the seminars, course development, workshops and conferences."
"1350397","CAREER: Theoretical and practical aspects of quantum communication protocols","CCF","INFORMATION TECHNOLOGY RESEARC, ALGORITHMIC FOUNDATIONS, SOFTWARE & HARDWARE FOUNDATION, QUANTUM COMPUTING, EPSCoR Co-Funding","05/15/2014","11/02/2018","Mark Wilde","LA","Louisiana State University & Agricultural and Mechanical College","Continuing grant","Dmitri Maslov","08/31/2019","$500,000.00","","mwilde@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1640, 7796, 7798, 7928, 9150","1045, 7928, 7942, 9150","$0.00","This NSF award aims to further our understanding of quantum information theory and quantum communication. All of the work proposed is theoretical in nature, but some of the threads are practical and may find future application in quantum communication systems. At the same time, there is a strong educational component to this award in line with the vision of the PI to establish Louisiana State University as a leading center for quantum information research and study.<br/><br/>This award first seeks to sharpen our understanding of the limitations on communication using quantum-mechanical systems as the carriers of information. The PI will do so by proving so-called ""strong converse"" theorems for various communication capacities of quantum channels. The quantum information community now understands the communication capacities of several channels well, but what is lacking in some cases is a strong converse theorem. These theorems are conceptually rich, and they also find application in proving the security of particular models of cryptography. The PI will also investigate areas such as lossy quantum data compression theory and quantum communication over networked systems. Both topics are foundational in the subject and will likely underly future quantum technologies. The important theoretical questions in these respective fields are still left unanswered and the research conducted on behalf of this award will offer several ways of addressing them. The PI also will consider new exciting communication capacity questions to consider, such as the ""locking capacity"" of a quantum channel for sending data securely. The PI also seeks to improve the understanding of the practically relevant class of optical communication channels, which underly free-space or fiber-optic communication technologies.<br/><br/>The broader significance of this award is in line with the NSF Mission ""to promote the progress of science; to advance the national health, prosperity, and welfare; to secure the national defense; and for other purposes."" Indeed, research at the intersection of physics, information theory, computer science, and mathematics has advanced science in remarkable ways, pushing the limits of what is possible experimentally and what is possible in principle. Research in quantum communication is critical to national defense as well, given the important application of quantum secured communication. The PI will be advising and teaching several graduate students in the quantum science and technologies group at Louisiana State University."
"1837120","FMitF: Collaborative Research: User-Centered Verification and Repair of Trigger-Action Programs","CCF","FMitF: Formal Methods in the F","09/01/2018","08/28/2018","Blase Ur","IL","University of Chicago","Standard Grant","Dan Cosley","08/31/2022","$666,666.00","Shan Lu, Ravi Chugh","blase@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","094Y","062Z, 071Z","$0.00","Modern data-centric systems, ranging from Internet-of-Things devices to online services, can benefit from helping people make clear their intent for how their devices and services should behave and interact with each other.  Generally, this requires people to engage in some amount of end-user programming, or programming by people who are not typically trained in programming.  Common examples of this include specifying that a light should only turn on when a room is occupied or that emails with certain words in the subject line should be routed into a particular folder.  Trigger-action programming (TAP), which consists of ""if-this-then-that"" rules, is the most common model for end-user programming because it is relatively easy to write simple TAP programs.  However, as the number and complexity of both rules and devices increases, TAP programs increasingly suffer from bugs and dependability problems and are hard to correct for inexperienced and trained programmers alike.  This project's goal is to make TAP programming, and thus people's ability to interact with devices that act on their behalf, more robust through developing a better understanding of end users' needs and abilities to write and debug TAP programs, computational techniques to both better model user intents and suggest TAP programs that meet them, and tools that use those techniques to help people more easily create correct TAP programs.  Apart from the potential benefits to people's well-being, the project will also provide educational benefits by developing course materials that increase awareness of both human aspects of, and formal methods for, programming.  Further, the tangible nature of such devices and the familiarity of popular online services are a fertile domain for engaging the public and training undergraduate students, K-12 students, and early-career graduate students in the computer science research lifecycle.<br/><br/>To accomplish these goals, the work combines techniques from formal methods, human-computer interaction, and machine learning. Contributions to formal methods include the design of systematic solutions to unique program repair, synthesis, and specification-refinement problems in the context of end-user programming. Contributions to cyber human systems include empirical studies and the design of data-driven interfaces for more accurately expressing intent. Specifically, the empirical human subjects studies seek to understand and improve the debugging process for trigger-action programming, create and distribute needed data sets of user-centric collections of trigger-action programs, and comparatively evaluate proposed interfaces. The interfaces developed in this work use data-driven methods to help users pinpoint and understand bugs in trigger-action programs, as well as to choose among candidates for automatically repaired trigger-action programs. Underlying these interfaces will be formal models of trigger-action programs, which are verified against specified properties written in linear temporal logic. The system developed will systematically synthesize program repairs, taking into account users' experiences and preferences. The system will also use a combination of machine learning and formal methods to automatically generate trigger-action programs and summarize specifications based on historical traces of user interaction with the system. In sum, helping non-technical users accurately communicate their intent through trigger-action programming benefits widely deployed end-user-programming systems for integrating internet-connected devices and online services.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836948","FMitF: Collaborative Research: User-Centered Verification and Repair of Trigger-Action Programs","CCF","FMitF: Formal Methods in the F","09/01/2018","08/28/2018","Michael Littman","RI","Brown University","Standard Grant","Dan Cosley","08/31/2022","$333,332.00","","mlittman@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","094Y","062Z, 071Z, 9150","$0.00","Modern data-centric systems, ranging from Internet-of-Things devices to online services, can benefit from helping people make clear their intent for how their devices and services should behave and interact with each other.  Generally, this requires people to engage in some amount of end-user programming, or programming by people who are not typically trained in programming.  Common examples of this include specifying that a light should only turn on when a room is occupied or that emails with certain words in the subject line should be routed into a particular folder.  Trigger-action programming (TAP), which consists of ""if-this-then-that"" rules, is the most common model for end-user programming because it is relatively easy to write simple TAP programs.  However, as the number and complexity of both rules and devices increases, TAP programs increasingly suffer from bugs and dependability problems and are hard to correct for inexperienced and trained programmers alike.  This project's goal is to make TAP programming, and thus people's ability to interact with devices that act on their behalf, more robust through developing a better understanding of end users' needs and abilities to write and debug TAP programs, computational techniques to both better model user intents and suggest TAP programs that meet them, and tools that use those techniques to help people more easily create correct TAP programs.  Apart from the potential benefits to people's well-being, the project will also provide educational benefits by developing course materials that increase awareness of both human aspects of, and formal methods for, programming.  Further, the tangible nature of such devices and the familiarity of popular online services are a fertile domain for engaging the public and training undergraduate students, K-12 students, and early-career graduate students in the computer science research lifecycle.<br/><br/>To accomplish these goals, the work combines techniques from formal methods, human-computer interaction, and machine learning. Contributions to formal methods include the design of systematic solutions to unique program repair, synthesis, and specification-refinement problems in the context of end-user programming. Contributions to cyber human systems include empirical studies and the design of data-driven interfaces for more accurately expressing intent. Specifically, the empirical human subjects studies seek to understand and improve the debugging process for trigger-action programming, create and distribute needed data sets of user-centric collections of trigger-action programs, and comparatively evaluate proposed interfaces. The interfaces developed in this work use data-driven methods to help users pinpoint and understand bugs in trigger-action programs, as well as to choose among candidates for automatically repaired trigger-action programs. Underlying these interfaces will be formal models of trigger-action programs, which are verified against specified properties written in linear temporal logic. The system developed will systematically synthesize program repairs, taking into account users' experiences and preferences. The system will also use a combination of machine learning and formal methods to automatically generate trigger-action programs and summarize specifications based on historical traces of user interaction with the system. In sum, helping non-technical users accurately communicate their intent through trigger-action programming benefits widely deployed end-user-programming systems for integrating internet-connected devices and online services.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763734","CIF: Medium: Foundations of Learning from Paired Comparisons and Direct Queries","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2018","03/09/2018","Nihar Shah","PA","Carnegie-Mellon University","Continuing grant","Phillip Regalia","05/31/2022","$286,629.00","Aarti Singh, Sivaraman Balakrishnan","nihars@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7924, 7935","$0.00","Direct queries and paired comparisons are popular means of data acquisition in various scientific disciplines. These two types of data have been studied separately, however, several modern applications -- particularly those involving human judgments -- require analysis of a combination of these two types of data. Such an analysis presents novel opportunities and challenges for stochastic modeling, experimental design and algorithm development. This research involves establishing a unified view of learning from direct measurements and paired comparisons with the aim of understanding fundamental limits and tradeoffs for various problems of interest, and developing and implementing practical algorithms that can leverage both types of data. The utility of the algorithms will be demonstrated by application to diverse domains including material science and crowdsourcing.<br/><br/>This project focuses on the specific technical problems of function estimation, feature selection, and optimization that can leverage passively and actively acquired direct queries and paired comparisons, with the following objectives. The first objective is to establish fundamental information-theoretic limits of learning from a combination of paired and direct measurements under various statistical models. These fundamental limits also involve understanding the inherent tradeoffs between the two forms of measurements that accounts for the different noise and costs. The second objective is to develop scalable and computationally efficient algorithms whose performance attains these fundamental limits. The third objective is to transfer the theory to practice in applications such as material design using direct experimental and pairwise expert feedback.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1703487","SHF: Medium: Scalable Holistic Autotuning for Software Analytics","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2017","07/21/2017","Timothy Menzies","NC","North Carolina State University","Continuing grant","Sol J. Greenspan","06/30/2021","$898,349.00","Xipeng Shen","tim.menzies@gmail.com","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7924, 7944","$0.00","Software analytics distills large quantities of low-value data down to smaller sets of higher value data that shed important insights for software quality enhancement. It is essential for software effort estimation, risk management, defect prediction, project resource management and many other tasks.  Software analytics is a complex, time-consuming process. Recent research has tried to alleviate the issue through intelligent optimizers that make better use of existing computational resources. The space of possible options for optimization is very large, and spans over multiple layers: all possible settings for algorithms, compilers, and execution time options. To complicate matters, there are many competing goals that could be used to guide that tuning; e.g. reducing CPU usage while increasing the predictive power of the learned model. Existing research has mainly focused on limited optimizers that explore just a few options at mostly one level while trying to improve on just one or two goals, leaving the large potential of optimizations untapped.<br/><br/>This research proposes to advance the state of the art to holistic scalable intelligent optimization for software analytics (SHASA). SHASA tunes all levels of options for multiple optimization objectives at the same time. It achieves this ambitious goal through the development of a set of novel techniques that efficiently handle the tremendous tuning space. These techniques take advantage of the synergies between all those options and goals by exploiting relevancy filtering (to quickly dispose of unhelpful options), locality of inference (that enables faster updates to outdated tunings) and redundancy reduction (that reduces the search space for better tunings). This research will produce algorithms and tools that are demonstrably more useful and efficient for software analytics research. Those techniques are generalizable beyond software analytics for use in computational science and engineering at large. An important broader impact is minimizing CPU and memory usage, ultimately reducing energy consumption in data centers, as data analytics computations grown significantly in scale and become computationally more demanding."
"1617610","CIF: Small: Complex-Valued Statistical Signal Processing with Dependent Data","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/28/2016","Jitendra Tugnait","AL","Auburn University","Standard Grant","Phillip Regalia","06/30/2019","$418,527.00","","tugnajk@eng.auburn.edu","310 Samford Hall","Auburn University","AL","368490001","3348444438","CSE","7797","7923, 7936, 9150","$0.00","Complex-valued random signals arise in many areas of science and engineering such as communications, radar, sonar, geophysics, oceanography, optics, electromagnetics, and acoustics. If the cross-covariance function of the signal with its complex conjugate vanishes, the signal is called proper, otherwise it is improper. If the underlying signals are improper, much can be gained in performance if they are treated as improper. If it is not known apriori whether a signal of interest is proper or improper, this information must be obtained from its noisy measurements. Existing approaches to determination of propriety are limited to the case where the measurements consist of a sequence of independent random vectors. Practical real-life signals do not typically consist of independent measurement samples. This research focuses on approaches designed to handle dependent data.<br/> <br/>Novel, efficient approaches are investigated in this research with emphasis on frequency-domain, improper signals, and applications. The signals are modeled as stationary but are not necessarily Gaussian. The following thrusts form the core of this research.  (1) Testing for impropriety of dependent multichannel data with arbitrary distribution unlike past work which is limited to independent sequences, typically assumed to be Gaussian. (2) Comparison of random complex signals involving statistical tests to ascertain if two multichannel random signals have the same second-order statistics. Application of such tests for user authentication in wireless networks is investigated. (3) Detection of multichannel complex signals in noise using a generalized likelihood ratio test formulation is studied, without requiring a structured model or Gaussian assumption. (4) This research also involves reexamination and modification of all aforementioned approaches to be robust with respect to additive or innovations outlier model."
"1651861","CAREER: Approximation Algorithms via SDP hierarchies","CCF","ALGORITHMIC FOUNDATIONS","02/01/2017","01/24/2017","Thomas Rothvoss","WA","University of Washington","Continuing grant","Rahul Shah","01/31/2022","$178,293.00","","rothvoss@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","1045, 7926","$0.00","Computational problems have become pervasive in all fields of science where huge amounts of data have to be processed efficiently. For computationally hard problems where computing the exact optimum solution is intractable, the next best option is to design efficient approximation algorithms that find solutions that are provably close to the optimum. In this project, the PI and the supported graduate student will design such approximation algorithms based on the still poorly understood Lasserre semidefinite programming hierarchy. The goal is to provide better algorithms for several key problems that are studied in combinatorial optimization. Practical applications of approximation algorithms can be found in many areas of science and industry. Moreover, this proposal includes the integration of research and teaching by means of graduate courses that cover the Lasserre hierarchy in a manner accessible for graduate students outside of theoretical computer science.<br/> <br/>More concretely, the goal is to develop approximation algorithms based on the Lasserre SDP for outstanding unsolved problems like Directed Steiner Tree, Graph Coloring, Unrelated Machine Scheduling and Unique Games. In particular this means designing new rounding schemes to extract valid integral solutions and developing the analytical techniques needed to study their effectiveness. A key ingredient will be the development of a better understanding of the vector embeddings for higher moments that are provided by the Lasserre SDP. <br/><br/>Keywords: Approximation algorithms; Semidefinite programs; Integrality gaps; Lasserre hierarchy"
"1816542","SHF: SMALL: A Novel Algorithm for Automated Synthesis of Passive, Causal, and Stable Models for Optical Interconnects","CCF","SOFTWARE & HARDWARE FOUNDATION, EPSCoR Co-Funding","10/01/2018","06/22/2018","Ata Zadehgol","ID","University of Idaho","Standard Grant","Sankar Basu","09/30/2021","$250,000.00","","azadehgol@uidaho.edu","Office of Sponsored Programs","MOSCOW","ID","838443020","2088856651","CSE","7798, 9150","7923, 7945, 9150","$0.00","With the explosive growth of Big Data and Internet of Things, and the ever-growing demand for high-performance computing, mobility, security, and high-fidelity experiences, there is an increasing need for high-bandwidth and low-loss interconnect technologies that enable efficient data transfer across the chip. The optical fiber and its significant capacity for high-bandwidth data transfer are well-known in the telecommunications industry, and widely exploited in their long-distance network of interconnects. Scaling down optical interconnects to fit in nano-scale silicon chips would clearly be advantageous for achieving high-bandwidth data transfer on-chip -- however, a major hurdle is the dearth of accurate and efficient computable stochastic electromagnetic models for simulation of optical interconnect structures comprised of multiple tightly-coupled nano-scale silicon-on-insulator (SOI) wave-guides that carry information signals (i.e., TeraHertz electromagnetic waves) across on-chip transmitters and receivers. The overall goal of this proposal is to develop algorithms for a software tool to perform the design, analysis, and optimization of 3-dimensional (3-D) nano-scale optical interconnects based on SOI wave-guides exhibiting random surface roughness. The overall educational components of this project are to leverage the developed models and software to: (1) develop new graduate courses, (2) develop interactive learning objects and lab-based activities for undergraduate courses, and (3) stimulate undergraduate students' interest in science, and recruit and mentor diverse groups of students including women and minority groups. <br/><br/>The project team will develop the Optical Interconnect Designer Tool (OIDT) software, of which the input is comprised of: (1) the specified 3-D geometry representing the physical description of the multi-port optical interconnect system, (2) the random distribution for surface roughness of SOI wave-guide, and (3) the wave-guide material's electrical properties. The OIDT will autonomously synthesize two types of electrical models: (1) network scattering parameters for design optimization of the interconnect, in the frequency-domain, and (2) stable, passive, and causal SPICE equivalent circuit models for timing analysis and signal/power integrity analysis of the passive interconnect, integrated with active non-linear drivers and components, in the time-domain. The proposed Python-based software package may be used stand-alone, or integrated into existing computer aided design (CAD) tools and design-flows to facilitate design automation, and research and development of advanced microelectronics for a variety of applications including computing, communications, energy, security, sensing, health, etc. The numerical program will be hosted on GitHub as an open-source project, to facilitate and promote (inter)national optical device research. The education component expands public's scientific literacy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844628","CAREER: New Fundamentals in Coding Theory","CCF","ALGORITHMIC FOUNDATIONS, COMM & INFORMATION FOUNDATIONS","02/01/2019","02/01/2019","Mary Wootters","CA","Stanford University","Continuing grant","Tracy Kimbrel","01/31/2024","$100,000.00","","marykw@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796, 7797","1045, 7927, 7935","$0.00","Coding theory is the study of error-correcting codes, which encode data to protect it from noise.  Error-correcting codes have been studied since the 1940s; today, they are used everywhere from hard drives to satellite communication.  This goal of this research is to pose and attack new fundamental questions in coding theory, inspired by new applications of coding theory across computer science, mathematics, and engineering.  In answering these questions, this project not only addresses the application areas, but also develops insights to tackle long-standing open questions in coding theory.  This project is interdisciplinary, and includes a synergistic education plan aimed at bringing together students from different backgrounds.  The education plan incorporates course development of both graduate and undergraduate courses at Stanford University as well as training for graduate students and research opportunities for undergraduates.<br/><br/>In more detail, this project focuses on four new lenses into coding theory, motivated by applications in distributed storage, cryptography, algorithm design and pseudorandomness.  Each of these lenses raises new fundamental questions in coding theory.  These questions fall broadly into the categories of ""locality"" and ""list-decoding.""  Informally, locality refers to the ability to decode a small part of the original data from only a small amount of encoded data.  List-decoding refers to the setting where there is so much noise that the original data cannot be uniquely determined, and the decoder must do the best it can to narrow the possibilities to a relatively short list.  By answering these new fundamental questions, this research will both make progress on the motivating applications and will also develop new theory illuminating the structure of locality and list-decoding in error correcting codes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642658","CAREER: Reliability in Large-Scale Storage","CCF","COMM & INFORMATION FOUNDATIONS","03/01/2016","09/14/2018","Arya Mazumdar","MA","University of Massachusetts Amherst","Continuing grant","Phillip Regalia","01/31/2020","$511,173.00","","arya@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7797","1045, 7797, 7935","$0.00","With the advent of large scale distributed storage systems, cloud computing and commercial data storage applications, there is a renewed interest in the coding and information theory in reliabile issues data storage. In large networked databases, faster updates and quick repair requirements must be integrated with reliable data storage protocols. These requirements bring new dimensions and parameters to the traditional optimization problem of information theory.<br/><br/>In this project we propose, for the first time, a model of large-scale storage that accounts for the topology of storage networks. Previously, storage topology was never a concern of the code designers. Further, we study the update efficiency and local repair (fast recovery) properties of error-correcting codes suitable for storage. For all of these, we will analyze the fundamental limits of systems, as well as propose explicit (fast algorithmic) constructions of codes. Several tools from graph and network theory, combinatorics and optimization theory will be leveraged to find performance limits and devise coding algorithms.<br/><br/>By cutting redundancy and allowing faster processing, the codes developed from this project will directly save energy consumption in data centers. Existing and new collaborations of the principal investigator will facilitate industry cooperation and increase the transition to practice of results generated from this project. Elements of this endeavor will be integrated with the courses taught by the principal investigator. The findings will be disseminated through publications in peer-reviewed venues and made available in the form of technical reports for public access on the investigator's webpage. Finally, motivated by practical applications and extending across various disciplines, this project is representative of an intriguing area of engineering science, and will attract a diverse student base including undergraduate students."
"1807526","SemiSynBio: An On-Chip Nanoscale Storage System Using Chimeric DNA","CCF","SemiSynBio Semiconductor Synth, Genetic Mechanisms","10/01/2018","07/16/2018","Olgica Milenkovic","IL","University of Illinois at Urbana-Champaign","Continuing grant","Mitra Basu","09/30/2021","$375,000.00","Jean-Pierre Leburton, Xiuling Li, Charles Schroeder","milenkov@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","061Y, 1112","7465","$0.00","DNA-based data storage is an emerging recording paradigm that has received significant attention from the scientific community due to several recent demonstrations of the viability of storing information in macro-molecules. Unlike classical optical and magnetic storage technologies, DNA-based storage platforms offer extremely high recording densities, and they do not require electrical supply to maintain data integrity. Furthermore, under mild maintenance conditions, DNA retains its information content for centuries while still allowing users to retrieve the information independent of the specific reading technology. Still, despite the promises of DNA-based archival systems, several problems remain that prevent wide-scale deployment of the technology. These include the high cost of DNA synthesis, the lack of structural and distributed organization of data encoded in DNA, and the nonexistence of an integrated random access and readout mechanism. To address these issues, this collaborative project aims to test and implement a new molecular storage paradigm that combines unique ideas in polymer chemistry, coding theory and molecular dynamics modeling, as well as new nano-material and solid state nano-pore technologies. The accompanying interdisciplinary research and educational programs involve experts in chemistry, biophysics, electrical engineering and theoretical computer science and aim to train a new cadre of students able to address future scientific challenges in molecular storage and computing systems. <br/><br/>The technical goal of the proposed program is to reduce the cost-integration barrier between classical recorders and DNA-based data storage devices by developing a new system centered around chimeric DNA, comprising cheap native DNA and chemically modified nucleotides. Chemically-modified nucleotides extend the coding alphabet from four symbols to more than twenty. Chimeric DNA is stored and accessed using a novel implementation of self-rolled semiconductor micro-tubular grids, controlled by three-dimensional arrays of electrodes. Random access in such systems is achieved via voltage modulation, with selected DNA guided into a sample preparation and specialized nano-pore sequencing device. The implementation of such systems is aided by new software tools for molecular dynamics simulations. Additional system support is provided via new coding methods that combat the effects of chimeric DNA integration and nano-pore sensing errors. Particular research challenges include identifying chemical modifications in nucleotides amenable for detection via nano-pore sequencers, calculating electrostatic forces within the tubes and within the pores in the presence of chimeric DNA, and integrating the micro tubular chip with an on-chip sample preparation and sensing device. Supporting work on bioinformatics and coding theoretic algorithmic development are expected to ensure additional robustness and operational stability of the proposed system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1535851","AitF: FULL: Sparse Fourier Transform: From Theory to Practice","CCF","Algorithms in the Field","09/01/2015","08/18/2015","Piotr Indyk","MA","Massachusetts Institute of Technology","Standard Grant","Tracy J. Kimbrel","08/31/2019","$500,000.00","Dina Katabi","indyk@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7239","012Z","$0.00","The Discrete Fourier Transform (DFT) is a powerful tool used in many big data domains, including multimedia processing, medical imaging, genomics research, astronomy, seismology for oil and gas reserve discovery, and malicious traffic detection in cybersecurity domains.  Building upon a recent breakthrough by the researchers behind this project, this award will develop the algorithmic and system foundations for practical high-spped DFT over sparse data sets. Addressing this goal involves highly interdisciplinary research encompassing ideas and techniques from mathematics, theoretical computer science, software design, and specific application areas such as wireless networks.<br/><br/>The project will be multi-pronged, focusing on three main themes: (a) Algorithms: The PIs will develop a family of algorithms that are faster, simpler and more accurate than the current state of the art in sparse DFT. The new algorithms will be capable of incorporating priors on the structure of the data and apply to multi-dimensional data sets. (b) Software implementations: The PIs will develop software implementations of sparse FFT algorithms and explore algorithm parallelization for further reduction in power and processing time. (c) Applications: The PIs will apply these algorithms and empirically demonstrate them in the context of cost-effective networked system for delivering smart services for intelligent transportation systems using existing e-toll transponders."
"1527084","AF: Small: Allocation Algorithms in Online Systems","CCF","ALGORITHMIC FOUNDATIONS","09/01/2015","07/20/2015","Debmalya Panigrahi","NC","Duke University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$416,000.00","","debmalya.panigrahi@gmail.com","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796","7923, 7926, 9251","$0.00","In recent years, the Internet has undergone explosive growth --- in the number of users and connected devices, volume of traffic, geographical reach, and diversity of services --- and its role in enriching modern human societies is indisputable. Two key contributors to this growth and success are: (a) the unique economic model of the Internet that predominantly relies on advertising revenues instead of paid services thereby allowing multitudes of users affordable access to online services such as email and search, and connectivity via social networks; and (b) the large-scale computing infrastructure based on massive data centers capable of providing computing and connectivity services to billions of users across the globe at any given time. The success of these critical components of the Internet revolution is contingent on the development of efficient allocation algorithms --- for deciding which advertisement an ad exchange should show an online user to maximize the user's utility and generate revenue, and for scheduling user service requests on the available resources such as processors, storage devices, and network elements in a data center. In this project, the PI will develop novel algorithmic tools and techniques to address these problems, thereby advancing the state of the art in algorithmic research. Moreover, the PI will regularly consult with practitioners to create opportunities for technology transfer in Internet applications. This project will also train graduate and undergraduate researchers in algorithms and theoretical computer science, with a focus on problems motivated by real world applications.<br/><br/>Allocation problems in large online systems have emerged as a vibrant area of research. In this project, the focus is on two important domains: scheduling and load balancing with applications to data center management, and online matching and budgeted allocation with applications to Internet advertising. Both application domains have been at the forefront of the Internet revolution and have grown into multi-billion dollar industries. Moreover, from a technical perspective, these problems are characterized by some of the key challenges in modern algorithm design for real world problems: uncertainty and incompleteness of input data, the existence of multiple simultaneous objectives, and non-linear optimization requirements. This project will address technical problems in the above-mentioned application domains that exhibit one or more of these characteristics. Specific problems to be considered include vector scheduling and load balancing, online convex optimization and applications to non-linear scheduling objectives, multi-objective and stochastic versions of budgeted allocation and online matching problems, etc. The successful completion of this project will yield an algorithmic toolkit for allocation problems motivated by real world applications on the Internet."
"1525462","SHF: Small: Novel Architecture Energy Harvesting for Sustainable Spot Cooling and Energy Management","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/24/2015","Carole-Jean Wu","AZ","Arizona State University","Standard Grant","Yuanyuan Yang","07/31/2019","$419,066.00","","carole-jean.wu@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7798","7923, 7941","$0.00","Increased power dissipation in computing devices has led to a sharp rise in thermal hot spots on computer chips, creating a vicious cycle ? higher temperatures bring higher leakage power, and higher power dissipation increases temperature, thereby leading to thermal avalanches. To reduce the additional power dissipation and reliability concerns caused by high temperature, current heat management approaches apply cooling mechanisms to remove heat aggressively as well as devise dynamic management techniques that avoid thermal emergencies by slowing down heat generation of processors. However, current trends to squeeze more computing power, e.g., in the form of large data centers or mobile devices, stand in direct conflict to our ability to slow down demand for more energy. The shrinking of transistor sizes further exacerbates the problem of reduced energy efficiency. The solution proposed in this work is anticipated to not only reduce cooling expenses and ambient temperatures, but also increase energy utilization, device lifetime, and physical space utilization. The technology developed here can be applied to a broad range of computing devices, large or small. If the research is successful, it has the potential of having a significant economic benefit as well as a significant, positive impact on the environment. This is because performance improvement and power reduction of processors under thermal constraints will have a direct impact upon the cooling costs of huge data warehouses such as those of Google, Yahoo, Amazon, etc. Data centers in the US consume many tens of billion kWh of electricity and generate about nearly a billion metric tons of carbon dioxide. Even if this project resulted in a 5% improvement in the energy consumption of a modern high performance processor and therefore, in the millions of such processors housed in data centers, that itself could reduce the amount of carbon dioxide released into the atmosphere per year, and realize ten of millions of dollars in energy cost savings.  Furthermore, this energy harvesting research requires cross-disciplinary engagement in areas such as material engineering, VLSI architecture, system architecture, and mechanical engineering and will attract a diverse set of student researchers. Overall, the engineering and scientific contributions will also have important societal impacts, including the broadening of ASU?s engineering curriculum, the engagement of graduate and undergraduate students in research activities, the potential of creating high-school or middle-school scientific projects, and the increased representation of target underrepresented minorities in science and engineering.<br/><br/>This project addresses the heat management problem using an innovative approach ? rather than removing heat or slowing down heat generation, the proposed work transforms the waste heat into reusable energy for new applications such as self-powered spot-cooling. The main objective of this project is to design and implement a novel architectural framework to create the mechanisms, policies, and system support that allow waste heat generated by computing devices to be harvested efficiently, to achieve better energy utilization efficiency. This will be achieved by exploiting the thermal characteristics of modern computing nodes and by leveraging thermoelectric and pyroelectric energy harvesting materials. By leveraging the thermoelectric and pyroelectric effects at the architectural level, the varying spatial and temporal thermal gradients from computations are exploited to transform processor waste heat (that otherwise dissipates) into reusable energy. A novel application is also proposed that uses the newly introduced energy in the form of a self-sustaining cooling system for processors. This work evaluates the applicability of energy harvesting materials by considering the intricate electrical properties of the materials and heterogeneous temperature distribution of the components on a processor. The proposed methodology is generic and can be readily adopted with commercially-available thermoelectric and pyroelectric energy harvesting materials. Nonetheless, with breakthroughs in the energy conversion efficiency of the materials, the proposed framework could be applied directly with a further improved degree of harvested energy, leading to even higher system energy efficiency."
"1422045","CIF/AF: Small: Some fundamental complexity-inspired coding theory challenges","CCF","ALGORITHMIC FOUNDATIONS, COMM & INFORMATION FOUNDATIONS","09/01/2014","03/20/2015","Venkatesan Guruswami","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","02/29/2020","$515,912.00","","guruswami@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796, 7797","7923, 7926, 7935, 7937, 9251","$0.00","Error-correcting codes provide a judicious way to add redundancy to data in order to safeguard its utility even when corrupted by various forms of errors. Codes are crucial to the reliable communication and storage of data, and find widespread use in diverse applications. This project is aimed at tackling a collection of fundamental challenges concerning various aspects of error-correcting codes. The questions underlying the project are inspired and technically integrated by the PI?s ongoing work in complexity theory and algorithmic coding theory. Progress on these questions will improve our understanding of the power and limitations of error-correcting codes in several models, as well as strengthen the connections between coding theory and various other fields such as information theory, combinatorics, pseudorandomness, cryptography, graph theory, and complexity theory.<br/><br/>The project will study the performance of error-correcting codes in many settings, such as communication on discrete memoryless channels with a specific focus on convergence to Shannon capacity of polar codes; codes resilient against worst-case deletions; local testability and decodability; non-malleability against natural tampering attacks, etc.The discovery of new coding schemes has potential applications in data storage and communication. Non-malleable coding schemes can serve as building blocks for tamper-resilient cryptography. Locally decodable codes can improve the efficiency of distributed storage applications leading to substantial cost savings. The research will employ ideas from computer science in setting new directions for research in coding theory, thereby enhancing the connection between the computer science and information theory communities. On the education front, the project will engage several graduate students and provide a stimulating research environment for them, and help with the planned writing of a textbook on essential coding theory."
"1718336","SHF:  Small:  Collaborative Research:  Uncovering Vulnerabilities in Parallel File Systems for Reliable High Performance Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/08/2017","Yong Chen","TX","Texas Tech University","Standard Grant","Almadena Chtchelkanova","07/31/2020","$233,000.00","Dong Dai","yong.chen@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","7798","7923, 7942, 9251","$0.00","Many scientific problems (e.g., computational biology, high-energy physics, climate science) rely on high performance computing (HPC) systems to manage and process massive amounts of data. However, with the rapid increase in scale and complexity, even the specially-designed and well-maintained HPC platforms may fail. This research aims to design innovative methodologies that scrutinize parallel file systems, the major storage software which empowers HPC platforms, and uncover the issues in parallel file systems that can lead to data loss under various failure scenarios. Such an effort is a fundamental step towards building highly reliable HPC systems and meet the demand of data-driven scientific discovery. In addition, this project integrates the research activities with education and outreach efforts to train broadly inclusive and globally competitive science workforce.<br/><br/>More specifically, this project includes two synergistic research tasks, which enables automatic testing as well as diagnosing the issues in parallel file systems. The first task focuses on testing parallel file systems through a single-fault injection framework, which interrupts the normal workloads of the target parallel file system automatically, and examines if the interruption could lead to any issues that cannot be fixed by the corresponding checker of the parallel file system. Building on the first task, the second task focuses on diagnosing the issues uncovered in the previous task through a two-level provenance-based analysis. The first level analysis builds the coarse-grain, inter-node provenance, which provides a high-level picture of the entire system behavior. The second level analysis creates the fine-grain, intra-node provenance that contains causal paths within each individual node. In addition, multiple provenance traces are aligned and compared automatically to help locate the problematic code region with minimal human efforts."
"1844976","CAREER: Error-Free, Uniform and Composable Chemical Computation","CCF","COMPUTATIONAL BIOLOGY","07/01/2019","02/04/2019","David Doty","CA","University of California-Davis","Continuing grant","Mitra Basu","06/30/2024","$100,770.00","","doty@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7931","1045, 7931","$0.00","Computation by electronic computers has revolutionized relatively mundane aspects of life, such as shopping and commuting, by automating tasks that previously required human intervention. Similarly, the programming of chemicals-automating the processing of information embedded in molecules could remake the world: smart molecules controlled by programmable chemical reactions could achieve the same level of precise automated control over the configuration of matter at the molecular level. This may one day revolutionize, for example, therapeutic treatments applied within living cells or nanoscale materials fabricated by self-assembly. This project aims to develop the mathematical foundations of such chemical information processing, bringing the engineering of chemistry-based ""software"" closer to the reliability of modern electronic computing. Research will be directed toward key challenges that have faced practitioners in the field: namely error-prevention, reusability, and the ability to integrate separately designed chemical systems into a properly functioning whole. Insights gleaned from research in classical computer science will be applied to achieve these goals, but new insights, based on the laws of physics and chemistry, will be required to reason about the uniquely molecular interactions mediating chemical computation. The potential applications in medicine or nano-engineering include identifying and curing diseases, as well as self-assembling devices with nanoscale precision. Together with a tightly integrated educational plan based on training female undergraduate mathematics students in computer science research, and a data-driven approach to develop autograding software for undergraduate CS courses, this project will help train a new and diverse generation of interdisciplinary scientists and programmers, who can innovate robust nanoscale information technologies.<br/><br/>This project advances the theoretical foundation for programming chemical algorithms-that is, algorithms executed by artificially synthesized chemical reactions-ensuring they have three crucial properties that are lacking, or at best poorly understood, in existing systems: (1) error-free: implementable by real chemicals that faithfully execute the intended algorithm, (2) uniform: correct for any ""population size"", i.e., the total number of molecules, unlike many current algorithms where reactions must be tailored specifically to the population size and (3) composable: can be packaged into functional modules that are easily combined. As DNA nanotechnology and molecular computing mature, so too must our insight into their fundamental abilities and limitations. A rigorous theory of chemical computing will guide the experimental breakthroughs of the future. Development of this theory will be guided by a preference for substrate-independence, identifying the laws of computation obeyed by all chemical systems, sifting out artifacts of particular technologies from their shared unifying principles. The development of techniques to make chemical algorithms error-free, uniform, and composable could lead to a systematic path for programming chemistry, making it understandable and usable by non-chemists thus allowing the molecular computing revolution to take flight. Advances in programmable kinetic barriers in implementing an autocatalytic reaction could eventually lead to a significant technological breakthrough allowing error-resilient single-molecule detection.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844855","CAREER: Theory of Fast Graph Optimization","CCF","ALGORITHMIC FOUNDATIONS","02/01/2019","02/01/2019","Aaron Sidford","CA","Stanford University","Continuing grant","Tracy Kimbrel","01/31/2024","$112,603.00","","sidford@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","1045, 7926","$0.00","Graphs are one of the most fundamental mathematical abstractions used to model complex, highly-interconnected data. Whether scheduling airline flights, finding communities in social networks, routing internet traffic, architecting a deep network, or simply visualizing a large data set, graphs are essential tools used to perform these operations. Consequently, efficient graph-optimization algorithms are highly coveted, and graph-optimization problems are among the most well-studied problems across the theory and practice of computer science, operations research, numerical analysis, and scientific computing. Recent rapid growth of data set sizes has further elevated the demand for graph algorithms which can scale nearly optimally. However, despite decades of extensive research, obtaining such algorithms is a wide-open and extremely challenging problem. The goal of this project is to directly address these open problems and provide a broad graph-optimization toolkit to fuel the development of faster algorithms. This project will provide provably faster graph algorithms, better structural results about graphs, new optimization methods, and diverse educational material. This work will be made widely accessible to facilitate savings in time, energy, and valued resources to society at large. <br/><br/>This project will focus on providing provably faster algorithms for solving a range of fundamental, canonical, and pervasive graph-optimization problems, including the maximum-flow problem, Perron vector computation, and solving Markov decision processes. These are very well-studied problems that have historically served as a stepping stone towards broader algorithmic advances. To overcome historical barriers to efficient graph optimization, this project will leverage techniques from multiple communities, including continuous optimization techniques from operations research, randomized linear-algebra techniques from numerical analysis, and combinatorial optimization techniques from theoretical computer science. Each of these three disciplines and research communities brings a different perspective on graph optimization, and this project will improve upon existing results from each area and unify them to create faster algorithms and a more comprehensive and diverse approach to algorithm and optimization education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1853714","SHF:  Small:  Collaborative Research:  Uncovering Vulnerabilities in Parallel File Systems for Reliable High Performance Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","08/11/2018","09/15/2018","Mai Zheng","IA","Iowa State University","Standard Grant","Almadena Y. Chtchelkanova","07/31/2020","$226,660.00","","mai@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7798","7923, 7942, 9150, 9251","$0.00","Many scientific problems (e.g., computational biology, high-energy physics, climate science) rely on high performance computing (HPC) systems to manage and process massive amounts of data. However, with the rapid increase in scale and complexity, even the specially-designed and well-maintained HPC platforms may fail. This research aims to design innovative methodologies that scrutinize parallel file systems, the major storage software which empowers HPC platforms, and uncover the issues in parallel file systems that can lead to data loss under various failure scenarios. Such an effort is a fundamental step towards building highly reliable HPC systems and meet the demand of data-driven scientific discovery. In addition, this project integrates the research activities with education and outreach efforts to train broadly inclusive and globally competitive science workforce.<br/><br/>More specifically, this project includes two synergistic research tasks, which enables automatic testing as well as diagnosing the issues in parallel file systems. The first task focuses on testing parallel file systems through a single-fault injection framework, which interrupts the normal workloads of the target parallel file system automatically, and examines if the interruption could lead to any issues that cannot be fixed by the corresponding checker of the parallel file system. Building on the first task, the second task focuses on diagnosing the issues uncovered in the previous task through a two-level provenance-based analysis. The first level analysis builds the coarse-grain, inter-node provenance, which provides a high-level picture of the entire system behavior. The second level analysis creates the fine-grain, intra-node provenance that contains causal paths within each individual node. In addition, multiple provenance traces are aligned and compared automatically to help locate the problematic code region with minimal human efforts."
"1544542","SHF: PROJECT DARWIN_ Towards Principled Language Evolution","CCF","ADVANCES IN BIO INFORMATICS, , COMPUTATIONAL PHYSICS, PROGRAM EVALUATION, SOFTWARE & HARDWARE FOUNDATION, Software Institutes","07/01/2016","05/25/2016","Jan Vitek","MA","Northeastern University","Standard Grant","Sol J. Greenspan","06/30/2020","$1,099,726.00","","j.vitek@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1165, 1798, 7244, 7261, 7798, 8004","010Z, 1206, 7433, 7569, 7944, 8004, 8009, 8084","$0.00","R is a large language ecosystem consisting of statistical and graphical capabilities, a programming language, and all the other artifacts present in an open source programming environment. R is used by millions of scientific/engineering programmers in academia and is used heavily in business and industry.  R is hitting limits and needs major renovations if it is to keep up with, and  advance, the pace of science. This proposal proposes to modernize several aspects of the R language and to integrate new infrastructure elements. <br/><br/>The modernization effort will focus on  the code base of the language  implementation, the compiler and the memory subsystem. The new features will center around the addition of type annotations to improve correctness and performance of R programs as well as to assist with distributed  execution. The PI is well-connected to the R core developers, and there is a clear path to releasing versions into production. The enhancements will help R support the pace of science, as problems become more computationally complex and use rapidly growing data sets.  Scientific computations programmed in R  will run faster, be capable of processing much larger, distributed data sets, and the implementation will be easier to maintain by modernizing the language.  The experience of reimplementing a live, full-scale language like R will be interesting to the programming language research community, as well as providing a more sustainable infrastructure for science/engineering programming."
"1845349","CAREER: Pseudorandom Objects and their Applications in Computer Science","CCF","ALGORITHMIC FOUNDATIONS","07/01/2019","02/01/2019","Xin Li","MD","Johns Hopkins University","Continuing grant","Tracy Kimbrel","06/30/2024","$108,719.00","","lixints@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7796","1045, 7927","$0.00","One of the most successful paradigms in computer science since the 1970s is the use of random bits (coin flips) in computation, which can be demonstrated from several broad aspects. For example, many simple randomized algorithms perform better than sophisticated deterministic algorithms, and random bits are widely used in modern cryptography to ensure security. Moreover, in certain applications regarding designing combinatorial objects (such as highly connected sparse networks), simply choosing a random object often achieves the best parameters. However, the use of random bits comes at a price: in practice high quality random bits are often too costly to obtain, and many applications such as the example of designing a sparse network require deterministic constructions rather than randomized ones. The overarching goal of this project is to understand the fundamental question of when and how one can replace the use of random bits or randomized objects by pseudorandom objects, which are objects that are deterministically constructed but behave like random ones. This will lead to a deeper understanding of the nature of random bits in computation, as well as more efficient and secure solutions to important questions both in theory and in practice. Examples of benefits include faster algorithms for handling large data sets, more robust networks, and more reliable communications in hostile environments. The project also involves plans for mentoring PhD students, integration of the research topics into courses and books that appeal to students from a variety of different backgrounds, and support of under-represented groups of students in computer science.<br/><br/>The project contains three sets of specific goals. The first set of goals seeks to understand how to reduce the quantity or quality of random bits in computation generally, using two kinds of  pseudorandom objects known as pseudorandom generators and randomness extractors. A pseudorandom generator is a function that stretches a short random seed into a long string that looks random to certain functions, and it can be used to reduce the quantity of random bits required. A randomness extractor is a function that transforms imperfect random sources into high quality random bits. The second set of goals investigates the connections of these pseudorandom objects to computational complexity theory. Specifically, the goal is to use the random-like property of these objects to give new constructions of deterministic objects that circumvent barriers in long-standing open problems, such as the question of sequential computation versus parallel computation. The third set of goals involves development of new techniques for constructions of error-correcting codes, which can be used both to protect against various tampering attacks from adversaries, and to achieve privacy or security in cryptographic systems. The study of these topics is based on techniques from several related areas such as probability theory, information theory, cryptography, combinatorics, and harmonic analysis, and will further foster the interactions among these areas towards breakthroughs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618999","CIF: Small: Dynamic Networks: Learning, Inference, and Prediction with Nonparametric Bayesian Methods","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/27/2016","Petar Djuric","NY","SUNY at Stony Brook","Standard Grant","Phillip Regalia","06/30/2019","$465,439.00","","petar.djuric@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7797","7923, 7936","$0.00","Complex networks are all around us. They can be physical, biological, social, and virtual. All the species in the world live in societies that can be represented as networks. Almost all complex systems, natural or man-made, are networks of interconnected components. The principal ingredients of all networks are their nodes and the links among the nodes. Most networks change with time. New nodes can be created and old ones can be eliminated. Similarly, new links can be established and existing ones removed. The nodes can form communities which they may leave later in time. The nodes may join another community or create a new one. Communities can emerge and disappear. All these phenomena can create very rich network dynamics. Understanding the common principles of these dynamics, the time-varying network structures and the functionalities that regulate network behaviors is of foremost importance in many fields of science and engineering. The theory that addresses these phenomena is a part of Network Science.<br/><br/>One of the main objectives of Network Science is to exploit statistical signal processing for inferential modeling of physical, biological, and social phenomena. The aspirations are to improve the understanding and prediction of these phenomena. In this project the investigator proposes to advance Network Science by introducing novel models for dynamic networks and novel ways for making inference and learning about them. The PI proposes to work with a methodology where the complexity of the network model is not predefined a priori but instead, it is determined by the observed data. Furthermore, the investigator proposes to work with Monte Carlo-based methods that can meet the most difficult challenges of the models in terms of their nonlinearities and dimensionalities."
"1350616","CAREER: Learning from Coarse, Nonmetric, and Incomplete Data","CCF","COMM & INFORMATION FOUNDATIONS","05/15/2014","05/22/2018","Mark Davenport","GA","Georgia Tech Research Corporation","Continuing grant","Phillip Regalia","04/30/2019","$515,304.00","","mdav@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797","1045, 7935, 7936, 9251","$0.00","In recent years, we have witnessed an explosion in the amounts of data being acquired and analyzed in a wide variety of contexts. Data-driven techniques are increasingly applied, not only in the traditional quantitative sciences, but also throughout the social sciences and in a variety of other non-traditional scenarios that challenge many of our common assumptions. For example, in contexts such as collaborative filtering, personalized and predictive medicine, and personalized learning systems, we face a variety of challenges due largely to the fact that an important -- often the only -- source of data is people. In these and many other modern applications, we want to learn about people using the data that people supply.<br/><br/>This presents several difficulties, including the fact that such data is often very ""coarse"" or heavily ""quantized"". It might even be binary or entirely nonmetric data consisting of categories or comparisons. Moreover, in many of these cases it is impossible to fully sample the data, and the underlying data of interest may be constantly changing, necessitating approaches that can handle incomplete observations and dynamic data models. This research confronts these difficulties by building on recent progress in the design of efficient algorithms for exploiting low-dimensional structure to perform inference, often using highly incomplete and coarse observations.  This research addresses a number of fundamental theoretical and algorithmic questions in the context of low-rank matrix recovery, nonmetric multidimensional scaling, unfolding, and low-dimensional dynamic models. It has applications in contexts such as collaborative filtering, personalized and predictive medicine, and personalized learning systems."
"1814706","AF: Small: Collaborative Research: Boolean Function Analysis Meets Stochastic Design","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/14/2018","Anindya De","IL","Northwestern University","Standard Grant","Tracy Kimbrel","05/31/2021","$333,497.00","","de.anindya@gmail.com","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7796","7923, 7926, 7927, 7932","$0.00","A central goal in the field of optimization is to develop effective procedures for decision-making in the presence of constraints. These constraints are often imposed by real-world data, but it is frequently the case that the relevant data is not completely known to the agent performing the optimization; it is natural to model such settings using probability distributions associated with the data. In such analyses, the constraints associated with the optimization problem are now themselves ""stochastic,"" and a standard goal is to maximize the likelihood of satisfying all the constraints while incurring minimum cost. (As a motivating example, an airline may wish to operate as few flights as possible while ensuring that with 99% probability, no passenger is bumped.)  Apart from modeling uncertainty, optimization with stochastic constraints also provides a way to succinctly model constraints whose standard description is very large; design problems in voting theory, where there are very many voters, are examples of this kind.  This project studies both of these kinds of problems, called stochastic design problems, from a unified new perspective based on techniques from computational complexity theory. The project also trains graduate students who will achieve fluency both in complexity theory and in optimization, and will promote cross-disciplinary activities between operations research and theoretical computer science. <br/> <br/>The motivating insight which underlies this project is that Boolean function analysis -- a topic at the intersection of harmonic analysis, probability theory, and complexity theory -- provides a useful suite of techniques for stochastic design problems. The investigators will study two broad topics. The first one is on chance-constrained optimization:  In problems of this sort, one is given a set of stochastic constraints and the aim is to satisfy all the constraints with at least a certain fixed threshold probability. While previous work on such problems has typically achieved computationally efficient algorithms by relaxing the actual set of constraints, the investigators will focus on algorithms which exactly satisfy the original given set of stochastic constraints. This line of work will address the chance-constrained versions of fundamental optimization problems such as bin packing, knapsack, and linear programming. The second broad topic is that of inverse problems in social choice theory:  Game theorists use so-called ""power indices"" to measure the influence of voters in voting schemes. A basic algorithmic problem is to design efficient algorithms for the inverse problem, in which, given a set of prescribed power indices, the goal is to construct a voting game with these indices. The investigators will study questions such as (a) to what extent is a given voting scheme specified by its power indices? (b) what is the complexity of exactly reconstructing an unknown target voting game given its power indices? (c) when and to what extent is reconstruction possible in a partial information setting?<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1439126","XPS: FULL: FP: Collaborative Research: Taming parallelism: optimally exploiting high-throughput parallel architectures","CCF","Exploiting Parallel&Scalabilty","09/01/2014","08/06/2014","Milind Kulkarni","IN","Purdue University","Standard Grant","Anindya Banerjee","08/31/2019","$329,571.00","","milind@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8283","","$0.00","Title: XPS: FULL: FP: Collaborative Research: Taming parallelism: Optimally exploiting high-throughput parallel architectures<br/><br/>Over the past decade, computer manufacturers have focused on producing ""multicore"" chips, that package multiple, powerful computing cores on a single chip. Researchers have invested significant effort in developing methods for writing programs that can run efficiently on these cores. The basic idea is to allow programmers to write programs using a high-level programming model and to rely on an underlying compiler and runtime system to efficiently schedule these programs on multicore platforms. However, due to power and heat dissipation concerns, emerging ""throughput-oriented"" computing systems increasingly rely on far simpler computing cores to deliver parallel computing performance. These cores are much more efficient than traditional multicores, and can deliver much higher performance. Practitioners across numerous fields -- bioinformatics, data analytics, machine learning, etc. -- are deploying these systems to harness their power. Unfortunately, existing high level programming models are targeted to multicore chips, and do not produce code that can run effectively on these new systems. As a result, practitioners are forced to rewrite their applications, with painstaking low-level optimization and scheduling. This project will develop schemes to adapt applications written for multicore systems to run efficiently on throughput-oriented processors. The intellectual merits are novel program optimizations that will transform multicore-oriented programs into forms that map efficiently to throughput-oriented processors, scheduling mechanisms that ensure that these throughput-oriented processors do not waste computational resources, and scheduling policies that ensure that the mechanisms are used effectively. The project's broader significance and importance are that programmers will be able to write portable, high-performant and energy-efficient programs for both traditional multicore systems as well as throughput-oriented systems. Moreover, high-level programming models will be used to program the throughput-oriented machines, thus leading to significant reduction of programming effort for practitioners in many science and engineering disciplines. Finally, outreach efforts enhance the project by providing training and mentoring to a diverse group of students.<br/><br/>Languages like Cilk provide support for ""dynamic multithreading"", which allows programmers to identify all of the parallelism in their program, while relying on sophisticated runtime systems to map that parallelism to available parallel execution hardware at runtime. However, Cilk-style execution is inappropriate for the vector-based parallelism found in SIMD units, GPUs and the Xeon Phi; vector parallelism requires finding identical computations performed on different data units. This project investigates a series of transformations that will morph Cilk-style programs into programs that expose vectorizable parallelism, allowing dynamic multithreading programs to be mapped to emerging throughput-oriented architectures. The enabling transformation involves transforming task parallel applications into data-parallel applications by identifying similar tasks being performed at different points in the computation. This project develops a series of scheduling mechanisms and provably efficient scheduling policies that ensure that parallelizing dynamic multithreading applications on throughput-oriented architectures are effective. In this manner, this project enables portable applications that run efficiently both on multicores and on vector-based architectures."
"1814873","AF: Small: Collaborative Research: Boolean Function Analysis Meets Stochastic Design","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/14/2018","Rocco Servedio","NY","Columbia University","Standard Grant","Tracy J. Kimbrel","05/31/2021","$166,270.00","","rocco@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7923, 7926, 7927, 7932","$0.00","A central goal in the field of optimization is to develop effective procedures for decision-making in the presence of constraints. These constraints are often imposed by real-world data, but it is frequently the case that the relevant data is not completely known to the agent performing the optimization; it is natural to model such settings using probability distributions associated with the data. In such analyses, the constraints associated with the optimization problem are now themselves ""stochastic,"" and a standard goal is to maximize the likelihood of satisfying all the constraints while incurring minimum cost. (As a motivating example, an airline may wish to operate as few flights as possible while ensuring that with 99% probability, no passenger is bumped.)  Apart from modeling uncertainty, optimization with stochastic constraints also provides a way to succinctly model constraints whose standard description is very large; design problems in voting theory, where there are very many voters, are examples of this kind.  This project studies both of these kinds of problems, called stochastic design problems, from a unified new perspective based on techniques from computational complexity theory. The project also trains graduate students who will achieve fluency both in complexity theory and in optimization, and will promote cross-disciplinary activities between operations research and theoretical computer science. <br/> <br/>The motivating insight which underlies this project is that Boolean function analysis -- a topic at the intersection of harmonic analysis, probability theory, and complexity theory -- provides a useful suite of techniques for stochastic design problems. The investigators will study two broad topics. The first one is on chance-constrained optimization:  In problems of this sort, one is given a set of stochastic constraints and the aim is to satisfy all the constraints with at least a certain fixed threshold probability. While previous work on such problems has typically achieved computationally efficient algorithms by relaxing the actual set of constraints, the investigators will focus on algorithms which exactly satisfy the original given set of stochastic constraints. This line of work will address the chance-constrained versions of fundamental optimization problems such as bin packing, knapsack, and linear programming. The second broad topic is that of inverse problems in social choice theory:  Game theorists use so-called ""power indices"" to measure the influence of voters in voting schemes. A basic algorithmic problem is to design efficient algorithms for the inverse problem, in which, given a set of prescribed power indices, the goal is to construct a voting game with these indices. The investigators will study questions such as (a) to what extent is a given voting scheme specified by its power indices? (b) what is the complexity of exactly reconstructing an unknown target voting game given its power indices? (c) when and to what extent is reconstruction possible in a partial information setting?<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1350823","CAREER: Algorithms for Risk Mitigation in Networks","CCF","INFORMATION TECHNOLOGY RESEARC, ALGORITHMIC FOUNDATIONS","05/15/2014","05/17/2018","Evdokia Nikolova","TX","University of Texas at Austin","Continuing grant","Tracy Kimbrel","05/31/2019","$561,540.00","","nikolova2009@gmail.com","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1640, 7796","1045, 7926, 7932, 9251, CL10","$0.00","As the world becomes increasingly connected and embedded in computational systems, network algorithms have the potential for tremendous impact. A key challenge is that, despite the enormous progress in computing, connectivity and data analytics, uncertainty remains pervasive in all aspects of life and we need a fundamental shift in the definition of what solutions we seek.  For example, what is the optimal route under uncertain traffic?  That depends on the risk-averseness of a user, who would seek to balance minimizing expected delay and the variability of the route. How can we compute this risk-minimizing route? More generally, how can we compute risk-minimizing solutions in complex networks and how is risk defined?  Risk has been at the forefront of research and practice in finance and economics. However, there is still a major need for designing computational approaches corresponding to these risk models, as well as developing new risk models and solution techniques that are specific to networked systems.  <br/><br/>The goal of this CAREER project is to lay the algorithmic foundation of a new area of risk mitigation for networked systems (such as transportation, telecommunications, energy, etc.) via an interdisciplinary approach that unifies Computer Science, Operations Research, Economics and Finance.  The technical milestones are to: (1) Develop a comprehensive theory of risk models for networked systems, in part inspired by risk models in finance and economics, and in part driven by the specific requirements of networked systems; (2) Advance the classic theory of algorithms, in which all input data is available upfront, by integrating uncertainty and risk---this will be achieved by developing novel techniques for nonlinear and nonconvex combinatorial optimization; (3) Leverage dynamic data to improve adaptive decision-making, using and advancing tools from Markov Decision Processes and developing new tools for approximating the optimal solutions; (4) Further the theory of online algorithms for repeated decision-making by developing reductions from nonlinear (risk-averse) formulations to the standard linear formulations.<br/><br/>On a high level, the transformative potential of the proposed research is to fundamentally shift thinking about stochastic problems in the field of network algorithms away from expected performance and instead towards understanding and mitigating risk.  The research is motivated by problems of national importance in transportation, telecommunications and energy.  It has the potential to improve a variety of applications that involve uncertainty and risk-averse users, for example, reducing congestion in transportation and telecommunication networks, improving the operation of the smart grid, etc. The PI will actively work on building bridges to other disciplines, for example, via organizing interdisciplinary workshops.  The PI will also participate in high-school outreach programs, summer camps for undergraduates and programs for increasing the participation of women and underrepresented minorities in computing."
"1533828","XPS: Full: FP: Collaborative Research: Sphinx: Combining Data and Instruction Level Parallelism through Demand Driven Execution of Imperative Programs","CCF","SOFTWARE & HARDWARE FOUNDATION, Exploiting Parallel&Scalabilty","08/01/2015","08/18/2016","Soner Onder","MI","Michigan Technological University","Standard Grant","Anindya Banerjee","07/31/2020","$575,876.00","","soner@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","7798, 8283","7943, 9251","$0.00","Title: XPS: Full: FP: Collaborative Research: Sphinx: Combining Data and Instruction Level Parallelism through Demand Driven Execution of Imperative Programs<br/><br/>It has become increasingly difficult to improve the performance of processors so that they can meet the demands of existing and emerging workloads. Recent emphasis has been towards enhancing the performance through the use of multi-core processors and Graphics Processing Units. However, these processors remain difficult to program and inflexible to adapt to dynamic changes in the available parallelism in a given program. Although the computer architecture and programming language community continues to innovate and make important gains towards better programmability and better designs, it remains that parallel programming is inherently costly and error prone, and automatic parallelization of programs is not always feasible or effective. The intellectual merits of this project are the development of a new program execution paradigm and the establishment of critical compiler and micro-architecture mechanisms so that one can design processors that can be easily programmed using existing programming languages and at the same time surpass the performance of existing parallel computers. The project's broader significance and importance are wide-spread: the deployment of such processors will push the limits of computation in every field of science and commerce.<br/><br/>The execution paradigm under consideration is a previously unexplored execution model, the demand-driven execution of imperative programs (DDE). The DDE paradigm rests on a solid theoretical framework and promises to efficiently deliver very high-levels of fine-grain parallelism. This parallelism is extracted from a program written in an imperative language such as C, and it is realized by means of an effective compiler-architecture collaboration mechanism using a common, single-assignment form for the program representation. DDE processors can extract instruction-level parallelism much more efficiently than existing superscalar processors as the paradigm does not require dynamic dependency checking. Such processors can fetch, buffer, and execute many more instructions in parallel than current superscalar processors. Owing to its dependence-driven instruction fetching and execution, the paradigm leads to extremely scalable designs, as the communication is naturally localized and synchronization is inherent in the model. Conventional thread-level parallelism (TLP) is orthogonal to DDE, and thus DDE designs can exploit both ILP and TLP. DDE architectures thus represent promising building blocks for extreme-scale machines."
"1533846","XPS: Full: FP: Collaborative Research: Sphinx: Combining Data and Instruction Level Parallelism through Demand Driven Execution of Imperative Programs","CCF","SOFTWARE & HARDWARE FOUNDATION, Exploiting Parallel&Scalabilty","08/01/2015","11/24/2017","David Whalley","FL","Florida State University","Standard Grant","Anindya Banerjee","07/31/2020","$347,000.00","","whalley@cs.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7798, 8283","7943, 9251","$0.00","Title: XPS: Full: FP: Collaborative Research: Sphinx: Combining Data and Instruction Level Parallelism through Demand Driven Execution of Imperative Programs<br/><br/>It has become increasingly difficult to improve the performance of processors so that they can meet the demands of existing and emerging workloads. Recent emphasis has been towards enhancing the performance through the use of multi-core processors and Graphics Processing Units. However, these processors remain difficult to program and inflexible to adapt to dynamic changes in the available parallelism in a given program. Although the computer architecture and programming language community continues to innovate and make important gains towards better programmability and better designs, it remains that parallel programming is inherently costly and error prone, and automatic parallelization of programs is not always feasible or effective. The intellectual merits of this project are the development of a new program execution paradigm and the establishment of critical compiler and micro-architecture mechanisms so that we can design processors that can be easily programmed using existing programming languages and at the same time surpass the performance of existing parallel computers. The project's broader significance and importance are wide-spread: the deployment of such processors will push the limits of computation in every field of science and commerce.<br/><br/>The execution paradigm under consideration is a previously unexplored execution model, the demand-driven execution of imperative programs (DDE). The DDE paradigm rests on a solid theoretical framework and promises to efficiently deliver very high-levels of fine-grain parallelism. This parallelism is extracted from a program written in an imperative language such as C, and it is realized by means of an effective compiler-architecture collaboration mechanism using a common, single-assignment form for the program representation. DDE processors can extract instruction-level parallelism much more efficiently than existing superscalar processors as the paradigm does not require dynamic dependency checking. Such processors can fetch, buffer, and execute many more instructions in parallel than current superscalar processors. Owing to its dependence-driven instruction fetching and execution, the paradigm leads to extremely scalable designs, as the communication is naturally localized and synchronization is inherent in the model. Conventional thread-level parallelism (TLP) is orthogonal to DDE, and thus DDE designs can exploit both ILP and TLP. DDE architectures thus represent promising building blocks for extreme-scale machines."
"1826519","CIF: Small: Inverse Methods for Parametric Mixture Models","CCF","COMM & INFORMATION FOUNDATIONS","01/01/2018","03/01/2018","Yuejie Chi","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","07/31/2019","$213,168.00","","yuejiechi@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7797, 7923, 7926","$0.00","Many imaging modalities in emerging science and engineering applications involve super-resolving low-resolution observations of point sources coming from mixed memberships encoded by different point spread functions. A notable example is super-resolution fluorescence microscopy, whose importance is recognized by the 2014 Nobel Prize in Chemistry, due to its ability of noninvasive imaging of complex biological processes at the nanometer scale. The next frontier, which is three-dimensional super-resolution single-molecule microscopy, allows reconstruction of three-dimensional structures from two-dimensional images, using engineered point spread functions to encode the axial information of different molecules, e.g. by introducing a cylindrical lens. The algorithmic challenge is therefore to simultaneously separate and resolve as many molecules as possible from their superposition in order to enhance the time resolution of imaging.<br/><br/>This research program will develop a unified framework to understand when separation and super-resolution in such mixture models is simultaneously possible, as well as develop algorithms that are computationally efficient, provably correct, and robust to noise. Algorithms will be implemented on real data of three-dimensional super-resolution single-molecule imaging with collaborators at the Dorothy M. Davis Heart and Lung Research Institute at OSU. The project provides interdisciplinary opportunities for students training, where students will develop expertise in mathematical signal processing, optimization, and biomedical data analysis. The results of this project will be integrated into graduate-level courses on inverse problems and high-dimensional data analysis at OSU.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1716400","AF:Small: Novel Geometric Techniques for Several Biomedical Problems","CCF","ALGORITHMIC FOUNDATIONS","09/15/2017","09/07/2017","Jinhui Xu","NY","SUNY at Buffalo","Standard Grant","Rahul Shah","08/31/2020","$451,802.00","","jinhui@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7796","7923, 7929","$0.00","Recent progress in biomedicine has relied heavily on computer science technology. As the territory of biomedicine is rapidly enlarging, more powerful computational techniques are needed to foster its continuous growth. This project develops efficient computer algorithms for three fundamental geometric problems arising in several biomedical applications: (1) Truth Discovery, (2) Abnormal Clusters Detection, and (3) Resource Allocation Voronoi Diagram. Problem (1) develops quality-guaranteed polynomial time solutions to a key problem in data crowdsourcing, finding trustworthy information from multiple data sources, which is also motivated by the biomedical problem of learning critical information for improving treatment planning of endovascular intervention. Problem (2) detects extremely small-sized abnormal clusters from large datasets, which is motivated by detecting genomic structure variants from large populations. Problem (3) investigates new generalizations of the classical Voronoi diagram, which are motivated by a segmentation problem of biological images. This project will provide educational and research opportunities to undergraduate and graduate students (including those from under-represented groups), and develop a teaching evaluation tool for improving the quality of education. <br/> <br/>This project uses computational geometry techniques to develop novel algorithms for the proposed problems. It will introduce several general algorithmic techniques to the area of computational geometry, enriching and prodding its further development. These algorithmic techniques are also likely to be used in other areas, such as machine learning, computer vision, data mining, and pattern recognition, and bring new ideas to these areas. This project could lead to several long term impacts. It could potentially improve the quality of endovascular intervention, help identifying potential genomic structural variants in some genetic disorders, and provide more accurate quantitative information for medical image analysis."
"1717523","AF: CQIS: Small: Theoretical Problems in Quantum Information","CCF","QUANTUM COMPUTING","06/01/2017","05/22/2017","Yaoyun Shi","MI","University of Michigan Ann Arbor","Standard Grant","Dmitri Maslov","05/31/2020","$451,640.00","","shiyy@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7928","7923, 7928","$0.00","This project addresses the following four sets of fundamental questions in quantum information.<br/>(1) Randomness extraction in the presence of quantum side information.<br/>To secure communication security, it is necessary to generate cryptographic keys that cannot be predicted by an adversary. Such keys are usually obtained through a randomness extraction process, which converts a weakly random input to a (almost) perfectly random output.<br/>The team is researching the most efficient methods for this task, with the focus on the setting that the adversary?s knowledge about the input is quantum.<br/>(2) Quantum cryptographic hash functions (qchash). Hash functions are powerful tools that can be used to verify the authenticity and the integrity of a message. Quantum hash functions extend (classical) hash functions by using a quantum state to ?tag? a classical message. The team is working to identify the advantages of such functions, especially in the scenario where classical side information is leaked to the adversary.<br/>(3) Quantum homomorphic encryption (QHE). Homomorphic encryption allows encrypted data to be processed by multiple parties without the need for decryption or for interactions with the owner of the data.<br/>The team is looking for the most efficient methods for achieving this task, when the data are quantum and the adversary is assumed to be all-powerful.<br/>(4) Quantum algorithms for combinatorial optimization. Many optimization problems from practice are extremely difficult to solve, even if one is content with an approximate answer. The team is examining to what extent quantum algorithms can speed up the computation or increase quality of approximation. A focus is on the recently proposed algorithmic framework of quantum approximate optimization algorithms (QAOA).<br/><br/>Positive solutions to the above questions may lead to new and powerful quantum information applications for safeguarding information security and for solving optimization problems. The project also trains multiple students to become experts on quantum information science."
"1566513","CRII: RI: Efficient Structure Learning and Approximation of Networks of Causally Interacting Processes","CCF","","05/01/2016","04/22/2016","Christopher Quinn","IN","Purdue University","Standard Grant","Phillip Regalia","04/30/2019","$175,000.00","","cjquinn@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","026y","7797, 7936, 8228","$0.00","The study of networks is important in numerous scientific domains: neuroscience, microbiology, social science, and economics, to name a few.  A major challenge in these fields is to identify causal influences in the networks.  Experimentation can directly determine causal influences.  However, it can be more costly and less practical than passively recording activity in the network and inferring influences from those observations.  There are numerous methods that can identify correlations from observational data, though identifying causal relationships often requires expert knowledge or strong modeling assumptions.  There is a need for computationally efficient and statistically robust causal inference methods to extract relevant information from network time-series data to facilitate human analysis.<br/><br/>This research aims to significantly advance the state of the art in inferring causal influences between time-series.  The research develops new and efficient algorithms to learn and approximate the structure of a recently proposed probabilistic graphical model: the directed information graph.  The algorithms find optimal or near-optimal approximations of the network topology that have user-controlled sparsity levels, such as the number of edges in the graph or the amount of computation performed.  The quality of approximation is measured using Kullback-Leibler divergence.  The work also involves proving correctness of the algorithms and developing variations that find provably-good approximations which are robust to noisy or limited data.  To achieve these goals, the project develops new bounds for directed information."
"1439062","XPS: FULL: FP: Collaborative Research: Taming parallelism: optimally exploiting high-throughput parallel architectures","CCF","Exploiting Parallel&Scalabilty","09/01/2014","08/06/2014","Kunal Agrawal","MO","Washington University","Standard Grant","Anindya Banerjee","08/31/2019","$330,250.00","","kunal@cse.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","8283","","$0.00","Title: XPS: FULL: FP: Collaborative Research: Taming parallelism: Optimally exploiting high-throughput parallel architectures<br/><br/>Over the past decade, computer manufacturers have focused on producing ""multicore"" chips, that package multiple, powerful computing cores on a single chip. Researchers have invested significant effort in developing methods for writing programs that can run efficiently on these cores. The basic idea is to allow programmers to write programs using a high-level programming model and to rely on an underlying compiler and runtime system to efficiently schedule these programs on multicore platforms. However, due to power and heat dissipation concerns, emerging ""throughput-oriented"" computing systems increasingly rely on far simpler computing cores to deliver parallel computing performance. These cores are much more efficient than traditional multicores, and can deliver much higher performance. Practitioners across numerous fields -- bioinformatics, data analytics, machine learning, etc. -- are deploying these systems to harness their power. Unfortunately, existing high level programming models are targeted to multicore chips, and do not produce code that can run effectively on these new systems. As a result, practitioners are forced to rewrite their applications, with painstaking low-level optimization and scheduling. This project will develop schemes to adapt applications written for multicore systems to run efficiently on throughput-oriented processors. The intellectual merits are novel program optimizations that will transform multicore-oriented programs into forms that map efficiently to throughput-oriented processors, scheduling mechanisms that ensure that these throughput-oriented processors do not waste computational resources, and scheduling policies that ensure that the mechanisms are used effectively. The project's broader significance and importance are that programmers will be able to write portable, high-performant and energy-efficient programs for both traditional multicore systems as well as throughput-oriented systems. Moreover, high-level programming models will be used to program the throughput-oriented machines, thus leading to significant reduction of programming effort for practitioners in many science and engineering disciplines. Finally, outreach efforts enhance the project by providing training and mentoring to a diverse group of students.<br/><br/>Languages like Cilk provide support for ""dynamic multithreading"", which allows programmers to identify all of the parallelism in their program, while relying on sophisticated runtime systems to map that parallelism to available parallel execution hardware at runtime. However, Cilk-style execution is inappropriate for the vector-based parallelism found in SIMD units, GPUs and the Xeon Phi; vector parallelism requires finding identical computations performed on different data units. This project investigates a series of transformations that will morph Cilk-style programs into programs that expose vectorizable parallelism, allowing dynamic multithreading programs to be mapped to emerging throughput-oriented architectures. The enabling transformation involves transforming task parallel applications into data-parallel applications by identifying similar tasks being performed at different points in the computation. This project develops a series of scheduling mechanisms and provably efficient scheduling policies that ensure that parallelizing dynamic multithreading applications on throughput-oriented architectures are effective. In this manner, this project enables portable applications that run efficiently both on multicores and on vector-based architectures."
"1162124","SHF: Medium: Compiling Parallel Algorithms to Memory Systems","CCF","COMPILERS, SOFTWARE & HARDWARE FOUNDATION","04/01/2012","01/18/2018","Stephen Edwards","NY","Columbia University","Continuing grant","Almadena Y. Chtchelkanova","03/31/2019","$1,206,634.00","Martha Kim","sedwards@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7329, 7798","7329, 7924, 7942","$0.00","This project aims to improve the practice of parallel programming - perhaps the central problem facing computer science in the 21st century.  While the sequential model first introduced by Von Neumann and others has served us well, its inefficiency has been brought into sharp focus by the availability of billion-transistor chips, which are greatly underutilized yet power-hungry when running sequential algorithms.<br/><br/>This project aims to improve the programmability and efficiency of distributed memory systems, a key issue in the execution of parallel algorithms.  While it is fairly easy to put, say, thousands of independent adders on a single chip, it is far more difficult to supply them with useful data to add, a task that falls to the memory system.  This research will develop compiler optimization algorithms able to configure and orchestrate parallel memory systems able to utilize such parallel computational resources.<br/><br/>To make more than incremental progress, this project departs from existing hegemony in two important ways.  First, its techniques will be applied only to algorithms expressed in the functional style, a more abstract, mathematically sound representation that enables precise reasoning about parallel algorithms and very aggressive optimizations.  Second, it targets field-programmable gate arrays (FPGAs) rather than existing parallel computing platforms.  FPGAs provide a highly flexible platform that enables exploring parallel architectures far different than today's awkward solutions, which are largely legacy sequential architectures glued together.  While FPGAs are far too flexible and power-hungry to be the long-term ""solution""<br/>to the parallel computer architecture question, their use grounds this project in physical reality and will produce useful hardware synthesis algorithms as a side-effect.<br/><br/>Judicious and efficient data movement is the linchpin of parallel computing.  This project attacks that challenge head on, establishing the constructs and algorithms necessary for hardware and software to efficiently manipulate data together.  This research will lay the groundwork for the next generation of storage and instruction set architectures, compilers, and programming paradigms -- the bedrock of today's mainstream computing."
"1717349","AF:Small: Fundamental High-Dimensional Algorithms","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/31/2017","Santosh Vempala","GA","Georgia Tech Research Corporation","Standard Grant","Rahul Shah","08/31/2020","$400,000.00","","vempala@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","7923, 7926","$0.00","The availability of high-dimensional data in important application areas has made efficient tools to handle such data the need of the century. This proposal addresses some of the most basic questions arising from this need. The topics targeted in this project are on the frontier of research in algorithms, targeting well-known open problems with promising new ideas. Progress on these problems is sure to unravel mathematical structure and is likely to yield new tools. As the field of algorithms continues to expand (and extend its reach beyond computer science), such tools have become indispensable.<br/><br/>The PI was the founding director of the Algorithms and Randomness Center (ARC) and continues in-depth collaborations with scientists from other fields to identify problems and ideas that could play a fundamental role in understanding the complexity of computation.  The topics and findings of this project will be used to design graduate courses and contribute to undergraduate ones. The graduate courses will be the basis for textbooks to benefit the research community. In addition, up-to-date surveys on these topics will be prepared by the PI and collaborators.<br/><br/>Sampling, Learning and Optimization in high dimension are intricately linked at many levels: reductions between problems from one topic to another, insights from one that apply to another, common analysis techniques and similar contexts (e.g., large, high-dimensional data). This project is motivated by quest for a theory of efficient algorithms, a theory that would include algorithmic tools, lower bounds and analysis techniques, in addition to questions that arise from the quest but are of independent mathematical interest and provide new ideas for classical fields.<br/><br/>Specifically, the project seeks to find efficient algorithms for sampling in the oracle model as well as for explicit polytopes, faster sampling and optimization using Riemannian geometry; algorithms for learning polyhedra, the analysis of neural networks with a single hidden layer, robust estimation and unsupervised learning in the presence of noise, and algorithmic considerations in the representation and analysis of very large (but not dense) graphs."
"1715671","CIF: Small: The Interplay Between Convex Feasibility Problems and Minimization Problems in Signal Recovery","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/28/2017","Patrick Combettes","NC","North Carolina State University","Standard Grant","Phillip Regalia","06/30/2020","$362,067.00","","plc@math.ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7797","7923, 7935","$0.00","Signal recovery encompasses the large body of inverse problems in which a signal is to be restored or reconstructed from the observation of data consisting of measurements physically or mathematically related to it. The importance of this field stems from its pervasiveness in numerous areas of science and engineering, including medical imaging, geophysics, astronomy, electron microscopy, nondestructive testing, seismology, telecommunications, social media analysis, and homeland security. This project investigates foundational principles guiding the formulation of signal recovery problems as convex optimization problems and develops new strategies and methodologies for data processing that significantly improve the efficiency of existing techniques and broadens their scope.<br/><br/>This research focuses on the interplay between two prominent frameworks that coexist in relative independence in signal recovery, namely convex feasibility problems and convex minimization problems. These two approaches employ different principles to exploit the prior knowledge and the data, their mathematical formalizations lead to distinct fixed point paradigms, and the algorithms used to solve them do not rely on the same techniques. The investigator shows that, despite these profound divergences, fruitful connections can be established between the two formalisms, that are mutually beneficial and suggest new models and algorithms. An important outcome of this research is a relaxation model that bridges the gap between feasibility and minimization formulations. Another highlight is a novel proximal geometric framework for solving structured minimization problems using a deep cutting plane technology adapted from convex feasibility algorithms. The impact of the theoretical findings and of the new algorithms resulting from this research is illustrated through applications to concrete signal recovery problems."
"1527371","AF:  Small:  Linear Algebra++ and applications to machine learning","CCF","ALGORITHMIC FOUNDATIONS","06/15/2015","05/25/2016","Sanjeev Arora","NJ","Princeton University","Standard Grant","Tracy Kimbrel","05/31/2019","$466,000.00","","arora@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7796","7923, 7926, 9251","$0.00","Many areas of unsupervised learning (i.e, learning with data that has not been labeled by humans) currently rely on heuristic algorithms that lack provable guarantees on solution quality or running time. In fact the underlying problems - as currently formulated - are often known to be computationally intractable (NP-hard, to use a technical term). This proposal identifies a big set of these problems that can be seen as twists - involving constraints such as nonnegativity and sparsity - on classical linear algebra problems like solving linear systems, rank computation, and eigenvalues/eigenvectors. The PI has proposed calling this set of problems collectively as Linear Algebra++. The project will seek to develop  algorithms with provable guarantees for these problems. The methodology will be to make suitable assumptions about the structure of realistic inputs.  The algorithms will be applied to problems in unsupervised learning, in areas such as topic modeling, natural language processing, semantic embeddings, sparse principle components analysis (PCA), deep nets, etc.<br/><br/>The work will lead to new and more efficient algorithms for big data processing that will come with provable guarantees of quality. It will bring new rigorous approaches to machine learning. (Some recent work of the PI shows that the new rigorous approaches can be quite practical.) It will advance the state of art in theoretical computer science by expanding its range and its standard toolkit of algorithms. It will contribute fundamentally new primitives to classical linear algebra and applied mathematics.<br/><br/>The project will train a new breed of graduate students who will be fluent both in theoretical algorithms and machine learning. The PI has a track record in this kind of work and training during the past few years and will continue this including working with undergrads and Research Experiences for Undergraduates (REU) students.  Any new algorithms discovered as part of this project will be released as open source code. The PI also plans a series of other outreach activities in the next few years including (a) A workshop. (b) A special semester or year at the Simons Institute in 2016-17 on provable bounds in machine learning which he will coorganize. (c) A new book on graduate algorithms based upon his new grad course, which tries to re-orient algorithms training for today's computer science problems. (d) A series of talks aimed at broad audiences, of which he gives several each year.<br/><br/>The techniques will build upon recent progress by the PI and others on problems such as nonnegative matrix factorization, sparse coding, alternating minimization etc. They involve average case analysis, classical linear algebra, convex optimization, numerical analysis, etc., as well involve completely new ideas. They could have a transformative effect on machine learning and algorithms."
"1218189","CIF: Small: Adaptive Information: Sequential Sensing and Active Learning Theory,  Methods and Applications","CCF","COMM & INFORMATION FOUNDATIONS, ","09/01/2012","09/21/2018","Robert Nowak","WI","University of Wisconsin-Madison","Standard Grant","Phillip Regalia","08/31/2019","$626,505.00","","rdnowak@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7797, L564","7923, 7936, 9251","$0.00","Complex systems, such as living cells and the Internet, involve interactions and associations between large numbers of individual components. Specific examples include interactions between proteins in biological cells and connections between users in communication networks. Developing theory and methods to model and analyze large-scale networked systems is a grand challenge for the 21st century. For instance, new mathematical methods will lead to better models of biological systems and thus improvements in disease prevention. Measuring the signals of such systems is a crucial step in developing good models, and a major hurdle is that it is often impractical or impossible to measure all variables in large systems. This project addresses the hurdle by developing adaptive methods that automatically adjust the measurement process by using information gleaned from previously collected data in order to focus and optimize the gathering of new information. Ultimately, these methods will dramatically accelerate the pace of discovery in science and engineering.<br/><br/>The main theme of the project is an investigation of the role of adaptive measurement, sensing and experimentation in large complex systems of many variables. Adaptive methods are sequential procedures that optimize the selection of the next measurements or experiments based on previously gathered data. The research involves the development of a general theory for adaptive measurement that is applicable to various domains of engineering and science. The main goals are to mathematically characterize and quantify the advantages of adaptive measurements relative to non-adaptive methods and to design optimal adaptive measurement procedures. The investigation also explores the potential of adaptive methods by studying specific problems and applications in biology, national security, and human-computer interaction."
"1718389","CCF-BSF:CIF: Small: Coding for Fast Storage Access and In-Memory  Computing","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","08/30/2017","Lara Dolecek","CA","University of California-Los Angeles","Standard Grant","Phillip Regalia","08/31/2020","$470,000.00","","dolecek@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7797","7923, 7935, 9102","$0.00","Part 1:<br/><br/>Driven by the needs of mobile and cloud computing, demand for data storage is exhibiting steep growth, both in the direction of higher storage density as well as a simultaneous ambitious increase in access performance. A related exciting emerging trend driven by access challenges is in-memory computing, whereby computations are offloaded from the main processing units to the memory to reduce transfer time and energy. The challenges of future rapid storage access and in-memory computing cannot be addressed by the conventional storage architectures that inevitably trade off reliability and capacity for latency. This bottleneck calls for innovative research contributions that can simultaneously maximize the storage density, access performance, and computing functionality. This project addresses this imminent challenge by developing principled mathematical foundations that will underpin future computing systems possessing qualities necessary to address new data-intensive applications, focusing on fundamental performance bounds, algorithms, and practical channel coding methods. The results of this project will be demonstrated on modern data-driven and machine learning applications, will advance the repertoire of mathematical techniques in information sciences, and will directly impact future computer system architectures to meet the growing and wide ranging societal and scientific needs for computing and rapid data processing. Additionally, the proposal offers several mechanisms for broader impacts, including engagement with data storage and memory industry through the existing research center that the principal investigator is leading at UCLA, curriculum development and the introduction of new graduate courses in the UCLA on-line master's program in engineering, engagement of undergraduate researchers, and dissemination of the results through survey-style articles and tutorials.<br/><br/><br/>Part 2: <br/><br/>The project has the following three complementary research goals:<br/><br/>1) Invention of new channel codes for reliable and fast memory access for latency sensitive applications, with the study spanning general memories and specific schemes for resistive memories in particular. The proposed schemes will offer non-trivial extensions to vibrant coding subjects: codes with locality (algebraic and graph-based) and constrained coding; <br/><br/>2) Invention of new channel codes for which the decoding is performed directly in memory to enable simultaneously satisfying competing requirements on latency and reliability. Here, the decoder itself is subject to computational errors, themselves manifested in a data dependent sense. The analysis will lead to bounds and practical code designs robust to data-dependent errors. An exemplar will be codes designed using spatial coupling and decoded using windowed message passing decoders;<br/><br/>3) Development of novel fundamental bounds, algorithms, and channel codes for robust in-memory computing, with the focus on quantifying the robustness of computing primitives in statistical inference and other machine learning algorithms used in modern data-driven applications.  These include fundamental performance limits and new coding-based methods to simultaneously combat sneak paths and computing noise. Analysis will include coding for (noisy) Hamming/Euclidean similarity calculations, evaluated in the context of practical machine learning applications.<br/><br/>Results from this project will also contribute to the curriculum development at UCLA and will offer new opportunities for the engagement of undergraduate researchers from underrepresented groups."
"1814609","SHF: Small: PAW: Novel Functionality in Programming Models to Productively Abstract Wavefront Parallel Pattern","CCF","SOFTWARE & HARDWARE FOUNDATION, EPSCoR Co-Funding","10/01/2018","07/16/2018","Sunita Chandrasekaran","DE","University of Delaware","Standard Grant","Almadena Y. Chtchelkanova","09/30/2021","$399,703.00","","schandra@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","7798, 9150","7798, 7923, 7942, 9150","$0.00","With the rapid and globally competitive development of faster computing systems that can compute up to one quintillion floating-point operations/second, it becomes imperative to update the computer programs that direct how data is analyzed. However, it has been a challenge for established and time-tested legacy scientific code, filling up hundreds to thousands of lines of code, to adapt and alter to exploit the rich computing capacity of these systems. This is largely a manpower issue as the adaptation of codes requires application developers to constantly re-write their program codes. The steep learning curve associated with both the intricacies of hardware and the ever evolving programming languages puts pressure on the developers and impedes the progress of science. The biggest challenge developers face is the inability to maintain a ""write-once reuse multiple times"" software. With all eyes on the development of an exascale machine - one that can compute data at the speed of the human brain - it is imperative to address this fundamental challenge. The aim of this project is to design high-level abstractions that can adapt scientific code to current and upcoming systems in a manner that enhances the performance of these machines, thus ensuring that these ""fast-as-the human-brain"" systems are flexible and adaptable enough to encourage the broader scientific community.<br/>  <br/>The goal of this project to enable high performance, memory-efficient, portable and productive software framework for parallelizing complex parallel patterns such as 'wavefronts', commonly found in large scientific applications such as neutron radiation transport, bioinformatics and atmospheric science. To achieve this goal, the investigator is addressing critical performance portable questions at the algorithmic-level, programming framework-level and at the software design level. The project studies the applicability of well-explored polyhedral transformation frameworks along with task-based environments on novel hardware systems, importantly on pre- and upcoming exascale systems. The studies are also suggestive of shortcomings in current programming models paving the way to developing novel insights towards high-level software abstractions for multi-use in different/diverse projects simultaneously.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836666","RCN: DIMACS/Simons Collaboration on Lower Bounds in Complexity Theory","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","02/26/2019","Rebecca Wright","NJ","Rutgers University New Brunswick","Standard Grant","Tracy Kimbrel","09/30/2021","$499,490.00","Shafrira Goldwasser, Eric Allender, Tamra Carpenter","rwright@barnard.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7796","7927","$0.00","One of the main goals of theoretical computer science is to understand the computational resources needed to solve a given computational problem. Along with the development of new algorithms and new algorithmic techniques, this involves discovering lower bounds on computational resource requirements. The DIMACS/Simons Collaboration on Lower Bounds in Complexity Theory, funded by this award, will speed up progress on some of the most vexing problems in theoretical computer science by enabling sustained collaboration and intense focus over a period of several years and will strengthen the research community whose work is dedicated to proving lower bounds on computational complexity. The research questions addressed by the project are of profound theoretical interest in and of themselves, and are also motivated by extremely pressing practical problems: for example, the cryptographic protocols that currently underlie the internet-based economy all rely on the unproven computational intractability of problems in the complexity class NP.<br/><br/><br/>The project seeks to focus attention on lower bounds because there have been some remarkable recent breakthroughs in proving lower bounds in Boolean circuit complexity, arithmetic circuit complexity, and communication complexity, and on the complexity of data structure access mechanisms. The project begins with an intensive program at the Simons Institute for the Theory of Computing at Berkeley in the fall semester of 2018 and continues with a 2.5-year special focus led by DIMACS at Rutgers that will expand the project to include more people and more topics. By providing opportunities to sustain collaborations and share ideas over a span of years, the RCN contributes to research that strives for a more complete and unified theory of the techniques for proving lower bounds, more powerful abstractions, and ultimately, new breakthroughs in computational lower bounds. This Research Collaboration Network enables expansion of the Fall 2018 Simons program, including additional long-term participants and new activities for graduate students and early-career fellows. The DIMACS special focus includes three workshops (Meta-Complexity, Barriers, and Derandomization; Arithmetic and Boolean Circuit Complexity; and Information-Theoretic Methods in Complexity Theory), a day of tutorials in conjunction with the 2019 Conference on Computational Complexity, a focused working group on Data Structure Lower Bounds, support for the twice-yearly New York Area Theory Day, a robust visitor program, and summer research for undergraduates.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740235","E2CDA: Type I: Collaborative Research: Nanophotonic Neuromorphic Computing","CCF","Energy Efficient Computing: fr","09/15/2017","07/16/2018","Volker Sorger","DC","George Washington University","Continuing grant","Sankar Basu","08/31/2020","$368,597.00","Tarek El-Ghazawi, Vikram Narayana","sorger@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","015Y","7945","$0.00","With this program, the National Science Foundation has challenged the scientific community to help solve the problem of energy efficient computing. Data centers around the world spend almost equal amount of overhead power in cooling and power conversion for every Watt of computation. As society's appetite for information continues to grow, data centers will eventually consume a significant portion of the world's electricity. Today, the entire information technology industry relies on digital electronics, i.e. electrons carrying digital bits, which impose a barrier for how efficient a computational task can be, regardless of how brilliantly software routines can be engineered. This project will explore the physics of lightwaves to craft a ""photonic"" processor that breaks that efficiency barrier. This interdisciplinary endeavor will attempt to generate foundational science which will support future innovation in the tech industry. To the end of help building the workforce for the future, the investigators will continue their commitments in mentoring graduate and undergraduate students, and support broadening participation of women, minorities and underrepresented groups through STEM outreach, and NSF REU sites.<br/><br/>The approach unites neuromorphic (neural network-inspired) architectures, nanophotonic devices, and emerging fabrication platforms. The technical challenge requires vertical integration of new optical devices, logic units, and control software. It utilizes diverse expertise of faculty members in nanophotonic devices, photonic systems and computer engineering. Specifically, the key points of innovation will include: 1) ultra-efficient electro-optic modulators that will act as neurons at the nanoscale; 2) reconfigurable hardware interconnection fabric on-chip that uses light as information carrier; and 3) creation of control software and benchmarks that are required to orient the project within the broader field of efficient computing. A strong experimental thrust to design, build, and demonstrate these proposed systems will serve to reduce the risk of development of this technology and pave the way for other researchers to join the emerging field of nanophotonic neuromorphic computing."
"1740262","E2CDA: Type I: Collaborative Research: Nanophotonic Neuromorphic Computing","CCF","Energy Efficient Computing: fr","09/15/2017","07/05/2018","Paul Prucnal","NJ","Princeton University","Continuing grant","Sankar Basu","08/31/2020","$222,689.00","","prucnal@ee.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","015Y","7945","$0.00","With this program, the National Science Foundation has challenged the scientific community to help solve the problem of energy efficient computing. Data centers around the world spend almost equal amount of overhead power in cooling and power conversion for every Watt of computation. As society's appetite for information continues to grow, data centers will eventually consume a significant portion of the world's electricity. Today, the entire information technology industry relies on digital electronics, i.e. electrons carrying digital bits, which impose a barrier for how efficient a computational task can be, regardless of how brilliantly software routines can be engineered. This project will explore the physics of lightwaves to craft a ""photonic"" processor that breaks that efficiency barrier. This interdisciplinary endeavor will attempt to generate foundational science which will support future innovation in the tech industry. To the end of help building the workforce for the future, the investigators will continue their commitments in mentoring graduate and undergraduate students, and support broadening participation of women, minorities and underrepresented groups through STEM outreach, and NSF REU sites.<br/><br/>The approach unites neuromorphic (neural network-inspired) architectures, nanophotonic devices, and emerging fabrication platforms. The technical challenge requires vertical integration of new optical devices, logic units, and control software. It utilizes diverse expertise of faculty members in nanophotonic devices, photonic systems and computer engineering. Specifically, the key points of innovation will include: 1) ultra-efficient electro-optic modulators that will act as neurons at the nanoscale; 2) reconfigurable hardware interconnection fabric on-chip that uses light as information carrier; and 3) creation of control software and benchmarks that are required to orient the project within the broader field of efficient computing. A strong experimental thrust to design, build, and demonstrate these proposed systems will serve to reduce the risk of development of this technology and pave the way for other researchers to join the emerging field of nanophotonic neuromorphic computing."
"1714497","SHF: Small: Molecular Classifier Circuits for Disease Diagnostics","CCF","COMPUTATIONAL BIOLOGY","09/15/2017","06/15/2017","Georg Seelig","WA","University of Washington","Standard Grant","Mitra Basu","08/31/2020","$440,000.00","","gseelig@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7931","7923, 7946","$0.00","Changes in the levels of RNA and protein molecules are associated with a large number of human diseases. Monitoring such changes enables clinicians to perform diagnosis, evaluate therapeutic efficacy and predict disease recurrence. Sometimes, detection of just a single molecular marker can be indicative of a disease state, but more commonly it is necessary to interpret a combination of markers via complex algorithms to obtain a reliable diagnosis. In a traditional diagnostic workflow, markers of interest are first detected and quantitated using tools such as RNA sequencing or microarrays. A computer is then used to make a diagnosis, for example by comparing the measurement results to a previously established benchmark. Despite their widespread use in medical research, these methods remain cost-prohibitive for a large number of medical applications where recurrent monitoring or regular screenings are necessary. To overcome these limitations, this work introduces a novel type of diagnostic tool where the computation and diagnosis is performed by a ""molecular computer"", minimizing the need for complex instrumentation.<br/><br/>This research is tightly integrated with an outreach program that has two main goals. The first goal is to develop an educational program dedicated to teaching the interdisciplinary skills that are necessary to be successful in molecular programming. A second and longer term goal is to increase the enrollment of women in engineering research. A key aim is to motivate students with backgrounds in electrical engineering and computer science to engage in molecular programming research by demonstrating that molecular systems can be ""programmed"" just as we program electronic systems. To achieve these goals the PI is participating in engineering outreach programs and systematically pushes research results into the classroom, both through specialized classes (e.g. synthetic biology) and by incorporating molecular programming modules in core electrical engineering and computer science classes.<br/><br/>The goal of this proposal is to demonstrate that molecular computation could become practically useful for disease diagnosis. The proposed approach integrates computation in silico with computation in the test tube. The workflow begins with the training of a computational classifier --- a support vector machine (SVM) --- on publicly available gene expression data. Then, the in silico classifier is mapped onto a set of DNA strands and complexes that realize the same classifier at the molecular level, resulting in a novel kind of molecular computation architecture. Finally, the molecular classifier is tested on different types of molecular data. In preliminary work, PI has constructed a molecular SVM that can, in principle, be used to distinguish between bacterial and viral infections based on analysis of seven host transcripts. The goal of the current proposal is to optimize and automate classifier design and testing and to bring such technology closer to practical applications."
"1616584","AF: Small: Graph Routing, Vertex Sparsifiers, and Connections to Graph Theory","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","05/25/2016","Julia Chuzhoy","IL","Toyota Technological Institute at Chicago","Standard Grant","Rahul Shah","08/31/2019","$449,720.00","","cjulia@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7926","$0.00","Graphs are basic combinatorial objects that are widely used in computer science, engineering, and beyond. Many problems, both theoretical and practical, can be abstracted through graphs, and graphs are also used to represent data. Several basic optimization problems on graphs naturally arise in many different contexts, and it is important to build and expand a toolkit of algorithmic ideas and techniques for addressing such problems. This project will focus on two broad classes of graph optimization problems: graph routing and graph sparsification. Graph routing problems are used as abstractions in many applications, for example, when designing VLSI chips, in multi-robot motion planning, and routing traffic across optical networks. Graph sparsification can be a useful tool in analyzing large and complex networks, including social ones, and in designing faster algorithms for a variety of graph problems. Graph routing and sparsification problems were studied in several different areas, such as approximation algorithms, fixed parameter tractability, and graph theory. In addition to designing better algorithms for such problems, another goal of the project is to increase the flow of ideas and collaboration between these different communities, by studying problems lying in the intersection of these areas. The project will involve graduate and possibly undergraduate students from TTIC and University of Chicago, as well as students from other institutions who participate in TTIC's summer internship program. The PI will also participate in activities aimed at increasing the participation of women in theoretical computer science.<br/><br/>Typically in a graph routing problem one is given a graph G and a collection of pairs of vertices, called demand pairs, that need to be connected to each other. The goal is to connect as many of the pairs as possible via paths, usually subject to some restrictions on the maximum load on graph vertices or edges. These are fundamental problems that have been studied extensively by both theoretical computer science and graph theory communities. Unfortunately, there are still wide gaps in our understanding of some of the most basic problems in this area, and this project aims to make progress on them. In graph sparsification problems, one is given a graph G and a small set T of its vertices called terminals. The goal is to represent the graph G by a much smaller graph H (called a sparsifier), that approximately preserves the properties of G with respect to T. The specific types of properties one would like to preserve give rise to several types of sparsifiers (for example, we may want to preserve cuts, flows or integral routings between the terminals). Problems in this area naturally connect to approximation algorithms (where sparsifiers can be used to obtain improved approximation factors to various problems), fixed-parameter tractability (where they give a black-box way to design faster algortihms), and to graph theory (many sparsification problems can be cast in graph theoretic terms and have been studied by the graph theory community). There are still many open problems in this area, and this project will attempt to improve the state of the art on several of them."
"1533644","XPS: FULL: FP: A profile-centric IDE for science-based performance engineering in the cloud","CCF","Exploiting Parallel&Scalabilty, ","10/01/2015","08/22/2018","Charles Leiserson","MA","Massachusetts Institute of Technology","Standard Grant","Marilyn McClure","09/30/2019","$1,170,615.00","Robert Miller","cel@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8283, R287","","$0.00","Scientists developing compute-intensive multicore applications find it difficult to parallelize their code, a problem that is exacerbated if they wish to take maximum advantage of the potential provided by cloud computing.  Part of the problem is that bad codes cause the generation of incorrect hypotheses while reading, writing, and debugging code, wasting time, energy, and resources.   <br/><br/>This research plans to meld advanced profiling methods for multithreaded programming with modern user-interface technology to produce a highly usable open-source integrated development environment (IDE) for the performance-engineering of multicore software applications in the cloud.  The goal is to provide programmers with continuous profile data for scalability and other performance profiling relevant to parallel programming.  They plan to embed an IDE into a profiling framework to produce a profile-centric IDE, continuously providing performance feedback so that developers can see and compare the results of recent runs of their program as they edit their code.<br/><br/>The project has the potential to enable science-based performance engineering of multicore applications in the cloud.  The vast majority of computer users, not just expert computer scientists, will be able to develop highly efficient parallel software applications, broadly impacting every computing application in every walk of life.  The software produced by this project will be made freely available to anyone on the World Wide Web using a liberal open-source license."
"1445755","CIRCLE: Catalyzing and Integrating Research, Collaboration, and Learning in Computing, Mathematics, and their Applications","CCF","SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF, CYBERINFRASTRUCTURE, IIS SPECIAL PROJECTS","08/15/2014","07/31/2018","Rebecca Wright","NJ","Rutgers University New Brunswick","Continuing grant","Mitra Basu","07/31/2019","$1,059,999.00","Tamra Carpenter","rwright@barnard.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1714, 2878, 7231, 7484","7361, 7482, 7556, 8090","$0.00","The DIMACS Project on Catalyzing and Integrating Research, Collaboration, and Learning in Computing, Mathematics, and their Applications provides a resource for a large community of researchers and educators in computer science and related mathematical and statistical areas as well as their collaborators in fields such as biology, chemistry, physics, engineering, public health, business, and the social sciences. The project enhances and encourages programs that emphasize fundamental methods and theory to advance computer science and related mathematics while also encouraging their application in areas with the potential to impact the infrastructure and resources society depends on, including healthcare, environmental sustainability, homeland security, information security, energy, and business. The project also reflects the importance of education at DIMACS and works to build and support communities that facilitate entry into and promote retention and success in computing disciplines.  This award partially supports the DIMACS Center infrastructure required to achieve project goals. The programs enabled by the infrastructure this project supports will directly reach an estimate of roughly 2000 people each year, and through them, many others.<br/><br/>Research programs enabled by the project are organized around special focus programs consisting of workshops, research workshop groups, tutorials, and a visitor program, on topics including Cybersecurity; Information Sharing and Dynamic Data Analysis; Algorithms and Energy; Sustainability; Theoretical Foundations and Scalability of Machine Learning; Health and Medical Informatics; Cryptography; and Analytics of Preference, Opinion, Recommendation, and Comparison. Educational programs integrate research and education across levels from precollege through postdoctoral via activities including an extensive Research Experiences for Undergraduates program; a Reconnect program run at satellite locations around the country for 2-year and 4-year college faculty highlighting recent research topics relevant to the classroom; a year-round program of workshops for middle and high school teachers; and development of classroom materials in areas including bio-mathematics, computational thinking, and planning for sustainability.  Connections with researchers in education enhance understanding of the cognitive mechanisms at play when students master computational thinking skills, allowing creation of research-informed materials that facilitate teaching and enhance learning. Several DIMACS programs use multi-layer mentoring models for providing mentoring to students while instilling in them the capacity to mentor, both enhancing their own skills and serving to build excitement for computing and computing-related careers in the next generation."
"1750140","CAREER: New Directions in Graph Algorithms","CCF","ALGORITHMIC FOUNDATIONS","02/01/2018","01/12/2018","Debmalya Panigrahi","NC","Duke University","Continuing grant","Rahul Shah","01/31/2023","$102,075.00","","debmalya.panigrahi@gmail.com","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796","1045, 7926, 9251","$0.00","Networks such as the Internet, social networks, transportation maps, and communication backbones have an ubiquitous presence in modern life. Graph algorithms play a crucial role in these networks by providing a range of basic services such as navigation, traffic management, and robustness against physical failures. Moreover, graphs are useful in modeling interactions in a variety of systems that arise in physical, biological, and social sciences. This project identifies a set of common themes in the algorithmic challenges that arise in modern networks -- uncertainty of data, complex failure patterns, and gigantic scale -- and seeks generic solutions that address these core issues. The project is expected to provide new insights into classical graph optimization problems, while also creating new models, problem formulations, and research directions that embrace these broad challenges. This project will also train graduate and undergraduate researchers in theoretical computer science, with an emphasis on gender diversity and participation of underrepresented groups. <br/><br/>For over fifty years, graph algorithms have played a central role in the advancement of computer science, both in theory and practice. Modern networks have evolved in scale, structure, and functionality, inspiring new models, problems, and algorithms. This project focuses on three key research thrusts for modern graph algorithms:  (a) network design under unreliable or imprecise future predictions, by developing generic optimization techniques for uncertain and dynamic inputs; (b) the analysis of correlation effects in network failures by expanding the scope of classical metrics like minimum cuts to incorporate correlated failures of multiple network components; and (c) the design of highly efficient algorithms for large networks, focusing on the tradeoff between approximation and efficiency for fundamental graph optimization problems. The project will integrate tools from diverse areas such as combinatorial optimization, probability theory, mathematical programming, and continuous optimization to model and address these algorithmic questions, and the project is expected to shed new light on related questions in these domains as well."
"1814797","CIF: SMALL: Toward a Molecular Computer: Scaling up Programmable single-molecule Junctions Based on DNA self-assembly","CCF","COMPUTATIONAL BIOLOGY","10/01/2018","07/30/2018","Risheng Wang","MO","Missouri University of Science and Technology","Standard Grant","Mitra Basu","09/30/2021","$349,999.00","","wangri@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","7931","7923, 7946, 9150","$0.00","The field of molecular electronics aims to exploit single-molecules as the main building blocks for creating electronic circuitry. This would allow the unprecedented miniaturization of computers and many electronics with enhanced data transfer speed, storage efficacy, and signal processing. While some single molecule- and carbon nanotube-transistors have been created, no complex and functional electrical circuits have been fabricated. One fundamental challenge in the fabrication of single-molecule electronics is to create a metal electrode junction with nanometer gaps in a reliable, reproducible, and mass-producible manner. The goal of this project is to discover ways to fabricate such junctions using DNA-based origami nanostructures as templates. DNA based self-assembly has the potential to dramatically decrease the manufacturing cost of electronic devices compared with lithography techniques, while reducing the environmental footprint. This project will provide interdisciplinary research training for graduate and undergraduate students from computer science, biochemistry, material science, to nanofabrication, which is relevant for industrial and academic careers. Through ""Outreach and Pre-college Programs"" high school students from minority backgrounds will participate in laboratory research and take classes during the summer months to become acquainted with the fields of computer science, engineering and nanotechnology. <br/><br/>This award will use self-assembled DNA nanostructures as templates to organize anisotropic nanoparticles into rationally-designed architectures with precisely controlled position and orientation at the nanometer scale. This work will provide a novel platform for fabricating scalable and cost-effective metallic electrodes which could be used to: a) integrate multiple single-molecule components in parallel, and b) study electron transferring of various molecules in a reliable and reproducible manner. The research and findings constitute an important step towards the fabrication of the new generation of integrated molecular circuits for miniaturization of computers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815607","AF: Small: Group Theory and Representation Theory in Matrix Multiplication and Generalized DFTs","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/21/2018","Christopher Umans","CA","California Institute of Technology","Standard Grant","Tracy J. Kimbrel","05/31/2021","$500,000.00","","umans@cs.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7796","7923, 7926, 7927","$0.00","This project addresses faster solutions to two prominent algorithmic problems: matrix multiplication and the generalized Discrete Fourier Transform (DFT). In both cases, the challenge is to obtain fast algorithms. Matrix multiplication is a central problem in theoretical computer science, both because of its intrinsic mathematical appeal, and because improved algorithms for this fundamental problem would lead immediately to improved algorithms for a broad variety of related problems. The generalized DFT is similarly fundamental, and concerns transforming data in certain mathematically meaningful ways.  Optimally fast algorithms for both problems have been longstanding open questions.  Such algorithms would have consequences and applications both within and beyond computer science. The project explicitly aims to increase fruitful interactions between computer science and mathematics, and to integrate appropriate aspects of the research program into teaching and training of students at all levels.<br/><br/>The project's goal is to achieve ""nearly-linear"" time algorithms for both problems, and in the case of the DFT, nearly-linear time algorithms with respect to all finite groups. Both problems possess rich structure that is susceptible to a sophisticated mathematical treatment. The DFT inherently involves group theory and representation theory, while the project's approach to matrix multiplication employs these well-developed areas of mathematics to obtain fast matrix multiplication algorithms. A major technical goal is to develop and leverage a recent breakthrough in combinatorics (the resolution of the ""Cap Set Conjecture"") as a tool in the effort the find or construct a suitable family of groups that can yield the desired nearly-linear time algorithm for matrix multiplication.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1553605","CAREER: The power and limitations of randomness","CCF","ALGORITHMIC FOUNDATIONS","02/01/2016","03/09/2018","Raghu Meka","CA","University of California-Los Angeles","Continuing grant","Tracy J. Kimbrel","01/31/2021","$291,070.00","","raghum@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7796","1045, 7926, 7927","$0.00","This project addresses three foundational questions in computer science: 1) Pseudo- randomness: When is randomness necessary for efficient computing? 2) Hardness of approximation: Which optimization problems are computationally hard? 3) Communication complexity: Which problems can be solved with little communication?<br/><br/>At first glance, these questions appear quite disparate. However, there is a strong and deep connection between them through a hidden, more basic, theme: identifying structure in randomness. The central motif is that understanding what role randomness and pseudorandomness have in computation can be a guiding approach to questions in complexity theory, communication complexity, algorithm design, and more. <br/><br/>The proposed research has potential for impacting several areas such as complexity theory, optimization, streaming algorithms, cryptography, and communication complexity; these areas in turn touch several core fields of computer science that impact all of science and even our daily lives. For example, pseudorandom generators are useful for saving space in streaming algorithms which in turn are important for processing massive amounts of data as is done in many modern applications. An integral part of the proposed research plan is to educate both undergraduate and graduate students. The PI intends to involve students at all levels in performing the research outlined in the proposal by actively advising PhD students as well as guiding undergraduate students on research projects.<br/><br/>In more detail, this project aims to address the following three questions:<br/><br/>1) Pseudorandomness: Can randomness in algorithms be removed at the expense of a constant-factor increase in space? Recent work has led to the resolution of several longstanding challenges in this context and the new techniques can potentially lead to further progress.<br/><br/>2) Optimization hierarchies and hardness of approximation: Semi-definite programming (SDP) hierarchies are some of the most powerful techniques in algorithm design. Can a comprehensive theory to understand the power and limitations of the semi-definite hierarchies be developed for problems in approximation algorithms? This is a particularly pressing issue for problems where we do not have NP-hardness results as is the case for uniform sparsest cut, the unique games problem, or average-case problems like the planted clique problem.<br/><br/>3) Communication complexity: Can we characterize precisely the communication complexity of ?lifted problems??one of the most studied classes of functions in this context? Such a characterization will likely simplify the task of analyzing communication costs significantly. <br/><br/>There is a rich history of interaction between the above pivotal areas and these connections should be investigated anew in light of the recent progress in the respective fields over the last few years."
"1442728","CyberSEES: Type 2: SEA-MASCOT: Spatio-temporal Extremes and Associations : Marine Adaptation and Survivorship under Changes in extreme Ocean Temperatures","CCF","CyberSEES","09/01/2014","07/21/2014","Jennifer Dy","MA","Northeastern University","Standard Grant","Phillip Regalia","08/31/2019","$1,199,617.00","Aidong Ding, Tarik Gouhier, Auroop Ganguly","jdy@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","8211","","$0.00","This proposal develops novel computational and statistical models to address a science question in marine ecology: How will marine organisms adapt and survive under extreme climate stressors, in particular, rising ocean temperatures and their extremes?  Addressing this marine ecology question requires prediction of extreme climate temperature variables at scales of one to a few meters; whereas, current global climate models only yield credible insights at 100 kilometers.  Key to addressing these science questions is to develop computational models for discovering associations and predictive models from nonlinear and relatively non-stationary systems, where the dependence structures can be complex in space and time.  In this project, we propose novel statistical dependence measures that capture nonlinear dependencies and non-stationary properties common in extremes and spatio-temporal applications.  In particular, we investigate dependence measures based on copulas that satisfy the equitability property (a new concept in statistics describing measures that are invariant to transformations) and develop computational models that utilize this dependence measure to perform feature selection to identify relevant variables and remove redundant ones on high-dimensional climate and marine ecology data.  We then develop novel prediction models, leveraging on advances in sparse models, Bayesian nonparametrics, and knowledge of the physics and science of climate and marine ecology. <br/><br/>All the novel computational methods on feature selection and prediction will enable the discovery of associations and prediction of climate extremes at finer resolutions relevant for marine ecology survivorship prediction.  Besides broader impact to society through better marine ecology prediction models, we also provide broader impact to education by leveraging our multi-disciplinary team in offering cross-discipline education and encouraging mentoring of women and minority students into our research program."
"1614023","AF: Small: Rare Events - New Probabilistic and Algorithmic Techniques","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","05/18/2016","Shachar Lovett","CA","University of California-San Diego","Standard Grant","Tracy J. Kimbrel","06/30/2019","$450,000.00","","slovett@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7796","7923, 7926","$0.00","Randomness is a powerful tool in mathematics and computer science. In mathematics, it underlies the ""probabilistic method,"" a method to prove that certain mathematical objects with desired properties exist, by simply picking an object at random, and arguing that with a positive probability, the object has the required properties. In computer science, randomness is a powerful tool in algorithm design. Randomized algorithms are often simpler or perform better than their deterministic counterparts, and have found applications in many fields such as graph algorithms, linear programming, routing algorithms, coding theory, communication protocols, approximation algorithms, cryptography, and many more.<br/><br/>One of the main reasons that randomness is ubiquitous in algorithm design, is that typically the probabilistic method shows that the desired events, which control the success of the algorithm, not only occur with a positive probability (which would be sufficient to prove existence), but that they in fact occur with overwhelming probability. As such, it immediately lends itself to the design of randomized algorithms.<br/><br/>The focus of this project is on regimes where this is not the case. There are several probabilistic techniques which can prove the existence of ""rare events."" That is, they show that the desired events occur with a positive (yet tiny) probability. Although we do not have many such techniques, they have proven to be extremely valuable, with applications in many domains: combinatorial optimization, learning theory, approximation algorithms, distributed algorithms, computational geometry, numerical analysis, and more. The main reason is that such techniques, and more importantly, their algorithmic realizations, provide algorithm designers with new sets of tools that they can apply that go beyond ""vanilla"" probabilistic techniques.<br/><br/>This project will focus both on the development of new mathematical and algorithmic tools and techniques, as well as on their assimilation by students and researchers. This involves mentoring students, creating and teaching relevant classes, writing expository surveys, and organizing workshops.<br/><br/>On the technical side, the project focuses on two main domains. The first is discrepancy theory. Discrepancy theory studies irregularities within distributions, and has intimate relations with rounding techniques for integer programs, and more generally with combinatorial optimization. There are several important open problems in discrepancy theory (most notably the Komlos conjecture), which this project sets a concrete plan to resolve.<br/><br/>The second domain is pseudo-randomness. Pseudo-randomness is the study of deterministic objects which attain certain properties satisfied by random objects. As such, this is a wide area of study, with many applications both in mathematics and computer science. In this project, we focus on pseudorandom objects which, for some bounded family of tests, behave exactly like random objects. While in some settings such objects are deeply understood and widely applied (for example, k-wise independence, which has applications in coding theory, data structures, de-randomization, and more), in many other settings they are much less understood. This project explores new approaches to better understand such objects and to develop new algorithmic applications of them."
"1615489","CIF: Small: Low-Dimensional Structure Learning for Tensor Data with Applications to Neuroimaging","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/27/2016","Selin Aviyente","MI","Michigan State University","Standard Grant","Phillip Regalia","06/30/2020","$500,000.00","Mark Iwen","aviyente@egr.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7797","7923, 7936, 8089","$0.00","Advances in information technology are making it possible to collect increasingly massive amounts of multidimensional, multi-modal data across a diverse range of disciplines including bioinformatics, neuroscience, and the social sciences. One particular area where such multidimensional, multi-modal, and often nonlinear data is collected is neuroscience, in particular human brain connectomics. Connectomics aims to offer a comprehensive framework to describe neuronal connectivity by constructing networks from multi-modal and multi-subject, as well as both temporal and spatial, data. These high dimensional datasets pose a challenge to the signal processing community to develop data reduction methods that can exploit their rich structure in order to extract meaningful summarizations. Over the past several decades a tremendous amount of work has focused on the analysis and compression of high dimensional point-cloud datasets via low-dimensional manifold and subspace techniques. Although the research on low-dimensional structure learning from vector-type data is well developed, the direct application of these methods to higher order data poses significant challenges, including both increased computational complexity, and their inability to capture the couplings across different modes. This research addresses these problems through a tensor-based framework for data reduction and low-dimensional structure learning with a particular focus on reducing dynamic functional connectivity networks (dFCNs) into physiologically meaningful network components.<br/><br/>The investigators develop two complementary approaches to address this high order data reduction problem: 1) Robust low-rank+sparse linear structure learning algorithms for tensors; 2) Multi-scale, locally linear adaptive tensor decomposition algorithms for compressing and learning structure from tensor data. Finally, this tensor based framework is applied to dFCNs constructed from electroencephalogram (EEG) data to assess well-known salience and control functional networks associated with affective regulation and cognitive control."
"1750920","CAREER: Advances in Graph Learning and Inference","CCF","COMM & INFORMATION FOUNDATIONS","02/01/2018","01/29/2018","Chinmay Hegde","IA","Iowa State University","Continuing grant","Phillip Regalia","01/31/2023","$160,413.00","","chinmay@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7797","1045, 7935","$0.00","Graph-based data processing algorithms impact a variety of application domains ranging from transportation networks, artificial intelligence systems, cellphone networks, social networks, and the Web. Nevertheless, the emergent big-data era poses key conceptual challenges: several existing graph-based methods used in practice exhibit unreasonably high running time; several other methods operate in the absence of correctness guarantees. These challenges severely imperil the safety and reliability of higher-level decision-making systems of which they are a part. This research introduces an innovative new computational framework for graph learning and inference that addresses these challenges. Specific applications studied in this project include: better approaches for monitoring roadway congestion and identify traffic incidents in a timely manner; root-cause analysis of complex events in social networks; and design of better personalized learning systems, lowering educational costs and increasing quality nationwide. Activities include integrated programs to increase participation of women and under-represented minorities in the computational sciences. <br/><br/>From a technical standpoint, the investigator pursues three research themes: (i) designing scalable non-convex algorithms for learning the edges (and weights) of an unknown graph given a sequence of independent static and/or time-varying local measurements; (ii) designing new approximation algorithms for utilizing the structure of a given graph to enable scalable post-hoc decision making in complex systems; (iii) developing provable algorithms for training special families of artificial neural networks, and filling gaps between rigorous theory and practice of neural network learning. Progress in each of the above themes will be extensively evaluated using real-world data from engineering applications including social network data, highway monitoring data, and fluid-flow simulation data. Collaborations with domain experts in each of these application areas will ensure that the new theory, tools, and software emerging from this project will lead to meaningful societal benefits."
"1755874","CRII: SHF: A Memory-Centric Hardware Accelerator for Large Scale Data Clustering","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2018","01/11/2018","Mahdi Nazm Bojnordi","UT","University of Utah","Standard Grant","Almadena Chtchelkanova","01/31/2020","$174,918.00","","bojnordi@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798","7941, 8228","$0.00","Clustering is a crucial tool for analyzing data in virtually every scientific and engineering discipline. The U.S. National Academy of Sciences (NAS) has recently announced ""the seven giants of statistical data analysis"" in which data clustering plays a central role. This report also emphasizes that more scalable solutions are required to enable time and space clustering for the future large scale data analyses. As a result, hardware and software innovations that can significantly improve energy-efficiency and performance of the data clustering techniques are necessary to make the future large scale data analysis practical. To this goal, the proposed research demonstrates a radically different vision of solving data clustering problems in the future, where large-scale clustering problems are mapped onto a memory-centric, non-Von Neumann computation substrate and solved in situ within the data arrays, with orders of magnitude greater performance and energy efficiency than contemporary computer systems.<br/><br/>The proposed project will leverage recent developments in resistive random access memory (RRAM) and algorithmic approaches for reformulating clustering problems within massively parallel frameworks, such as bit serial rank order filters, to build an extremely low power and fast memory substrate for future clustering applications. At the software level, novel algorithms will be developed to map different types of heterogeneous data clustering problems (including numerical and non-numerical data points) from scientific and engineering domains onto the proposed memristive accelerator. Programming models, software modules, and application libraries for hardware-software co-design, dynamic resource management, and memory allocation will be developed to give the user control of the data clustering process at runtime. At the hardware level, we will investigate techniques for optimizing power and performance of the memory modules constructed from novel resistive cells and interconnection networks. Architecture and software innovations will be disseminated to the broader research community through published papers, as well as tutorials on the emerging in situ computing platforms and software-hardware interfaces in non-Von Neumann computer systems. The educational component of this project will involve integrating the cell structure, the interconnection networks, the hierarchical software interface, and the control policies into the syllabus of an advanced computer architecture course."
"1409258","CIF: Medium: Collaborative Research: Tracking low-dimensional information in data streams and dynamical systems","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2014","09/04/2015","Michael Wakin","CO","Colorado School of Mines","Continuing grant","Phillip Regalia","08/31/2019","$349,662.00","","mwakin@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","7797","7924, 7936","$0.00","Many applications of significant societal impact are modeled by<br/>complex dynamical system behavior, including the (life, physical<br/>and social) sciences, medicine, economics, law, urban development,<br/>international politics and global conflict. Fortunately, recent<br/>advances in sensor technology have allowed observation of these<br/>phenomena at an unprecedented scale. Unfortunately, the volume and<br/>complexity of available data present many challenges to extracting<br/>meaningful information about these systems. Low-dimensional models<br/>serve as a useful structure for understanding the information in<br/>high-dimensional signals and systems. However, this information<br/>often changes over time, and so these models can further be<br/>improved by exploiting temporal dynamics. This project is concerned<br/>with developing new methods for tracking changing low-dimensional<br/>structure in data streams and dynamical systems, particularly in<br/>settings where the observations may be missing, incomplete,<br/>corrupted, or compressed.<br/><br/>The first research aim in this project is to develop new and<br/>substantially improve existing techniques for tracking<br/>low-dimensional structure and, in particular, to extend tracking<br/>capabilities far beyond conventional signals to much more general<br/>data sets with intrinsic low-dimensional structure. A second<br/>research aim is to develop new tools for tracking low-dimensional<br/>structure in systems jointly with estimating the content of<br/>time-varying signals and data sets. A third research aim is<br/>concerned with higher-dimensional and more complex dynamical<br/>systems, and the goal is to develop improved methods that exploit<br/>low-dimensional structure to perform quantitative and qualitative<br/>analysis in systems that are too complex and high-dimensional for<br/>system identification. In a fourth, educational aim, accessible<br/>K-12 outreach materials are being developed for dissemination<br/>through an online digital library."
"1618551","CIF: Small: Source Separation with an Adaptive Structure for Multi-Modal Data Fusion","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/27/2016","Tulay Adali","MD","University of Maryland Baltimore County","Standard Grant","Phillip Regalia","05/31/2019","$461,642.00","","adali@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7797","7923, 7936","$0.00","In many fields today, information about a given phenomenon is obtained through different types of acquisition techniques and experimental conditions, and the availability of such multimodal data has been growing. Joint analysis of this data---its fusion---promises a more comprehensive and informative view of the task at hand, and, if probed carefully, may open new venues and answer questions we might not have even thought of asking when working with a single modality. There is now significant activity in data fusion in disciplines spanning medical imaging, remote sensing, speech processing, behavioral sciences, and metabolomics, to name a few. A common challenge across multiple disciplines is determining how and to what degree different datasets are related, i.e., identifying the common and distinct subspaces across multiple datasets in terms of the information they provide for the given task.<br/><br/>The current work provides a powerful solution to this key challenge enabling identification of the relationship among multiple datasets in a completely data-driven manner such that both the common and distinct subspaces within each dataset can be robustly identified. By combining this adaptive scheme with the well defined but flexible framework of independent vector analysis (IVA), a new powerful framework, IVA with an adaptive subspace structure (IVA-AS) is developed for effective fusion of both the multiset and multimodal data. The successful application of this framework is demonstrated using a unique medical dataset that allows the study of commonalities and differences in brain function while driving with distractions or under the influence of alcohol."
"1409422","CIF: Medium: Collaborative Research: Tracking low-dimensional information in data streams and dynamical systems","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2014","09/04/2015","Christopher Rozell","GA","Georgia Tech Research Corporation","Continuing grant","Phillip Regalia","08/31/2019","$370,009.00","","crozell@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797","7924, 7936","$0.00","Many applications of significant societal impact are modeled by<br/>complex dynamical system behavior, including the (life, physical<br/>and social) sciences, medicine, economics, law, urban development,<br/>international politics and global conflict. Fortunately, recent<br/>advances in sensor technology have allowed observation of these<br/>phenomena at an unprecedented scale. Unfortunately, the volume and<br/>complexity of available data present many challenges to extracting<br/>meaningful information about these systems. Low-dimensional models<br/>serve as a useful structure for understanding the information in<br/>high-dimensional signals and systems. However, this information<br/>often changes over time, and so these models can further be<br/>improved by exploiting temporal dynamics. This project is concerned<br/>with developing new methods for tracking changing low-dimensional<br/>structure in data streams and dynamical systems, particularly in<br/>settings where the observations may be missing, incomplete,<br/>corrupted, or compressed.<br/><br/>The first research aim in this project is to develop new and<br/>substantially improve existing techniques for tracking<br/>low-dimensional structure and, in particular, to extend tracking<br/>capabilities far beyond conventional signals to much more general<br/>data sets with intrinsic low-dimensional structure. A second<br/>research aim is to develop new tools for tracking low-dimensional<br/>structure in systems jointly with estimating the content of<br/>time-varying signals and data sets. A third research aim is<br/>concerned with higher-dimensional and more complex dynamical<br/>systems, and the goal is to develop improved methods that exploit<br/>low-dimensional structure to perform quantitative and qualitative<br/>analysis in systems that are too complex and high-dimensional for<br/>system identification. In a fourth, educational aim, accessible<br/>K-12 outreach materials are being developed for dissemination<br/>through an online digital library."
"1740796","Collaborative Research:  TRIPODS Institute for Optimization and Learning","CCF","TRIPODS Transdisciplinary Rese","01/01/2018","08/27/2018","Katya Scheinberg","PA","Lehigh University","Continuing grant","Tracy Kimbrel","12/31/2020","$597,811.00","Frank Curtis, Martin Takac","kas410@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","041Y","047Z, 062Z, 9102","$0.00","This Phase I project forms an NSF TRIPODS Institute, based at Lehigh University and in collaboration with Stony Brook and Northwestern Universities, with a focus on new advances in tools for machine learning applications.  A critical component for machine learning is mathematical optimization, where one uses historical data to train tools for making future predictions and decisions.  Traditionally, optimization techniques for machine learning have focused on simplified models and algorithms.  However, recent revolutionary leaps in the successes of machine learning tools---e.g., for image and speech recognition---have in many cases been made possible by a shift toward using more complicated techniques, often involving deep neural networks.  Continued advances in the use of such techniques require combined efforts between statisticians, computer scientists, and applied mathematicians to develop more sophisticated models and algorithms along with more comprehensive theoretical guarantees that support their use. In addition to its research goals, the institute trains Ph.D. students and postdoctoral fellows in statistics, computer science, and applied mathematics,  and hosts interdisciplinary workshops and Winter/Summer schools.  <br/><br/>The research efforts in Phase I are on the analysis of nonconvex machine learning models, the design of optimization algorithms for training them, and on the development of nonparametric models and associated algorithms.  The focus is on deep neural networks (DNNs), mostly in general, but also with respect to specific architectures of interest.  The institute's research efforts emphasize the need to develop connections between state-of-the-art approaches for training DNNs and statistical performance guarantees (e.g., on generalization errors), which are currently not well understood. Optimization algorithms development centers on second-order-derivative-type techniques, including (Hessian-free) Newton, quasi-Newton, Gauss-Newton, and their limited memory variants.  Recent advances have been made in the design of such methods; the PIs' work builds upon these efforts with their broad expertise in the design and implementation (including in parallel and distributed computing environments) of such methods.  The development of nonparametric models promises to free machine learning approaches from restrictions imposed by large numbers of user-defined parameters (e.g., defining a network structure or learning rate of an optimization algorithm).  Such models could lead to great advances in machine learning, and the institute's work in this area also draws on the PIs expertise in derivative-free optimization methods, which are needed for training in nonparametric settings.<br/><br/>In this TRIPODS institute, the PIs approach all of these research directions with a unified perspective in the three disciplines of statistics, computer science, and applied mathematics.  Indeed, as machine learning draws so heavily from these areas, future progress requires close collaborations between optimization experts, learning theorists, and statisticians---communities of researchers that, as yet, have tended to operate separately with differing terminology and publication venues.  With an emphasis on deep learning, this institute aims to foster intercollegiate and interdisciplinary collaborations that overcome these hindrances.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740762","Collaborative Research:  TRIPODS Institute for Optimization and Learning","CCF","TRIPODS Transdisciplinary Rese","01/01/2018","08/26/2018","Francesco Orabona","NY","SUNY at Stony Brook","Continuing grant","Tracy Kimbrel","12/31/2020","$189,872.00","","fo@bu.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","041Y","047Z, 062Z","$0.00","This Phase I project forms an NSF TRIPODS Institute, based at Lehigh University and in collaboration with Stony Brook and Northwestern Universities, with a focus on new advances in tools for machine learning applications.  A critical component for machine learning is mathematical optimization, where one uses historical data to train tools for making future predictions and decisions.  Traditionally, optimization techniques for machine learning have focused on simplified models and algorithms.  However, recent revolutionary leaps in the successes of machine learning tools---e.g., for image and speech recognition---have in many cases been made possible by a shift toward using more complicated techniques, often involving deep neural networks.  Continued advances in the use of such techniques require combined efforts between statisticians, computer scientists, and applied mathematicians to develop more sophisticated models and algorithms along with more comprehensive theoretical guarantees that support their use. In addition to its research goals, the institute trains Ph.D. students and postdoctoral fellows in statistics, computer science, and applied mathematics,  and hosts interdisciplinary workshops and Winter/Summer schools.  <br/><br/>The research efforts in Phase I are on the analysis of nonconvex machine learning models, the design of optimization algorithms for training them, and on the development of nonparametric models and associated algorithms.  The focus is on deep neural networks (DNNs), mostly in general, but also with respect to specific architectures of interest.  The institute's research efforts emphasize the need to develop connections between state-of-the-art approaches for training DNNs and statistical performance guarantees (e.g., on generalization errors), which are currently not well understood. Optimization algorithms development centers on second-order-derivative-type techniques, including (Hessian-free) Newton, quasi-Newton, Gauss-Newton, and their limited memory variants.  Recent advances have been made in the design of such methods; the PIs' work builds upon these efforts with their broad expertise in the design and implementation (including in parallel and distributed computing environments) of such methods.  The development of nonparametric models promises to free machine learning approaches from restrictions imposed by large numbers of user-defined parameters (e.g., defining a network structure or learning rate of an optimization algorithm).  Such models could lead to great advances in machine learning, and the institute's work in this area also draws on the PIs expertise in derivative-free optimization methods, which are needed for training in nonparametric settings.<br/><br/>In this TRIPODS institute, the PIs approach all of these research directions with a unified perspective in the three disciplines of statistics, computer science, and applied mathematics.  Indeed, as machine learning draws so heavily from these areas, future progress requires close collaborations between optimization experts, learning theorists, and statisticians---communities of researchers that, as yet, have tended to operate separately with differing terminology and publication venues.  With an emphasis on deep learning, this institute aims to foster intercollegiate and interdisciplinary collaborations that overcome these hindrances.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740735","Collaborative Research: TRIPODS Institute for Optimization and Learning","CCF","TRIPODS Transdisciplinary Rese","01/01/2018","09/09/2018","Han Liu","IL","Northwestern University","Continuing grant","Tracy J. Kimbrel","12/31/2020","$196,715.00","","hanliu@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","041Y","047Z, 062Z","$0.00","This Phase I project forms an NSF TRIPODS Institute, based at Lehigh University and in collaboration with Stony Brook and Northwestern Universities, with a focus on new advances in tools for machine learning applications.  A critical component for machine learning is mathematical optimization, where one uses historical data to train tools for making future predictions and decisions.  Traditionally, optimization techniques for machine learning have focused on simplified models and algorithms.  However, recent revolutionary leaps in the successes of machine learning tools---e.g., for image and speech recognition---have in many cases been made possible by a shift toward using more complicated techniques, often involving deep neural networks.  Continued advances in the use of such techniques require combined efforts between statisticians, computer scientists, and applied mathematicians to develop more sophisticated models and algorithms along with more comprehensive theoretical guarantees that support their use. In addition to its research goals, the institute trains Ph.D. students and postdoctoral fellows in statistics, computer science, and applied mathematics,  and hosts interdisciplinary workshops and Winter/Summer schools.  <br/><br/>The research efforts in Phase I are on the analysis of nonconvex machine learning models, the design of optimization algorithms for training them, and on the development of nonparametric models and associated algorithms.  The focus is on deep neural networks (DNNs), mostly in general, but also with respect to specific architectures of interest.  The institute's research efforts emphasize the need to develop connections between state-of-the-art approaches for training DNNs and statistical performance guarantees (e.g., on generalization errors), which are currently not well understood. Optimization algorithms development centers on second-order-derivative-type techniques, including (Hessian-free) Newton, quasi-Newton, Gauss-Newton, and their limited memory variants.  Recent advances have been made in the design of such methods; the PIs' work builds upon these efforts with their broad expertise in the design and implementation (including in parallel and distributed computing environments) of such methods.  The development of nonparametric models promises to free machine learning approaches from restrictions imposed by large numbers of user-defined parameters (e.g., defining a network structure or learning rate of an optimization algorithm).  Such models could lead to great advances in machine learning, and the institute's work in this area also draws on the PIs expertise in derivative-free optimization methods, which are needed for training in nonparametric settings.<br/><br/>In this TRIPODS institute, the PIs approach all of these research directions with a unified perspective in the three disciplines of statistics, computer science, and applied mathematics.  Indeed, as machine learning draws so heavily from these areas, future progress requires close collaborations between optimization experts, learning theorists, and statisticians---communities of researchers that, as yet, have tended to operate separately with differing terminology and publication venues.  With an emphasis on deep learning, this institute aims to foster intercollegiate and interdisciplinary collaborations that overcome these hindrances.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1739635","E2CDA: Type I: Probabilistic Spin Logic for Low-Energy Boolean and Non-Boolean Computing","CCF","Energy Efficient Computing: fr, SOFTWARE & HARDWARE FOUNDATION","10/01/2017","07/11/2018","Joerg Appenzeller","IN","Purdue University","Continuing grant","Sankar Basu","09/30/2020","$1,651,836.00","Ronald DeMara, Ramamoorthy Ramesh, Chris Kim","appenzeller@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","015Y, 7798","7798, 7945, 9251","$0.00","Various components of hardware and software have come together to make this electronic revolution surrounding smartphones and the like possible. To further expand our capabilities, to enter a new era of even more comprehensive networking between individual users and the cloud, as well as to solve problems of a complexity level that cannot be tackled even with today's computers, there is the need to push the frontiers of data processing beyond anything that can be achieved through ""conventional"" electronics. One approach is to improve the hardware further. However the latter has been historically mainly focused on making devices ever smaller, thus gaining the benefit of a more compact but very power hungry electronics. The current project revolves around a novel device that uses electron spins rather than electron charges for information processing, and holds the promise to achieve the above mentioned goals at a power consumption level that is far smaller than what can be envisioned with conventional technologies. At the same time the project aims at a new level of education and dissemination through the concept that we call ""atoms to systems"". A new generation of engineers requires skill sets and a knowledge base that is different from what was learned previously. A high priority of the project is thus to make the models and ideas that underlie them accessible across science and engineering.<br/><br/>There is increasing interest in a fundamentally different form of brain-like logic based on probabilistic inference that is far more effective and energy efficient in dealing with the problems of search and recognition posed by the ever increasing amounts of ""big data"". Probabilistic logic is currently implemented with software algorithms that run on a deterministic computing platform. The goal is to lay the foundation for a new P(robabilistic)-computing platform using unstable multiferroics, a manmade material combination that combines distinct electrical, mechanical and magnetic properties. Unlike quantum computers, P-computers should operate robustly at room temperature, while providing some aspects of the ""quantum parallelism"" that facilitates the solution of hard problems. The team will work on the material, device and circuit development of this novel probabilistic computing idea. Technical work related to the characterization of novel material properties, the impact of various structural parameters and architectural aspects will all be explored in parallel by the group of experts."
"1521925","Collaborative Research: Evolable Living Computing: Understanding and Qunatifying Synthetic Biological Systems' Applicability, Performance and Limits","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","02/11/2019","Ron Weiss","MA","Massachusetts Institute of Technology","Continuing grant","Mitra Basu","11/30/2020","$3,000,000.00","Domitilla Del Vecchio, Timothy Lu","rweiss@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7723","7723","$0.00","Successful computing systems leverage their underlying technologies to solve problems humans simply cannot. Electronic systems harness the power of radio waves and electrons. Mechanical systems use physical force and physical interactions. Biological systems represent a computing paradigm that can harness evolution, adaptation, replication, self-repair, chemistry, and living organisms. Engineered, living biological systems which make decisions, process ""data"", record events, adapt to their environment, and communicate to one another will deliver exciting new solutions in bio-therapeutics, bio-materials, bio-energy, and bio-remediation. This project will create a quantitative set of freely available design principles, computational tools, mathematical models, physical biological artifacts, educational resources, and outreach activities. Once available, these resources will allow for novel, living biological solutions to be built more quickly, perform better, be more reliable to manufacture, and cost less to produce. This project is unique in that these resources will be explicitly developed to validate key computational concepts to understand how well these concepts can be applied rigorously and repeatedly to biology. This project decomposes these concepts into three areas: Computing Paradigm (digital, analog, memory, and communication), Computing Activity (specification, design, and verification), and Computing Metric (time, space, quality, and complexity). Once complete, this project will provide the most comprehensive, freely available, and computationally relevant set of building blocks to engineer biological systems to date. <br/><br/>By developing the tools, techniques, and materials outlined in this project, this research will fundamentally change the way biological systems are specified, designed, assembled, and tested. Advanced bio-energy, bio-sensing, bio-therapeutics, and bio-materials all will become increasingly viable commercial technologies that can be made better, cheaper, faster, and more safely as a result of this project. The education of an entire new generation of engineers will occur through workshops, coursework, and community engagement activities.  This new generation will have access to these approaches which will influence how biological computation is done and how that process is communicated to the community.  This project will bring computational questions and methods to the forefront of biotechnology via an interdisciplinary research team focused not on one-off solutions but on foundational computing principles. <br/><br/>Explicitly five unanswered questions will be addressed in this project: (1) What computational models are available to biology, what are their limits, and how do they perform?  (2) What communication mechanisms are available to biology, what are their limits, and how do they perform? (3) What are the theoretical and empirical measures of quality, scale, time, and space in biological computing systems? (4) How generalizable are the concepts and ""design rules"" which can be learned from studying biological systems?  (5) How can the results (data and learnings) from biological specification, design, and verification be authoritatively disseminated to the community as design principles and grand challenges? This project addresses these questions with an interdisciplinary team with expertise in theoretical computer science, electronic design automation, bio-physics/chemistry, control theory, and molecular cell biology. For more information visit www.programmingbiology.org."
"1521759","Collaborative Research: Evolvable Living Computing - Understanding and Quantifying Synthetic Biological Systems' Applicability, Performance, and Limits","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","02/11/2019","Peter Carr","MA","Massachusetts Institute of Technology","Continuing grant","Mitra Basu","11/30/2020","$1,000,000.00","","carr@ll.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7723","7723","$0.00","Successful computing systems leverage their underlying technologies to solve problems humans simply cannot. Electronic systems harness the power of radio waves and electrons. Mechanical systems use physical force and physical interactions. Biological systems represent a computing paradigm that can harness evolution, adaptation, replication, self-repair, chemistry, and living organisms. Engineered, living biological systems which make decisions, process ""data"", record events, adapt to their environment, and communicate to one another will deliver exciting new solutions in bio-therapeutics, bio-materials, bio-energy, and bio-remediation. This project will create a quantitative set of freely available design principles, computational tools, mathematical models, physical biological artifacts, educational resources, and outreach activities. Once available, these resources will allow for novel, living biological solutions to be built more quickly, perform better, be more reliable to manufacture, and cost less to produce. This project is unique in that these resources will be explicitly developed to validate key computational concepts to understand how well these concepts can be applied rigorously and repeatedly to biology. This project decomposes these concepts into three areas: Computing Paradigm (digital, analog, memory, and communication), Computing Activity (specification, design, and verification), and Computing Metric (time, space, quality, and complexity). Once complete, this project will provide the most comprehensive, freely available, and computationally relevant set of building blocks to engineer biological systems to date. <br/><br/>By developing the tools, techniques, and materials outlined in this project, this research will fundamentally change the way biological systems are specified, designed, assembled, and tested. Advanced bio-energy, bio-sensing, bio-therapeutics, and bio-materials all will become increasingly viable commercial technologies that can be made better, cheaper, faster, and more safely as a result of this project. The education of an entire new generation of engineers will occur through workshops, coursework, and community engagement activities.  This new generation will have access to these approaches which will influence how biological computation is done and how that process is communicated to the community.  This project will bring computational questions and methods to the forefront of biotechnology via an interdisciplinary research team focused not on one-off solutions but on foundational computing principles. <br/><br/>Explicitly five unanswered questions will be addressed in this project: (1) What computational models are available to biology, what are their limits, and how do they perform?  (2) What communication mechanisms are available to biology, what are their limits, and how do they perform? (3) What are the theoretical and empirical measures of quality, scale, time, and space in biological computing systems? (4) How generalizable are the concepts and ""design rules"" which can be learned from studying biological systems?  (5) How can the results (data and learnings) from biological specification, design, and verification be authoritatively disseminated to the community as design principles and grand challenges? This project addresses these questions with an interdisciplinary team with expertise in theoretical computer science, electronic design automation, bio-physics/chemistry, control theory, and molecular cell biology. For more information visit www.programmingbiology.org."
"1513915","CIF: Medium: Collaborative Research: Feedback Communication: Models, Designs, and Fundamental Limits","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2015","10/23/2018","Sergio Verdu","NJ","Princeton University","Continuing grant","Phillip Regalia","05/31/2020","$400,000.00","","verdu@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7797","7924, 7935","$0.00","Claude Shannon?s ?A Mathematical Theory of Communication? is the landmark event that paved the way for the development of modern communication systems. Shannon analyzed the fundamental redundancy that must be added to data in order to achieve reliable communication in the presence of noise. Since then his vision has guided the practical design of virtually all aspects of modern communication systems such as forward error correction, spectrally-efficient communication, multiuser and inter-symbol interference, multiple-antenna systems, opportunistic communication, and joint compression/transmission. However, while feedback is present in virtually all modern communication systems, the field of information theory has had relatively little impact on how feedback is employed in practice.  The proposed work will advance knowledge by developing a more complete understanding of how feedback should be employed in communication systems and what quantitative improvements in delay, complexity, and transmitted power one can expect from its effective use, under realistic delay constraints such as those found in high-speed wireless data. In summary, the successful completion of the project is expected to contribute new mathematical tools, designs, viewpoints, and models to the field of information theory.<br/><br/>The goal of this research is to bring the insight, design guidance, and performance bounds for which information theory is known to bear on the design of systems with feedback. By providing new design principles and feedback codes, this research has the potential not only to advance basic science but also to improve the efficiency and reliability of our communications infrastructure, including consumer technology such as WiFi and smartphones. Given the proliferation of personal communication devices, such improvements would augment the efficiency with which crucial resources such as energy and radio frequency bandwidth are currently utilized."
"1716388","Approximate Message Passing Algorithms and Networks","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/28/2017","Philip Schniter","OH","Ohio State University","Standard Grant","Phillip Regalia","06/30/2020","$499,570.00","","schniter@ece.osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7797","7923, 7936","$0.00","A problem of paramount importance in engineering, science, and medicine is that of recovering information signals from high-dimensional measurements.  This problem manifests in many forms, e.g., reconstructing a high-quality image from a few noisy Fourier projections, determining which features in patient data are most likely associated with a given disease, or classifying which objects are present within an image.  Until recently, the dominant approach to signal recovery was algorithmic.  But nowadays, algorithms are increasingly being replaced by deep neural networks (DNNs), which can learn optimal inference strategies directly from the data.  This project researches algorithmic as well as deep-neural-network (DNN) approaches to high-dimensional signal recovery, leveraging connections between them to make advances in both.<br/><br/>On the algorithmic front, this project investigates the vector approximate message passing (VAMP) algorithm.  Like the original AMP algorithm of Donoho, Maleki, and Montanari, the VAMP algorithm enjoys low complexity and a scalar state-evolution that rigorously and concisely characterizes its behavior.  However, VAMP is applicable to a much larger class of problems than AMP.  On the DNN front, this project investigates DNNs whose architecture is inspired by the processing steps within VAMP.  The resulting DNNs are highly interpretable and, for some simple applications, statistical optimal.  This project aims to develop this VAMP-based DNN design framework to work with more complex applications."
"1442777","CyberSEES: Type 2: Collaborative Research: Connecting Next-generation Air Pollution Exposure Measurements to Environmentally Sustainable Communities","CCF","ALGORITHMIC FOUNDATIONS, CyberSEES","09/01/2014","07/20/2018","Robert Dick","MI","University of Michigan Ann Arbor","Standard Grant","Rahul Shah","08/31/2019","$203,171.00","","dickrp@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7796, 8211","8208, 9251","$0.00","Ambient exposure to ground-level air pollution is linked to adverse health effects in many populated areas of the world. However, advances in relating air pollution exposure to sustainable communities are hindered by limited direct observations of exposure and the coarseness of regional and global air quality models used for decision making. As a result, existing models do not resolve the scales of variability in either pollutant concentrations or population distributions necessary to accurately assess exposure nor provide the type of probabilistic uncertainty bounds required for policy.<br/><br/>This project aims to assimilate comprehensive cyber information for use in air quality management. It advances the interdisciplinary field of cyber-environmental research through investigation into (1) cyber-scale data analysis to harness and distill valuable cyber information to support community-scale air pollution modeling; (2) micro-environment targeted sensing to augment community-scale studies with accurate, on-demand, and in situ sensing capabilities; and (3) scalable exposure modeling and analysis by solving a complex, spatiotemporally varying problem with high-dimensional data containing cyber, sensing, and model outputs.<br/><br/>Advances in cyber-environmental research have the potential to improve government policy making, regulations, and personal choices with regard to environmentally sustainable community development. Results of this project can apply across a wide spectrum of sectors including energy, transportation, and healthcare. This project broadens vertical research and education integration across information technologies and environmental science and engineering, to both graduate and undergraduate students. Through in-field trials, this project offers unique real-world education and research opportunities to attract students from underrepresented groups and industrial professionals."
"1815896","CIF: Small: Sequential and Compound Estimation for Computational Imaging Systems","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2018","05/22/2018","Vivek Goyal","MA","Trustees of Boston University","Standard Grant","Phillip Regalia","06/30/2021","$490,462.00","","goyal@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7797","7923, 7935","$0.00","The widespread use of imaging technologies across biology, chemistry, material science, medicine, and other fields is motivated by the great innate human ability for visual information processing.  Imaging is often limited, however, by the damage to a sample caused by the imaging process, such as sputtering due to the incident ion or electron beam in scanned beam microscopy.  With conventional methods, a sample may be destroyed before enough information has been gained to form a useful image.  This project develops methods for reducing the necessary dose in microscopy systems.  This may make it possible to image molecular or biological species not before imaged, or to increase the speed of imaging so that new dynamic phenomena are revealed.  The project also includes dissemination of results to broad audiences through expository writing, general-audience talks, cross-disciplinary training, and production of teaching materials.<br/> <br/>Poisson models are often exploited when measured signals are due to small numbers of detected particles.  A major premise of this project is to combine the Poisson modeling of such detection counts with Poisson modeling of the numbers of interacting particles in low-current beams.  The project develops theory and methods for the resulting compound estimation problems.  The project explores the use of time-resolved sensing to reduce the uncertainty due to the source shot noise in compound processes.  Furthermore, time-resolved sensing enables the decomposition of a data collection process into a sequence of less-damaging steps, and the project will develop theory and methods for sequential estimation and adaptive data collection.  Analysis will inspire and be informed by proof-of-concept experiments.  The ultimate limit of learning about a sample without damaging it is achieved with certain quantum-mechanical interaction-free measurements.  The project will develop theory for sequential interaction-free measurements to establish when they may be superior to conventional measurements.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1513858","CIF: Medium: Collaborative Research: Feedback Communication: Models, Designs, and Fundamental Limits","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2015","08/08/2017","Aaron Wagner","NY","Cornell University","Continuing grant","Phillip Regalia","05/31/2019","$392,777.00","","wagner@ece.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","7924, 7935","$0.00","Claude Shannon?s ?A Mathematical Theory of Communication? is the landmark event that paved the way for the development of modern communication systems. Shannon analyzed the fundamental redundancy that must be added to data in order to achieve reliable communication in the presence of noise. Since then his vision has guided the practical design of virtually all aspects of modern communication systems such as forward error correction, spectrally-efficient communication, multiuser and inter-symbol interference, multiple-antenna systems, opportunistic communication, and joint compression/transmission. However, while feedback is present in virtually all modern communication systems, the field of information theory has had relatively little impact on how feedback is employed in practice.  The proposed work will advance knowledge by developing a more complete understanding of how feedback should be employed in communication systems and what quantitative improvements in delay, complexity, and transmitted power one can expect from its effective use, under realistic delay constraints such as those found in high-speed wireless data. In summary, the successful completion of the project is expected to contribute new mathematical tools, designs, viewpoints, and models to the field of information theory.<br/><br/>The goal of this research is to bring the insight, design guidance, and performance bounds for which information theory is known to bear on the design of systems with feedback. By providing new design principles and feedback codes, this research has the potential not only to advance basic science but also to improve the efficiency and reliability of our communications infrastructure, including consumer technology such as WiFi and smartphones. Given the proliferation of personal communication devices, such improvements would augment the efficiency with which crucial resources such as energy and radio frequency bandwidth are currently utilized."
"1657420","CRII: CIF: Limits and Robustness of Nonconvex Low-Rank Estimation","CCF","CRII CISE Research Initiation","02/15/2017","02/10/2017","Yudong Chen","NY","Cornell University","Standard Grant","Phillip Regalia","01/31/2020","$175,000.00","","yc2272@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","026Y","7797, 7936, 8228","$0.00","The objective of this research is to significantly broaden the algorithms and theory for nonconvex low-rank estimation. Low-rank estimation problems are ubiquitous in science and engineering. Recently developed nonconvex methods promise great computational gains on large-scale datasets, but the algorithmic and theoretical foundation has not yet reached the same level of maturity as their convex counterpart. This research attacks this deficit in two research thrusts: pushing the limits of nonconvex methods for greater flexibility through a unified theoretical framework, and developing new algorithms robust to data corruption. <br/><br/>To achieve greater flexibility and generality for the nonconvex approach, the investigator develops a new unifying paradigm that explains when and why nonconvex methods succeed. This is accomplished by a novel reinterpretation of various nonconvex methods through a two-step procedure. This flexible framework unifies several existing algorithms including gradient descent and alternating minimization, and opens the door for designing new algorithms. Theoretically this unified view allows for a decoupling of the statistical and optimization analysis. The investigator will explore the consequences of this approach by (a) providing a simpler and modular analysis of the convergence and statistical properties of existing algorithms, (b) studying the global behaviors of nonconvex methods and the role of initialization, and (c) designing new algorithms that are more efficient and general. The second thrust of this project studies the robustness of nonconvex methods. To protect against arbitrary corruption in data, the investigator designs new robust nonconvex formulations by viewing corruption as a superimposed structure and leveraging sparsity in the optimization objectives. This result will be further expanded through the use of nonsmooth nonconvex formulations and a complete rethinking of existing analytic techniques."
"1652257","CAREER:   New Methods for Central Streaming Problems","CCF","ALGORITHMIC FOUNDATIONS","02/01/2017","02/27/2019","Vladimir Braverman","MD","Johns Hopkins University","Continuing grant","Rahul Shah","01/31/2022","$322,966.00","","vova@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7796","1045, 7926","$0.00","The streaming model is a powerful model of computation that has made a significant impact on computer science over the past decade. Recent developments demonstrate the critical need for streaming methods in numerous applications such as networking, machine learning, astronomy and statistical inference. The project will develop new streaming and sketching algorithms that will be applicable in the aforementioned areas. The project will support undergraduate research and engage students in working on cutting-edge theoretical problems. The project will promote STEM education by collaborating with Independence School Local 1 (IHS), a public charter high school in Baltimore city, where minority students constitute about 60 percent of the student body. This project will help to organize (I) a workshop for first generation students and (II) an annual Sublinear Algorithms Workshop at Johns Hopkins University. The project will promote core education and will introduce new advanced courses and seminars that will convey the principles of algorithms to non-theory students.<br/><br/>In 1996, Alon, Matias and Szegedy published a fundamental paper on streaming algorithms. The paper introduced the problem of approximating frequency moments in the streaming model and asked the open question, ?What other frequency-based functions can be approximated on streams?? Since 1996 the research on data streams has resulted in great progress. Despite this progress, our understanding of many fundamental streaming problems is far from being complete. The main technical objective of this project is to develop new algorithms that will resolve central problems and overcome existing barriers of streaming methods. The specific goals are the following: (1) Answer the main open question of Alon, Matias and Szegedy and obtain a zero-one law for all frequency-based functions. (2) Discover the relation between the sliding window model and the unbounded model. Extend this knowledge to the decay and distributed models. (3) Design new sampling methods for data streams. Extend the sampling methods for the sliding window model to decay models, improve the weighted and distributed sampling."
"1522074","Collaborative Research: Evolvable Living Computing - Understanding and Quantifying Synthetic Biological Systems' Applicability, Performance, and Limits","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","02/12/2019","Douglas Densmore","MA","Trustees of Boston University","Continuing grant","Mitra Basu","11/30/2020","$3,999,190.00","","doug.densmore@gmail.com","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7723","7723","$0.00","Successful computing systems leverage their underlying technologies to solve problems humans simply cannot. Electronic systems harness the power of radio waves and electrons. Mechanical systems use physical force and physical interactions. Biological systems represent a computing paradigm that can harness evolution, adaptation, replication, self-repair, chemistry, and living organisms. Engineered, living biological systems which make decisions, process ""data"", record events, adapt to their environment, and communicate to one another will deliver exciting new solutions in bio-therapeutics, bio-materials, bio-energy, and bio-remediation. This project will create a quantitative set of freely available design principles, computational tools, mathematical models, physical biological artifacts, educational resources, and outreach activities. Once available, these resources will allow for novel, living biological solutions to be built more quickly, perform better, be more reliable to manufacture, and cost less to produce. This project is unique in that these resources will be explicitly developed to validate key computational concepts to understand how well these concepts can be applied rigorously and repeatedly to biology. This project decomposes these concepts into three areas: Computing Paradigm (digital, analog, memory, and communication), Computing Activity (specification, design, and verification), and Computing Metric (time, space, quality, and complexity). Once complete, this project will provide the most comprehensive, freely available, and computationally relevant set of building blocks to engineer biological systems to date. <br/><br/>By developing the tools, techniques, and materials outlined in this project, this research will fundamentally change the way biological systems are specified, designed, assembled, and tested. Advanced bio-energy, bio-sensing, bio-therapeutics, and bio-materials all will become increasingly viable commercial technologies that can be made better, cheaper, faster, and more safely as a result of this project. The education of an entire new generation of engineers will occur through workshops, coursework, and community engagement activities.  This new generation will have access to these approaches which will influence how biological computation is done and how that process is communicated to the community.  This project will bring computational questions and methods to the forefront of biotechnology via an interdisciplinary research team focused not on one-off solutions but on foundational computing principles. <br/><br/>Explicitly five unanswered questions will be addressed in this project: (1) What computational models are available to biology, what are their limits, and how do they perform?  (2) What communication mechanisms are available to biology, what are their limits, and how do they perform? (3) What are the theoretical and empirical measures of quality, scale, time, and space in biological computing systems? (4) How generalizable are the concepts and ""design rules"" which can be learned from studying biological systems?  (5) How can the results (data and learnings) from biological specification, design, and verification be authoritatively disseminated to the community as design principles and grand challenges? This project addresses these questions with an interdisciplinary team with expertise in theoretical computer science, electronic design automation, bio-physics/chemistry, control theory, and molecular cell biology. For more information visit www.programmingbiology.org."
"1253362","CAREER: A Transactional Software Ecosystem","CCF","SOFTWARE & HARDWARE FOUNDATION, PROGRAMMING LANGUAGES","08/01/2013","07/20/2017","Michael Spear","PA","Lehigh University","Continuing grant","Anindya Banerjee","07/31/2019","$458,674.00","","spear@cse.lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","7798, 7943","1045, 7943, 9251","$0.00","The recent adoption of Transactional Memory (TM) in mainstream microprocessors and programming languages heralds a new era in parallel programming.  Simply put, support for low-overhead speculative execution of critical sections will enable greater productivity, more straightforward designs for scalable data structures, and simplified reasoning about the correctness of programs.  The full promise of TM, however, will only be realized through the development of a complete software ecosystem to enable scalable and conflict-free execution of transactions at any and all levels of the systems stack; including operating systems, language-level runtime libraries, and end-user code.  The design of such an ecosystem is the focus of this research.<br/><br/>This research explores algorithms and software systems that provide a seamless environment for transactional programming.  Through a focus on both the C++ and Java languages, this project will invent both data structures and supporting libraries (such as garbage collectors) that can be used both to leverage TM support in programs that are, themselves, unaware of modern transactional features, and to exploit TM support in programs that are explicitly parallel and transactional.  The project also considers programming models, with a focus on delivering a purely transactional model of program execution, via parallel open and closed-nested transactions that can exploit the first generation of transactional hardware.  The outcomes of this research will influence the design of second generation TM hardware, and will result in prototypes and source code that will be distributed as open-source software.  In addition, a broad array of educational and outreach activities are planned, to include deeper integration of parallel programming into undergraduate courses and activities in the local community that aim to widen the pipeline of students considering degrees and careers in science and technology."
"1553056","CAREER: iMPACT: Metaphysical and Probabilistic-Based Computing Transformation with Emerging Spin-Transfer Torque Device Technology","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2016","05/15/2018","Mingjie Lin","FL","University of Central Florida","Continuing grant","Sankar Basu","04/30/2021","$319,288.00","","mingjie@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7798","1045, 7945, 8089","$0.00","Effectively tackling the upcoming ""zettabyte"" data explosion requires a quantum leap in computing power and energy efficiency. However, with the Moore's law dwindling quickly, the physical limits of CMOS technology make it almost impossible to achieve high energy efficiency if the traditional ""deterministic and precise"" computing model still dominates. This CAREER proposal is inspired by applications in brain-like computing and aims at developing an alternative, non-Boolean, non-CMOS computing paradigm capable of overcoming the limitations in computational efficiency of digital CMOS technology posed by quantum-related device physics. This research, if successful, will offer a solution to tackle the upcoming ""zettabyte"" data explosion. In addition, the PI approaches the challenge of broadening participation from underrepresented minority groups in both bottom-up (public STEM education) and top-down (PhD students recruiting) manner. In particular, the PI and his team will use Orlando Science Center as the main platform to stimulate public interests through innovative exhibits and mini-lectures as well as engage in STEM education in the context of computing. <br/><br/>This CAREER proposal revolves around the Metaphysical and Probabilistic Computing Transformation, an innovative paradigm that transcends deterministic computing by natively exploiting randomness-driven physical phenomenon, either from CMOS devices under extreme conditions or from emerging spin-torque devices. The ultimate goal is to achieve, for a wide-range of perception-based computing applications - and thus for applications where the human Brain performs better than current machines - more than 10 times improvement in computing performance, more than 100 times in energy efficiency, and more than 10 times in hardware robustness over the existing state-of-the-art."
"1405564","AF: Medium: Collaborative Research: Sparse Polynomials, Complexity, and Algorithms","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","06/17/2016","Daqing Wan","CA","University of California-Irvine","Continuing grant","Tracy Kimbrel","08/31/2019","$250,000.00","","dwan@math.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7796","7924, 7927, 7933","$0.00","Solving equations quickly is what gets modern technology off the ground: Transmitting conversations between cellphones, sending data from space-craft back to earth, navigating aircraft, and making robots move correctly, all rely on solving equations quickly. In each setting, the equations have their own personality -- a special structure that we try to take advantage of, in order to find solutions more quickly. In this project, the principle investigators will study equations involving sparse polynomials -- polynomials that have few terms but very high degree.<br/><br/>But solving equations is more than just calculating quickly -- it also means understanding, and using, computational hardness. For example, classical results in Number Theory and Algebraic Geometry give us specially structured equations that, after centuries of research, still can not be solved quickly. These are the equations that are actually the most useful in Cryptography and complexity theory: Computational hardness can be used to secure sensitive data by forcing an adversary to spend a prohibitively large effort before successfully stealing anything. However, truly understanding hardness is subtle: Every day, codes and cryptosystems are broken because of a missed theoretical detail or a newly discovered backdoor.<br/><br/>The principal investigators on this project are world experts in Algebraic Geometry, Number Theory, Complexity Theory, and specially structured equations. They bring sophisticated new tools, never used before in Complexity Theory, in order to better classify what kinds of algebraic circuits define intractable equations. Their interdisciplinary approach is well-suited toward attracting mathematically talented students to theoretical Computer Science, Cryptography, and Number Theory."
"1617626","AF:Small:Algorithmic Foundations for Evolutionary Tree Comparison and Assembly","CCF","ALGORITHMIC FOUNDATIONS","06/01/2016","05/25/2016","Oliver Eulenstein","IA","Iowa State University","Standard Grant","Mitra Basu","05/31/2020","$429,998.00","","oeulenst@cs.iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7796","7923, 7931, 9150","$0.00","Synthesizing evolutionary trees for thousands of species from an ever-increasing pool of evolutionary information is not only beneficial throughout biology, but also holds enormous promise for science and society at large. For instance, the predictive power of large-scale evolutionary trees plays an important role in leading advances in human health, increasing agricultural production, and to inform decisions about natural resource management. Despite these promises, inferring such large trees from genetic data is confronting the computational biology community with the most challenging and complex computational problems in evolutionary biology today. Tree assembly problems have emerged as a powerful tool to address these challenges. The project investigates such problems that, given a collection of typically smaller evolutionary trees, seek an assembled tree that reconciles the overall conflict in these input trees. This conflict is often measured by a problem-specific tree comparison measure. Mathematical and computational properties of such measures play a critical role in the efficient computation, credibility, and analysis of assembled trees, and therefore, are the focal research point of the project. Scalability and accuracy of the methods developed in the project will be analyzed in practice using data provided by phylogenetic databases. <br/><br/>Research results will be widely disseminated within the computational biology and evolutionary biology communities. Graduate and undergraduate training opportunities offered through the project will enrich the educational experience of students through extensive interdisciplinary collaborations by acquiring a balanced perspective of biological, mathematical, and algorithmic challenges in evolutionary biology. Underrepresented minorities will be engaged through presentations in the bioinformatics programs at Iowa State University dedicated to such minorities, incorporating minority students into the project through rotations and research assistantships, and by attracting minority students to the field of computational biology and bioinformatics through public events at Iowa State University, like Major Fairs, or the student organized Bioinformatics and Computational Biology Symposia. The project will also engage in computational thinking workshops at Iowa State University for K-12 students by involving topics in computational biology and bioinformatics.<br/><br/>The developed methods will enable biologists to assemble larger and more credible evolutionary trees. The ability of these methods to handle unrooted and possibly erroneous evolutionary trees will largely extend on their applicability in practice. Evolutionary tree assembly often relies on comparison metrics and their largest possible distances, called diameters. Identifying diameters for tree comparison measures and computing these diameters efficiently will allow for compensation of shape-biases that can radically alter the outcome of tree assembly methods. For several comparison measures only weak upper bounds on their diameters are known, and the research will investigate into tightening these bounds. Identified properties of assembly methods will support biologists in their difficult choice of appropriate assembly methods. Such properties will also lead to mathematical characterizations and parameterizations of efficiently solvable instances of tree assembly methods. Novel linear time algorithms that provide evaluations of all possible rootings under various tree comparison measures will support biologists in their challenging task of identifying such rootings accurately. Finally, the theoretical results of the research will advance the algorithmic foundations of tree assembly and comparative phylogenetics."
"1844951","CAREER: Coding Theory for Robust Large-Scale Machine Learning","CCF","COMM & INFORMATION FOUNDATIONS","05/01/2019","12/11/2018","Dimitrios Papailiopoulos","WI","University of Wisconsin-Madison","Continuing grant","Phillip Regalia","04/30/2024","$86,947.00","","dimitris@ece.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7797","1045, 7935","$0.00","Coding theory has played a critical role in modern information technology by supporting robustness of information against a backdrop of multifaceted uncertainty. Following recent successes in machine learning, robustness has emerged as a desired principle, but now in the context of large-scale computation.  Challenges related to robustness are prevalent when deploying machine learning solutions in real applications and non-curated settings, which are often non-ideal environments. This project aims to address these challenges by developing novel solutions based on coding theory for computation. These solutions offer provable robustness guarantees, can outperform more traditional solutions in practice, and extend to machine learning systems the gains that have transformed communication and storage systems. Existing and new collaborations of the investigator will facilitate industry cooperation and increase the transition to practice for the frameworks and algorithms generated from this project. The research will be strongly coupled with educational developments guided by recent advances in education science, alongside an outreach program within the Wisconsin Institute for Discovery. <br/><br/>This project aims to develop novel coding-theoretic solutions and fundamental trade-offs for robust large-scale machine learning. The research program is centered around three thrusts. The first thrust focuses on robustness during distributed optimization in the presence of delays and straggler nodes, where the speed of convergence is affected by nodes in the system that are significantly slower than average. The second thrust focuses on robustness during distributed optimization in the presence of Byzantine nodes and worst-case failures. Recent studies proposed robust aggregation rules to filter out the effect of worst-case or adversarial failures. This project develops coding-theoretic solutions that can be orders of magnitude faster, and give rise to unexplored trade-offs between computation and Byzantine tolerance. The third thrust focuses on adversarial perturbations during prediction that can force state-of-the-art models to consistently mis-classify events/data. The coding-theoretic approach of this project pursues provable defense mechanisms against adversarial attacks through ensembles of models with inherent redundancy and through data augmentation. The proposed theoretical and algorithmic solutions are afforded by an interdisciplinary mix of tools from information and coding theory, distributed optimization, and machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1657598","CRII: AF: Characterization and Complexity of Information Elicitation","CCF","CRII CISE Research Initiation, ALGORITHMIC FOUNDATIONS","06/01/2017","05/29/2018","Rafael Frongillo","CO","University of Colorado at Boulder","Standard Grant","Tracy J. Kimbrel","05/31/2020","$182,300.00","","raf@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","026Y, 7796","7796, 7932, 8228, 9251","$0.00","The way one judges the accuracy of predictions can greatly impact what predictions people or computers make.  For example, Glenn Brier argued in 1950 that the way meteorologists were evaluated would actually give them an incentive to distort the true probability of rain.  Brier's study inspired a growing body of work in statistics, economics, and now computer science, which studies evaluation metrics that incentivize accurate reports from people or machines.  These evaluation metrics are also used in machine learning, a branch of artificial intelligence, where a designer implicitly tells the computer what statistic to predict by providing only the evaluation metric itself.  This project seeks to mathematically characterize this link between statistics and evaluation metrics, and moreover, to understand the computational and statistical difficulty of evaluating different statistics.  A precise understanding of this link would provide new evaluation metrics with the potential to increase predictive power across a vast array of applications such as climate simulations and smart cities.  In particular, metrics for statistics that quantify uncertainty or risk could improve decision making in many fields, including healthcare, engineering, and finance.<br/><br/>A dominant algorithmic paradigm in machine learning, encompassing most regression techniques and classification algorithms, is that of empirical risk minimization (ERM): choosing a model from some class that best fits the data, according to some evaluation metric called a loss function.  A thread of research in theoretical machine learning called property elicitation gives a mathematical formalism to describe the link between loss functions and their corresponding statistics.  In these terms, this project seeks to characterize the statistics which have calibrated loss functions, and determine how many regression parameters or data points are required for the calibration to hold.  These questions are particularly relevant to machine learning when restricting attention to certain classes of loss functions which can be easily optimized or which have desirable statistical learning guarantees.  The class of statistics from mathematical finance known as risk measures, which are used to regulate banks, form an important focus of the project."
"1740525","AF: Small: Graphs and structures for distance estimation","CCF","ALGORITHMIC FOUNDATIONS","01/20/2017","05/30/2017","Virginia Williams","MA","Massachusetts Institute of Technology","Standard Grant","Tracy J. Kimbrel","08/31/2019","$219,028.00","","virgi@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7923, 7926, 9102","$0.00","Distance computation and estimation in networks is one of the most basic and fundamental tasks in network analysis. However, storing the distance between every pair of nodes of a graph is infeasible, especially for today's ""big data."" Small sketches of a graph have been developed so that a good estimate of any pairwise distance can be retrieved from the sketch. This project aims to advance the state-of-the art of such sketches. The main objects of study are spanners, distance oracles and their fault-tolerant variants. A spanner is a sparse subgraph that does not stretch any distance of the original graph by much. A distance oracle is a data structure that has small space usage and is capable of answering (approximate) distance queries efficiently. Both spanners and distance oracles compress the distance information. The main difference between them is that one can still run graph algorithms on a spanner, but not on a distance oracle, whereas one can obtain any distance from a distance oracle instantly, but in a spanner one would have to actually compute it.<br/><br/>In practice, networks are dynamic in nature. To address this, graph sketches would have to tolerate faults, i.e. edge and vertex deletions. There are fault-tolerant versions of spanners and distance oracles-- these structures estimate distances for any given (typically fixed size) subset of failed edges or nodes. This project will provide algorithms for constructing new low-space spanners and oracles with improved guarantees and will strive to develop new techniques for fault-tolerance and distance estimation in general. Spanners and distance oracles have many applications, e.g. in parallel and distributed algorithms for distance computation, network routing, and simulating synchronized protocols in unsynchronized networks. A better understanding of network routing along short paths could guide the design of next-generation Internet protocols. The research is also closely tied to the field of metric embedding, and thus extends beyond the strict boundaries of computer science. Material from this research will be integrated into core undergraduate and graduate courses, and will lead to the development of new courses on the topic. The lecture notes and project materials will be available on the course website for the general public."
"1409294","AF: Medium: Collaborative Research: Sparse Polynomials, Complexity, and Algorithms","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","07/29/2016","Qi Cheng","OK","University of Oklahoma Norman Campus","Continuing grant","Tracy J. Kimbrel","08/31/2019","$223,168.00","","qcheng@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","7796","7924, 7927, 7933, 9150","$0.00","Solving equations quickly is what gets modern technology off the ground: Transmitting conversations between cellphones, sending data from space-craft back to earth, navigating aircraft, and making robots move correctly, all rely on solving equations quickly. In each setting, the equations have their own personality -- a special structure that we try to take advantage of, in order to find solutions more quickly. In this project, the principle investigators will study equations involving sparse polynomials -- polynomials that have few terms but very high degree.<br/><br/>But solving equations is more than just calculating quickly -- it also means understanding, and using, computational hardness. For example, classical results in Number Theory and Algebraic Geometry give us specially structured equations that, after centuries of research, still can not be solved quickly. These are the equations that are actually the most useful in Cryptography and complexity theory: Computational hardness can be used to secure sensitive data by forcing an adversary to spend a prohibitively large effort before successfully stealing anything. However, truly understanding hardness is subtle: Every day, codes and cryptosystems are broken because of a missed theoretical detail or a newly discovered backdoor.<br/><br/>The principal investigators on this project are world experts in Algebraic Geometry, Number Theory, Complexity Theory, and specially structured equations. They bring sophisticated new tools, never used before in Complexity Theory, in order to better classify what kinds of algebraic circuits define intractable equations. Their interdisciplinary approach is well-suited toward attracting mathematically talented students to theoretical Computer Science, Cryptography, and Number Theory."
"1649087","EAGER: Using Machine Learning to Increase the Operational Efficiency of Large Distributed Systems","CCF","Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","09/01/2016","09/01/2016","Evgenia Smirni","VA","College of William and Mary","Standard Grant","Almadena Y. Chtchelkanova","08/31/2019","$299,994.00","","esmirni@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7354, 7798","7916, 7942","$0.00","Large, distributed systems are nowadays ubiquitous and part of sustainable IT solutions to a broad range of customers and applications.  Data centers in the private or public cloud and high performance computing systems are two examples of complex, highly distributed systems: the former are used by almost everyone on a daily basis, the latter are used by computational scientists for advancing science and engineering.  High availability and reliability of these complex systems are important for the quality of user experience.  Efficient management of such systems contributes to their availability and reliability, and relies on a priori knowledge of the timing of the collective demands of users and a priori knowledge of certain performance measures (e.g., usage, temperature, power) of various systems components.<br/><br/>This  project aims to provide a systematic methodology to improve the operational efficiency of complex, distributed systems by developing neural networks that can efficiently and accurately predict the incoming workload within fine and coarse time scales. Such workload prediction can dramatically improve the operational efficiency of data centers and high performance systems by driving proactive management strategies that specifically aim to enhance reliability.  For datacenters, the focus is on actively reducing performance tickets that are automatically triggered by pro-actively managing virtual machine resizing and migration. For high performance computing systems the focus is on predicting hardware faults to autonomically improve the scheduler's efficiency, direct cooling, and improve performance and memory bandwidth."
"1651236","CAREER: Information-Theoretic Methods for RNA Analytics","CCF","COMM & INFORMATION FOUNDATIONS","03/15/2017","05/11/2018","Sreeram Kannan","WA","University of Washington","Continuing grant","Phillip Regalia","02/28/2022","$189,537.00","","ksreeram@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7797","1045, 7935","$0.00","The development of high-throughput sequencing has ushered in a new era in molecular biology, enabling inexpensive study of the genome. Furthermore, in recent years, RNA sequencing has enhanced our ability to quantify the dynamics of gene expression with transcript-level precision and single-cell resolution. This has applications in diverse areas like evolutionary biology, developmental biology, medical transcriptomics as well as synthetic biology. These advances in biotechnology necessitate corresponding advances in the development of computational algorithms that perform inference on these new datasets. Information theory offers a natural lens to study such problems as it can quantify the amount of data required to make accurate inferences, as well as leading to optimality. The main research objective of this project is to adapt, apply and create new information-theoretic and algorithmic methods to solve inference problems arising in RNA sequence analytics. The project will also have a significant educational component to integrate these new discoveries into graduate and undergraduate courses that can expose electrical engineering and computer science students to sequencing problems, in addition to exposing high-school and undergraduate students to this research area by outreach and mentoring.<br/><br/>The project will study inference problems at two different levels of RNA-sequencing: assembly and downstream analytics. The typical method for RNA-sequencing involves fragmentation of RNA into short fragments that are then sequenced. The first thrust of this project will be in studying the informational limits and algorithms for this ?assembly? problem, particularly in studying the role of errors and repeated regions in the genome. The second thrust will be to study informational limits and algorithms for the downstream task of utilizing single-cell RNA-sequence data to understand gene-regulation and cell-differentiation."
"1800723","AF: Medium: Algorithms for Scalable Phylogenetic Network Inference","CCF","SPECIAL PROJECTS - CCF, COMPUTATIONAL BIOLOGY","06/15/2018","09/13/2018","Luay Nakhleh","TX","William Marsh Rice University","Continuing grant","Mitra Basu","05/31/2022","$481,716.00","","nakhleh@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","2878, 7931","7924, 7931","$0.00","Species phylogenies model how species split and diverge and provide important insight into fundamental biological phenomena and processes, including biodiversity, and trait evolution, while gene trees provide insight into protein structure and function as well as systems biology. Advances in sequencing technologies and assembly methods and the availability of whole-genome datasets have opened up the possibility of transformative improvements in accuracy for estimating species phylogenies and gene trees. Phylogenetic networks extend phylogenetic trees to provide an appropriate model of reticulate evolutionary histories. Reticulate evolution describes the origination of a lineage through partial merging of two ancestor lineages. Recently developed methods allow for statistical inference of phylogenetic networks in order to account for other processes that could be at play during the evolution of the genomes. However, these methods can handle fewer than a handful of genomes. This award will develop methods for estimating large-scale phylogenetic networks from sequence data as well as gene tree estimates. The award will stimulate research in computer science and statistics and will have a major impact on evolutionary biology. The award will contribute open-source code to the PhyloNet software package. Lectures and tutorials will be given to the community on the developments made in the award and on the use of PhyloNet. The award will provide ample opportunities for training students and post-doctoral fellows in cutting-edge, interdisciplinary algorithmic research.<br/><br/>The project will be carried out through five activities that are intertwined throughout the lifetime of the award. (1) Development of novel algorithmic techniques for scalable inference of phylogenetic networks that allow for analyzing data sets with tens and even hundreds of genomes. (2) Implementation and of all methods in the PhyloNet software package. (3) Thorough evaluation of the methods in terms of accuracy and computational requirements. (4) Mentoring and training of students and post-doctoral fellows. (5) Dissemination of the results through an open-source software package, publications in peer-reviewed journals and conference proceedings, lectures and tutorials, and course materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1715443","SHF: Small: Efficient and Accurate Learning with Low-Precision Components: A Cortex-Inspired Approach","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2017","07/07/2017","Yu Cao","AZ","Arizona State University","Standard Grant","Sankar Basu","06/30/2020","$450,000.00","Jae-sun Seo, Shimeng Yu","Yu.Cao@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7798","7923, 7945","$0.00","Achieving high performance and high-energy efficiency with a small footprint is a central challenge of computer engineering. This project targets to develop technologies with cutting-edge nanoscale devices towards a self-learning chip. It will be integrated with front-end sensors, process the information in real-time, and consume ultra-low energy. The success is likely to have an impact on the society, bringing broad benefits to multiple emerging applications, mobile vision and autonomous vehicles to name a few. The interdisciplinary nature of this project, as well as the frequent interaction with industry, will provide an ideal platform for education and training of state-of-the-art science and technology. It will improve the knowledge base of intelligent system design through new curriculum development, engaging undergraduate and minority students in research and practice, and participating in outreach programs that are customized for K-12 students. Furthermore, this project will advocate the web-based interface and workshops to disseminate the latest research outcome. <br/><br/>Microprocessors have been a ubiquitous and vitally important part in our modern-day life. However, they are facing severe issues in artificial intelligent systems, which require tremendous amount of energy and data to train and operate the sophisticated algorithm. On the contrary, animal brains at various sizes achieve remarkable feats of learning and accuracy at energy costs much lower than human-engineered systems. Therefore, the central theme of this project is to transfer the latest knowledge of the structure and function of brains into neuromorphic design, generate novel insights for improvement of the engineered system, and achieve high accuracy and high energy efficiency despite the severe precision constraints of the nanoscale components. These neurobiological principles include approximate learning rules with low-precision synapses, neural motifs of excitation and inhibition, and hierarchical network models. The goal is to accomplish complex computation with much less data volume and resources, and promise magnitudes of improvement in energy efficiency and performance than microprocessors today."
"1442971","CyberSEES: Type 2: Collaborative Research: Connecting Next-generation Air Pollution Exposure Measurements to Environmentally Sustainable Communities","CCF","EDUCATION AND WORKFORCE, ALGORITHMIC FOUNDATIONS, COMM & INFORMATION FOUNDATIONS, CyberSEES","09/01/2014","05/03/2018","Qin Lv","CO","University of Colorado at Boulder","Standard Grant","Rahul Shah","08/31/2019","$698,765.00","Michael Hannigan, Li Shang, Daven Henze","Qin.Lv@Colorado.EDU","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7361, 7796, 7797, 8211","8208, 8231, 9251","$0.00","Ambient exposure to ground-level air pollution is linked to adverse health effects in many populated areas of the world. However, advances in relating air pollution exposure to sustainable communities are hindered by limited direct observations of exposure and the coarseness of regional and global air quality models used for decision making. As a result, existing models do not resolve the scales of variability in either pollutant concentrations or population distributions necessary to accurately assess exposure nor provide the type of probabilistic uncertainty bounds required for policy.<br/><br/>This project aims to assimilate comprehensive cyber information for use in air quality management. It advances the interdisciplinary field of cyber-environmental research through investigation into (1) cyber-scale data analysis to harness and distill valuable cyber information to support community-scale air pollution modeling; (2) micro-environment targeted sensing to augment community-scale studies with accurate, on-demand, and in situ sensing capabilities; and (3) scalable exposure modeling and analysis by solving a complex, spatiotemporally varying problem with high-dimensional data containing cyber, sensing, and model outputs.<br/><br/>Advances in cyber-environmental research have the potential to improve government policy making, regulations, and personal choices with regard to environmentally sustainable community development. Results of this project can apply across a wide spectrum of sectors including energy, transportation, and healthcare. This project broadens vertical research and education integration across information technologies and environmental science and engineering, to both graduate and undergraduate students. Through in-field trials, this project offers unique real-world education and research opportunities to attract students from underrepresented groups and industrial professionals."
"1712633","CIF:Small: Ramanujan-sums in signal representation","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/28/2017","P. Vaidyanathan","CA","California Institute of Technology","Standard Grant","Phillip Regalia","06/30/2020","$450,000.00","","ppvnath@systems.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7797","7923, 7936","$0.00","This research explores new directions in the representation of signals, based on certain<br/>mathematical ideas introduced by Ramanujan many decades ago. In particular the applicability of<br/>Ramanujan-sums in the representation of digital signals is studied in detail. Signals acquired and<br/>stored in many scientific areas such as speech, music, medical, genomic, and finance often<br/>contain hidden periodic patterns bearing important information. These are difficult to identify and<br/>localize because of the large volume of data, and extreme localization of information.<br/>Conventional representations of signals based on Fourier and wavelet transforms are not efficient<br/>for this purpose. This research involves the use of Ramanujan-sums to develop new<br/>representations that are efficient and economical. These offer insights into the role of<br/>mathematics, especially number theory, in representation of signals in science and technology,<br/>with emphasis on particular hidden structures such as periodicities. Such representations are<br/>expected to have significant impact in scientific applications involving large data bases.<br/><br/>More specifically, the research goes deep into Ramanujan subspaces, nested periodic subspaces,<br/>and Ramanujan filter banks, which are crucial to the representation of periodic sequences. This<br/>enables one to address some fundamental research problems in signal processing. This includes<br/>the minimum information that should be gathered from a discrete-time signal in order to identify<br/>multiple periodicities, the design of digital filter banks that serve as projection operators onto<br/>periodicity subspaces, and optimal design of dictionaries of atoms to represent sums of periodic<br/>signals with unknown integer periods. The research also involves the theory and implementation<br/>of gridless methods to achieve super resolution in period-estimation for continuous-time signals,<br/>and extension to two- and higher-dimensional signals."
"1523768","CIF: Small: Statistical Inference via Convex Optimization","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2015","07/30/2015","Arkadi Nemirovski","GA","Georgia Tech Research Corporation","Standard Grant","Phillip Regalia","07/31/2019","$460,111.00","Vladimir Koltchinskii","nemirovs@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797","7797, 7923, 7936","$0.00","In a variety of applications in modern science and technology, there is a strong need in accurate statistical inferences from massive sets of high-dimensional data. To meet this need, it is a must to develop novel methods combining provably (nearly) optimal statistical performance with computational efficiency and scalability. The project aims at designing innovative Convex Optimization based inference techniques meeting the above requirements and utilizing these techniques in important applications (Positron Emission Tomography, Nanoscale Fluorescent Microscopy, Quantum Statistics,...). Challenges to be addressed combined with clear ""applied appeal"" make the project a good training ground for Ph.D. students. Project?s outcomes could make a valuable contribution to the computational tools of ""Big Data.""<br/><br/>The approach is based on designing statistical tests with near-optimal risk for multiple composite hypotheses in a class of statistical models where observation is: (a) affine image of unknown vector (""signal"") corrupted by Gaussian noise; (b) random  vector with independent Poisson entries, the underlying parameters being affine functions of the signal;  (c) random variable taking finitely many values with probabilities affinely depending on the signal; (d) direct products of models (a) - (c). While restrictive with respect to the allowed models, the approach is highly permissive with respect to the number and the structure of the hypotheses. The proposed efficiently computable and scalable tests and their risks stem from optimal solutions to explicit convex programs, and can be used as building blocks in more complicated inferential problems. The research agenda includes the design of sequential tests and dynamical tests; change point detection; estimating functionals of a signal; ""sparsity-oriented"" testing/estimation; applications to Poisson Imaging, Active Learning, and Quantum Statistics."
"1513883","CIF: Medium: Collaborative Research: Feedback Communication: Models, Designs, and Fundamental Limits","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2015","06/06/2017","Tara Javidi","CA","University of California-San Diego","Continuing grant","Phillip Regalia","05/31/2019","$382,714.00","","tara@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7924, 7935","$0.00","Claude Shannon?s ?A Mathematical Theory of Communication? is the landmark event that paved the way for the development of modern communication systems. Shannon analyzed the fundamental redundancy that must be added to data in order to achieve reliable communication in the presence of noise. Since then his vision has guided the practical design of virtually all aspects of modern communication systems such as forward error correction, spectrally-efficient communication, multiuser and inter-symbol interference, multiple-antenna systems, opportunistic communication, and joint compression/transmission. However, while feedback is present in virtually all modern communication systems, the field of information theory has had relatively little impact on how feedback is employed in practice.  The proposed work will advance knowledge by developing a more complete understanding of how feedback should be employed in communication systems and what quantitative improvements in delay, complexity, and transmitted power one can expect from its effective use, under realistic delay constraints such as those found in high-speed wireless data. In summary, the successful completion of the project is expected to contribute new mathematical tools, designs, viewpoints, and models to the field of information theory.<br/><br/>The goal of this research is to bring the insight, design guidance, and performance bounds for which information theory is known to bear on the design of systems with feedback. By providing new design principles and feedback codes, this research has the potential not only to advance basic science but also to improve the efficiency and reliability of our communications infrastructure, including consumer technology such as WiFi and smartphones. Given the proliferation of personal communication devices, such improvements would augment the efficiency with which crucial resources such as energy and radio frequency bandwidth are currently utilized."
"1813624","AF: Small: Collaborative Research: Scalable and Topologically Versatile Material Point Methods for Complex Materials in Multiphysics Simulation","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","08/17/2018","Chenfanfu Jiang","PA","University of Pennsylvania","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$250,000.00","","cffjiang@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7796","7923, 7933","$0.00","Computational simulation of natural phenomena is a ubiquitous tool in the natural sciences (environmental modeling, prediction of earthquakes or avalanches), bio-mechanics (modeling the musculoskeletal system, organ function, or tissue damage), manufacturing (product design, prototyping, and verification) as well as in computer graphics/animation. This project advances the science of simulation using a technique called Material Point Methods (MPM), by increasing the number of materials and range of phenomena that can be simulated, and optimizing the performance of numerical algorithms used for simulations at larger scales. The enhancements from this project will enable studies in terrain dynamics, design and prototyping of vehicles and agricultural implements interacting with complex soils, modeling of material failure and fracture scenarios including biological settings such as skin tearing, surgical incisions, or vascular rupture. The optimization and scaling will allow computational studies of simulated physical systems at levels of resolution that were previously only afforded to large enterprises.<br/><br/>This project extends MPM simulation to: a) materials with multi-phase interactions influenced by thermodynamics; and b) media with complex multi-scale geometric features, including porosity (e.g. water-soil interactions) or aggregates dominated by grains of non-spherical geometry. This project extends MPM simulation to phenomena that include intricate frictional contact (beyond the no-slip contact model embedded in traditional MPM), dynamic crack propagation, and de-cohesion. This research thread leverages the team's prior work on non-manifold data structures for storing implicit geometry representations, allowing the background grids of MPM to incorporate a richer set of topological features (e.g. tears and incisions) than those incorporated by conventional array-based regular lattices. Finally, this research boosts the scale of MPM simulations that can be accommodated in modern multiprocessors, improving detail, resolution as well as parallel efficiency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763299","AF: Medium: Collaborative Research:  Exploiting Opportunities in Pseudorandomness","CCF","ALGORITHMIC FOUNDATIONS","03/01/2018","03/02/2019","Salil Vadhan","MA","Harvard University","Continuing grant","Tracy Kimbrel","02/28/2022","$309,332.00","","salil_vadhan@harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7924, 7926, 7927","$0.00","This project seeks to exploit new opportunities to advance the theory of pseudo-randomness, which is the theory of generating objects that ""look random"" despite being constructed using little or no randomness.  The computational theory of pseudo-randomness originated in the foundations of cryptography in the early 1980s and has since developed into a rich sub-field of theoretical computer science in its own right. The notions and constructs studied in the theory of pseudo-randomness have implications for many different areas of research in computer science, communications, and mathematics, including cryptography, computational complexity, coding theory, additive number theory, metric embeddings, streaming and sketching algorithms, and graph theory.  The project puts high value on education, service to the research community, and wide dissemination of knowledge.  The research activities will be accompanied by and integrated with curriculum development, research advising, service, and outreach.  In addition, the research also relates to national priorities of importance to society, such as security and privacy.<br/><br/>Specifically, the project seeks to exploit new opportunities for progress on several fundamental questions, including: 1) The RL vs. L problem: trying to prove, unconditionally, that every randomized algorithm can be made deterministic with only a constant-factor loss in space efficiency; 2) Explicit constructions: seeking explicit load-balancing hash functions, batch codes, and depth-robust graphs that achieve substantial parameter improvements and have qualitative significance for applications, and 3) Applications: improving and extending the applications of pseudo-randomness to cryptography and data structures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1637585","AitF: Collaborative Research: Algorithms for Probabilistic Inference in the Real World","CCF","Algorithms in the Field","09/01/2016","08/30/2016","Aravindan Vijayaraghavan","IL","Northwestern University","Standard Grant","Tracy Kimbrel","08/31/2020","$399,939.00","","aravindv@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7239","","$0.00","Statistical models provide a powerful means of quantifying uncertainty, modeling prior beliefs, and describing complex dependencies in data.  The process of using a model to answer specific questions, such as inferring the state of several random variables given evidence observed about others, is called probabilistic inference.  Probabilistic graphical models, a type of statistical model, are often used in diverse applications such as medical diagnosis, understanding protein and gene regulatory networks, computer vision, and language understanding.  On account of the central role played by probabilistic graphical models in a wide range of automated reasoning applications, designing efficient algorithms for probabilistic inference is a fundamental problem in artificial intelligence and machine learning.<br/> <br/>Probabilistic inference in many of these applications corresponds to a complex combinatorial optimization problem that at first glance appears to be extremely difficult to solve.  However, practitioners have made significant strides in designing heuristic algorithms to perform real-world inference accurately and efficiently.  This project focuses on bridging the gap between theory and practice for probabilistic inference problems in large-scale machine learning systems.  The PIs will identify structural properties and methods of analysis that differentiate real-world instances from worst-case instances used to show NP-hardness, and will design efficient algorithms with provable guarantees that would apply to most real-world instances.  The project will also study why heuristics like linear programming and other convex relaxations are so successful on real-world instances.  The efficient algorithms for probabilistic inference developed as part of this project have the potential to be transformative in machine learning, statistics, and more applied areas like computer vision, social networks and computational biology.  To help disseminate the research and foster new collaborations, a series of workshops will be organized bringing together the theoretical computer science and machine learning communities.  Additionally, undergraduate curricula will be developed that use machine learning to introduce students to concepts in theoretical computer science."
"1845125","CAREER: The Polynomial Method in Complexity and Cryptography","CCF","ALGORITHMIC FOUNDATIONS","06/01/2019","01/30/2019","Justin Thaler","DC","Georgetown University","Continuing grant","Tracy Kimbrel","05/31/2024","$70,001.00","","jt1157@georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","7796","1045, 7927","$0.00","The ""polynomial method"" in computer science refers to the study of the algebraic structure of Boolean functions, in the form of approximation or exact computation by low-degree polynomials. It has led to many celebrated results in computer science over the last 50 years, in areas as diverse as machine learning, quantum computing, and circuit lower bounds. As one example, quantum computers have the potential to be vastly more efficient (for some problems) than today's classical computers, and the polynomial method has proven to be one of the most promising tools available for understanding their power. This is because any function that can be efficiently evaluated by a quantum computer must be well-approximated in a precise sense by low-degree polynomials. This project seeks to improve our understanding of known formulations of the polynomial method, and to develop new formulations to solve major open problems in the aforementioned application domains.<br/><br/>The objectives of this project are separated into two classes. The first class focuses on a basic formulation of the polynomial method called approximate degree, which has many applications. The project will develop a relatively new technique for proving approximate degree lower bounds, called the method of dual polynomials, that is poised to resolve the approximate degrees of many basic functions. Via established connections, this will impact the fields of quantum algorithms (where it is likely to imply the optimality of a variety of quantum algorithms), circuit complexity (where it will resolve the complexity of shallow circuits under basic complexity measures), and learning theory (where it will characterize the power of important objects including halfspace learners, noise-tolerant learners, and deep nets). The project will also investigate new polynomial-based notions of structure with the potential to solve additional major open problems in these areas.<br/><br/>The second class of objectives focuses on a different application of the polynomial method, to verifiable computing (VC). VC refers to cryptographic protocols enabling an untrusted prover to guarantee to a verifier that the prover performed a computation correctly. Efficient VC systems would enable a wide variety of applications. For example, entities that offload data processing to the cloud could obtain guarantees that the cloud is operating correctly. Seminal theoretical results used the polynomial method to show that VC protocols can be dramatically more efficient than static proofs, and the last decade has seen major progress in building VC systems verging on practicality, and even commercial deployment of such systems. Still, many existing VC systems have high costs or lack several key properties, limiting their applicability. The project will explore new formulations of the polynomial method to overcome these limitations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1423306","AF: Small: Minimalist cryptography","CCF","ALGORITHMIC FOUNDATIONS","08/01/2014","07/21/2014","Allison Bishop","NY","Columbia University","Standard Grant","Tracy J. Kimbrel","07/31/2019","$399,999.00","Tal Malkin","abl2156@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7923, 7926","$0.00","Modern cryptography offers an impressive virtual buffet to a consumer who is wealthy in resources, with powerful tools like fully homomorphic encryption (which allows a provider to compute with encrypted values while keeping the client's data safe) and general purpose obfuscation (which allows one to hide the purpose of a given computation). But for more modestly minded users, who seek to perform less lofty tasks using more affordable computing resources or under more time-tested assumptions, the offerings are comparatively paltry. Many fundamental and practically motivated questions remain unanswered, such as what are the lowest complexity classes (essentially, the weakest, and thus cheapest, computing models) containing basic cryptographic primitives? How much protection against adversarial intrusions can information-theoretic techniques provide for interactive protocols? Or, what levels of provable security are achievable within fixed efficiency constraints? This project will advance a theory of cryptography for the minimalist: a cryptography that is nimble enough to offer attractive value propositions to conservative consumers.<br/><br/>The PIs will focus investigation on three main themes. First, the PIs will study complexity (e.g., several notions of circuit complexity) of pseudrandom primitives such as pseudorandom generators, pseudorandom functions, and weak pseudorandom functions. Secondly, the PIs will develop Information-theoretic techniques for robustness in adversarial environments, deepening connections between error-resilient communication and computation. Finally, the PIs will improve tradeoffs between security and efficiency in settings where overly stringent security requirements obstruct practicality, by studying incrementally relaxed notions of security and the gains in efficiency that they afford. <br/><br/>The proposed research activity is tightly weaved with an educational and outreach plan for K-12, undergraduate, and graduate students, with an emphasis on encouraging participation of under-represented minorities. The PIs are involved in computer science programs for elementary school children and for high school girls, both being taught by undergraduate women in the department of Computer Science, who will be mentored by the PIs.   The educational plan additionally includes integration with the undergraduate and graduate curriculum, through incorporation of the research into the introductory and advanced cryptography classes taught by the PIs, and through individual research projects and mentoring."
"1812944","AF: Small: Collaborative Research: Scalable and Topologically Versatile Material Point Methods for Complex Materials in Multiphysics Simulation","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","08/17/2018","Eftychios Sifakis","WI","University of Wisconsin-Madison","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$249,710.00","","sifakis@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7796","7923, 7933","$0.00","Computational simulation of natural phenomena is a ubiquitous tool in the natural sciences (environmental modeling, prediction of earthquakes or avalanches), bio-mechanics (modeling the musculoskeletal system, organ function, or tissue damage), manufacturing (product design, prototyping, and verification) as well as in computer graphics/animation. This project advances the science of simulation using a technique called Material Point Methods (MPM), by increasing the number of materials and range of phenomena that can be simulated, and optimizing the performance of numerical algorithms used for simulations at larger scales. The enhancements from this project will enable studies in terrain dynamics, design and prototyping of vehicles and agricultural implements interacting with complex soils, modeling of material failure and fracture scenarios including biological settings such as skin tearing, surgical incisions, or vascular rupture. The optimization and scaling will allow computational studies of simulated physical systems at levels of resolution that were previously only afforded to large enterprises.<br/><br/>This project extends MPM simulation to: a) materials with multi-phase interactions influenced by thermodynamics; and b) media with complex multi-scale geometric features, including porosity (e.g. water-soil interactions) or aggregates dominated by grains of non-spherical geometry. This project extends MPM simulation to phenomena that include intricate frictional contact (beyond the no-slip contact model embedded in traditional MPM), dynamic crack propagation, and de-cohesion. This research thread leverages the team's prior work on non-manifold data structures for storing implicit geometry representations, allowing the background grids of MPM to incorporate a richer set of topological features (e.g. tears and incisions) than those incorporated by conventional array-based regular lattices. Finally, this research boosts the scale of MPM simulations that can be accommodated in modern multiprocessors, improving detail, resolution as well as parallel efficiency.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835986","Workshops on Geometry of Polynomials","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","10/01/2018","08/28/2018","Shafrira Goldwasser","CA","University of California-Berkeley","Standard Grant","Rahul Shah","09/30/2019","$60,000.00","","shafi.goldwasser@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","2878, 7796","7556, 7926","$0.00","This award provides support for organizing three workshops around the theme of Geometry of Polynomials at Simons Institute - Berkeley. Simons Institute at University of California, Berkeley has taken a leading role in advancing the state-of-the-art in Theoretical Computer Science by bringing together the most prolific and upcoming researchers. This effort will bridge ideas from mathematics and computer science and develop new techniques with broad applicability. Researchers will engage in diverse research areas like combinatorics, probability, statistical physics, optimization and real algebraic geometry. These workshops will be open to all potential participants, and video recordings of presentations will be distributed to the public for comment and engagement. The organizers will invite students and scientists from a diversity of backgrounds to participate.<br/><br/>Geometry of polynomials is a topic which has shown a lot of promise recently in solving Asymmetric Traveling Salesman Problems, Constructing Ramanujam Graphs and other problems.  The power of this framework stems from the fact that certain classes of these polynomials are general enough to encode a variety of interesting combinatorial, probabilistic and geometric data, and special enough to have useful global properties and structure theory. To further develop, enrich and popularize these techniques, this project will organize three workshops around the topic: (1) Beyond randomized rounding and the probabilistic method (2) Deterministic Counting, Probability and Zeros of Partition Functions and (3) Hyperbolic Polynomials and Hyperbolic Programming.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813930","AF: Small: Research in Complexity Theory","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/21/2018","Emanuele Viola","MA","Northeastern University","Standard Grant","Tracy Kimbrel","05/31/2021","$499,896.00","","viola@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7796","7923, 7927","$0.00","Computational inefficiency is a common experience: the computer cannot complete a certain task due to lack of resources such as time, memory, or bandwidth. The theory of computational complexity classifies -- or aims to classify -- computational tasks according to their inherent inefficiency. Inefficiency can also be harnessed to our advantage. Indeed, modern cryptography and electronic commerce rely on the (presumed) inefficiency of certain computational tasks. From the study of computational complexity there have arisen questions that stand as grand challenges of contemporary science. The objective of this project is to enrich the theory of computational complexity with new directions and techniques, and to use these techniques to make progress on long-standing open problems. Specific areas of investigation include the complexity of sampling tasks and of distributed tasks, and randomness. The investigator has a track record of fruitful exchanges with the mathematics community and will foster further cross-fertilization between mathematics and computer science. The project will develop publicly-available educational material, including lecture notes, surveys, slides, and videos, both at the advanced and at the introductory level. <br/> <br/>In more detail, the project will seek to prove computational lower bounds for sampling tasks. These lower bounds provide an under-explored angle for attacking problems on extractors and data structures. Preliminary results by the investigator have already found application in a breakthrough known as two-source extractors. The project will also bring a new set of techniques in group theory to bear on communication complexity and cryptography. This project will strengthen these techniques and develop more applications. Finally, the project will explore extensions of small bias generators, which have the potential to answer central open questions in pseudo-randomness and beyond.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763311","AF: Medium: Collaborative Research: Exploiting Opportunities in Pseudorandomness","CCF","ALGORITHMIC FOUNDATIONS","03/01/2018","03/02/2019","Omer Reingold","CA","Stanford University","Continuing grant","Tracy Kimbrel","02/28/2022","$318,814.00","","omreing@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","7924, 7926, 7927","$0.00","This project seeks to exploit new opportunities to advance the theory of pseudo-randomness, which is the theory of generating objects that ""look random"" despite being constructed using little or no randomness.  The computational theory of pseudo-randomness originated in the foundations of cryptography in the early 1980s and has since developed into a rich sub-field of theoretical computer science in its own right. The notions and constructs studied in the theory of pseudo-randomness have implications for many different areas of research in computer science, communications, and mathematics, including cryptography, computational complexity, coding theory, additive number theory, metric embeddings, streaming and sketching algorithms, and graph theory.  The project puts high value on education, service to the research community, and wide dissemination of knowledge.  The research activities will be accompanied by and integrated with curriculum development, research advising, service, and outreach.  In addition, the research also relates to national priorities of importance to society, such as security and privacy.<br/><br/>Specifically, the project seeks to exploit new opportunities for progress on several fundamental questions, including: 1) The RL vs. L problem: trying to prove, unconditionally, that every randomized algorithm can be made deterministic with only a constant-factor loss in space efficiency; 2) Explicit constructions: seeking explicit load-balancing hash functions, batch codes, and depth-robust graphs that achieve substantial parameter improvements and have qualitative significance for applications, and 3) Applications: improving and extending the applications of pseudo-randomness to cryptography and data structures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1442858","CyberSEES: Type 2: Ocean Wave Energy and the Power Grid: Optimization and Integration","CCF","CyberSEES","01/15/2015","10/03/2014","Shalinee Kishore","PA","Lehigh University","Standard Grant","Rahul Shah","12/31/2019","$900,000.00","Rick Blum, Lawrence Snyder, Arindam Banerjee, Alberto Lamadrid","skishore@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","8211","8208","$0.00","The objective of this project is to develop sensing, prediction, optimization, and control techniques for arrays of ocean wave energy conversion devices, known as wave farms, to make them reliable, efficient, economical, and environmentally benign.  The project focuses on operational questions about the cyber-infrastructure required to control and coordinate a wave farm's many autonomous sensing and decision-making functions.  Research topics include designing computational tools to assess real-time hydrodynamic interactions of wave energy converters; developing short-term and long-term wave prediction algorithms; using wave condition forecasts to dynamically control arrays of wave energy converters and energy storage systems within wave farms; enabling rapid fault detection tests that use wave sensor data to assess system state; and automating these decision-making processes within a wave farm with a multi-agent control system.  The project also assesses and optimizes the sustainability and electricity market potential of wave farms by quantifying the opportunities and gains that wave power producers may avail in the electricity market due to the predictability and consistency of waves, and the effect of optimal wave farm design and control on CO2 emissions.   The project includes an educational and workforce development plan to develop and administer an in-ocean course for high school students from the Lehigh Valley, PA at the Bermuda Institute of Ocean Sciences on the topic of wave energy; and to engage in community outreach via educational modules and a unique exhibit on wave energy for patrons of the Da Vinci Science Center in Allentown, PA.<br/><br/>If successful, the project will optimize and estimate the extent to which wave energy can provide green, predictable, dispatchable power to the electricity grid. The results will yield models, algorithms, control strategies, and system architectures to improve the output power of wave farms, reduce operating costs, maximize their environmental benefits, and provide guidance for farm operators in pursuing market opportunities. The project will validate the economic and environmental feasibility of wave power and drive forward the significant research and development efforts currently underway to bring the potential of wave energy conversion to grid-scale fruition.  Efficient and economic harvesting of the kinetic energy in ocean waves offers an electricity future with a more diverse supply portfolio, reduced greenhouse gas emissions, and higher sustainability impacts.  The educational and workforce development components of this project will broaden understanding of the technical and environmental aspects of wave energy among a new generation of engineers and scientists, as well as among the general public."
"1659807","REU SITE: Research Experiences for Undergraduates in Software Systems and Analysis","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/01/2017","01/17/2017","Guowei Yang","TX","Texas State University - San Marcos","Standard Grant","Rahul Shah","02/29/2020","$360,000.00","Anne Ngu","gyang@txstate.edu","601 University Drive","San Marcos","TX","786664616","5122452314","CSE","1139","9250","$0.00","This project sets up an REU site focused on software systems and analysis at Texas State University. This REU site provides opportunities for a cohort of undergraduate students to participate in research and development of innovative software systems and analysis technologies that would have enormous public benefits. The site emphasizes the participation of a diverse group of students, in particular, women, minorities, first generation, and non-traditional students. The project includes workshops, entrepreneurship forum, student presentations and posters, field trips and other professional development opportunities. The goal is to increase retention of computer science students, instill in them the spirit of innovation, improve their career perspectives, engage them to participate in cutting-edge research in software systems and analysis, and motivate them to enter graduate computer science programs.<br/><br/>Software affects nearly every aspect of our lives and holds the key to many scientific and engineering challenges. As software is increasingly pervasive nowadays, software-related problems have become much more prevalent. This project will study new trends in software systems including mobile software systems, green computing, internet of things, big data and parallel systems while analyzing various software qualities such as reliability, performance, usability, safety, maintainability, and energy efficiency."
"1657377","CRII: AF: Developing and Applying Connections Between Communication Complexity and Query Complexity","CCF","CRII CISE Research Initiation, ALGORITHMIC FOUNDATIONS","07/01/2017","03/22/2018","Thomas Watson","TN","University of Memphis","Standard Grant","Rahul Shah","06/30/2020","$182,911.00","","Thomas.Watson@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","026Y, 7796","7796, 7927, 8228, 9251","$0.00","The aim of computational complexity is to prove theorems that explain the fundamental limits of computation under resource constraints. Two such resources, communication (between different parties) and queries (to the input), have become increasingly important with the trends toward cloud computing and big data. Recent research by the PI and others has uncovered deep connections between these two subfields of complexity theory, namely communication complexity and query complexity, and has applied these connections to resolve several fundamental and long-standing open problems in theoretical computer science and beyond. This project will further develop these connections and explore new applications, which will help identify which problems can and cannot be solved by efficient computational processes in other areas of computer science. The broader impacts of this project include training and supporting the research careers of graduate students, broad dissemination of research findings through online reference resources and expository articles, curriculum development at the institutional level for the integration of research and teaching, involvement of minorities in research, collaboration with mathematicians, and outreach efforts.<br/><br/>The connections between communication complexity and query complexity are formalized as ""simulation theorems"" showing that in certain situations, decision trees can simulate communication protocols for related problems. The PI will develop the mathematical techniques needed to prove new simulation theorems and obtain quantitative improvements to existing ones. These results will contribute to a ""unified theory"" of communication lower bounds, showing a separation of concerns in which simple problem-specific query complexity lower bounds can be combined with generic (but deep) machinery for handling communication protocols. The project will also explore new applications of such connections, such as obtaining new lower bounds and separations for communication complexity measures, developing a theory of ""fine-grained extension complexity"" for linear programming, answering structural questions about communication and about how efficiently the complexity of a given function can be estimated, as well as studying fundamental open questions about the behavior of query complexity measures."
"1651225","CAREER:  The Next 700 Solver-Aided Languages","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2017","02/11/2019","Emina Torlak","WA","University of Washington","Continuing grant","Anindya Banerjee","01/31/2022","$287,501.00","","emina@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7798","1045, 7943","$0.00","Software is a critical part of modern infrastructure, and programming is an essential part of knowledge work in many fields, from physics to biology to social science. Yet translating algorithms and ideas into code is no easy task, and mistakes can be costly. A bug in a script can invalidate scientific results, and a bug in a file system can cause catastrophic loss of data. This project makes programming easier for systems programmers and scientists alike, through a novel approach to programming that automates domain-specific languages (DSLs) with solver-aided tools for program verification and synthesis. The intellectual merits are to advance knowledge in programming support for domain-specific verification and synthesis, in co-design of languages and tools, and in applying solver-aided programming to new domains. The project's broader significance and importance are to extend the reach of solver-aided programming by orders of magnitude and to thousands of programmers, facilitating new applications with societal, educational, and industrial impact.<br/><br/>The project's key idea is to make verification and synthesis tools, which are usually hand-crafted by computer science PhDs, as simple to build as DSLs, which are developed by a broad spectrum of programmers. The PI's prior work on solver-aided languages has demonstrated that this is possible, enabling a wide range of programmers, from professional developers to high-school students, to rapidly construct synthesis and verification tools for a variety of domains, from radiation therapy software to low-power computing to K-12 education. The resulting tools are based on reduction to Satisfiability Modulo Theories (SMT) solving, and as such, rely on technology that is (1) fundamentally intractable and (2) requires years of experience and training to use effectively. The goal of this proposal is thus to address the central challenge of solver-aided programming: enabling non-experts to diagnose and optimize the performance of solver-aided tools. To achieve this goal, the project develops automatic techniques for (1) symbolic profiling to provide diagnostic information about the causes of scalability bottlenecks across the solver-aided stack; (2) symbolic optimization to mitigate the identified scalability bottlenecks via code refactoring, (meta)sketch mining, and combination of solving engines; and (3) applications to serve as new challenge problems for evaluating symbolic profiling and optimization, and as demos for attracting a diverse population of users, from computer architects to education experts."
"1750127","CAREER: New Mathematical Programming Techniques in Approximation and Online Algorithms","CCF","ALGORITHMIC FOUNDATIONS","02/01/2018","02/07/2019","Viswanath Nagarajan","MI","University of Michigan Ann Arbor","Continuing grant","Rahul Shah","01/31/2023","$219,248.00","","viswa@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7796","1045, 7926, 9251","$0.00","Numerous applications in engineering, science and business are modeled and solved as combinatorial optimization problems. In today?s digital age, data collection and storage is inexpensive. The bottleneck lies in society's ability to solve increasingly larger problem instances, where the challenge typically stems from computational or informational limitations. This intractability can often be overcome by searching for approximately optimal instead of exactly optimal solutions. This project will design new mathematical-programming-based techniques that are broadly applicable in approximating combinatorial optimization problems. The project will strengthen connections of theoretical computer science to various fields of mathematics such as discrepancy theory, geometry, graph theory, optimization and probability. The PI will also collaborate with industry colleagues to disseminate the theoretical findings on some of the studied problems and assess their practical impact. The educational aspect of this project includes training undergraduate and graduate students, developing new course material and organizing a workshop for high-school teachers. <br/> <br/>Despite the wide range of possible combinatorial optimization problems, a common approach underlying numerous results in approximation and online algorithms is mathematical programming and rounding. This project will develop new techniques in this area by investigating (1) algorithms based on convex-programming hierarchies, (2) rounding algorithms based on recent advances in algorithmic discrepancy and (3) an online primal-dual framework for convex objectives. This project involves designing general algorithmic techniques (and identifying problem classes to which they apply) as well as improving the state-of-art on central problems such as k-Median, directed Steiner tree, the Beck-Fiala conjecture, unsplittable flow and online multicommodity routing. This project will also expand the applicability of the resulting techniques to areas such as combinatorics and operations research."
"1525943","AF: Small: Is the Simulation of Quantum Many-Body Systems Feasible on the Cloud?","CCF","ALGORITHMIC FOUNDATIONS","08/01/2015","08/21/2017","Pawel Wocjan","FL","University of Central Florida","Standard Grant","Dmitri Maslov","07/31/2019","$385,434.00","Dan Marinescu, Eduardo Mucciolo","wocjan@cs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7796","7923, 7928, 9251","$0.00","Simulating quantum mechanics with its unique effects such as superposition, interference, and entanglement is a hard problem for classical computing systems including supercomputers and computer clouds with a very large number of servers. To efficiently simulate large quantum-mechanical systems using a computer cloud one has to overcome major obstacles. This research investigates optimal algorithms for contracting tensor networks which arising in the study of condensed matter physics. These algorithms minimize the required communication between the nodes of the computer cloud and exploit its hierarchical organization. The broader research objective in this effort is to optimally exploit the architecture of hierarchically organized systems for big data applications that exhibit fine-grained parallelism.<br/>This research project aims to find new efficient methods for simulating large quantum systems that are important for quantum information processing, condensed matter physics, materials science, and chemistry. Its goals are to design and implement novel parallel and distributed simulation algorithms optimized for cloud computing environments such as Amazon Web Services and the National Science Foundation?s future cloud for scientific computing. The ultimate motivation of this project is to enable researchers world-wide to significantly push the boundary in terms of the size of quantum systems that they can simulate reliably and within a reasonable time and with a reasonable budget. While the research mainly concentrates on efficient algorithms and their implementation for the study of properties of condensed matter systems, it also attempts to derive generic strategies to other classes of applications, for instance, in artificial intelligence and in machine learning."
"1749864","EAGER: Probabilistic Models and Algorithms","CCF","ALGORITHMIC FOUNDATIONS","09/15/2017","09/05/2017","Aravind Srinivasan","MD","University of Maryland College Park","Standard Grant","Rahul Shah","02/29/2020","$129,000.00","","srin@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7796","7916, 7926","$0.00","The power of randomness in the computational context is one of the key discoveries of computer science. This project studies algorithms for fundamental problems that will further explore and use such power offered by randomness. One example of such a basic problem considered is in facility location: how can we place facilities at a low cost in order to minimize the total commute time for the customers? This classical problem has significant modern applications as well: e.g., in clustering data for machine learning, and in placing services on the Internet cloud. The PI aims to resolve how best this -- and closely-related problems in clustering and facility location -- can be solved via new probabilistic techniques. The PI will also study other such fundamental problems (e.g., in efficient resource allocation) in optimization through randomized algorithms, as well as study how such investigations improve and/or develop broadly-applicable probabilistic techniques.<br/> <br/>This project aims to have broader impact also through human-resource development. A graduate student will be directly involved in almost all this proposed research. The PI has advised two multi-year ``Gemstone"" undergraduate research teams: both teams have had their research published in national conferences. The PI proposes to start advising a new such team around 2018 and continue to expose them to the interplay between algorithms, randomness, and networks. Furthermore, the PI will mentor two high-school students; these students will continue to learn algorithms, randomness and applications from their basics up to cutting-edge research. The research of one of these high-school students, co-mentored by the PI, was invited to compete in the Intel International Science and Engineering Fair in 2017. <br/> <br/>A substantial part of this project will be on developing improved approximation algorithms through (new techniques in) randomization: approximation algorithms are those that are provably efficient and deliver solutions that are within a provable distance from optimal. Two representative examples of problems considered in this regard are in facility location (opening a subset of a given set of facilities in order to minimize the sum of the facility-opening costs and the total commute-time of the customers, as well as variants of this classical problem), and assigning jobs to servers in a general setting, in order to minimize the sum of the completion times of the jobs (weighted by individual priorities of the jobs). Methodologically, this project proposes to develop new techniques to carefully ""round"" infeasible solutions to feasible solutions via randomization, to further understand the power of information-theoretic notions (e.g., when one has a range of choices of probability distribution for the random choices to be made, can choosing a distribution that maximizes the entropy offer additional power?), and the interplay between linear-algebraic and probabilistic arguments. The ""rounding"" approach is flexible and general: e.g., for the facility location and job-assignment problems, it is computationally easy to start with near-optimal but infeasible solutions, and the key open question is how to optimally round to a feasible solution. Advances in such rounding techniques have had numerous consequences in approximation algorithms; a key goal of this project is to contribute to further such advances."
"1723344","AitF: Collaborative Research: Algorithms for Probabilistic Inference in the Real World","CCF","Algorithms in the Field","01/01/2017","12/28/2016","David Sontag","MA","Massachusetts Institute of Technology","Standard Grant","Tracy J. Kimbrel","08/31/2020","$399,999.00","","dsontag@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7239","","$0.00","Statistical models provide a powerful means of quantifying uncertainty, modeling prior beliefs, and describing complex dependencies in data.  The process of using a model to answer specific questions, such as inferring the state of several random variables given evidence observed about others, is called probabilistic inference.  Probabilistic graphical models, a type of statistical model, are often used in diverse applications such as medical diagnosis, understanding protein and gene regulatory networks, computer vision, and language understanding.  On account of the central role played by probabilistic graphical models in a wide range of automated reasoning applications, designing efficient algorithms for probabilistic inference is a fundamental problem in artificial intelligence and machine learning.<br/> <br/>Probabilistic inference in many of these applications corresponds to a complex combinatorial optimization problem that at first glance appears to be extremely difficult to solve.  However, practitioners have made significant strides in designing heuristic algorithms to perform real-world inference accurately and efficiently.  This project focuses on bridging the gap between theory and practice for probabilistic inference problems in large-scale machine learning systems.  The PIs will identify structural properties and methods of analysis that differentiate real-world instances from worst-case instances used to show NP-hardness, and will design efficient algorithms with provable guarantees that would apply to most real-world instances.  The project will also study why heuristics like linear programming and other convex relaxations are so successful on real-world instances.  The efficient algorithms for probabilistic inference developed as part of this project have the potential to be transformative in machine learning, statistics, and more applied areas like computer vision, social networks and computational biology.  To help disseminate the research and foster new collaborations, a series of workshops will be organized bringing together the theoretical computer science and machine learning communities.  Additionally, undergraduate curricula will be developed that use machine learning to introduce students to concepts in theoretical computer science."
"1408673","AF: Medium: Collaborative Research: On the Power of Mathematical Programming in Combinatorial Optimization","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","08/20/2018","David Steurer","NY","Cornell University","Continuing grant","Tracy J. Kimbrel","08/31/2019","$366,166.00","","dsteurer@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7796","7924, 7927","$0.00","Mathematical programming is a powerful tool for attacking combinatorial problems.  One transforms a discrete task into a related continuous one by casting it as optimization over a convex body.  Linear and semi-definite programming (LP and SDP) form important special cases and are central tools in the theory and practice of combinatorial optimization. These approaches have achieved spectacular success in computing approximately optimal solutions for problems where finding exact solutions is computationally intractable.<br/><br/>While there are very strong bounds known on the efficacy of particular families of relaxations, it remains possible that adding a small number of variables or constraints could lead to drastically improved solutions.  We propose the development of a theory to unconditionally capture the power of LPs and SDPs without any complexity-theoretic assumptions.  Our approach has the potential to show something remarkable:  For many well-known problems, the basic LP or SDP is optimal among a very large class of algorithms.  More concretely, we suggest a method that could rigorously characterize the power of polynomial-size LPs and SDPs for a variety of combinatorial optimization tasks.   This involves deep issues at the intersection of many areas of mathematics and computer science, with the ultimate goal of significantly extending our understanding of efficient computation.<br/><br/>Mathematical programming is of major importance to many fields---this is especially true for computer science and operations research.  These methods have also seen dramatically increasing use in the analysis of ""big data"" from across the scientific spectrum.  From a different perspective, LPs and SDPs can be thought of as rich proof systems, and characterizing their power is a basic problem in the theory of proof complexity. Thus the outcomes of the proposed research are of interest to a broad community of scientists, mathematicians, and practitioners."
"1712788","CCF-BSF:CIF:Small:Signal Processing and Machine Learning on Manifolds, with Applications to Invariant Detection and Covariant Estimation","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/28/2017","Louis Scharf","CO","Colorado State University","Standard Grant","Phillip Regalia","06/30/2020","$449,993.00","Edwin Chong, Christopher Peterson","Louis.Scharf@ColoState.EDU","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7797","7923, 7935, 7936","$0.00","In many fields of engineering and applied science the problem is to extract relevant information from a signal or image. Certainly this describes the problem of identifying cyber attackers in data networks, spotting objects of interest in closed-circuit TV records, and classifying anomalies in medical images. The goal of this project is to develop a signal processing theory for detecting and classifying images that have undergone geometric transformations. Practical and topical examples are medical features viewed in variable magnification and orientation, and images of people in arbitrary orientations in crowded scenes. The solution to classification problems under such imaging conditions will advance medical practice and national defense. As a broader impact, the project prepares students for careers in mathematics and electrical engineering, with an expertise in signal processing and imaging science.<br/><br/>This project develops a theory of matched manifold detectors, based on a universal manifold embedding that extracts a subspace basis from an image. The basis itself codes for the coordinate transformation of the image, but its span is invariant to the transformation. Consequently the extracted subspace is an invariant statistic for detection, and the basis is a covariant statistic for the parameters of the transformation. Classification is then a problem of subspace matching on a Grassmann manifold, and identification of coordinate transformation is a problem of analyzing a subspace basis in a Stiefel manifold. We aim to adapt this theory to other problems where a transformation group turns out an orbit of images, all of which are to be classified as equivalent. The objective is to develop a theory of signal processing on manifolds that is as broad in its scope and as precise in its methodologies as modern subspace signal processing. Such a theory will augment statistical reasoning with geometrical reasoning, and bring new mathematical methods into play."
"1422840","AF: Small: Programmable Nanowalkers:Models and Simulations","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2014","05/18/2016","Darko Stefanovic","NM","University of New Mexico","Continuing grant","Mitra Basu","06/30/2019","$400,000.00","","darko@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","7798","7923, 7946, 9150","$0.00","All nanoscale devices are subject to random diffusive forces that lead to slow, uncontrollable transport of materials and information.  Synthetic nanoscale systems designed for computational  and self-assembly tasks, require more precise control over energy- and information-carrying molecules.  Such systems include nanoscale devices currently being developed for delivery of diagnostic logic circuits to cells, querying the state of health of specific cells or subcellular structures, and conditional release of therapeutic cargo molecules. Ideally, such systems could employ programmable synthetic molecular motors to ferry information and materials in directed motion over a complex network of tracks, analogous to natural molecular motors in living cells, thus enabling behaviors otherwise not possible in a purely diffusion-driven environment. In this project computational models, simulation algorithms, and data visualization tools will be developed that will help synthetic chemists build the next generation of nanoscale walker systems. In the project, students at all levels (high-school to postdoctoral) will be trained in interdisciplinary research. High-school students will be engaged on the project through already established, tracked science involvement programs that emphasize participation of traditionally underrepresented groups. In the context of the project, a regular seminar will be developed on nanoscale technology, molecular computing, and molecular robotics within the biomedical engineering degree program, to educate future generations of students (undergraduate and graduate) in this emerging field of science.<br/><br/>Previous work has shown that simple DNA-enzyme driven synthetic walkers can move superdiffusively along nanoscale tracks, and can do mechanical work. In this project more advanced walker designs with large, complex body shapes and heterogeneity in their interactions with other walkers and molecules in their environment will be modeled and simulated.  This class of structured walker scaffolds will exhibit modes of motion not available in symmetrical walkers, e.g., rotational persistence, orientation-aware sorting, chiral walker-walker and walker-track interactions.  These features will be used to break symmetries in the walker's local environment, leading to stronger directional biases and more efficient directional transport. <br/><br/>The goal of the project is to understand the algorithmic basis of how a walker's shape and structure affect its motion, and how these features can be composed, modularly, into larger nanoscale transportation systems with programmable control, to achieve directed transport even under the randomizing and disorienting influence of Brownian motion. The models developed will enable more complex nano systems to be engineered to take advantage of programmable nanoscale transport.<br/><br/>The approach taken in the project is computational. Walker motion is treated as a continuous-time Markov process, at a level of abstraction that balances physical detail and computational tractability.  A hierarchy of Monte Carlo simulations will be developed to approximate physical and chemical processes at the appropriate relative scales, while maintaining computational tractability."
"1834218","NSF Student Travel Grant for 2018 IEEE International Conference on Bioinformatics and Biomedicine","CCF","COMPUTATIONAL BIOLOGY","07/01/2018","06/20/2018","Chi-Ren Shyu","MO","University of Missouri-Columbia","Standard Grant","Mitra Basu","06/30/2019","$12,000.00","","shyuc@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","7931","7556, 7946","$0.00","This award will enable up to 12 students from educational institutions in the US to attend the 12th Annual Institute of Electrical and Electronics Engineers (IEEE) International Conference on Bioinformatics and Bio-medicine (BIBM 2018).  The conference is one of the premier research conferences in bioinformatics and bio-medicine.  The BIBM community strives to build an international platform to foster experience sharing of multi-disciplinary research, education, and outreach activities through scientific presentations, keynote speeches, and workshop and panel discussions. Cutting-edge research shared at the conference will impact the science and technology of many fields, from agricultural and environmental to pharmaceutical and medical sciences. Due to the high cost of international travel, which often precludes student authors from participation due to insufficient support from their mentors and institutions, the travel grants will greatly encourage students, especially under-represented and disabled participants, from US institutions to attend the meeting. <br/><br/>The student awardees will have the opportunity to present their research, listen to talks, both keynotes and regular paper presenters, from the scientific program, which will highlight the following seven themes: 1. Bioinformatics and Computational Biology of Molecular Structure, Function and Evolution, 2. Computational Systems Biology, 3. Next Generation Sequencing and High-throughput Methods, 4. Cheminformatics and pharmacogenomics, 5. Cross-Cutting Computational Methods and Bioinformatics Infrastructure, 6. Medical Informatics and Bioinformatics of Disease, and 7. Healthcare Informatics.  These themes are designed to provide breadth, depth and synergy for research collaboration. In addition, to foster open discussions about research, education, and outreach opportunities and barriers, the students will join special panels in topics related to Big Data, Actionable AI, and Consumerable Discoveries.  The panels will cover opportunities for young investigators and provide tips to early career researchers and encourage the sharing of informatics tools dissemination.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814629","CCF-BSF: AF: CIF: Small: Low Complexity Error Correction","CCF","ALGORITHMIC FOUNDATIONS, COMM & INFORMATION FOUNDATIONS","10/01/2018","05/18/2018","Mary Wootters","CA","Stanford University","Standard Grant","Tracy J. Kimbrel","09/30/2021","$500,000.00","","marykw@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796, 7797","7923, 7926, 7927, 7935, 9102","$0.00","Error correcting codes are a fundamental tool for protecting data in communication and storage. Since the seminal works of Shannon and Hamming in the 1950s, error correcting codes have found uses throughout computer science and engineering, including in algorithm design, complexity theory, and cryptography. The goal of this project is to develop extremely efficient decoding algorithms for error correcting codes, in a variety of settings.  In addition to addressing fundamental problems in algorithmic coding theory, this research will have applications algorithm design, complexity theory and pseudorandomness.  Further, this award advances education by supporting graduate students and by providing research opportunities for undergraduates. This project is an international collaboration, made possible through joint funding with the US-Israel Binational Science Foundation (BSF). The project brings together one US investigator and one Israeli investigator, both of whom are experts in algorithmic coding theory, and who have a history of successful collaboration.<br/><br/>This project develops linear-time and sublinear-time algorithms for decoding error correcting codes in scenarios where these codes operate at capacity: that is, where they attain the best possible combinatorial and/or information-theoretic trade-offs.  The approach is to bring together algebraic, graph-theoretic, and information-theoretic techniques.  Traditionally, graph-theoretic and information-theoretic techniques have been useful in developing linear-time or near-linear-time algorithms for decoding, while algebraic techniques have been useful in developing sublinear-time algorithms.  By combining these techniques, this project will make progress on fundamental questions in algorithmic coding theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750808","CAREER: A Theory of Error Correction for Interactive Communication","CCF","ALGORITHMIC FOUNDATIONS","02/15/2018","02/05/2018","Bernhard Haeupler","PA","Carnegie-Mellon University","Continuing grant","Phillip Regalia","01/31/2023","$106,427.00","","haeupler@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","1045, 7934, 7935","$0.00","This project aims at developing a theory and methods for correcting errors in interactive communications. Shannon's influential ""A Mathematical Theory of Communication"" established such a theory for one-way communications. Coding theory has subsequently produced computationally efficient methods for reliable data transmissions over unreliable channels. While error correcting codes have transformed communication technologies over the decades, modern communication settings often go beyond one-way transmissions and instead require many interleaved rounds of interactive communication. The development of interactive equivalents of good error correcting codes for such modern systems is a much harder task, which has attracted attention recently and witnessed encouraging initial successes. This project will further advance the fundamental questions underlying possibilities, limitations, and theoretical underpinnings of reliable interactive communication in the presence of noise, and thus contribute to a solid mathematical and computational theory of reliable interactive communication. The project also has a strong educational component which supports several initiatives to stimulate undergraduates, graduate students, the scientific community, and the general public through education and outreach.<br/><br/>The project attacks a diverse set of ""classical"" questions, such as determining the fundamental communication rate limits for error correction in interactive communications, and explicit constructions of tree codes, a powerful but elusive type of online code. The project also considers the broader context of interactive coding schemes and their wide ranging applicability in other areas of theoretical computer science, including cryptography, privacy, and memory efficient error resilient computations."
"1618502","AF: Small: Non-revelation Mechanism Design","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","06/28/2016","Jason Hartline","IL","Northwestern University","Standard Grant","Tracy Kimbrel","06/30/2019","$450,000.00","","hartline@eecs.northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7796","7923, 7932","$0.00","Mechanism Design governs the design of protocols for problems of allocation-of-goods to strategic (i.e., selfish) agents and has applications in both computer science and economics.  Such allocation problems are all around: advertisers compete in an auction through online search engines to post their links beside query responses; Internet traffic competes for router access; elementary students compete for a limited number of openings in an elite magnet school. These processes can aim to achieve different objectives: search engines want to maximize revenue from ad sales; the Internet wants to minimize the total communication delays of all users; school districts want to respect fairness in school assignments.<br/><br/>Historically, research in mechanism design focuses almost exclusively on mechanisms that are simple for the agents, requiring that it is in each agent's best interest to truthfully reveal its preference. Such mechanisms are known as revelation mechanisms.  The mechanisms that result often both have complex rules and are dependent on detailed assumptions about the environment.  This project develops the theory for design and analysis of non-revelation mechanisms, which may require strategic optimization by the agents, but are simple and robust.  Broader impacts of this project include contribution to the economics literature, development of theory that informs the design of mechanisms in industry, and the training of undergraduates and Ph.D. students who will use the developed skills in software development jobs and research.<br/><br/>This project will address a number of foundational questions in non-revelation mechanism design.  First, simple non-revelation mechanisms may have parameters that need to be tuned to the environment and this environment may not be stationary.  For example, both supply and demand may be evolving.  A goal of the project is to understand families of mechanisms that both have good performance and can be tuned directly from historical data from the mechanism, and to identify statistically efficient procedures for performing this tuning.<br/><br/>Second, an especially robust criterion for design is that the mechanism perform well under any environmental conditions; i.e., without any parameterization. The project will investigate the design of robust non-revelation mechanisms and quantify the extent to which robust guarantees for non-revelation mechanisms may be better than the best robust guarantees possible for revelation mechanisms.  <br/><br/>Third, in non-revelation mechanisms, where truthful revelation is not an agent's best response, agents will need to take actions strategically.  Agents may find strategically good actions may only come after some trial and error, e.g., via learning algorithms.  When all agents are behaving thus, the resulting actions will be correlated.  The project aims to study the performance of mechanisms under such natural dynamics and their convergence properties."
"1652276","CAREER: Software Hardware Architecture Co-Design for Smart Environment Operation and Management","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2017","03/14/2018","Fan Ye","NY","SUNY at Stony Brook","Continuing grant","Yuanyuan Yang","01/31/2022","$183,975.00","","fan.ye@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7798","1045, 7941","$0.00","""Internet-of-Things"" (IoT) embedded with sensing, actuation, computing, communication and storage resources are expected to transform our homes, offices, neighborhoods and communities into ""smart environment"", providing novel functions, services and creating potential economic impacts in trillions. Despite recent booming IoT business, there is a lack of foundational software/hardware architecture suitable for fine grained access, performance assurances and management critically demanded in smart environment. This research takes a principled approach to design and develop such a software/hardware architecture serving as a foundation for future IoT technology and applications, thus alleviating some of the critical technical obstacles impeding the trillions of economic potentials in diverse IoT applications. The PI will collaborate with the police and industry partners for technology pilots to assist officers for agile emergency response in their daily duties, and seniors to age at home with improved life quality. Students from Ph.D., M.S., to undergraduate and high school levels will be involved and trained during the research. Outreach to K-12 and local communities will leverage the research prototypes for science projects and maker's clubs to foster a culture of technology and innovation.<br/><br/>This research promotes smart object command operations as first class citizens. It will develop software/hardware architecture and systems firmly rooted in flexible, fine-grained access, command execution assurances and scalable management demanded in smart environment. States and information related to access constraints are bundled into self-sufficient and self-protected command data units. The research will explore and develop mechanisms to ensure formal IoT command execution properties; essential hardware traffic forwarding and security functions to ensure low latency and protection of IoT operations; and configuration, monitoring, maintenance essential in smart environment management."
"1914717","EAGER: Recomputation-Based Checkpointing for Sparse Matrices","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","02/01/2019","Yan Solihin","FL","University of Central Florida","Standard Grant","Almadena Chtchelkanova","04/30/2020","$236,234.00","","yan.solihin@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7798","7916, 7942","$0.00","High-performance computing (HPC) is essential for maintaining the US international competitive edge and leadership in science, technology, engineering, and mathematics (STEM). Advances in HPC are vital to national interests by providing infrastructure for scientific discovery that improves the national health, prosperity, welfare, and defense. To solve large-scale scientific problems, HPC relies on an increasing number of nodes and components, which makes it likelier for long-running computation to be interrupted with failures before completing. A critical technique to ensure computation completion is checkpointing. Checkpointing allows snapshots of the computation to be saved so that when a failure occurs, computation state can be restored from the last snapshot and continues execution, rather than restarting from the beginning. The research in this project seeks to advance the state-of-the-art checkpointing technique by making it significantly faster and lowering its cost. This project also plans to contribute to the training of future workforce by providing students with exposure to the mechanisms and inefficiencies of current checkpointing mechanisms on NVMM, and the new in-place checkpointing. The project seeks to increase participation of minority and under-represented groups and involves undergraduates in research.<br/><br/>Prior approaches to checkpointing rely on taking a snapshot of the system state (system-level checkpointing) or the application state (application-level checkpointing) and saving it to secondary non-volatile storage. With the advent of non-volatile main memory (NVMM), a new approach to checkpointing becomes possible.  In contrast to traditional approaches to checkpointing that rely on storing separate snapshots in a separate secondary storage, the project uses a new approach where checkpoints can be constructed in-place in the NVMM  utilizing the working data structures used by the applications. This allows only very minimal additional state beyond what the program already saves to memory, making checkpointing significantly faster and incurring lower cost, in turn providing further HPC scaling.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1704828","CIF: Medium: Collaborative Research: Nonconvex Optimization for High-Dimensional Signal Estimation: Theory and Fast Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","09/19/2017","Yudong Chen","NY","Cornell University","Continuing grant","Phillip Regalia","08/31/2021","$179,752.00","","yc2272@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","7924, 7936","$0.00","High-dimensional signal estimation plays fundamental roles in various engineering and science applications, such as medical imaging, video and network surveillance. Estimation procedures that maintain both statistical and computational efficacy are of great practical value, which translate into desiderata such as less time patients need to spend in a medical scanner, faster response to cyber attacks, and capabilities to handle very large datasets. While a lot of signal estimation tasks are naturally formulated as nonconvex optimization problems, existing results for nonconvex methods have several fundamental limitations, and the current state of the art is still limited in terms of when, why and which nonconvex approaches are effective for a given problem. The goal of this research program is to significantly deepen and broaden the understanding and applications of nonconvex optimization for high-dimensional signal estimation.<br/><br/>In this project, the investigators will study high-dimensional signal estimation via direct optimization of nonconvex, and potentially nonsmooth, loss functions, without resorting to convex relaxation. This research will explore geometric structures shared by nonconvex functions commonly encountered in signal estimation, and study the fundamental roles these structures play in determining the algorithmic convergence. These results will then be exploited as guidelines to develop fast and provably correct algorithms for estimating high-dimensional signals with physically induced structures and under streaming data observations. Specifically, the research program consists of three major thrusts: (1) understanding the geometric structures of important classes of nonconvex loss surfaces, and characterizing their impact on the convergence of optimization algorithms; (2) developing fast algorithms and the associated theory for the recovery of structured low-rank matrices; (3) designing new online algorithms that are time and space efficient under a streaming setting, with the capability of detecting and tracking the time-varying signals of interest."
"1637598","AitF: Algorithmic challenges in smart grids: control, optimization & learning","CCF","Algorithms in the Field","10/01/2016","08/18/2016","Steven Low","CA","California Institute of Technology","Standard Grant","Tracy Kimbrel","09/30/2020","$750,000.00","Adam Wierman, Venkat Chandrasekaran, Yisong Yue","slow@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7239","","$0.00","This project will tackle the algorithmic challenges underlying the transformation of the power grid.  Society is at the cusp of a historic transformation of our energy systems, driven by sustainability. Daunting challenges arise in the stable, reliable, secure, and efficient operation of the future grid that will be much more distributed, dynamic, and open. This project will push the boundaries of control, optimization, and learning to develop practical solutions to some of these difficulties.  It will advance state of the art in both the science of general cyber-physical systems and its application to smart grids.  It will support education and diversity through a tight integration of the research with educational courses and the training of female and minority students. <br/><br/>The theory and algorithms to be developed in this project will contribute directly towards the historic transformation of energy systems to a more sustainable future. Specifically, the project will focus on three core algorithmic challenges facing cyber-physical networks such as a smart grid: control, optimization, and learning. First, this project will develop an optimization-based approach to the design of feedback controllers for cyber-physical systems so that the closed-loop system is asymptotically stable, and every equilibrium point of the closed-loop system is an optimal solution of a given optimization problem.  Second, this project will develop a new hierarchy of convex relaxations for exponential programs based on relative entropy optimization. This will immediately yield a fundamentally new approach for solving Optimal Power Flow (OPF) problems, which underlie numerous power system applications and are non-convex and NP-hard in general.  Third, this project will develop methods to learn a policy that is near-optimal efficiently, despite not having access to the objective function at run time. This will allow power systems to ""learn to optimize"" in real time, addressing one of the biggest challenges in power systems -- that data about the system is too expensive or impossible to obtain in real time."
"1652491","CAREER: Beyond Worst-Case Analysis: New Approaches in Approximation Algorithms and Machine Learning","CCF","ALGORITHMIC FOUNDATIONS","03/15/2017","03/02/2019","Aravindan Vijayaraghavan","IL","Northwestern University","Continuing grant","Tracy Kimbrel","02/28/2022","$261,737.00","","aravindv@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7796","1045, 7926","$0.00","Combinatorial optimization problems such as clustering, and unsupervised learning of probabilistic models are important computational problems that arise in diverse areas including machine learning, computer vision, operations research and data analysis. However, there is a large disconnect between our theoretical and practical understanding of these problems -- while theory tells us that many interesting computational problems in combinatorial optimization and machine learning are intractable in the worst case, practitioners in areas such as machine learning and computer vision have made significant progress in solving such theoretically hard problems. This project focuses on bridging the fundamental gap between theory and practice by developing paradigms and machinery that will allow us to reason about the performance of algorithms on real-world instances. This research has the potential to have broad impact on both theory and practice of computational problems across different areas of computer science, machine learning and statistics. The project will involve students at all levels of research, will integrate aspects of average-case analysis in both graduate and undergraduate courses, and will include outreach activities in high schools in Evanston and the broader Chicago area.<br/><br/>The PI will study several problems in machine learning and combinatorial optimization by using realistic average-case models and smoothed analysis. Broad goals include designing new model-independent algorithms with provable guarantees for realistic average-case models of graph partitioning and clustering and challenging average-case settings where there is no unique or planted solution. These algorithms will also lead to new algorithmic techniques for learning probabilistic models such as mixtures of Gaussians and stochastic block models that are robust to various kinds of modeling errors and noise. Another focus of the project is on developing new efficient algorithms for learning latent variable models and for reasoning about the performance of algorithms using smoothed analysis."
"1806154","CIF: Medium: Collaborative Research: Nonconvex Optimization for High-Dimensional Signal Estimation: Theory and Fast Algorithms","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","01/01/2018","07/17/2018","Yuejie Chi","PA","Carnegie-Mellon University","Continuing grant","Phillip Regalia","08/31/2021","$295,887.00","","yuejiechi@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878, 7797","7797, 7924, 7935, 7936","$0.00","High-dimensional signal estimation plays fundamental roles in various engineering and science applications, such as medical imaging, video and network surveillance. Estimation procedures that maintain both statistical and computational efficacy are of great practical value, which translate into desiderata such as less time patients need to spend in a medical scanner, faster response to cyber attacks, and capabilities to handle very large datasets. While a lot of signal estimation tasks are naturally formulated as nonconvex optimization problems, existing results for nonconvex methods have several fundamental limitations, and the current state of the art is still limited in terms of when, why and which nonconvex approaches are effective for a given problem. The goal of this research program is to significantly deepen and broaden the understanding and applications of nonconvex optimization for high-dimensional signal estimation.<br/><br/>In this project, the investigators will study high-dimensional signal estimation via direct optimization of nonconvex, and potentially nonsmooth, loss functions, without resorting to convex relaxation. This research will explore geometric structures shared by nonconvex functions commonly encountered in signal estimation, and study the fundamental roles these structures play in determining the algorithmic convergence. These results will then be exploited as guidelines to develop fast and provably correct algorithms for estimating high-dimensional signals with physically induced structures and under streaming data observations. Specifically, the research program consists of three major thrusts: (1) understanding the geometric structures of important classes of nonconvex loss surfaces, and characterizing their impact on the convergence of optimization algorithms; (2) developing fast algorithms and the associated theory for the recovery of structured low-rank matrices; (3) designing new online algorithms that are time and space efficient under a streaming setting, with the capability of detecting and tracking the time-varying signals of interest."
"1560137","REU Site: Interdisciplinary Software Engineering","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2016","01/20/2016","Joshua Sunshine","PA","Carnegie-Mellon University","Standard Grant","Rahul Shah","01/31/2020","$359,364.00","Claire Le Goues","josh.sunshine@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1139","9102, 9250","$0.00","This project sets up a new REU site at Carnegie-Mellon University focused on multi-faceted aspects of software engineering. The program will be recruiting a diverse set of students based on stop-past-the-post strategy successfully employed previously by other sites. The site will focus on recruiting women students and students from undergraduate-only institutions with limited research exposure. The students will not only be acquiring research skills through the training program, but also gain exposure to a broader research landscape through interactions and seminars. The students will be exposed to faculty life and research career paths.<br/><br/>Skills in software engineering have always been core to computer science, programming and software development. The project will offer research opportunity in secure languages, requirement engineering, program repair, variational data structures and API usability. The projects emphasize separate modules on training, exposure and graduate life mentoring. The project will build on the experience of many of the faculty mentors who have worked with undergraduate students in their individual research. The students will be encouraged to disseminate their research findings by publishing in quality venues and giving presentations."
"1618648","AF:III: small: Convex optimization for protein-protein interaction network alignment","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","06/28/2016","Jinbo Xu","IL","Toyota Technological Institute at Chicago","Standard Grant","Mitra Basu","06/30/2019","$299,994.00","Qixing Huang","j3xu@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7931","$0.00","High-throughput experimental techniques have been producing a large amount of protein-protein interaction (PPI) data. Comparative analysis (e.g., alignment) of PPI networks greatly benefits the understanding of evolutionary relationship among species, helps identify functional modules and provides information for protein function annotations. The research goal of this proposal is to study optimization methods that can align PPI networks much more accurately than existing methods. This proposal will apply several elegant and powerful optimization techniques to understand the mathematical structure of the problem and develop efficient alignment algorithms. This proposal will also develop software implementing the proposed algorithms. <br/><br/>The proposed algorithms will be implemented as both a standalone program and Cytoscape plugin so that they can be easily used by biologists. The resultant software and plugin shall benefit a broad range of biological and biomedical applications, such as protein functional annotation, understanding of disease processes, design of novel diagnostics and drugs, and precision medicine. The research results will be disseminated to the optimization, computer vision/graphics and biology communities through a variety of venues. The source code will be released so that it can be useful to other network analysis researchers who want to adapt the code for their own research projects and to other optimization method researchers who want to work on biological network analysis. This project will train a few PhD students and summer interns, who will receive training in the intersection of optimization techniques, network biology and programming. Undergraduate and underrepresented students will be recruited through our summer intern program, CRA-W and collaborators. The research results will be integrated into course materials and used in an Illinois online bioinformatics program that has trained many underrepresented students. <br/><br/>This proposal will study a novel convex optimization algorithm for the alignment of two or multiple PPI networks. This convex method distinguishes itself from the widely-used seed-and-extension or progressive alignment strategy in that it simultaneously aligns all the input networks and proteins while the latter methods use a greedy strategy to build an alignment. A greedy strategy may introduce alignment errors at an early stage that cannot be fixed later, but this convex method can avoid this. Due to its simultaneous alignment strategy, this convex method shall detect many more proteins that are functionally conserved across all input PPI networks than existing methods and produce more accurate pairwise alignments of multiple networks. This proposal will also study a few methods to speed up the proposed convex alignment method, by making use of special topology properties of PPI networks and exploring low-rank representation of proteins. Finally, this proposal will implement the proposed algorithms as a standalone software package and Cytoscape plugin to greatly facilitate the application of comparative network analysis to biological and biomedical science discovery."
"1718474","SHF:Small: Collaborative Research: Exploring 3-Dimensional Integration Strategies of STTRAM","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2017","05/08/2018","Swaroop Ghosh","PA","Pennsylvania State Univ University Park","Standard Grant","Sankar Basu","07/31/2020","$233,000.00","","szg212@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7798","7923, 7945, 9102, 9251","$0.00","The development of high-density memory is fueled by recent advancements in several critical areas ranging from big data analytics, Internet-of-Things (IoT), wearable electronics, smart phones, assistive devices, cybersecurity to weather tracking systems. Currently available conventional memory technologies face serious challenges in addressing these needs due to increased energy consumption, scalability limitations, and limited bandwidth. The objective of this proposal is to develop novel memory devices by enabling 3D integration of spin-torque transfer RAM devices (STTRAM) with appropriate selector diodes (SD). These type of memory devices will be massively scalable, energy-efficient, and fast which will open tremendous opportunities to integrate them in existing memory architectures to enable various futuristic applications. The project will integrate education by incorporating research outcomes in PI's academic courses. Industrial feedback will be sought via collaborations. Under-represented groups will be involved in science and engineering through several outreach programs at Penn State University (PSU) and University of Cincinnati (UC) such as Summer Research Opportunities Program (SROP) at PSU, and Emerging Ethnic Engineers (E3) program at UC which provides opportunities to the high-school students from the inner-city schools with economically challenged background to participate in internships and lab-experience activities. Undergraduate students will be involved in research via Research Experiences for Undergraduates (REU) program at both the universities. Dissemination of results will be accomplished by publications in high-impact scientific journals, conference presentations, and YouTube lecture videos. <br/><br/>The intellectual merit of the project is in understanding the integration compatibility of STTRAM devices with SD and developing highly scalable 3D crossbar arrays of memory technologies based on integrated STTRAM and SD. The objectives will be achieved by executing the following specific aims: (i) modeling and simulation of STTRAM-SD arrays, (ii) resilience analysis and optimization studies to minimize the impact of device-level variabilities on performance metrics, (iii) optimization of STTRAM-SD architectures for high-performance computing and IoT, (iv) designing, fabrication, testing, and modeling of novel SD devices based on energy band-engineered and doping-engineered transition metal oxide and electrode stacks, (v) designing and fabrication of small-size STTRAM-SD arrays, electrical testing and modeling to benchmark simulations and experimental results. It is anticipated that the successful completion of this project will lead to a fundamental understanding of the compatibility of STTRAM with SD and provide a platform-technology to develop high-density memories which will have transformative impact on commercializing and advancing the futuristic applications."
"1717606","AF: Small: The Complexity of Random CSPs","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/30/2017","Ryan O'Donnell","PA","Carnegie-Mellon University","Standard Grant","Tracy Kimbrel","08/31/2020","$450,000.00","","odonnell@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7923, 7926, 7927","$0.00","This project is concerned with understanding the computational difficulty of solving very large, randomly generated tasks called ""constraint satisfaction problems.""  One major motivation for this study is cryptography.  Cryptographic protocols used to transmit and compute on securely encrypted data rely on our ability to easily generate random, hard-to-solve computational tasks.  As an example, many cryptographic protocols rely on the assumption that if you multiply together two random prime numbers of many digits, it is computationally difficult to factor the resulting product.  Recent research in cryptography has sought new sources of easy-to-generate hard problems, both for added flexibility and for facilitating different types of cryptographic tasks.  However, although we have a fairly good understanding of what kinds of computational tasks can be hard to solve in the worst case, we do not know nearly as much about what kinds of computational tasks are expected to be hard when they are chosen at random.  The class of ""constraint satisfaction problems (CSPs)"" seems to be a good and potentially useful candidate for a class of problems that is hard when chosen at random.  This project will involve furthering our understanding of the computational feasibility of random CSPs, studying how the various parameters involved (such as the ratio of constraints to variables, the kinds of constraints, etc.) affects their easiness/difficulty.  An additional aspect of the project will be scientific and educational training for computer science undergraduate and graduate students at Carnegie Mellon University, as well as wide dissemination of the research produced.<br/><br/>At a more technical level, the project has several lines of inquiry related both to the computational complexity of random CSPs, as well as algorithms for their solution.  The PI will consider the tradeoffs between constraint density, constraint type, quality of approximation/refutation algorithms, and running time.  Particularly, the PI will investigate the power and limitations of the powerful ""Sum-of-Squares (SOS) semidefinite programming hierarchy"" in the context of random CSPs.  The PI will also investigate potential new hardness results for non-CSPs and learning theory problems, based on the assumed intractability of random CSPs."
"1423111","AF: Small: Motion Planning Techniques for Protein Motion","CCF","ALGORITHMIC FOUNDATIONS, COMPUTATIONAL BIOLOGY","07/01/2014","05/03/2018","Nancy Amato","TX","Texas A&M Engineering Experiment Station","Standard Grant","Mitra Basu","06/30/2019","$432,000.00","Lawrence Rauchwerger, Shawna Thomas","namato@illinois.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7796, 7931","7923, 7931, 9251","$0.00","Protein motions play an essential role in many biochemical processes.  For example, as proteins fold to their native, functional state, they sometimes undergo critical conformational changes that affect their functionality, e.g., diseases such as Mad Cow disease or Alzheimer's disease are associated with protein misfolding and aggregation.  Proteins also undergo conformational change when interacting with other molecules as they transition between bound and unbound states.  Knowledge of the mechanics of these motion processes may help provide insight into how and why proteins misfold, how binding regulation is communicated through the protein structure, and how to design more effective drugs.  For example, a better understanding of protein misfolding and aggregation has the potential to provide insight into neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, prion diseases, and related diseases that have a major impact on society.  An important, and more immediate, goal of the project is to share detailed results generated by the new methods with the community in a publicly available database of protein motions. <br/><br/>This project will develop new modeling, simulation and analysis tools that specialize and apply a novel computational method for studying molecular motions that has been developed and validated against experimental data in preliminary work.  This method represents a trade-off between methods such as molecular dynamics and Monte Carlo simulations that provide detailed individual folding trajectories and techniques such as statistical mechanical methods that provide global landscape statistics.  The approach, derived from robotic motion planning methods, builds a graph that encodes many (typically thousands) of motion pathways.  The proposed work involves both algorithmic research to further develop and optimize the techniques and research necessary to apply them to study issues of current interest in protein science.  While the algorithmic research will be performed by computer scientists, the application and testing of the techniques will benefit from collaborations with labs currently studying these problems.  The main research goals include: (i) The development of new and/or improved metrics and analysis techniques for conformations, pathways, and roadmaps that can be applied to modeling more complex motion applications.  These methods will be validated and applied to protein transitions, decoy database improvement, and ligand binding. (ii) New methods for modeling and simulating constrained motion and incorporating greater bond flexibility in areas of legitimate need (neither supported by current framework).  These methods will be applied to modeling protein transitions, and ligand binding. (iii) Strategies for employing high-performance computing to increase the size and complexity of the systems that can be studied."
"1618762","SHF: Small: Collaborative Research: GOALI: Multiscale CAD Framework of Atomically Thin Transistors for Flexible Electronic System Applications","CCF","GRANT OPP FOR ACAD LIA W/INDUS, INFORMATION TECHNOLOGY RESEARC, SOFTWARE & HARDWARE FOUNDATION","07/01/2016","06/28/2016","Jing Guo","FL","University of Florida","Standard Grant","Sankar Basu","06/30/2019","$225,000.00","Shu-Jen Han","guoj@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1504, 1640, 7798","1504, 7923, 7945","$0.00","Establishing the foundation for electronics technology based on atomically thin two-dimensional (2D) materials, such as layered transition metal dichalcogenides (TMDCs), may prove to be transformative in many technological areas relying on flexible electronic and nanoelectronic systems. This project will establish a critical knowledge base for future 2D TMDC electronics technology for a broad range of applications, such as low power computing, flexible display, and wearable electronics. The project has a direct industrial impact through the respective collaboration and technology transfer between the participating universities and the industrial partner. It will also offer interdisciplinary research opportunities for training graduate students, as well as undergraduate and high school students, in a collaborative research environment between university and industry, and provide valuable resources for research and educational community by disseminating web-based learning modules, simulators, and experimental data on TDMC-based electronics.<br/><br/>While TMDC materials are promising for many potential applications in nanoelectronics and flexible electronics due to their mechanical bendability, atomically thin thickness, and excellent intrinsic carrier transport properties, major gaps exist on translating early science of  such materials into practical circuit and system technologies. The objective of this project is to develop compact model and circuit-simulation platform for new 2D TMDC-based devices and systems, and to explore its applications in flexible and wearable electronic systems through experimental demonstration and collaboration with IBM T. J. Watson Research Center as the industrial partner. The proposal will undertake the following tasks: (i) develop a multiscale simulation framework that integrates atomistic device simulations with compact circuit models for TMDC transistors, (ii) fabricate, characterize and simulate basic TMDC circuits, (iii) model the variability and defect mechanisms and their correlations in TMDC transistors, and (iv) design and experimentally demonstrate TMDC driving circuits for transparent flexible display."
"1553354","CAREER: From Discrepancy to Optimization and Approximations in Geometric Hypergraphs","CCF","ALGORITHMIC FOUNDATIONS","02/01/2016","02/07/2019","Esther Ezra","GA","Georgia Tech Research Corporation","Continuing grant","Rahul Shah","01/31/2021","$484,645.00","","eezra3@math.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","1045, 7929","$0.00","How does one evenly distribute points on a sphere?  Given points, how does one color them red and blue, so no circular region has too many of one color? The first question is open-ended until one defines evenly; the second is fully-specified, but hard -- there are many possible colorings and many regions to check. In these, and many similar problems, even when one is trying to be fair to a number of players, some will win a little and some will lose a little; the mathematical concept of ""discrepancy"" captures the amount of deviation from uniformity that must occur.  Through what sometimes seem abstract problems, such as coloring, this mathematics explores the limits of how well can a sample represent a whole -- an important question for computation, since algorithms are often made efficient by doing initial computation on a sample of the input data. <br/><br/>In this project, the PI will explore questions of geometric discrepancy and their connections to algorithms for optimization and approximation. She has three foci: (1) Relative discrepancy, in which bounds depend on set size -- eg., the number of points in a circle. (In two and three dimensions, the PI has previously nearly optimal bounds in two- and three-dimensions.) Algorithms to find good colorings will also be sought, especially for geometric objects with characteristics (like ""fatness"") that apply to models for manufacturing or simulation.  (2) Extending epsilon-nets, which are samples that approximate a whole (e.g., a subset S from points P is an epsilon-net if any circle that contains an epsilon fraction of P must contain a point of S) so that you have some idea how many points from S lie in a circle that contains a larger fraction of P. Epsilon-nets not only help approximate a whole, but also indicate where more computation is needed in divide and conquer algorithms. (3) Leveraging improved epsilon nets for better approximation algorithms. It has been known for some time that improved bounds on epsilon nets in various geometric settings (e.g. axis aligned rectangles, etc) immediately give improved bounds for various NP-hard packing and covering problems in those same geometric settings. The PI has some of the strongest bounds (e.g. she proved optimal bounds in the case of axis-aligned rectangles, and has also found some practical applications of these to problems in networking), and aims to make progress on a number of still open questions in this domain, such as the size of weak epsilon-nets for axis aligned rectangles. <br/><br/>This work pulls together several themes in mathematics and computation, including when local solutions can be extended to global solutions.  Progress in understanding discrepancy can have broad and unexpected impact: fairness of resource allocation affects more than just science and engineering simulations."
"1841780","NSF Student Travel Grant for 2018 International Workshop on Computational Network Biology: Modeling, Analysis, and Control (CNB-MAC)","CCF","INFORMATION TECHNOLOGY RESEARC, COMPUTATIONAL BIOLOGY","08/01/2018","07/29/2018","Ranadip Pal","TX","Texas Tech University","Standard Grant","Mitra Basu","07/31/2019","$10,000.00","Byung-Jun Yoon, Xiaoning Qian","ranadip.pal@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","1640, 7931","7556, 7931, 7946","$0.00","The Fifth International Workshop on Computational Network Biology: Modeling, Analysis, and Control (CNB-MAC 2018) will be held in Washington, D.C., August 29, 2018. The workshop will be organized in conjunction with the 9th Association for Computing Machinery (ACM) Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB 2018). CNB-MAC 2018 is a continuation of the first, second, third and fourth CNB-MAC workshops in 2014, 2015, 2016 and 2017, which have been successfully organized in conjunction with ACM-BCB 2014 (in Newport Beach, CA), ACM-BCB 2015 (in Atlanta, GA), ACM-BCB 2016 (in Seattle, WA) and ACM-BCB 2017 (in Boston, MA) respectively. The continuing partnership with ACM-BCB will enhance the visibility of the workshop and attract researchers from various disciplines across engineering, computer science, statistics, biology, and medicine. This project will provide travel support for eight graduate students to attend CNB-MAC 2018, present their latest research findings, and actively engage in interactions with other researchers in the field.<br/><br/>CNB-MAC 2018 aims to provide an international scientific forum for presenting recent advances in computational network biology and boost the awareness of the importance of rigorous mathematical modeling in transforming big biomedical data into reproducible and meaningful scientific knowledge. One of the missions of CNB-MAC that is of foremost importance is to foster next-generation scientists in the emerging field of computational network biology. The workshop aims to offer research talks to provide future research directions for participating graduate students and post-doctoral researchers, especially women and minorities. Utilizing this award, CNB-MAC 2018 will provide travel awards to encourage graduate and undergraduate students to participate in the workshop and present their latest research findings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640227","E2CDA: Type I: Collaborative Research: Energy Efficient Computing with Chip-Based Photonics","CCF","Energy Efficient Computing: fr","09/01/2016","07/23/2018","Yeshaiahu Fainman","CA","University of California-San Diego","Continuing grant","Sankar Basu","08/31/2019","$411,708.00","","fainman@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","015Y","7945","$0.00","Amidst today's data explosion, demand for computing power is accelerating, and the energy requirements for solving critical problems in science, engineering, business, and intelligent processing are increasing dramatically.  In order to address this critical issue, the scientific and engineering community is beginning to explore new approaches to computing, such as mimicking the brain's structure or the dynamical behavior of coupled particles.  However, implementing these approaches with conventional computer architectures is highly inefficient from both energy and computing standpoints. As a result, there is a pressing need to develop revolutionary computer architectures that overcome these severe roadblocks. An ambitious program is to be pursued within the framework of this project in which light, rather than electrons, is used to realize new computing paradigms with superior energy scalability. Specifically, computing architectures will be explored that exploit the wave nature of light (i.e., its amplitude and phase) harnessing the remarkable advancements over the past decade in nanofabrication of complex photonic chips with thousands of high-performance devices. These photonic platforms would have the potential to be computationally powerful, operate with unparalleled energy efficiency, and are scalable and highly reconfigurable.<br/><br/>To fulfill this vision of energy efficient photonic computing, the proposed research efforts will focus on the following two types of photonic processors: (1) Ising Machine and (2) Neuromorphic Computing Machine.  The unifying aspect of these photonic processors is that they consist of dynamic networks of coupled photonic units and rely on the wave nature of light to solve problems which has no analogy in electron-based computing systems. Beyond developing new types of optical processors, photonics technology will be developed as the cornerstone of future computer systems.  A novel architecture will be explored that integrates photonic processors and interconnects with electronic memory and processors to maximize the benefits of each technology. Research will also be undertaken to map complete, challenging problems to combinations of photonic accelerators and electronic processors, and to determine how to best scale them to maximize full-system performance.  By innovating at all levels of the system - from devices to architectures including systems, compilers, and algorithms -  the project would aim to achieve advances that cannot be realized within any single field."
"1829142","EAGER: Recomputation-Based Checkpointing for Sparse Matrices","CCF","SOFTWARE & HARDWARE FOUNDATION","05/15/2018","05/03/2018","Yan Solihin","NC","North Carolina State University","Standard Grant","Almadena Chtchelkanova","03/31/2019","$298,716.00","","yan.solihin@ucf.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7916, 7942","$0.00","High-performance computing (HPC) is essential for maintaining the US international competitive edge and leadership in science, technology, engineering, and mathematics (STEM). Advances in HPC are vital to national interests by providing infrastructure for scientific discovery that improves the national health, prosperity, welfare, and defense. To solve large-scale scientific problems, HPC relies on an increasing number of nodes and components, which makes it likelier for long-running computation to be interrupted with failures before completing. A critical technique to ensure computation completion is checkpointing. Checkpointing allows snapshots of the computation to be saved so that when a failure occurs, computation state can be restored from the last snapshot and continues execution, rather than restarting from the beginning. The research in this project seeks to advance the state-of-the-art checkpointing technique by making it significantly faster and lowering its cost. This project also plans to contribute to the training of future workforce by providing students with exposure to the mechanisms and inefficiencies of current checkpointing mechanisms on NVMM, and the new in-place checkpointing. The project seeks to increase participation of minority and under-represented groups and involves undergraduates in research.<br/><br/>Prior approaches to checkpointing rely on taking a snapshot of the system state (system-level checkpointing) or the application state (application-level checkpointing) and saving it to secondary non-volatile storage. With the advent of non-volatile main memory (NVMM), a new approach to checkpointing becomes possible.  In contrast to traditional approaches to checkpointing that rely on storing separate snapshots in a separate secondary storage, the project uses a new approach where checkpoints can be constructed in-place in the NVMM  utilizing the working data structures used by the applications. This allows only very minimal additional state beyond what the program already saves to memory, making checkpointing significantly faster and incurring lower cost, in turn providing further HPC scaling.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1657175","CRII: SHF: ACI: Performance-in-Depth Sparse Solvers for Heterogeneous Parallel Platforms.","CCF","CRII CISE Research Initiation","02/15/2017","07/07/2018","Maryam Mehri Dehnavi","NJ","Rutgers University New Brunswick","Standard Grant","Almadena Chtchelkanova","01/31/2020","$175,000.00","Maryam Mehri Dehnavi","maryam.mehri@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","026Y","7942, 8228","$0.00","Sparse numerical computations are at the heart of many science and engineering simulations. However, the complex irregularities in sparse methods limit the performance of many scientific software. This project integrates mathematical reformulation, algorithm redesign, and performance engineering to develop high-performance sparse solvers for heterogeneous parallel platforms. The outcomes of this research are innovative tools and methodologies that advance the field of large-scale scientific simulations. In addition, the project has a broader impact in training graduate students to perform interdisciplinary research.<br/> <br/>The project conducts an in-depth investigation of performance bottlenecks in sparse solvers and reformulates their standard variants to deliver end-to-end performance. Cross-layer solutions are developed to improve data locality, reduce communication, and increase inherent parallelism in sparse linear solvers. The solutions involve multi-level algorithm restructuring and performance tuning to significantly improve the scalability and performance of sparse computations while preserving their numerical accuracy, convergence, and stability. The proposed methods and algorithms are implemented as domain-specific high-performance software and a benchmark suite to promote iterative improvements of the developed algorithms and codes."
"1850404","CRII: CIF: Robust, Principled, and Practical Adaptive Sampling with Mobile Sensors","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2019","01/31/2019","John Lipor","OR","Portland State University","Standard Grant","Phillip Regalia","08/31/2021","$174,990.00","","lipor@pdx.edu","1600 SW 4th Ave","Portland","OR","972070751","5037259900","CSE","7797","7797, 7935, 8228","$0.00","A fundamental challenge to modern science and engineering is that of rapidly and accurately sensing the environment. Harmful algae blooms impair access to drinking water, wildfires present a persistent threat to safety in the western United States, and traffic and industry-related air pollutants pose a risk to growing urban populations. To combat these issues, researchers are turning increasingly to mobile sampling devices to provide safe, persistent estimation of environmental phenomena. However, given the vast spatial regions these devices are tasked with covering, a key open problem is that of designing sampling strategies to provide the highest quality of information given the available resources. This project aims to enable mobile sampling devices to perform efficient, autonomous monitoring of such phenomena, thereby permitting scientists and other concerned actors to make informed, data-driven decisions. The project also facilitates undergraduate involvement through the deployment of the developed algorithms on an unmanned aerial system, as well as workshops to engage underrepresented community college students in the project goals.<br/><br/>A key task in environmental sensing is that of determining where a phenomenon lies above a certain threshold (for example, regions where the pollution level is above a safe limit), a problem known as level-set estimation (LSE). Existing LSE algorithms fall short by either (1) failing to incorporate sampling costs associated with mobile sensors, (2) lacking theoretical guarantees, or (3) relying on strong modeling assumptions. The technical aim of this project is the development of practical adaptive sampling algorithms for LSE with well-understood theoretical properties. The first thrust of the project will consider the problem of estimating the change point of a step function in one dimension, drawing on connections between active learning and robotic path planning to develop and analyze algorithms capable of incorporating previously-ignored costs and handling noisy measurements. The second thrust will consider the two-dimensional LSE problem directly via a graph-based approach. By combining recent techniques from graph-based active learning with Markov decision processes and reinforcement learning, realistic sampling costs based on the distance traveled, the number of measurements taken, and the need to return to a base station for battery recharging will be incorporated.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763399","SHF: Medium: DeepSEA: A Language for Programming and Synthesizing Certified Software","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","04/23/2018","Zhong Shao","CT","Yale University","Continuing grant","Anindya Banerjee","05/31/2022","$388,503.00","Ruzica Piskac, Vilhelm Sjoberg","zhong.shao@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7798","7924, 7943","$0.00","Building certifiably reliable and secure software is one of the grand challenges facing today's computing community. Despite the extensive progress in programming languages in the past few decades, today's mainstream operating systems and hypervisors are still written in C-like low-level languages.  There seems to be an inherent conflict between high-level formal reasoning and low-level systems programming: the former relies on a rich theory at a high abstraction level while the latter must manipulate and manage low-level effects and hardware resources.  The chief novelties of this project are (1) to design and implement a new language (named DeepSEA) that can be used to tackle this inherent conflict and directly program and synthesize certified software, and (2) to develop a DeepSEA toolchain and apply it to build certified OS kernels and Ethereum-style smart contracts. The project's impacts are demonstrated in multiple ways. The technology for building certified software will have a profound impact on the software industry and the society in general; it will dramatically improve the reliability and security of many key components in the world's critical infrastructure. The project would catalyze a change in the way computing science is taught at U.S. universities, by pushing new courses on formal methods into the existing curriculum; it would broaden the participation of underrepresented groups and give U.S. students a unique combination of technical training and international experience in this cutting-edge field.<br/><br/>Certified programming is a unique challenge for language design: both operating systems and smart contracts are inherently low-level and effectful, while software verification requires high-level abstractions and pure functions.  Recent projects on OS kernel verification required writing (manually) the actual kernel in a C-like language and a formal specification of the kernel in a proof-assistant language; a large part of the verification effort is then spent on showing that the implementation indeed satisfies the specification.  DeepSEA bridges this chasm automatically---from a single input program, one can derive the relation between abstract data types and bytes, and between functional specification and concrete implementation. Instead of having to choose between high- and low-level languages, DeepSEA can have the best of both.  The DeepSEA language provides native support for layered specification and abstraction refinement, full equational reasoning, a functional model of effects (including concurrency), and effect encapsulation and composition: consequently it directly supports certified programming at multiple abstraction levels.  Using DeepSEA, a programmer need only to write the formal specification of a desirable system; then the DeepSEA compiler will automatically compile the DeepSEA program into a certified artifact consisting of a C program (which is then compiled into assembly by the verified C compiler, CompCert), a Coq specification, and a formal (Coq) proof that the C program satisfies the specification. The project opens up a new space of language designs that can directly support the development of correct-by-construction system software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640075","E2CDA: Type I: Collaborative Research: Energy Efficient Computing with Chip-Based Photonics","CCF","Energy Efficient Computing: fr","09/01/2016","08/07/2018","Benjamin Lev","CA","Stanford University","Continuing grant","Sankar Basu","08/31/2019","$241,653.00","","benlev@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","015Y","7945","$0.00","Amidst today's data explosion, demand for computing power is accelerating, and the energy requirements for solving critical problems in science, engineering, business, and intelligent processing are increasing dramatically.  In order to address this critical issue, the scientific and engineering community is beginning to explore new approaches to computing, such as mimicking the brain's structure or the dynamical behavior of coupled particles.  However, implementing these approaches with conventional computer architectures is highly inefficient from both energy and computing standpoints. As a result, there is a pressing need to develop revolutionary computer architectures that overcome these severe roadblocks. An ambitious program is to be pursued within the framework of this project in which light, rather than electrons, is used to realize new computing paradigms with superior energy scalability. Specifically, computing architectures will be explored that exploit the wave nature of light (i.e., its amplitude and phase) harnessing the remarkable advancements over the past decade in nanofabrication of complex photonic chips with thousands of high-performance devices. These photonic platforms would have the potential to be computationally powerful, operate with unparalleled energy efficiency, and are scalable and highly reconfigurable.<br/><br/>To fulfill this vision of energy efficient photonic computing, the proposed research efforts will focus on the following two types of photonic processors: (1) Ising Machine and (2) Neuromorphic Computing Machine.  The unifying aspect of these photonic processors is that they consist of dynamic networks of coupled photonic units and rely on the wave nature of light to solve problems which has no analogy in electron-based computing systems. Beyond developing new types of optical processors, photonics technology will be developed as the cornerstone of future computer systems.  A novel architecture will be explored that integrates photonic processors and interconnects with electronic memory and processors to maximize the benefits of each technology. Research will also be undertaken to map complete, challenging problems to combinations of photonic accelerators and electronic processors, and to determine how to best scale them to maximize full-system performance.  By innovating at all levels of the system - from devices to architectures including systems, compilers, and algorithms -  the project would aim to achieve advances that cannot be realized within any single field."
"1136993","The Computing Community Consortium II","CCF","INFORMATION TECHNOLOGY RESEARC","10/01/2012","09/13/2018","Gregory Hager","DC","Computing Research Association","Cooperative Agreement","Nina Amla","09/30/2019","$8,374,862.00","Fred Schneider, Susan Graham, Anita Jones, Edward Lazowska, Andrew Bernat, Elizabeth Mynatt, Ann Drobnis","hager@cs.jhu.edu","1828 L St., NW","Washington","DC","200360000","2022662949","CSE","1640","1640, 7798, 9218, HPCC","$0.00","The Computing Community Consortium (CCC) is a catalyst and enabler for the computing research community. Its various activities strive to unite the community to contribute to shaping the future of the field; provide leadership for the community, facilitating revolutionary, high-impact research; encourage the alignment of computing research with pressing national priorities and national challenges (many of which cross disciplines); give voice to the community, communicating to a broad audience the myriad ways in which advances in computing will create a brighter future; and grow new leaders for the field as a whole. The CCC operates under a Cooperative Agreement between the National Science Foundation and the Computing Research Association (CRA), a membership organization of over 200 computing research entities in academia, industry, and government.<br/><br/>During the founding years of its existence, the activities of the CCC had a significant impact on the status, direction, and prospects of the computing research community, catalyzing new Federal initiatives in robotics and big data to name a few. Opportunities during this second phase of the CCC are every bit as great. The CCC is an investment that promises to pay off in important ways for the field and for the nation."
"1640012","E2CDA: Type I: Collaborative Research: Energy Efficient Computing with Chip-Based Photonics","CCF","Energy Efficient Computing: fr","09/01/2016","06/26/2018","Marin Soljacic","MA","Massachusetts Institute of Technology","Continuing grant","Sankar Basu","08/31/2019","$635,460.00","Dirk Englund, Daniel Sanchez Martin","marin@alum.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","015Y","7945","$0.00","Amidst today's data explosion, demand for computing power is accelerating, and the energy requirements for solving critical problems in science, engineering, business, and intelligent processing are increasing dramatically.  In order to address this critical issue, the scientific and engineering community is beginning to explore new approaches to computing, such as mimicking the brain's structure or the dynamical behavior of coupled particles.  However, implementing these approaches with conventional computer architectures is highly inefficient from both energy and computing standpoints. As a result, there is a pressing need to develop revolutionary computer architectures that overcome these severe roadblocks. An ambitious program is to be pursued within the framework of this project in which light, rather than electrons, is used to realize new computing paradigms with superior energy scalability. Specifically, computing architectures will be explored that exploit the wave nature of light (i.e., its amplitude and phase) harnessing the remarkable advancements over the past decade in nanofabrication of complex photonic chips with thousands of high-performance devices. These photonic platforms would have the potential to be computationally powerful, operate with unparalleled energy efficiency, and are scalable and highly reconfigurable.<br/><br/>To fulfill this vision of energy efficient photonic computing, the proposed research efforts will focus on the following two types of photonic processors: (1) Ising Machine and (2) Neuromorphic Computing Machine.  The unifying aspect of these photonic processors is that they consist of dynamic networks of coupled photonic units and rely on the wave nature of light to solve problems which has no analogy in electron-based computing systems. Beyond developing new types of optical processors, photonics technology will be developed as the cornerstone of future computer systems.  A novel architecture will be explored that integrates photonic processors and interconnects with electronic memory and processors to maximize the benefits of each technology. Research will also be undertaken to map complete, challenging problems to combinations of photonic accelerators and electronic processors, and to determine how to best scale them to maximize full-system performance.  By innovating at all levels of the system - from devices to architectures including systems, compilers, and algorithms -  the project would aim to achieve advances that cannot be realized within any single field."
"1640108","E2CDA: Type I: Collaborative Research: Energy Efficient Computing with Chip-Based Photonics","CCF","Energy Efficient Computing: fr","09/01/2016","07/20/2018","Alexander Gaeta","NY","Columbia University","Continuing grant","Sankar Basu","08/31/2019","$1,145,616.00","Luca Carloni, Michal Lipson, Keren Bergman","a.gaeta@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","015Y","7945","$0.00","Amidst today's data explosion, demand for computing power is accelerating, and the energy requirements for solving critical problems in science, engineering, business, and intelligent processing are increasing dramatically.  In order to address this critical issue, the scientific and engineering community is beginning to explore new approaches to computing, such as mimicking the brain's structure or the dynamical behavior of coupled particles.  However, implementing these approaches with conventional computer architectures is highly inefficient from both energy and computing standpoints. As a result, there is a pressing need to develop revolutionary computer architectures that overcome these severe roadblocks. An ambitious program is to be pursued within the framework of this project in which light, rather than electrons, is used to realize new computing paradigms with superior energy scalability. Specifically, computing architectures will be explored that exploit the wave nature of light (i.e., its amplitude and phase) harnessing the remarkable advancements over the past decade in nanofabrication of complex photonic chips with thousands of high-performance devices. These photonic platforms would have the potential to be computationally powerful, operate with unparalleled energy efficiency, and are scalable and highly reconfigurable.<br/><br/>To fulfill this vision of energy efficient photonic computing, the proposed research efforts will focus on the following two types of photonic processors: (1) Ising Machine and (2) Neuromorphic Computing Machine.  The unifying aspect of these photonic processors is that they consist of dynamic networks of coupled photonic units and rely on the wave nature of light to solve problems which has no analogy in electron-based computing systems. Beyond developing new types of optical processors, photonics technology will be developed as the cornerstone of future computer systems.  A novel architecture will be explored that integrates photonic processors and interconnects with electronic memory and processors to maximize the benefits of each technology. Research will also be undertaken to map complete, challenging problems to combinations of photonic accelerators and electronic processors, and to determine how to best scale them to maximize full-system performance.  By innovating at all levels of the system - from devices to architectures including systems, compilers, and algorithms -  the project would aim to achieve advances that cannot be realized within any single field."
"1649242","EAGER:   Deep Learning for Microarchitectural Prediction","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/27/2016","Daniel Jimenez","TX","Texas A&M Engineering Experiment Station","Standard Grant","Yuanyuan Yang","07/31/2019","$150,000.00","","djimenez@cse.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7798","7916, 7941","$0.00","Computer programs often have highly predictable behavior.  Microprocessors use predictors to improve program performance and efficiency. Decisions made by a program can often be predicted with good accuracy, and patterns of data usage can be predicted to improve system efficiency and performance. However, incorrect predictions can lead to poor performance or lost opportunities for improving efficiency. This project proposes to use deep learning to improve prediction in microprocessors. Deep learning is a technology that has been used to improve computer vision and other pattern recognition tasks in large computing systems, but so far it has not been applied at the very small scale and tight timing margins of improving microprocessors. The project will likely result in improved microprocessors, as well as educational, mentoring, and career opportunities for under-represented groups in computer science. The PI will incorporate the research into classroom teaching. The Ph.D. students trained through this project will enhance industrial and academic workforce. The PI will continue to recruit women and minority graduate students into his research program for this project. Outreach to under-represented groups will include PI leadership and participation at CRA-W mentoring workshops for women and minority graduate students.<br/><br/>The goal of the proposed research is to exploit deep learning to design new microarchitectural predictors capable of exploiting previously untapped levels of predictability in program behavior to improve performance, power, and energy. Deep neural networks will be used to greatly improve the accuracy of microarchitectural predictors. This project will first explore latency-tolerant cache locality predictors, then move to control-flow prediction that has tighter timing constraints. Proposed predictors will be evaluated in a variety of contexts representing modern workloads at scales from mobile phones to datacenters. The research incurs a high-risk because no deep neural network has even been developed to operate at the sub-nanosecond level. However, the research offers a high-payoff due to the tremendous potential to improve performance. Results will be manifested through students' theses and dissertations as well as publication in top-tier architecture venues."
"1813188","AF: Small: New Directions in Algorithmic Game Theory","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/22/2018","Tim Roughgarden","CA","Stanford University","Standard Grant","Tracy J. Kimbrel","05/31/2021","$451,617.00","","tar2147@columbia.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","7923, 7926, 7932","$0.00","The goal of this research project is to develop new results in theoretical computer science that are useful for reasoning about fundamental economic problems.  For example, consider the problem of selling items in an auction --- such as collectables on online auction websites, wireless spectrum to telecommunication companies in a government auction, or sponsored links to advertisers through a search engine auction.  How can a seller use past experience to design better auctions?  To what extent can simple and practical solutions substitute for complex but theoretically justified solutions? Another example of a fundamental economic problem is fair division (as seen, for example, when dividing an estate among its heirs).  What's the most sensible way of defining a ""fair outcome,"" and how much work is required to find one? This research project will develop theory to help answer all of these questions.  The research project also involves the mentoring of PhD students, innovation in graduate teaching, the dissemination of lecture videos and notes for graduate courses, and the teaching of algorithmic fundamentals through massive online open courses.<br/><br/>The specific goals of the project are divided into four categories.  The first set of goals seeks relatively simple solutions to economic design problems that provably approximate the theoretical optimum. The project includes two new directions for this area: the design of simple contracts for principal agent settings, and the design of resale-proof revenue-maximizing auctions.  The second set of goals investigates the question of how to best learn near-optimal auctions from data, focusing on the important but unresolved algorithmic and strategic issues of the problem.  The third set of goals develops complexity-theoretic barriers in economics, with specific applications to equilibria in markets with divisible goods, to extended formulations for the polytope of interim allocation rules, and to the inherent complexity of optimal prior-free digital goods auctions. The fourth set of goals concern fair division, where the goal is to distribute resources among competing players in a ""fair"" way, with a focus on the multi-party communication complexity of the problem with indivisible items.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740519","AF: Medium: Collaborative Research: Hardness in Polynomial Time","CCF","ALGORITHMIC FOUNDATIONS","01/20/2017","08/04/2017","Virginia Williams","MA","Massachusetts Institute of Technology","Continuing grant","Tracy J. Kimbrel","08/31/2019","$453,050.00","","virgi@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7924, 7926","$0.00","A central endeavor of theoretical computer science is to classify computational problems according to the resources (such as running time and storage space) needed to solve them.Although the field of algorithm design has been highly successful in discovering efficient, polynomial-time algorithms for problems of practical interest, little evidence has been shown for the optimality of most algorithms.The goal of this project is to build a useful complexity theory for the class of polynomial-time solvable problems (called P), by proving equivalences between problems and proving conditional lower bounds on specific problems, assuming the validity of certain plausible mathematical conjectures.<br/><br/>Known lower bounds for specific problems in P are conditioned on some complexity-theoretic assumption such as the (Strong) Exponential Time Hypothesis (concerning the complexity of k-CNF-SAT), the conjecture that dense all-pairs shortest paths (APSP) requires cubic time, or that 3SUM requires quadratic time.The goals of this project are threefold.The first goal is to establish conditional lower bounds on problems in diverse areas (such as graph optimization, string matching, geometry, and dynamic data structures) using standard hardness conjectures. The second goal is to search for better hardness conjectures that are both plausible and versatile, and to discover relationships (implications or equivalences) between nominally unrelated conjectures.The last goal is to investigate the plausibility of these conjectures by attempting to disprove them.<br/><br/>The curricular portion of this project involves developing lecture material suitable for introductory algorithms and complexity courses at both the undergraduate and graduate level."
"1553428","CAREER: Fast Graph Algorithms and Continuous Optimization","CCF","ALGORITHMIC FOUNDATIONS","01/01/2016","03/05/2018","Aleksander Madry","MA","Massachusetts Institute of Technology","Continuing grant","Rahul Shah","12/31/2020","$323,438.00","","madry@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","1045, 7926","$0.00","Graphs are at the heart of computer science and many other fields. They model various complex systems, such as the structure of web links, social networks, road systems, protein interactions, and the brain. Also, many elements of our everyday life, such as web search, driving directions, online advertising and parcel delivery, crucially rely upon graph algorithms. Despite this central role and extensive research on graphs, however, some of the most prominent questions in algorithmic graph theory remain unresolved. Additionally, in the era of big data, the graphs that we have to deal with are often massive - they have hundreds of billions of edges - which renders many classic graph algorithms completely infeasible.<br/><br/>The goal of this project is to address these challenges by augmenting our traditional graph-algorithmic techniques with methods borrowed from continuous optimization. In particular, the PI aims to forge a unified toolkit that is powerful enough to make progress on a variety of fundamental graph problems. An integral part of this effort will be to broadly disseminate the acquired insights. Dissemination will encompass undergraduate and graduate curriculum development, production of freely available lecture notes/survey, as well as training students, of all levels, to equip them with the skill set that is essential for tackling modern graph-algorithmic challenges.<br/><br/>From a more technical point of view, the main focus of this project will be to combine the classic combinatorial approaches of algorithmic graph theory with core continuous-optimization primitives, such as gradient descent methods and interior-point methods, to break longstanding running time barriers for a variety of fundamental flow and matching questions. Additionally, the PI expects that this work will shed new light on one of the central challenges of mathematical programming: the convergence behavior of interior-point methods, and will also lead to novel extensions of that optimization framework."
"1718494","CIF: Small: Capacity via Symmetry","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2017","06/28/2017","Henry Pfister","NC","Duke University","Standard Grant","Phillip Regalia","07/31/2020","$514,176.00","Galen Reeves","henry.pfister@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7797","7923, 7935, 9251","$0.00","Information theory studies the fundamental laws that govern information processing systems. For instance, the Shannon Capacity of a noisy channel is the largest rate (in bits per channel use) that bits can be reliably communicated. Information theory has had an enormous impact on how information is acquired, processed, compressed, and transmitted. Examples include channel coding and data compression, both of which are used extensively in Internet and cellular communications. This project focuses on connections between performance and codebook symmetry. One part focuses on using symmetry to help low-complexity decoders achieve near-optimum performance for codes relevant to 5G cellular standards. This project will also train several graduate students, hence an important broader impact of this project is the production of highly-trained workers in electrical engineering and computer science.<br/><br/>Many theoretical results in information theory use random codebooks to encode messages. Researchers have long sought families of deterministic algebraic codes that provably achieve capacity. In a recent breakthrough, the PI and his coauthors showed that sequences of sufficiently symmetric codes achieve capacity on the binary erasure channel. Since Reed-Muller codes satisfy the required symmetry condition, we now know that such a code family was discovered by Muller in 1954, only 6 years after Shannon's introduction of channel capacity! The project focuses on whether symmetric codebooks can be used in more general settings to approach information-theoretic limits. In particular, the goals are to determine: (i) How general is the phenomenon of capacity via symmetry? (ii) What performance gains can be achieved in practice by exploiting symmetries? (iii) Can symmetry be utilized to approach the information-theoretic limits of other problems?"
"1714672","CCF-BSF: CIF: Small: Identification and Isolation of Malicious Behavior in Multi-Agent Optimization Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","07/15/2017","07/10/2017","Anna Scaglione","AZ","Arizona State University","Standard Grant","Phillip Regalia","06/30/2020","$180,000.00","Angelia Nedich","Anna.Scaglione@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7797","7923, 7937, 9102","$0.00","Harnessing effectively the power of cloud computing and the benefits of ubiquitous data collection requires parallel advances in parallel algorithms, also known as multi-agent optimization. Unfortunately, these methods are vulnerable to cyber-attacks: if one or more of the platforms used is compromised and spreads incorrect values to other servers, the answers produced will be incorrect. It is not always possible to prevent this kind of attacks from occurring by relying on authentication alone. <br/><br/>This project studies the vulnerabilities that exist in parallel algorithms, with the intent of understanding how to detect dysfunctional agents, isolate those that are compromised and restore the system functionality. The study of vulnerabilities in decentralized computation has important ramifications that go beyond the engineering discipline. In social science multi-agent optimization is used as a model for social learning. The study of malicious behavior will capture the effect of zealots in social settings that inject false information, steering the outcome of collective decisions towards specific actions that favors their interests."
"1442735","CyperSEES: Type 2: Integrative Sensing and Prediction of Urban Water for Sustainable Cities","CCF","CyberSEES","10/01/2014","08/25/2014","Dong-Jun Seo","TX","University of Texas at Arlington","Standard Grant","Rahul Shah","09/30/2019","$1,196,295.00","Michael Zink, Xinbao Yu, Zheng Fang, Jean Gao","djseo@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","8211","8208","$0.00","Many cities face tremendous water-related challenges due to urban population growth and climate fluctuations. Even moderate rainfall can quickly fill and overflow urban water reserves. Urban areas are particularly susceptible not only to excesses and shortages of water but also to variations in water quality. This project protects urban areas from the shocks of extreme precipitation cycles and urbanization by advancing our understanding of the urban water cycle through the integration of advanced computing and cyber-infrastructure, environmental modeling, geoscience, and information science. <br/>This project utilizes high-resolution precipitation information from the network of Collaborative Adaptive Sensing of the Atmosphere (CASA) radars available in the Dallas-Fort Worth area, crowdsourced water observations for ubiquitous sensing of surface water over a large urban area, and new innovative wireless sensors for water quantity, water quality and soil moisture to close the observation gaps. Cloud computing is then used for advanced high-resolution modeling, data optimization, and predictive analytics to assess water quantity and quality in both the short and long term. This project advances our understanding of urban sustainability and the associated challenges through environmental, social and economic responses of a large city as an uncertain dynamic system."
"1514383","AF:  Medium: Collaborative Research: Hardness in Polynomial Time","CCF","ALGORITHMIC FOUNDATIONS","09/01/2015","08/02/2017","Seth Pettie","MI","University of Michigan Ann Arbor","Continuing grant","Tracy J. Kimbrel","08/31/2019","$599,945.00","","pettie@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7796","7924, 7926","$0.00","A central endeavor of theoretical computer science is to classify computational problems according to the resources (such as running time and storage space) needed to solve them.  Although the field of algorithm design has been highly successful in discovering efficient, polynomial-time algorithms for problems of practical interest, little evidence has been shown for the optimality of most algorithms.  The goal of this project is to build a useful complexity theory for the class of polynomial-time solvable problems (called P), by proving equivalences between problems and proving conditional lower bounds on specific problems, assuming the validity of certain plausible mathematical conjectures. <br/><br/>Known lower bounds for specific problems in P are conditioned on some complexity-theoretic assumption such as the (Strong) Exponential Time Hypothesis (concerning the complexity of k-CNF-SAT), the conjecture that dense all-pairs shortest paths (APSP) requires cubic time, or that 3SUM requires quadratic time.  The goals of this project are threefold.  The first goal is to establish conditional lower bounds on problems in diverse areas (such as graph optimization, string matching, geometry, and dynamic data structures) using standard hardness conjectures. The second goal is to search for better hardness conjectures that are both plausible and versatile, and to discover relationships (implications or equivalences) between nominally unrelated conjectures.  The last goal is to investigate the plausibility of these conjectures by attempting to disprove them. <br/><br/>The curricular portion of this project involves developing lecture material suitable for introductory algorithms and complexity courses at both the undergraduate and graduate level."
"1619452","CIF:Small:Collaborative Research:Statistics of slow mixing Markov processes: theory and applications to community detection","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/10/2016","Narayana Santhanam","HI","University of Hawaii","Standard Grant","Phillip Regalia","05/31/2019","$499,468.00","Aleksandar Kavcic","nsanthan@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","CSE","7797","7923, 7935, 9150","$0.00","Historically, the field of statistics developed around multiple independent observations of phenomena. Yet progress in modern applications from biology to social networks increasingly requires deeper understanding of observations that are dependent. Dependent observations introduce biases unseen in independent sampling. If not interpreted properly, they often lead to misleading conclusions. This research develops a statistical framework to interpret samples generated by dependent observations. The problem is then turned around to develop algorithms that detect meaningful dependencies in data. Such algorithms are used, for example, to identify genes working together, or find the extent to which social networks are polarized on certain issues.  Not only is this research translated to classes, demonstrations and outreach programs, but also the location in Hawaii is leveraged to reach out to underrepresented communities in science and engineering, in particular, women and the Pacific Islander community.<br/><br/>Stationary properties of Markov processes may not be very well reflected by finite samples, no matter how large the sample size. This bias is often formalized as mixing, and is unseen in independent sampling.  This research analyzes Markov samples even before the samples reflect the asymptotic stationary properties, and develops a framework for inference, analysis and simulation of slow mixing Markov processes. Then, algorithmic primitives based on coupling from the past are developed for the widely studied task of community detection in a graph, where vertices are partitioned into clusters so that intra-cluster vertices are connected tighter than those in disparate clusters. As several resampling procedures in machine learning such as cross validation and bootstrap are premised on independent sampling and have no good analogs in the Markov setting, this research develops new resampling approaches for potentially slow mixing Markov processes."
"1453853","CAREER: Energy-Efficient and Energy-Proportional Silicon-Photonic Manycore Architectures","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2015","05/07/2018","Nikos Hardavellas","IL","Northwestern University","Continuing grant","Sankar Basu","04/30/2020","$368,957.00","","nikos@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7798","1045, 7945","$0.00","Increasing energy demands have put computing on an unsustainable technological, economic and environmental path. Unfortunately, a large fraction of this energy is wasted, with data transfers being one of the major contributors to energy consumption. At the same time, while the demand for computing grows, modern microprocessors are increasingly constrained by physical limitations, which prevent them from realizing their full potential. Area, power, thermal, off-chip bandwidth, and yield limitations constrain single-chip designs to a relatively small number of cores, beyond which scaling becomes impractical.  Multi-chip designs can overcome these limitations, but require a cross-chip interconnect with bandwidth, latency, and energy efficiency characteristics well beyond the reach of conventional electrical signaling.  Introduction of nano-photonic interconnects, as undertaken in this proposal, can meet these requirements and allow systems to break free of the limitations of single-chip designs. <br/><br/>Within the context of this research, a rigorous educational plan is also integrated into the research agenda that strongly connects research to education, and enhances the participation of minorities and undergraduates in research. This project capitalizes on existing collaborations with the Searle Center for Teaching Excellence at Northwestern University to implement innovative educational approaches, Northwestern?s Science in Society outreach initiatives for the general public, and Northwestern?s Office of STEM Education Partnerships to develop K-12 STEM outreach activities with outreach potential extending to 140+ schools in the Chicago metropolitan area, reaching 368 teachers and 30,000 students.<br/><br/>Specific technical aspects of this research aims to develop scalable, energy-efficient, and energy-proportional interconnects for future multicores. To achieve this vision, the research seeks to understand and mitigate the energy inefficiencies of the dominant power consumers in silicon-photonics. The project involves a cross-cutting approach to combine developments in novel materials, emerging devices, and 3D-stacking with research in architectural and micro-architectural techniques, memory systems, the runtime environment, and the operating system, to develop adaptive techniques that minimize the energy consumed by nano-photonic interconnects without sacrificing their performance. The overall effort culminates on the design of a virtual macro-chip, a disaggregated many-core design supported by a silicon-photonic interconnect that reaches scales of thousands of cores, at a performance and power level impossible to realize with conventional technology."
"1755847","CRII: AF: Towards Faster Algorithms for Large-scale Constrained Optimization","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","10/01/2018","08/31/2018","Ruoyu Sun","IL","University of Illinois at Urbana-Champaign","Standard Grant","Tracy J. Kimbrel","09/30/2020","$175,000.00","","ruoyus@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","2878, 7796","7933, 8228","$0.00","With the ever-growing amounts of data collected by ubiquitous sensors in today's world, there is an increasing need for efficient methods that can solve large-scale optimization problems. In the past few years, much effort has been spent on solving large-scale unconstrained optimization problems, where the decision variables are free (unconstrained). However, in a wide variety of applications, the decision variables must satisfy certain constraints, which could be physical constraints and/or the needs of the system designer. Classical methods such as interior-point methods can solve small- to medium-size constrained problems quite well, but for large-scale problems, they are unsuitable due to high per-iteration costs and high storage requirements. This project aims to design and analyze efficient algorithms for solving large-scale constrained optimization problems. This research will significantly broaden the current understanding of large-scale constrained optimization, and offer a new computational toolbox to practitioners in various fields such as computer science, engineering, healthcare, and economics.  <br/><br/>This research combines successful ideas in large-scale unconstrained optimization such as block decomposition with the special structure of constrained optimization. The emphasis is on the fundamental understanding of the convergence behavior of the algorithms. The investigator explores this approach by (1) designing efficient block decomposition versions of classical constrained optimization methods such as augmented Lagrangian multiplier methods and (2) developing a new convergence analysis framework for large-scale constrained optimization and analyzing the convergence behavior of the proposed methods. The investigator aims to apply the proposed methods to various application problems such as Markov decision processes, neural networks, and structured sparsity problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1719047","SHF:Small:New models, design, and test methods for long-term aging of nanometer VLSI","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/11/2017","Sandeep Gupta","CA","University of Southern California","Standard Grant","Sankar Basu","07/31/2020","$440,000.00","","sandeep@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","7923, 7945","$0.00","Transistor aging refers to the phenomenon that a transistor degrades with use over time. Hence, as digital chips are used, transistor aging reduces their performance and increases power consumption. Aging also causes some chips to fail prematurely during their expected lifetimes (called lifetime failures). Since such failures can corrupt user data, cause expensive system downtime, and require expensive maintenance, industry practice requires lifetime failure rates to be of the order of 10 to 100 parts per million. This project will develop completely new methods and CAD tools for design and testing of digital chips to combat aging in uniquely efficient ways. These new methods and tools will dramatically improve yield, dramatically reduce power, and provide chips with extremely low lifetime failure rates, even as aging continues to grow in severity as we approach the end of Moore's Law. In turn, the society at large will reap significant benefits of this research, since lower cost digital systems with low lifetime failure rates will help improve many essential services, especially health, security, and finance. In addition, this project will train students and industry experts in the art and science of aging and its mitigation and prepare them for the era near and beyond the end of Moore's Law. Also, significant outreach effort will ensure that undergraduate students and students from groups underrepresented in STEM will participate in this research.<br/><br/>This project has identified and will address three major limitations of existing aging research: serious inaccuracies of existing models in estimating long-term aging for real-life chip usage, the inability of existing memory designs to combat the most common type of memory aging, namely asymmetric aging, and the fact that existing approaches for post fabrication chip testing either provide extremely low yields or unacceptability high lifetime failure rates due to aging. Specifically, this project will develop completely new methods and tools for aging, demonstrate their effectiveness via extensive simulation studies and experiments on test chips, disseminate results and share new tools with academic and industry experts, and train students."
"1718428","SHF:Small: Collaborative Research: Exploring 3-Dimensional Integration Strategies of STTRAM","CCF","ELECT, PHOTONICS, & MAG DEVICE, COMMS, CIRCUITS & SENS SYS, SOFTWARE & HARDWARE FOUNDATION","08/01/2017","05/03/2018","Rashmi Jha","OH","University of Cincinnati Main Campus","Standard Grant","Sankar Basu","07/31/2020","$240,999.00","","jhari@ucmail.uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","1517, 7564, 7798","7923, 7945, 9102, 9251","$0.00","The development of high-density memory is fueled by recent advancements in several critical areas ranging from big data analytics, Internet-of-Things (IoT), wearable electronics, smart phones, assistive devices, cybersecurity to weather tracking systems. Currently available conventional memory technologies face serious challenges in addressing these needs due to increased energy consumption, scalability limitations, and limited bandwidth. The objective of this proposal is to develop novel memory devices by enabling 3D integration of spin-torque transfer RAM devices (STTRAM) with appropriate selector diodes (SD). These types of memory devices will be massively scalable, energy-efficient, and fast which will open tremendous opportunities to integrate them in existing memory architectures to enable various futuristic applications. The project will integrate education by incorporating research outcomes in PI's academic courses. Industrial feedback will be sought via collaborations. Under-represented groups will be involved in science and engineering through several outreach programs at Penn State University (PSU) and University of Cincinnati (UC) such as Summer Research Opportunities Program (SROP) at PSU, and Emerging Ethnic Engineers (E3) program at UC which provides opportunities to the high-school students from the inner-city schools with economically challenged background to participate in internships and lab-experience activities. Undergraduate students will be involved in research via Research Experiences for Undergraduates (REU) program at both the universities. Dissemination of results will be accomplished by publications in high-impact scientific journals, conference presentations, and YouTube lecture videos. <br/><br/>The intellectual merit of the project is in understanding the integration compatibility of STTRAM devices with SD and developing highly scalable 3D crossbar arrays of memory technologies based on integrated STTRAM and SD. The objectives will be achieved by executing the following specific aims: (i) modeling and simulation of STTRAM-SD arrays, (ii) resilience analysis and optimization studies to minimize the impact of device-level variabilities on performance metrics, (iii) optimization of STTRAM-SD architectures for high-performance computing and IoT, (iv) designing, fabrication, testing, and modeling of novel SD devices based on energy band-engineered and doping-engineered transition metal oxide and electrode stacks, (v) designing and fabrication of small-size STTRAM-SD arrays, electrical testing and modeling to benchmark simulations and experimental results. It is anticipated that the successful completion of this project will lead to a fundamental understanding of the compatibility of STTRAM with SD and provide a platform-technology to develop high-density memories which will have transformative impact on commercializing and advancing the futuristic applications."
"1761506","CIF: Medium: Collaborative Research: Nonconvex Optimization for High-Dimensional Signal Estimation: Theory and Fast Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2017","07/18/2018","Yingbin Liang","OH","Ohio State University","Continuing grant","Phillip Regalia","08/31/2021","$153,639.00","","liang.889@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7797","7924, 7936","$0.00","High-dimensional signal estimation plays fundamental roles in various engineering and science applications, such as medical imaging, video and network surveillance. Estimation procedures that maintain both statistical and computational efficacy are of great practical value, which translate into desiderata such as less time patients need to spend in a medical scanner, faster response to cyber attacks, and capabilities to handle very large datasets. While a lot of signal estimation tasks are naturally formulated as nonconvex optimization problems, existing results for nonconvex methods have several fundamental limitations, and the current state of the art is still limited in terms of when, why and which nonconvex approaches are effective for a given problem. The goal of this research program is to significantly deepen and broaden the understanding and applications of nonconvex optimization for high-dimensional signal estimation.<br/><br/>In this project, the investigators will study high-dimensional signal estimation via direct optimization of nonconvex, and potentially nonsmooth, loss functions, without resorting to convex relaxation. This research will explore geometric structures shared by nonconvex functions commonly encountered in signal estimation, and study the fundamental roles these structures play in determining the algorithmic convergence. These results will then be exploited as guidelines to develop fast and provably correct algorithms for estimating high-dimensional signals with physically induced structures and under streaming data observations. Specifically, the research program consists of three major thrusts: (1) understanding the geometric structures of important classes of nonconvex loss surfaces, and characterizing their impact on the convergence of optimization algorithms; (2) developing fast algorithms and the associated theory for the recovery of structured low-rank matrices; (3) designing new online algorithms that are time and space efficient under a streaming setting, with the capability of detecting and tracking the time-varying signals of interest."
"1813910","CIF: Small: Collaborative Research: Signal Processing for Nonlinear Diffractive Imaging: Acquisition, Reconstruction, and Applications","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2018","05/21/2018","Ulugbek Kamilov","MO","Washington University","Standard Grant","Phillip Regalia","06/30/2021","$265,293.00","","kamilov@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7797","7923, 7935, 9251","$0.00","There is a growing need in biomedical research to observe biological structure and processes on the length scales smaller than 100nm. Conventional optical systems cannot effectively provide such information, however, due to the infamous diffraction limit. The formulation of the diffraction limit fundamentally relies on the presumed linearity in the interaction between the illuminating light and the object. The goal of this project is to overcome the diffraction limit by exploiting nonlinearities from a modern signal processing perspective. This project will lay out theoretical, algorithmic, and instrumentation foundation for nonlinear diffractive imaging. The outcome of this project can open up many possibilities of high-resolution imaging in highly scattering media that are ubiquitous in science and medicine, including histology, cytometry, brain mapping, and drug discovery. The proposed activities will also promote education through the training of students in a wide range of topics within signal processing, computational imaging, and optics, and through related curriculum development efforts where the basic themes of the proposed research will be incorporated in courses taught by the investigators.<br/><br/>The focus of this research is to develop computational imaging methods that fully harness the information encoded in two types of nonlinearities: object-light interactions and prior assumptions on the objects. Concretely, the work will focus on: (a) efficient schemes for data acquisition, (b) scalable algorithms for 3D reconstruction, and (c) two novel application platforms for validation. Results from this research will lead to a single, holistic signal processing framework that can maximally extract the information encoded in nonlinear measurements.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1619053","CCF-BSF: CIF: Small: Coding Techniques for Emerging Storage Technologies.","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/28/2016","Paul Siegel","CA","University of California-San Diego","Standard Grant","Phillip Regalia","06/30/2019","$500,000.00","","psiegel@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7797, 7923, 7935","$0.00","Magnetic media and flash memories are currently the main platforms used for mass data storage. As the need for storage capacity grows, new methods are being explored to increase the density in such media, and new nano-scale technologies, such as memristor-based memories, are being developed. This project concerns the analysis of fundamental limits on the information storage capability of these emerging storage technologies, as well as the design of coding schemes to approach those limits. The theoretical component of the project requires new approaches in information theory and discrete mathematics, while the development of new coding techniques extends the reach of the theory into practice, hastening the evolution of new commercial technologies.  The project provides a stimulating research environment for graduate students, and contributes to curriculum development in electrical engineering and computer science. <br/><br/>In the domain of magnetic storage, the research addresses lower and upper bounds on the capacity of grain and Ising channels; the construction of codes that correct errors in these channels; and the extension of these results to non-binary channel models. The research on multilevel flash memories involves computing bounds on the capacity of the one-dimensional inter-cell interference channel; constructing codes that combat inter-cell interference in one, two, and three dimensions; and designing codes that mitigate the wear of cells and the erase-program asymmetry in flash memories. For memristor-based memories, research topics include the design of two-dimensional codes with weight constraints on the rows and columns; the design of fast methods for overwriting a two-dimensional weight-constrained array onto another constrained array; and the study of the notion of Hamming connectedness of constraints."
"1813848","CIF: Small: Collaborative Research: Signal Processing for Nonlinear Diffractive Imaging: Acquisition, Reconstruction, and Applications","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2018","05/21/2018","Lei Tian","MA","Trustees of Boston University","Standard Grant","Phillip Regalia","06/30/2021","$250,707.00","","leitian@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7797","7923, 7935","$0.00","There is a growing need in biomedical research to observe biological structure and processes on the length scales smaller than 100nm. Conventional optical systems cannot effectively provide such information, however, due to the infamous diffraction limit. The formulation of the diffraction limit fundamentally relies on the presumed linearity in the interaction between the illuminating light and the object. The goal of this project is to overcome the diffraction limit by exploiting nonlinearities from a modern signal processing perspective. This project will lay out theoretical, algorithmic, and instrumentation foundation for nonlinear diffractive imaging. The outcome of this project can open up many possibilities of high-resolution imaging in highly scattering media that are ubiquitous in science and medicine, including histology, cytometry, brain mapping, and drug discovery. The proposed activities will also promote education through the training of students in a wide range of topics within signal processing, computational imaging, and optics, and through related curriculum development efforts where the basic themes of the proposed research will be incorporated in courses taught by the investigators.<br/><br/>The focus of this research is to develop computational imaging methods that fully harness the information encoded in two types of nonlinearities: object-light interactions and prior assumptions on the objects. Concretely, the work will focus on: (a) efficient schemes for data acquisition, (b) scalable algorithms for 3D reconstruction, and (c) two novel application platforms for validation. Results from this research will lead to a single, holistic signal processing framework that can maximally extract the information encoded in nonlinear measurements.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814613","AF: Small: Collaborative Research: Matrix Signings and Algorithms for Expanders and Combinatorial Nullstellensatz","CCF","ALGORITHMIC FOUNDATIONS","09/01/2018","05/18/2018","Karthekeyan Chandrasekaran","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rahul Shah","08/31/2021","$249,986.00","","karthe@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7923, 7926","$0.00","This project will investigate spectral properties of graph-related matrices and their signings, which have become fundamental tools in computer science. The spectra of such matrices have had a tremendous impact in numerous areas including machine learning, data mining, web search and ranking, game theory, scientific computing, and computer vision and have influenced several algorithmic innovations. The project will have significant technical as well as educational impacts. The inherent mathematical and algorithmic nature of the project together with the plethora of potential practical applications will bring together researchers from varied areas such as mathematics and network design. The investigators will organize a workshop on Spectral Graph Theory to bring together experts in these areas. The project will support graduate students who will receive mentoring and extensive training in the design and analysis of algorithms. The investigators will also direct special efforts towards fostering diversity through educational activities targeting under-represented groups in STEM disciplines.<br/><br/>In this project, the investigators will design efficient algorithms for constructing various combinatorial structures that are guaranteed to exist through suitable signings of matrices. The combinatorial structures to be studied include expander graphs and several other applications of the algebraic method. Notably, the algorithmic problem of efficiently constructing of expander graphs is at the core of spectral graph theory. This project will develop a comprehensive understanding of the inherent difficulties, as well as propose algorithms for efficiently constructing expander graphs via signed adjacency matrices. Combinatorial Nullstellensatz is a powerful algebraic tool often used to show the existence of certain combinatorial structures. However, the non-constructive nature of its proof has been a barrier towards finding these structures efficiently. Existential proofs based on the algebraic method have resisted progress on the constructive front (unlike those based on probabilistic method). In this project, the investigators will break ground along this direction by obtaining efficient constructive proofs for restricted applications of Combinatorial Nullstellensatz.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812951","SHF: Small: Collaborative Research: RUI: Synchronicity: A Framework for Synthesizing Concurrent Software from Sequential and Cooperative Specifications","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","07/05/2018","Stephen Freund","MA","Williams College","Standard Grant","Nina Amla","09/30/2021","$199,999.00","","freund@cs.williams.edu","880 Main St.","Williamstown","MA","012672600","4135974352","CSE","7798","7923, 8206","$0.00","The nation's computing infrastructure utilizes multicore processors and multiprocessor hardware across the entire spectrum of systems from small mobile devices to huge data centers.  These systems offer increased performance and scaling over single-processor systems, but at a significant cost: writing correct concurrent software is notoriously challenging.  Programmers must take extreme care to orchestrate synchronization between concurrently running threads to avoid unintended interference while simultaneously eliminating synchronization whenever possible to avoid performance bottlenecks.  To address this challenge, this project develops the Synchronicity tool to automatically synthesize high-performance concurrent software from simple specifications of the desired behavior.  This research has the potential to reduce the costs of developing computing infrastructure, by eliminating the costly process of manually writing, testing, and reasoning about concurrent code, and it may reduce the hardware resources and energy required to meet computing needs.  <br/><br/>Synchronicity starts with an initial programmer-provided description of a software component suitable execution on a single thread.  It then uses counterexample-guided inductive synthesis to search for thread-safe concurrent components conforming to that specification. Synchronicity verifies thread safety using an extended form of Lipton's theory of reduction.  Multiple thread-safe concurrent solutions may be found, and Synchronicity automatically ranks according to their performance on a programmer-supplied workload. The project is committed to increasing access to science education for all students, including women, under-represented groups, and first-generation college students. The investigators include students from these groups in this research, at both the undergraduate and graduate level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1525178","SHF:  Small:  Causal Foundations of Statistical Fault Localization","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","06/16/2015","H. Andy Podgurski","OH","Case Western Reserve University","Standard Grant","Sol J. Greenspan","06/30/2019","$497,500.00","","podgurski@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","7798","7923, 7944","$0.00","The goal of this research is to improve the effectiveness of automated techniques that seek to locate the faults in software that caused observed failures (malfunctions) to occur during testing or operational use, so the faults can be repaired.  This goal is important because properly functioning software is critical in business, communications, national security, transportation, science, and many other activities.  The desired improvements are to be achieved by employing methodology that has been developed recently, across several disciplines, to enable the causal effects of various kinds of treatments, exposures, or interventions (e.g., medical treatments) upon outcomes of interest (e.g., diseases) to be estimated accurately and without bias.  If successful, the proposed research has the potential to help software developers to efficiently localize and repair faults in their products, thereby preventing harms such as economic loss, injury, and even death. The research will also help to disseminate sound causal inference methodology in the software engineering community.<br/> <br/>More specifically, the research will investigate and improve the foundations of causal statistical fault localization (CSFL), including the form of causal models, the abstraction of causal states, and the handling of iteration.  A value-based approach to CSFL will be developed, which involves profiling and analyzing the values of program variables, and this will be integrated with predicate-based CSFL, in order to more accurately estimate the failure-causing effects of program elements.  A new approach to CSFL will be explored that employs multilevel statistical models to integrate execution data of different types and granularity levels, both for a given program version and across versions.  Meta-analysis techniques will be applied to the set of suspiciousness scores obtained with CSFL, in order to take account of features of the score distribution and of factors that predict the credibility if individual scores.<br/>Also to be investigated is how the problem of selection bias affects SFL techniques in different settings and how it can be mitigated.  Finally, the research will explore the potential value of case-control methodology for improving the cost-effectiveness of SFL in scenarios where software failures are infrequent."
"1814524","AF: Small: Computational and Geometric aspects of Lattices","CCF","ALGORITHMIC FOUNDATIONS, Secure &Trustworthy Cyberspace","10/01/2018","05/23/2018","Oded Regev","NY","New York University","Standard Grant","Tracy J. Kimbrel","09/30/2021","$499,998.00","","or380@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7796, 8060","7923, 7926, 7927","$0.00","This project focuses on lattice-based cryptography, an area born in the late 1990s, and that has since grown tremendously. Lattices are mathematical objects defined as the the set of all integer combinations of some n linearly independent vectors in n-dimensional Euclidean space. For instance, the set of all integer points in n-dimensional Euclidean space forms a lattice. For n=2, this is the set of all points in the familiar Cartesian plane with integer coordinates. Lattices have attracted the attention of mathematicians for over two centuries, and have an impressive number of applications in mathematics and computer science, from number theory and Diophantine approximation to complexity theory and cryptography. Lattice-based cryptography is unique in that it is believed to be secure against attacks using quantum computers, a feature not shared by any of the traditional cryptographic schemes such as RSA (named for its inventors' initials). Moreover, recent work has shown that lattice-based cryptography is practical, and that it is amazingly versatile, leading to a remarkable number of applications such as fully homomorphic encryption, which allows computation on encrypted data.<br/><br/>The main goal of the project is establishing even stronger foundations for lattice-based cryptography by finding tighter hardness reductions and understanding the mysterious role quantum computing plays in it. The project also includes the development of new classical and quantum algorithms for lattice problems, as well as an investigation into some of the geometrical properties of lattices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1724992","CCF-BSF:SHF: Small: Timing Validation for Asyncronous Circuits","CCF","SOFTWARE & HARDWARE FOUNDATION","01/01/2017","03/24/2017","Rajit Manohar","CT","Yale University","Standard Grant","Sankar Basu","07/31/2019","$350,000.00","","rajit.manohar@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7798","7923, 7945","$0.00","Enabled by research and development in advanced materials, nano-manufacturing, and digital computation, low-cost, high-performance, and low-power electronic components have yielded smart-phones, wireless connectivity, high-throughput networks, inexpensive data centers, to just name a few aspects of our modern digital economy. However, as the current technology approaches limits set by the laws of physics, gains in performance and power efficiency are no longer realizable through conventional techniques. It is critical that a diverse group of students be trained in unconventional approaches, so that they can take new ideas to practice as part of the semiconductor industry and continue the phenomenal growth of the past decades known as the Moore's law. Asynchronous circuits and systems design, to be pursued in this project, is one such unconventional approach being studied as a way to improve computational efficiency. The project also collaborates with the Binational Science Foundation (BSF) of Israel to leverage complementary research expertise.<br/><br/>Automated timing validation is a critical component in physical realization of a digital circuit. Timing validation ensures that the physical implementation of the circuit is consistent with the intent of the designer, in spite of the uncertainties and constraints introduced by the manufacturing process. Two components are necessary for timing validation: a mathematical foundation, and software that realizes the validation process by implementing the mathematics. The goal of this effort is to develop these two components for the timing validation of asynchronous circuits.  The project brings expertise from two different disciplines to bear on this effort: (i) asynchronous circuit design and implementation, and (ii) the theory of asynchronous distributed systems. The project adapts the concept of potential causality from the distributed systems literature to the context of asynchronous circuits. The fusion of insights and techniques from the two disciplines promises to facilitate better design of fast and energy-efficient circuits, as well as improving the techniques for validating and verifying the correctness of systems built from them."
"1813133","SHF: Small: Collaborative Research: Synchronicity: A Framework for Synthesizing Concurrent Software from Sequential and Cooperative Specifications","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","07/05/2018","Cormac Flanagan","CA","University of California-Santa Cruz","Standard Grant","Nina Amla","09/30/2021","$299,962.00","","cormac@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7798","7923, 8206","$0.00","The nation's computing infrastructure utilizes multicore processors and multiprocessor hardware across the entire spectrum of systems from small mobile devices to huge data centers.  These systems offer increased performance and scaling over single-processor systems, but at a significant cost: writing correct concurrent software is notoriously challenging.  Programmers must take extreme care to orchestrate synchronization between concurrently running threads to avoid unintended interference while simultaneously eliminating synchronization whenever possible to avoid performance bottlenecks.  To address this challenge, this project develops the Synchronicity tool to automatically synthesize high-performance concurrent software from simple specifications of the desired behavior.  This research has the potential to reduce the costs of developing computing infrastructure, by eliminating the costly process of manually writing, testing, and reasoning about concurrent code, and it may reduce the hardware resources and energy required to meet computing needs.  <br/><br/>Synchronicity starts with an initial programmer-provided description of a software component suitable execution on a single thread.  It then uses counterexample-guided inductive synthesis to search for thread-safe concurrent components conforming to that specification. Synchronicity verifies thread safety using an extended form of Lipton's theory of reduction.  Multiple thread-safe concurrent solutions may be found, and Synchronicity automatically ranks according to their performance on a programmer-supplied workload. The project is committed to increasing access to science education for all students, including women, under-represented groups, and first-generation college students. The investigators include students from these groups in this research, at both the undergraduate and graduate level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618038","SHF: Small: Collaborative Research: GOALI: Multiscale CAD Framework of Atomically Thin Transistors for Flexible Electronic System Applications","CCF","GRANT OPP FOR ACAD LIA W/INDUS, INFORMATION TECHNOLOGY RESEARC, SOFTWARE & HARDWARE FOUNDATION","07/01/2016","06/28/2016","Han Wang","CA","University of Southern California","Standard Grant","Sankar Basu","06/30/2019","$225,000.00","","han.wang.4@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1504, 1640, 7798","1504, 7923, 7945","$0.00","Establishing the foundation for electronics technology based on atomically thin two-dimensional (2D) materials, such as layered transition metal dichalcogenides (TMDCs), may prove to be transformative in many technological areas relying on flexible electronic and nanoelectronic systems. This project will establish a critical knowledge base for future 2D TMDC electronics technology for a broad range of applications, such as low power computing, flexible display, and wearable electronics. The project has a direct industrial impact through the respective collaboration and technology transfer between the participating universities and the industrial partner. It will also offer interdisciplinary research opportunities for training graduate students, as well as undergraduate and high school students, in a collaborative research environment between university and industry, and provide valuable resources for research and educational community by disseminating web-based learning modules, simulators, and experimental data on TDMC-based electronics.<br/><br/>While TMDC materials are promising for many potential applications in nanoelectronics and flexible electronics due to their mechanical bendability, atomically thin thickness, and excellent intrinsic carrier transport properties, major gaps exist on translating early science of  such materials into practical circuit and system technologies. The objective of this project is to develop compact model and circuit-simulation platform for new 2D TMDC-based devices and systems, and to explore its applications in flexible and wearable electronic systems through experimental demonstration and collaboration with IBM T. J. Watson Research Center as the industrial partner. The proposal will undertake the following tasks: (i) develop a multiscale simulation framework that integrates atomistic device simulations with compact circuit models for TMDC transistors, (ii) fabricate, characterize and simulate basic TMDC circuits, (iii) model the variability and defect mechanisms and their correlations in TMDC transistors, and (iv) design and experimentally demonstrate TMDC driving circuits for transparent flexible display."
"1514177","AF: Medium: Statistical Inference of Complex Evolutionary Histories","CCF","ALGORITHMIC FOUNDATIONS, COMPUTATIONAL BIOLOGY","07/15/2015","07/03/2018","Luay Nakhleh","TX","William Marsh Rice University","Continuing grant","Mitra Basu","06/30/2019","$800,000.00","","nakhleh@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7796, 7931","7923, 7924, 7931","$0.00","Genes are an essential building block of all forms of life. Understanding how genes evolve and diversify their function would contribute significantly to elucidating many phenomena and processes in biology, including how diseases emerge and how to treat them. Genes undergo evolutionary events that range from small-scale ones (e.g., one nucleotide is replaced by another) to large-scale ones (e.g., a gene gets duplicated resulting in more than one copy of the same gene). Accurately identifying these evolutionary events for different gene families is the focus of this project. In particular, the project is aimed at devising mathematical models, computational techniques, and software products for mapping the trajectory of a gene through time in light of a variety of evolutionary processes. The project will have impact on biology and biomedicine, will result in publicly available software products that enable new analyses, and will train students at the intersection of computer science, statistics, and biology. <br/><br/><br/>Inferring accurate evolutionary histories, or phylogenies, of species is a major endeavor in evolutionary biology, and has implications on all aspects of biology. This inference used to be conducted by sequencing a certain region of interest from the genomes of species under investigation, building a genealogy, or gene tree, for the region, and declaring the tree to be the species phylogeny. In the post-genomic era, this practice has been replaced by utilizing hundreds of genomic regions. While this new practice promises to yield more accurate estimates of the species phylogeny, it also gives rise to a new major challenge, namely accounting for the different evolutionary processes that could be acting simultaneously on the different genomic regions. In particular, three evolutionary processes have been prominent in post-genomic evolutionary analysis: incomplete lineage sorting (ILS), horizontal transfer (or, gene flow), and gene duplication/loss (GDL). Currently, no statistical methods exist for the task of inferring evolutionary relationships of genes and genomes while accounting for all these three processes simultaneously. The overarching goal of the project is to develop mathematical models and algorithmic techniques for this task. The proposed project will produce mathematical and algorithmic results, as well as open-source software that would enable species phylogeny inference from genome-wide data while simultaneously accounting for ILS, gene flow, and GDL."
"1350206","CAREER: Leveraging Heterogeneous Manycore Systems for Scalable Modeling, Simulation and Verification of Nanoscale Integrated Circuits","CCF","INFORMATION TECHNOLOGY RESEARC, COMMS, CIRCUITS & SENS SYS, SOFTWARE & HARDWARE FOUNDATION","06/01/2014","05/22/2018","Zhuo Feng","MI","Michigan Technological University","Continuing grant","Sankar Basu","05/31/2019","$400,000.00","","zhuofeng@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","1640, 7564, 7798","1045, 7945","$0.00","The goal of this CAREER research project is to best unleash the power of emerging heterogeneous manycore CPU-GPU computing platforms. This will require revolutionizing the next-generation Electronic Design Automation (EDA) tools to deal with unprecedented complexity of circuits involving billions of components, making possible their modeling, analysis and verification tasks which would be prohibitively expensive and even intractable with methods in use today. The experience acquired in this research is also likely to contribute to advances in the use of computing in other areas of science and engineering, thus impacting areas such as complex system modeling and simulation, computational fluid dynamics, social computing, and systems biology. The PI will promote undergraduate and underrepresented student research, as well as K-12 education outreach, to motivate students in pursuing advanced engineering education or a career in STEM areas. Additionally, the PI will integrate the research outcomes into undergraduate and graduate curriculum development, and leverage interdisciplinary, industrial and international collaborations to effectively facilitate the proposed research work and broadly disseminate the results. <br/><br/><br/>Future nanoscale Integrated Circuit (IC) subsystems, such as clock distributions, power delivery networks, embedded memory arrays, as well as analog and mixed-signal systems, may reach an unprecedented complexity involving billions of circuit components, making their modeling, analysis and verification tasks prohibitively expensive and intractable with existing EDA tools. On the other hand, emerging heterogeneous manycore computing systems, such as the manycore CPU-GPU computing platforms that integrate a few large yet power-consuming general purpose processors with massive number of much slimmer but more energy-efficient graphics processors, can theoretically delivery teraflops of computing power. The proposal aims to accelerate a paradigm shift in EDA research to more energy-efficient heterogeneous computing regimes. Towards this end, the PI will develop systematic hardware/software approaches to achieve scalable integrated circuit modeling, simulation and verifications by inventing heterogeneous CAD algorithms and data structures, as well as exploiting hardware-specific and domain-specific runtime performance modeling and optimization approaches."
"1814385","AF: Small: Collaborative Research: Matrix Signings and Algorithms for Expanders and Combinatorial Nullstellensatz","CCF","ALGORITHMIC FOUNDATIONS","09/01/2018","05/18/2018","Alexandra Kolla","CO","University of Colorado at Boulder","Standard Grant","Rahul Shah","08/31/2021","$250,000.00","","alexkolla@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7796","7923, 7926, 9102","$0.00","This project will investigate spectral properties of graph-related matrices and their signings, which have become fundamental tools in computer science. The spectra of such matrices have had a tremendous impact in numerous areas including machine learning, data mining, web search and ranking, game theory, scientific computing, and computer vision and have influenced several algorithmic innovations. The project will have significant technical as well as educational impacts. The inherent mathematical and algorithmic nature of the project together with the plethora of potential practical applications will bring together researchers from varied areas such as mathematics and network design. The investigators will organize a workshop on Spectral Graph Theory to bring together experts in these areas. The project will support graduate students who will receive mentoring and extensive training in the design and analysis of algorithms. The investigators will also direct special efforts towards fostering diversity through educational activities targeting under-represented groups in STEM disciplines.<br/><br/>In this project, the investigators will design efficient algorithms for constructing various combinatorial structures that are guaranteed to exist through suitable signings of matrices. The combinatorial structures to be studied include expander graphs and several other applications of the algebraic method. Notably, the algorithmic problem of efficiently constructing of expander graphs is at the core of spectral graph theory. This project will develop a comprehensive understanding of the inherent difficulties, as well as propose algorithms for efficiently constructing expander graphs via signed adjacency matrices. Combinatorial Nullstellensatz is a powerful algebraic tool often used to show the existence of certain combinatorial structures. However, the non-constructive nature of its proof has been a barrier towards finding these structures efficiently. Existential proofs based on the algebraic method have resisted progress on the constructive front (unlike those based on probabilistic method). In this project, the investigators will break ground along this direction by obtaining efficient constructive proofs for restricted applications of Combinatorial Nullstellensatz.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814947","AF: Small: Multiparty Communication, Polynomials, and Noise","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/15/2018","Alexander Sherstov","CA","University of California-Los Angeles","Standard Grant","Tracy J. Kimbrel","05/31/2021","$500,000.00","","sherstov@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7796","7923, 7926, 7927","$0.00","Communication complexity theory studies the minimum amount of communication, measured in bits, required in order to compute functions whose arguments are distributed among several parties. In addition to the basic importance of studying communication as a bottleneck resource, the theory has found a vast number of applications in many areas, including machine learning, mechanism design, streaming algorithms, data structures, pseudo-random generators, and VLSI layouts. This project tackles fundamental questions whose resolution will have a significant impact on the discipline. This work will exploit insights from, and contribute new ideas to, other areas such as quantum computing, computational learning, and approximation theory. The project is an ample source of research problems at various levels of difficulty and will be used in advising graduate and undergraduate students.  The investigator will integrate this research into his graduate and undergraduate teaching, take an active part in scientific dissemination, and promote theoretical computer science in Southern California.<br/><br/>This project comprises two related components.  First, the investigator will tackle longstanding open problems in the study of multiparty communication, such as settling the communication requirements of the set disjointness problem and breaking the logarithmic barrier for multiparty communication lower bounds. The second, complementary component of this project will advance the study of analytic representations of Boolean functions. Here, the investigator aims to obtain tight lower bounds for the polynomial approximation and sign-representation of the k-element distinctness function, constant-depth circuits, and Boolean formulas of arbitrary depth.  The two research components of this project are intimately related in that they require the same class of analytic techniques. Indeed, major advances in communication complexity over the past two decades have been obtained by transforming, explicitly or implicitly, communication protocols into multivariate polynomials of comparable complexity and by analyzing the resulting approximation questions. The planned research on multiparty communication and polynomials is further unified by a focus on noise, in the sense of imperfect output or adversarial corruption of intermediate computations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618364","SHF: Small: Scalable Spectral Sparsification of Graph Laplacians and Integrated Circuits","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/10/2016","Zhuo Feng","MI","Michigan Technological University","Standard Grant","Sankar Basu","05/31/2019","$450,000.00","","zhuofeng@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","7798","7923, 7945","$0.00","This research is motivated by investigations on scalable methods for design simplifications of nanoscale integrated circuits (ICs). This is to be achieved by extending the associated spectral graph sparsification framework to handle Laplacian-like matrices derived from general nonlinear IC modeling and simulation problems. The results from this research may prove to be key to the development of highly scalable computer-aided design algorithms for modeling, simulation, design, optimization, as well as verification of future nanoscale ICs that can easily involve multi-billions of circuit components. The algorithms and methodologies developed will be disseminated to leading technology companies that may include semiconductor and Electronic Design Automation companies as well as social and network companies, for potential industrial deployments. <br/><br/><br/>Spectral graph sparsification aims to find an ultra-sparse subgraph (a.k.a. sparsifier) such that its Laplacian can well approximate the original one in terms of its eigenvalues and eigenvectors. Since spectrally similar subgraphs can approximately preserve the distances, much faster numerical and graph-based algorithms can be developed based on these ""spectrally"" sparsified networks. A nearly-linear complexity spectral graph sparsification algorithm is to be developed based on a spectral perturbation approach. The proposed method is highly scalable and thus can be immediately leveraged for the development of nearly-linear time sparse matrix solvers and spectral graph (data) partitioning (clustering) algorithms for large real-world graph problems in general. The results of the research may also influence a broad range of computer science and engineering problems related to complex system/network modeling, numerical linear algebra, optimization, machine learning, computational fluid dynamics, transportation and social networks, etc."
"1819935","AF: Medium: Algorithmic Explorations of Networks, Markets, Evolution, and the Brain","CCF","ALGORITHMIC FOUNDATIONS","12/01/2017","12/20/2017","Christos Papadimitriou","NY","Columbia University","Continuing grant","Tracy J. Kimbrel","03/31/2019","$205,727.00","","cp3007@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7926, 7927, 7932","$0.00","Computer science is not just the scientific discipline behind the information technology revolution; it is also an apt framework for understanding the world around us.  This project is about applying the point of view of algorithms -- and their antithesis, complexity -- to understanding phenomena and challenges in a variety of domains, including the Internet, markets, evolution, and the brain.  To understand the Internet and the networks and markets it entails and enables, one must combine algorithms with ideas from economics and game theory.  This research will focus on online markets and particularly their dynamic (that is, multi-stage) nature, on incentives for improving congestion in network routing and air traffic, on algorithms for propagating influence in social networks, as well as new kinds of algorithms that take their inputs from competitors (who may choose to misrepresent their data).  The PI will continue his research on how computational insights can shed light on some key problems in evolution,  including certain rigorous connections between natural selection, machine learning, and a problem in Boolean logic.  Finally, the PI will work to reconcile learning algorithms with new insights from neuroscience.<br/><br/>The project includes research on certain crucial problems at the interface between computation and game theory/economics/networks,  while continuing past work employing computational concepts to elucidate evolution and, more recently, neuroscience.  The PI will  study the important problem of dynamic mechanism design in economics from the point of view of computational complexity and approximate implementation. He will also study mechanisms for managing congestion, with possible applications to air traffic control.  The project will explore the computational and graph-theoretic properties of several novel and promising game-theoretic models of network creation.  It will study from the complexity standpoint Nash equilibria with continuous strategies, and extensions of the Nash equilibrium concept beyond utility theory.  The project will also explore new and timely modes of computation in which all inputs (ultimately, all computational components) are provided by selfish rational agents. In evolution, the PI will explore the connections between learning algorithms, games, and natural selection, and a different connection between Boolean satisfiability and the emergence of novelty. The PI also plans to develop a new genre of learning algorithms that are more faithful to the new insights we are gaining into the brain.  Finally, from the standpoint of algorithms and complexity, the PI will look at several computational problems ranging from network variants of the set cover problem to linear programming and optimizing multivariate polynomials."
"1750362","CAREER: Theoretical Foundations for Probabilistic Models with Dense Random Matrices","CCF","COMM & INFORMATION FOUNDATIONS","03/01/2018","02/15/2018","Galen Reeves","NC","Duke University","Continuing grant","Phillip Regalia","02/28/2023","$188,474.00","","galen.reeves@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7797","1045, 7935","$0.00","Many real-world scientific and engineering applications require sophisticated processing of large and complex data sets. Examples include wireless communications,  computational photography, and the training of multilayer networks for classification tasks. In some cases, performance is fundamentally limited by the amount of data. In other cases, the main limitation is the computational complexity of processing algorithms. A major challenge for researchers is to understand these fundamental limits. This project explores these limits by studying probabilistic models that describe the statistical relationship between the data and the unknown quantities of interest (e.g., transmitted message or correct label). The research involves combining ideas from information theory and statistical physics to compute fundamental limits and using these results to design efficient methods with improved performance. The interdisciplinary nature of the research is mirrored in the education activities of this project, which focuses on making connections between engineering, statistical physics, and the information sciences, as well as improving undergraduate education through exploratory data analysis.<br/><br/>The key conceptual idea behind this research is that statistical dependencies induced through multiplication by dense random matrices can be understood through connections with simpler models involving additive Gaussian noise. In a recent breakthrough, the investigator showed how ideas from information theory could provide rigorous proofs for behaviors that had been conjectured using the heuristic replica method from statistical physics. Building upon this insight, the research is organized around three thrusts: i) Developing new theoretical methods to provide rigorous and interpretable characterization of fundamental limits; ii) Designing new algorithms for inference, learning, and compression; and iii) Analyzing bi-linear and multi-layer inference problems with applications to deep learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1715027","AF:   Small:  Collaborative Research:  Personalized Environmental Monitoring of Type 1 Diabetes (T1D): A Dynamic System Perspective","CCF","COMPUTATIONAL BIOLOGY","08/01/2017","07/13/2017","Shuai Huang","WA","University of Washington","Standard Grant","Mitra Basu","07/31/2020","$158,300.00","","shuai.huang.ie@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7931","7923, 7931","$0.00","The progression of many chronic diseases, such as Type 1 diabetes (T1D), manifests dynamic processes that can be modified by environmental exposures. Modeling of the underlying disease progression holds critical values for better understanding of disease development, effective monitoring, and prevention. While the emerging big data studying these diseases provide great resources, current pace for translating these data into effective monitoring and intervention strategies has been slow due to the analytic challenges caused by potential multi-layer characteristics of disease progression processes, the high-dimensional exogenous factors, heterogeneous biomarker signals, and the complexity of continuous-time stochastic processes. To mitigate these challenges with the development of new models and computational algorithms, this research will provide the desired personalized monitoring and risk factor identification capability, which is crucial not only for increasing the situational awareness of the individuals who are at risk, but also for providing evidences for design, validation, and deployment of intervention strategies. Its generic nature will also help effective monitoring of many other dynamic systems in engineering and life sciences. The interdisciplinary nature of this research across data-driven risk monitoring, dynamic systems, high-dimensional variable selection, and healthcare, will prepare students with a diversified education background. <br/><br/>The objective of this project is to create a generic suite of computational approaches that can be applied for modeling, learning, and monitoring a set of dynamic diseases, whose progression processes may be modified by exogenous factors such as environmental exposures. Several methodological contributions are expected, including: (1) a novel rule-based monitoring methodology to convert high-dimensional complex biomarkers into disease risk evaluation, via the development of an efficient screening method for high-throughput rule discovery and an optimal design method for risk monitoring; (2) a multi-layer dynamic model that can investigate how the exogenous risk factors regulate the disease process, with integration of sparse multi-task learning to mitigate the high-dimensionality of exogenous factors; and (3) a high-dimensional robust risk factor identification framework that can identify exogenous factors with integration of knowledge learned from historical data, new measurements, and clinician's prognostics. These proposed methods will be evaluated with a practical example studying T1D in partnership with The Environmental Determinant of Diabetes in the Young (TEDDY) study."
"1714136","AF:  Small:  Collaborative Research:  Personalized Environmental Monitoring of Type 1 Diabetes (T1D):  A Dynamic System Perspective","CCF","COMPUTATIONAL BIOLOGY","08/01/2017","07/13/2017","Ji Liu","NY","University of Rochester","Standard Grant","Mitra Basu","07/31/2020","$157,999.00","","jliu@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7931","7923, 7931","$0.00","The progression of many chronic diseases, such as Type 1 diabetes (T1D), manifests dynamic processes that can be modified by environmental exposures. Modeling of the underlying disease progression holds critical values for better understanding of disease development, effective monitoring, and prevention. While the emerging big data studying these diseases provide great resources, current pace for translating these data into effective monitoring and intervention strategies has been slow due to the analytic challenges caused by potential multi-layer characteristics of disease progression processes, the high-dimensional exogenous factors, heterogeneous biomarker signals, and the complexity of continuous-time stochastic processes. To mitigate these challenges with the development of new models and computational algorithms, this research will provide the desired personalized monitoring and risk factor identification capability, which is crucial not only for increasing the situational awareness of the individuals who are at risk, but also for providing evidences for design, validation, and deployment of intervention strategies. Its generic nature will also help effective monitoring of many other dynamic systems in engineering and life sciences. The interdisciplinary nature of this research across data-driven risk monitoring, dynamic systems, high-dimensional variable selection, and healthcare, will prepare students with a diversified education background. <br/><br/>The objective of this project is to create a generic suite of computational approaches that can be applied for modeling, learning, and monitoring a set of dynamic diseases, whose progression processes may be modified by exogenous factors such as environmental exposures. Several methodological contributions are expected, including: (1) a novel rule-based monitoring methodology to convert high-dimensional complex biomarkers into disease risk evaluation, via the development of an efficient screening method for high-throughput rule discovery and an optimal design method for risk monitoring; (2) a multi-layer dynamic model that can investigate how the exogenous risk factors regulate the disease process, with integration of sparse multi-task learning to mitigate the high-dimensionality of exogenous factors; and (3) a high-dimensional robust risk factor identification framework that can identify exogenous factors with integration of knowledge learned from historical data, new measurements, and clinician's prognostics. These proposed methods will be evaluated with a practical example studying T1D in partnership with The Environmental Determinant of Diabetes in the Young (TEDDY) study."
"1718513","AF:   Small:   Collaborative Research:  Personalized Environmental Monitoring of Type 1 Diabetes (T1D): A Dynamic System Perspective","CCF","COMPUTATIONAL BIOLOGY, Smart and Connected Health","08/01/2017","07/13/2017","Xiaoning Qian","TX","Texas A&M Engineering Experiment Station","Standard Grant","Mitra Basu","07/31/2020","$183,700.00","Kendra Vehik","xqian@ece.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7931, 8018","7923, 7931","$0.00","The progression of many chronic diseases, such as Type 1 diabetes (T1D), manifests dynamic processes that can be modified by environmental exposures. Modeling of the underlying disease progression holds critical values for better understanding of disease development, effective monitoring, and prevention. While the emerging big data studying these diseases provide great resources, current pace for translating these data into effective monitoring and intervention strategies has been slow due to the analytic challenges caused by potential multi-layer characteristics of disease progression processes, the high-dimensional exogenous factors, heterogeneous biomarker signals, and the complexity of continuous-time stochastic processes. To mitigate these challenges with the development of new models and computational algorithms, this research will provide the desired personalized monitoring and risk factor identification capability, which is crucial not only for increasing the situational awareness of the individuals who are at risk, but also for providing evidences for design, validation, and deployment of intervention strategies. Its generic nature will also help effective monitoring of many other dynamic systems in engineering and life sciences. The interdisciplinary nature of this research across data-driven risk monitoring, dynamic systems, high-dimensional variable selection, and healthcare, will prepare students with a diversified education background. <br/><br/>The objective of this project is to create a generic suite of computational approaches that can be applied for modeling, learning, and monitoring a set of dynamic diseases, whose progression processes may be modified by exogenous factors such as environmental exposures. Several methodological contributions are expected, including: (1) a novel rule-based monitoring methodology to convert high-dimensional complex biomarkers into disease risk evaluation, via the development of an efficient screening method for high-throughput rule discovery and an optimal design method for risk monitoring; (2) a multi-layer dynamic model that can investigate how the exogenous risk factors regulate the disease process, with integration of sparse multi-task learning to mitigate the high-dimensionality of exogenous factors; and (3) a high-dimensional robust risk factor identification framework that can identify exogenous factors with integration of knowledge learned from historical data, new measurements, and clinician's prognostics. These proposed methods will be evaluated with a practical example studying T1D in partnership with The Environmental Determinant of Diabetes in the Young (TEDDY) study."
"1741683","CAREER: Compiler and Runtime Support for Irregular Applications on Many-core Processors","CCF","SOFTWARE & HARDWARE FOUNDATION","01/01/2017","02/25/2019","Michela Becchi","NC","North Carolina State University","Continuing grant","Almadena Chtchelkanova","01/31/2020","$424,999.00","","mbecchi@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","1045, 7942, 9150, 9251","$0.00","Many-core processors (such as GPUs) have been used to accelerate a wide variety of applications: molecular dynamics, image processing, data mining, option pricing and linear algebra, among others. Despite their widespread adoption, these devices are still considered relatively difficult to use, in that they require the programmer to be familiar both with parallel programming and with the operation of the hardware. In particular, the effective deployment of irregular applications on many-core devices is still far from understood. However, many established and emerging applications (from social and computer networking, electrical circuit modeling, discrete event simulation, compilers, and computational sciences) are irregular in nature, being based on data structures such as graphs and trees. <br/>This research proposes compiler and runtime techniques to support the deployment of graph and other irregular applications on many-core processors, while hiding from the programmer the complexity and heterogeneity of the underlying hardware and software stack. Since the degree of parallelism within irregular applications is heavily data dependent, the proposed compiler techniques aim to generate multiple platform-specific code variants starting from high-level platform-agnostic algorithmic descriptions. The runtime techniques focus on the selection of the most appropriate code variant and its tuning to the hardware and the input datasets. More specifically, this research covers three important issues related to irregular applications: (i) the effective handling of nested parallelism (in the form of parallelizable nested loops and recursive functions) within irregular applications; (ii) the design of a dynamic memory allocation library that can scale to the degree of multithreading offered by many-core devices, and of graph encoding schemes suitable for applications operating on dynamic datasets; and (iii) the effective handling of synchronization on many-core devices."
"1748988","CAREER: Associative In-Memory Graph Processing Paradigm: Towards Tera-TEPS Graph Traversal In a Box","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2018","02/11/2019","Jing (Jane) Li","WI","University of Wisconsin-Madison","Continuing grant","Sankar Basu","01/31/2023","$190,895.00","","jli587@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7798","1045, 7945","$0.00","Large-scale graph analytics, the class of big data analytics that essentially explores the relationship among a vast collection of interconnected entities (e.g., ""friends"" in a social network), is becoming increasingly important due to its broad applicability, from machine learning to web search, precision medicine, and social sciences. However, the performance of graph processing systems is severely limited by the irregular data access patterns in graph computations. The existing solutions that have been developed for mainstream parallel computing are generally ineffective for massive, sparse real-world graphs due to the conventional computer architecture (i.e., von Neumann architecture) itself. In this project, new, fundamental methods will be explored in both theoretical and practical implementations to address this problem. It uniquely advances multiple fundamental cross-disciplinary areas in device, circuit, computer-aided design, and computer architecture and can be applied to address some of the most challenging ""big data"" problems ranging from fundamental research to everyday life. The research framework will be extended into an educational platform, providing a user-friendly framework for a laboratory-based curriculum and will serve the educational objectives for K-12 students, undergraduate and graduate students.<br/><br/>In this research, a new computing paradigm will be developed to fundamentally address the challenge in processing large-scale graphs and to achieve ultra-high computing efficiency, orders of magnitude higher in performance per watt than state-of-art mainstream computer. To this end, a holistic co-design and optimization of algorithm, software and hardware will be developed to leverage the great potential of emerging nonvolatile memory technology. A new computing model will be proposed and theoretically proven to be more efficient in runtime/area/energy than traditional von Neumann architecture in performing graph computation. Detailed micro-architectures and circuits will be designed and evaluated to best implement the proposed computing model for concept proof."
"1637566","AitF: Collaborative Research: High Performance Linear System Solvers with Focus on Graph Laplacians","CCF","Algorithms in the Field","09/01/2016","01/29/2019","Yang Peng","GA","Georgia Tech Research Corporation","Standard Grant","Rahul Shah","08/31/2020","$266,666.00","","rpeng@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7239","","$0.00","Fast and robust solvers for systems of linear equations are the work horse of many communities in the sciences, engineering, business, and industry. Few pieces of software are so important of all these areas. Recent theoretical progress on efficient solvers for special cases of linear systems, including Symmetric Diagonally Dominant matrices, have sparked a renaissance in faster algorithms for wide classes of optimization problems that have not seen improvements in many years.<br/><br/>The main goal of this project is to take the next step to find and implement fast robust solvers that work in seconds on systems that are a factor of 100 to 1000 times larger than is now possible on a modern large workstation. For the applications mentioned above the solver may be called 100s or 1000s times for a single run. As a result, such a solver needs to meet several important requirements: 1) it must be robust enough to not need human intervention between these runs; 2) it must be fast enough to finish all work in a reasonable amount of time. 3) it must be able to handle the very different systems of equations that arise in applications.<br/><br/>This project aims to bridge the theoretical and practical aspects of designing efficient and robust solvers for linear systems in graph Laplacians. The PIs plan to develop code packages that have good practical performances as well as provable guarantees in the worst case. Doing so requires them to address a range of issues arising from numerical analysis, combinatorics, high performance computing, and data structures.<br/><br/>They plan to address shortcomings of existing packages for solving linear systems in graph Laplacians, specifically their robustness in the presence of widely varying edge weights. Resolving this issue is crucial for bridging the theory and practice of incorporating these solvers in optimization algorithms such as iterative least squares, mirror descent, and interior point methods. Specifically, they will study a variety of theoretical algorithmic tools from the perspective of high performance computing, focusing on topics at the core of data structures, high performance computing, numerical analysis, scientific computing, and graph theory. Progresses on them have the potential of opening up novel lines of investigations on well-studied topics for the team and the students that they will train."
"1637523","AitF: Collaborative Research: High Performance Linear System Solvers with Focus on Graph Laplacians","CCF","Algorithms in the Field","09/01/2016","08/18/2016","Gary Miller","PA","Carnegie-Mellon University","Standard Grant","Rahul Shah","08/31/2020","$266,666.00","","glmiller@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7239","","$0.00","Fast and robust solvers for systems of linear equations are the work horse of many communities in the sciences, engineering, business, and industry. Few pieces of software are so important of all these areas. Recent theoretical progress on efficient solvers for special cases of linear systems, including Symmetric Diagonally Dominant matrices, have sparked a renaissance in faster algorithms for wide classes of optimization problems that have not seen improvements in many years.<br/><br/>The main goal of this project is to take the next step to find and implement fast robust solvers that work in seconds on systems that are a factor of 100 to 1000 times larger than is now possible on a modern large workstation. For the applications mentioned above the solver may be called 100s or 1000s times for a single run. As a result, such a solver needs to meet several important requirements: 1) it must be robust enough to not need human intervention between these runs; 2) it must be fast enough to finish all work in a reasonable amount of time. 3) it must be able to handle the very different systems of equations that arise in applications.<br/><br/>This project aims to bridge the theoretical and practical aspects of designing efficient and robust solvers for linear systems in graph Laplacians. The PIs plan to develop code packages that have good practical performances as well as provable guarantees in the worst case. Doing so requires them to address a range of issues arising from numerical analysis, combinatorics, high performance computing, and data structures.<br/><br/>They plan to address shortcomings of existing packages for solving linear systems in graph Laplacians, specifically their robustness in the presence of widely varying edge weights. Resolving this issue is crucial for bridging the theory and practice of incorporating these solvers in optimization algorithms such as iterative least squares, mirror descent, and interior point methods. Specifically, they will study a variety of theoretical algorithmic tools from the perspective of high performance computing, focusing on topics at the core of data structures, high performance computing, numerical analysis, scientific computing, and graph theory. Progresses on them have the potential of opening up novel lines of investigations on well-studied topics for the team and the students that they will train."
"1637564","AitF: Collaborative Research: High Performance Linear System Solvers with Focus on Graph Laplacians","CCF","Algorithms in the Field","09/01/2016","08/18/2016","John Gilbert","CA","University of California-Santa Barbara","Standard Grant","Rahul Shah","08/31/2020","$266,597.00","","gilbert@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7239","","$0.00","Fast and robust solvers for systems of linear equations are the work horse of many communities in the sciences, engineering, business, and industry. Few pieces of software are so important of all these areas. Recent theoretical progress on efficient solvers for special cases of linear systems, including Symmetric Diagonally Dominant matrices, have sparked a renaissance in faster algorithms for wide classes of optimization problems that have not seen improvements in many years.<br/><br/>The main goal of this project is to take the next step to find and implement fast robust solvers that work in seconds on systems that are a factor of 100 to 1000 times larger than is now possible on a modern large workstation. For the applications mentioned above the solver may be called 100s or 1000s times for a single run. As a result, such a solver needs to meet several important requirements: 1) it must be robust enough to not need human intervention between these runs; 2) it must be fast enough to finish all work in a reasonable amount of time. 3) it must be able to handle the very different systems of equations that arise in applications.<br/><br/>This project aims to bridge the theoretical and practical aspects of designing efficient and robust solvers for linear systems in graph Laplacians. The PIs plan to develop code packages that have good practical performances as well as provable guarantees in the worst case. Doing so requires them to address a range of issues arising from numerical analysis, combinatorics, high performance computing, and data structures.<br/><br/>They plan to address shortcomings of existing packages for solving linear systems in graph Laplacians, specifically their robustness in the presence of widely varying edge weights. Resolving this issue is crucial for bridging the theory and practice of incorporating these solvers in optimization algorithms such as iterative least squares, mirror descent, and interior point methods. Specifically, they will study a variety of theoretical algorithmic tools from the perspective of high performance computing, focusing on topics at the core of data structures, high performance computing, numerical analysis, scientific computing, and graph theory. Progresses on them have the potential of opening up novel lines of investigations on well-studied topics for the team and the students that they will train."
"1422262","CIF:  Small: A comprehensive framework for dynamic network tracking and clustering with applications to functional brain connectivity","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2014","07/10/2014","Selin Aviyente","MI","Michigan State University","Standard Grant","Phillip Regalia","07/31/2019","$380,048.00","","aviyente@egr.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7797","7923, 7936","$0.00","Complex network theory has proved to be a versatile framework to represent and analyze relational data that is abundant in many disciplines including the social sciences, information systems, biology and neuroscience. Until recently, the research on network theory has mainly focused on static graphs, i.e. the relationships between nodes of the network do not change with time. However, almost all real networks are dynamic in nature such as social networks with connections that change over time or functional neural networks that reorganize rapidly in response to stimuli. Most of the current studies of dynamic networks focuses on two major and interrelated problems: anomaly or change point detection in a time series of graphs and evolutionary clustering to identify the time-varying structure of networks. This research addresses these two problems simultaneously in a unified framework to monitor and track the changes in network topology and to characterize each ?network state? with a single community structure. In particular, this research focuses on the functional connectivity networks (FCNs) of the human brain that reorganize themselves dynamically during perception, cognition and execution of mental processes.<br/><br/>The investigator develops two complementary approaches to address dynamic network monitoring problem: 1) A multi-scale framework for joint anomaly detection and network state identification in time-varying networks; and, 2) Consensus spectral clustering methods along with tensor decomposition for succinct topographic representation of network states. Finally, this dynamic network monitoring framework is applied to electroencephalogram (EEG) data collected using an experimental protocol designed to assess well-known salience and control functional networks associated with affective regulation and cognitive control."
"1839370","TRIPODS+X:RES: Collaborative Research: Thermodynamic Phases and Configuration Space Topology","CCF","TRIPODS Transdisciplinary Rese, OFFICE OF MULTIDISCIPLINARY AC, SPECIAL INITIATIVES, DMR SHORT TERM SUPPORT","10/01/2018","09/10/2018","Jeremy Kyle Mason","CA","University of California-Davis","Standard Grant","Christopher W. Stark","09/30/2021","$300,000.00","","jkmason@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","041Y, 1253, 1642, 1712","024E, 047Z, 062Z, 8037","$0.00","The purpose of this project is to use techniques developed within pure and applied topology to study unanswered questions relating to phase transitions. Phase transitions are unusual phenomena in the physical sciences; they are at once part of our everyday experience (e.g., the freezing of water) and yet continue to resist entirely satisfactory explanation. A phase is traditionally defined as a system throughout which all physical properties are effectively uniform. Certainly then, the transformation of water into steam should always involve a phase transition. Curiously, this is not the case; by constructing a path around the critical point in the pressure-temperature diagram of water, the liquid can be converted into the vapor without any discontinuous change in the density. How then can the definition of a phase be made more precise, and the presence or absence of a phase transition be understood? This project approaches the above question by considering a simplified system where molecules are spheres that interact by a hard sphere potential (i.e., do not overlap). Since this is often regarded as a prototype for simple fluids, this research could offer valuable insight into the factors governing solid-liquid phase transitions and glass transitions in other materials. Possible applications include prediction of the metallic alloy compositions most likely to form bulk metallic glasses, or a better understanding of the conditions leading to soil liquefaction during earthquakes.<br/><br/>More specifically, the project will study the statistical topology of the configuration space of the hard disk system. What is unique about this project is the emphasis on the use of topology for data exploration: functions on the configuration space govern the thermodynamic behavior, but the space itself is fantastically complicated and likely can only be adequately characterized with the help of computational and theoretical techniques that did not exist even a few years ago. First, the Reeb graph of the tautological function on the configuration space will be approximated using the mapper algorithm. The Reeb graph provides more information about the configuration space topology than the disconnectivity graphs common in the literature. Second, while configuration spaces of points have been studied for decades, giving the points positive radius presents new and interesting challenges for algebraic topologists. These challenges are expected to stimulate new developments in Morse theory.. Third, if the conjecture holds that a phase transition occurs where there is a non-analyticity in the diameter of the configuration space as measured by the diffusion distance, that would provide a dramatically different and more fundamental view of the nature of phase transitions in general. Any results pertaining specifically to the hard disk system will be tested for hard spheres in other domains to evaluate the generality of the conclusions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839358","TRIPODS+X:RES: Collaborative Research: Thermodynamic Phases and Configuration Space Topology","CCF","TRIPODS Transdisciplinary Rese, SPECIAL INITIATIVES, DMR SHORT TERM SUPPORT","10/01/2018","09/10/2018","Matthew Kahle","OH","Ohio State University","Standard Grant","Christopher W. Stark","09/30/2021","$300,000.00","Facundo Memoli","kahle.70@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","041Y, 1642, 1712","024E, 047Z, 062Z","$0.00","The purpose of this project is to use techniques developed within pure and applied topology to study unanswered questions relating to phase transitions. Phase transitions are unusual phenomena in the physical sciences; they are at once part of our everyday experience (e.g., the freezing of water) and yet continue to resist entirely satisfactory explanation. A phase is traditionally defined as a system throughout which all physical properties are effectively uniform. Certainly then, the transformation of water into steam should always involve a phase transition. Curiously, this is not the case; by constructing a path around the critical point in the pressure-temperature diagram of water, the liquid can be converted into the vapor without any discontinuous change in the density. How then can the definition of a phase be made more precise, and the presence or absence of a phase transition be understood? This project approaches the above question by considering a simplified system where molecules are spheres that interact by a hard sphere potential (i.e., do not overlap). Since this is often regarded as a prototype for simple fluids, this research could offer valuable insight into the factors governing solid-liquid phase transitions and glass transitions in other materials. Possible applications include prediction of the metallic alloy compositions most likely to form bulk metallic glasses, or a better understanding of the conditions leading to soil liquefaction during earthquakes.<br/><br/>More specifically, the project will study the statistical topology of the configuration space of the hard disk system. What is unique about this project is the emphasis on the use of topology for data exploration: functions on the configuration space govern the thermodynamic behavior, but the space itself is fantastically complicated and likely can only be adequately characterized with the help of computational and theoretical techniques that did not exist even a few years ago. First, the Reeb graph of the tautological function on the configuration space will be approximated using the mapper algorithm. The Reeb graph provides more information about the configuration space topology than the disconnectivity graphs common in the literature. Second, while configuration spaces of points have been studied for decades, giving the points positive radius presents new and interesting challenges for algebraic topologists. These challenges are expected to stimulate new developments in Morse theory.. Third, if the conjecture holds that a phase transition occurs where there is a non-analyticity in the diameter of the configuration space as measured by the diffusion distance, that would provide a dramatically different and more fundamental view of the nature of phase transitions in general. Any results pertaining specifically to the hard disk system will be tested for hard spheres in other domains to evaluate the generality of the conclusions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1533823","AitF: FULL: Collaborative Research: Provably Efficient GPU Algorithms","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS, EPSCoR Co-Funding","09/01/2015","06/26/2017","Nodari Sitchinava","HI","University of Hawaii","Standard Grant","Tracy J. Kimbrel","08/31/2019","$416,000.00","","nodari@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","CSE","7239, 7796, 9150","012Z, 7934, 9150, 9251","$0.00","Graphics processing units (GPUs) were originally developed as specialized hardware exclusively for graphics rendering. In recent years they have become massively parallel systems with hundreds of processing cores supporting thousands of threads. Given their computational potential, they are now used to support general-purpose computation via high-level programming languages.  As a result, they have become a standard platform for high-performance computing (HPC) simulations in natural sciences.<br/><br/>However, there is still very little understanding of what types of algorithms translate into efficient GPU programs, and many implementations rely on a limited number of design patterns and many rounds of trial-and-error. There is a need for simple but accurate algorithmic models to get a wider algorithmic community involved in GPU computing. The project will develop such a model, intended to have the transformative effect of enabling algorithms researchers to focus their efforts on creating algorithms for GPUs in a way that is currently not possible, increasing the algorithmic knowledgebase in GPU computing. Over time, more efficient algorithms will lead to better utilization of computing resources and reuse of code implemented as libraries. Such a model for GPUs will also enable teaching GPU computing to a wider group of students, similarly to how sequential and PRAM algorithms are currently taught.<br/><br/>This project will study the algorithmic aspects of GPU computing and will develop a simple but accurate theoretical model for GPUs, that will define clear guidelines and complexity metrics for algorithm evaluation. The PIs will develop and implement algorithms that will improve the state of the art code base of general purpose computation on GPUs in the areas of combinatorial algorithms, computational geometry, visualization, search algorithms, and data structures."
"1533564","AitF: FULL: Collaborative Research: Provably Efficient GPU Algorithms","CCF","Algorithms in the Field","09/01/2015","10/05/2018","John Iacono","NY","New York University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$400,000.00","John Iacono, Claudio Silva","iacono@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7239","012Z, 9150","$0.00","Graphics processing units (GPUs) were originally developed as specialized hardware exclusively for graphics rendering. In recent years they have become massively parallel systems with hundreds of processing cores supporting thousands of threads. Given their computational potential, they are now used to support general-purpose computation via high-level programming languages.  As a result, they have become a standard platform for high-performance computing (HPC) simulations in natural sciences.<br/><br/>However, there is still very little understanding of what types of algorithms translate into efficient GPU programs, and many implementations rely on a limited number of design patterns and many rounds of trial-and-error. There is a need for simple but accurate algorithmic models to get a wider algorithmic community involved in GPU computing. The project will develop such a model, intended to have the transformative effect of enabling algorithms researchers to focus their efforts on creating algorithms for GPUs in a way that is currently not possible, increasing the algorithmic knowledgebase in GPU computing. Over time, more efficient algorithms will lead to better utilization of computing resources and reuse of code implemented as libraries. Such a model for GPUs will also enable teaching GPU computing to a wider group of students, similarly to how sequential and PRAM algorithms are currently taught.<br/><br/>This project will study the algorithmic aspects of GPU computing and will develop a simple but accurate theoretical model for GPUs, that will define clear guidelines and complexity metrics for algorithm evaluation. The PIs will develop and implement algorithms that will improve the state of the art code base of general purpose computation on GPUs in the areas of combinatorial algorithms, computational geometry, visualization, search algorithms, and data structures."
"1740583","SHF: Small: Collaborative Research: The Automata Programming Paradigm for Genomic Analysis","CCF","SOFTWARE & HARDWARE FOUNDATION","03/01/2017","05/04/2017","Michela Becchi","NC","North Carolina State University","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$234,744.00","","mbecchi@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7923, 7942, 9150, 9251","$0.00","Inferring knowledge from genetic data will drive future advances in the life sciences.  However, DNA sequences are being generated faster than they can be analyzed with existing computing technologies and algorithms. The core computations performed by many genomic applications involve pattern matching. This operation is normally implemented using automata-based algorithms and can be efficiently mapped onto non-general purpose platforms such as Field Programmable Gate Arrays (FPGA) and Micron?s recently announced Automata Processor (AP). However, the lack of high-level programming interfaces for these devices hampers their adoption in the bioinformatics community.<br/><br/>This project fills this gap by developing novel programmatic descriptions of several genomic analyses and mapping them onto these two non-traditional architectures. The work advances the state-of-the-art in several ways. At an algorithmic level, new methods to address the biological problems of genome-scale orthology inference and regulatory motif search are being developed. At a computational abstraction level, the researchers are designing an extended finite automaton abstraction suitable to support diverse computations, and are mapping new and existing computational kernels onto it. At a hardware mapping level, automatic tuning techniques for the effective deployment of automata-based computations on FPGA and Micron?s AP are being developed. <br/><br/> This interdisciplinary project will facilitate the adoption of FPGA and Micron?s Automata Processor by biologists by providing a new library of pattern matching routines and a high-level automata-based programming interface for these platforms. In addition, the researchers are developing instructional material in a variety of topics, such as genomic analysis, pattern matching, automata processing and high-performance computing. Finally, this project provides research opportunities and access to pre-production hardware to undergraduate and graduate students, interdisciplinary training, and technology transfer to industry. The results of this research will be made available through the release of software tools and publication in international conferences and journals."
"1816027","AF: Small: Algorithmic Techniques for High-throughput Analysis of Long Reads","CCF","SOFTWARE & HARDWARE FOUNDATION, COMPUTATIONAL BIOLOGY","10/01/2018","07/20/2018","Srinivas Aluru","GA","Georgia Tech Research Corporation","Standard Grant","Mitra Basu","09/30/2021","$424,992.00","","aluru@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798, 7931","7923, 7931, 7942","$0.00","The field of DNA sequencing has rapidly evolved from Sanger sequencing (700-1000 base pairs, or bp, in length) to massively parallel high-throughput sequencing of short reads (100-200 bp) to the more recent advances in generating long (> 5,000 bp) and ultra-long (> 50,000 bp) reads. There is currently an urgent need to develop efficient algorithms for analyzing long-read datasets in the context of the myriad biological applications they enable. Long read technologies sustain high error rates but with more attractive error distribution characteristics that often permit probabilistic guarantees on the quality of results. This project will result in fundamental research advances in developing bioinformatics algorithms for long-read sequencing, along with distributable open-source software products to facilitate their immediate adoption by the life sciences community. The award will also support interdisciplinary training, and undergraduate participation in research.<br/><br/>The project seeks to advance mapping, assembly, and biological applications of long-read sequencing through the design of provably efficient algorithms, formal characterization of the quality of results, development of methods that scale to larger datasets, and methods that are robust to changes brought about by continued developments in long read sequencing. Problems addressed include (i) split-read mapping of ultra-long reads to a reference genome, (ii) development of data structures based on bloom filters to achieve space optimization and perfect statistical sensitivity, (iii) algorithms for mapping long reads to a collection of reference genomes represented by compact graph-based structures, (iv) algorithms for partitioning long reads to facilitate identification of haplotypes in diploid assemblies, and (v) long-read inspired alignment free algorithms for genome-to-genome comparison, as well as important biological applications enabled by these. The research will emphasize utilization of real datasets, relevance to practically encountered problems and applications, and independent validation by collaborators and other experts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1553248","CAREER: An Information Theoretic Perspective of Consistent Distributed Storage Systems","CCF","COMM & INFORMATION FOUNDATIONS","02/15/2016","01/24/2018","Viveck Cadambe","PA","Pennsylvania State Univ University Park","Continuing grant","Phillip Regalia","01/31/2021","$393,640.00","","VXC12@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7797","1045, 7935","$0.00","Broader Significance:<br/><br/>Key-value stores form an integral infrastructural component of numerous modern web-based applications including retail stores, multi-player games, reservation systems, news feeds, and social and professional networks. Cloud computing service providers commonly implement key-value stores over large scale distributed data storage systems. At the heart of key-value store implementations in distributed storage systems, there are carefully crafted algorithms that expose a consistent, current view of the stored data to a user who reads the data.  The main purpose of this project is to undertake a formal study of the storage costs incurred in distributed storage systems which aspire to present a consistent, current view of the stored data. The project has the long-term potential to aid the development of new data storage techniques that can benefit key-value store implementations by reducing their storage cost and energy consumption.<br/><br/>Technical Description:<br/><br/>An important requirement of a distributed data storage system is fault tolerance, that is, the data must be accessible even if the system components fail. In applications of distributed storage to distributed computing and implementation of key-value stores, the following property known as consistency is also critical: when the data is being constantly updated, a user that reads from the system should obtain the latest version of the data. Algorithms that ensure consistency and fault tolerance in storage systems have been extensively studied in distributed systems theory and practice. The goal of this project is to obtain, for the first time, an information theoretic understanding of the storage costs incurred in consistent, fault-tolerant distributed storage systems.<br/><br/>Building on preliminary work by the investigator, the project will develop and study several new information theoretic formulations inspired by distributed systems theory and practice. The proposed formulations naturally expose trade-offs between the degrees of redundancy and consistency, and other physical parameters of storage systems. New coding schemes and information theoretic converses for the proposed formulations will be developed using tools from algebra, combinatorics and network information theory. The project will also pursue the development of new bounds for network coding, which naturally apply to the newly proposed formulations and to other families of codes including locally repairable codes and regenerating codes. The project will likewise develop an education plan that eyes the long term goal of developing interdisciplinary researchers and engineers who are trained in information theory, coding theory, and the theory and design of distributed systems."
"1606557","XPS: EXPL: FP: Symmetric Queries as a Building Block for Efficient Parallel Query Evaluation","CCF","Exploiting Parallel&Scalabilty","07/01/2015","07/17/2018","Yuqing Wu","CA","Pomona College","Standard Grant","Aidong Zhang","08/31/2019","$297,592.00","","Melanie.Wu@pomona.edu","Alexander Hall","Claremont","CA","917114434","9096218328","CSE","8283","","$0.00","Today's applications frequently feature massive and heterogeneous data and complicated computational requirements. There have been many efforts towards efficient parallel query processing and optimization. However, the full potential of parallelism has not been realized by existing techniques and frameworks in scaling to massive datasets, especially for applications that inherently demand recursive data accesses. <br/><br/>The project offers a theoretical methodology for tackling the problem of parallel query evaluation on massive data. The PI conjectures that to maximize parallelizability of generic queries, e.g., queries that are used frequently in analytical and transactional applications, one needs to examine queries that are inherently parallelizable as the basic unit of study. She identifies symmetric queries as a set of queries that are potentially highly parallelizable and will use such queries as a stepping stone to study parallelizable query languages and leverage the findings to design techniques for efficient evaluation of generic queries. In particular, the project focuses on three separate, yet highly related tasks: (1) design and study a set of query languages whose queries are symmetric, investigate the properties of these languages, and propose and prove theoretical bounds on the computational complexity of the languages, in terms of scaling and data skew; (2) investigate and propose data structures and algorithms for efficiently evaluating queries of these languages in a parallel manner; and (3) propose strategies including query rewrite and optimization techniques for efficient evaluation of arbitrary queries, based on the new data structures and algorithms that  result from (2).<br/><br/>During the exploratory phase of this project, the PI is conducting research activities in key areas in all three aforementioned topics. These will build the theoretical foundation, form strong collaborations with experts in related areas, and lay the groundwork for an effort suitable for a full-size XPS project. The research result of this project will be beneficial to both the database and the parallel computing communities as a new way to approach the problem of integrating the techniques of each.<br/><br/>The research methodology and algorithms developed is to be integrated into the undergraduate- and graduate-level database courses the PI teaches, as course materials and topics for course projects. Graduate students are supported by the project as research assistants. The PI works with various initiatives to recruit and encourage undergraduate students to participate in research activities."
"1845763","CAREER: Parallel Algorithms and Frameworks for Graph and Hypergraph Processing","CCF","SOFTWARE & HARDWARE FOUNDATION","03/01/2019","02/20/2019","Julian Shun","MA","Massachusetts Institute of Technology","Continuing grant","Almadena Chtchelkanova","02/29/2024","$109,535.00","","jshun@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7798","1045, 7942","$0.00","Graphs are a tool used to model the interactions between various entities. Efficiently processing large graphs has attracted significant attention due to its applications in various domains, such as biology, chemistry, and social network analysis.  With the explosion in the volume of data, graphs have become very large and can contain hundreds of billions of vertices and trillions of edges. Various applications also benefit from modeling the underlying data as a hypergraph, which enables relationships among multiple entities to be represented better than a graph.  In addition, many of these data sets are changing rapidly in real-time and applications require computing results on the latest data.  It is crucial to design high-performance parallel algorithms to enable analysis to be done on graphs and hypergraphs in a timely fashion.  However, writing efficient parallel code is notoriously difficult. Furthermore, many parallel algorithms used in practice today do not have strong theoretical guarantees, causing them to perform extremely poorly on certain inputs. To address these challenges, this project involves creating high-level programming frameworks with highly-optimized backends to make it easier for non-experts to write high-performance parallel programs for graphs and hypergraphs dealing with static and streaming data. This project also involves designing parallel algorithms that are efficient both in theory and in practice, so that they can perform well under all possible inputs and across many machine parameters, and scale gracefully to larger data sets.  Using the resulting algorithms and frameworks, scientists will be able to use high-level tools to perform graph and hypergraph analytics on massive inputs much more efficiently than before, both in theory and in practice.<br/><br/>This project involves designing new parallel primitives and algorithms for many fundamental graph and hypergraph problems that are fast and memory-efficient, both in theory and in practice.  The new algorithms are being evaluated on the largest publicly-available data sets using large-scale multicore machines. The project also involves creating high-level abstractions and programming frameworks to support the implementation of theoretically-efficient algorithms on both static and streaming data. First, a domain specific language for graph computations that generates efficient and highly-optimized code from high-level specifications of algorithms and optimizations will be designed.  Second, a unified framework for streaming graph analytics that can efficiently support parallel updates to the graph (simultaneously running algorithms and updates), incremental algorithms, and temporal analysis will be developed.  Finally, a novel abstraction and framework for hypergraph processing that will support theoretically-efficient implementations of hypergraph algorithms will be designed.  The results will lead to fundamental advances in parallel algorithm design and programming frameworks for graph and hypergraph analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617691","CIF: Small: Efficient Discriminant Analysis Through Parsimonious Probabilistic Models","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/28/2016","Xin Zhang","FL","Florida State University","Standard Grant","Phillip Regalia","06/30/2020","$414,215.00","Qing Mai","henry@stat.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7797","7923, 7936","$0.00","In the Big-Data era, there is a pressing need to develop more efficient and easy-to-interpret data analysis methods to face the new challenges rising from real life applications, such as cancer diagnosis, brain connectivity network and neuroimaging studies. Besides their intimidating sizes, data sets in such applications demand researchers to understand the intrinsic and complex structure behind them. This research involves new directions in discriminant analysis and classification by exploring state-of-art statistical methods and contemporary computation techniques. The investigators will develop new statistical methods and computational tools to utilize the intrinsic structure of the data, such as high correlation, network structure and tensor/array data structure. The research results will advance the research in statistics, machine learning, text mining, bio-medical research, finance, neuroimaging analysis, among other fields. The research will also be integrated with substantial educational and outreach activities.<br/><br/>In this research, the investigators aim to develop parsimonious probabilistic discriminant analysis models for efficiently analyzing data with intrinsically complicated structures, and for improved estimation of parameters and accuracy in predictions. Three sets of problems will be investigated: (1) parsimonious linear discriminant analysis with envelope, aiming to integrate a nascent technique of envelope modeling with the classical linear discriminant analysis model; (2) simultaneous discriminant analysis and differential network estimation, aiming to develop a novel and unified framework for simultaneously studying differential networks, estimating multiple covariance matrices, and training quadratic discriminant analysis classifier; and (3) sparse tensor discriminant analysis with feature selection, aiming to develop a sparse discriminant analysis method for tensor-valued data that directly uses the tensor-valued features for discriminant analysis, while simultaneously achieving feature selection and preserving interpretable tensor structure."
"1922840","CIF: Small: Collaborative Research: Sparse and Low Rank Methods for Imbalanced and Heterogeneous Data","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2018","02/12/2019","Vishal Patel","MD","Johns Hopkins University","Standard Grant","Phillip Regalia","06/30/2019","$59,956.00","","vpatel36@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7797","7923, 7936","$0.00","In recent years, sparse and low-rank modeling techniques have emerged as powerful tools for efficiently processing visual data in non-traditional ways. A particular area of promise for these theories is visual recognition, where object detection and image classification approaches need to be able to deal with the highly diverse appearance of real-world objects. However, existing visual recognition methods generally succeed only in the presence of sufficient amounts of homogeneous and balanced training data that are well matched to the actual test conditions. In practice, when the data are heterogeneous and imbalanced, the performance of existing methods can be much worse than expected.<br/><br/>This project will develop a comprehensive framework for real-world visual recognition based on novel sparse and low-rank modeling techniques, which will be able to deal with imbalanced, heterogeneous and multi-modal data. Imbalanced data will be handled using convex optimization techniques that automatically divide a dataset into common and rare patterns, and select a small set of representatives for the common patterns that are then combined with the rare patterns to form a balanced dataset. Heterogeneous and multi-modal data will be handled using non-convex optimization techniques that learn a latent representation from multiple domains or modalities. Classification and clustering algorithms can be applied to the latent representation. Applications of these methods include image and video-based object recognition, activity recognition, video summarization, and surveillance."
"1733794","AitF: Provenance with Privacy and Reliability in Federated Distributed Systems","CCF","Algorithms in the Field","09/01/2017","08/11/2017","Sampath Kannan","PA","University of Pennsylvania","Standard Grant","Tracy Kimbrel","08/31/2021","$309,900.00","Val Tannen, Andreas Haeberlen","kannan@central.cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7239","","$0.00","The goal of this project is to investigate the question of provenance in federated distributed systems, such as networks and scientific workflows, where several self-interested entities generate and share data, and compute and make decisions on the shared data.  Computations and decisions give rise themselves to new shared data.  Provenance analysis allows us to understand the contributions of the various entities to the data.  This project seeks to put provenance on a sound mathematical foundation by unifying the theoretical notion of semiring provenance with practical approaches to network provenance.  The PIs would also like to design new algorithms and compression techniques for collecting and managing provenance data.  A second thrust of the project is to compute and associate reliability scores to each data item, in conjunction with their provenance.  In order to do this, the PIs will design techniques for assigning reliability scores to primitive data items, as well as a calculus based on sound axiomatic principles for assigning reliability scores to derived data items.  This project will achieve broad impact by allowing for networks to operate more reliably and by enhancing reproducibility in scientific workflows."
"1618677","CIF: Small: Collaborative Research: Sparse and Low Rank Methods for Imbalanced and Heterogeneous Data","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/30/2016","Vishal Patel","NJ","Rutgers University New Brunswick","Standard Grant","Phillip Regalia","03/31/2019","$249,152.00","","vpatel36@jhu.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7797","7923, 7936","$0.00","In recent years, sparse and low-rank modeling techniques have emerged as powerful tools for efficiently processing visual data in non-traditional ways. A particular area of promise for these theories is visual recognition, where object detection and image classification approaches need to be able to deal with the highly diverse appearance of real-world objects. However, existing visual recognition methods generally succeed only in the presence of sufficient amounts of homogeneous and balanced training data that are well matched to the actual test conditions. In practice, when the data are heterogeneous and imbalanced, the performance of existing methods can be much worse than expected.<br/><br/>This project will develop a comprehensive framework for real-world visual recognition based on novel sparse and low-rank modeling techniques, which will be able to deal with imbalanced, heterogeneous and multi-modal data. Imbalanced data will be handled using convex optimization techniques that automatically divide a dataset into common and rare patterns, and select a small set of representatives for the common patterns that are then combined with the rare patterns to form a balanced dataset. Heterogeneous and multi-modal data will be handled using non-convex optimization techniques that learn a latent representation from multiple domains or modalities. Classification and clustering algorithms can be applied to the latent representation. Applications of these methods include image and video-based object recognition, activity recognition, video summarization, and surveillance."
"1535821","AitF: FULL: Collaborative Research: Better Hashing for Applications: From Nuts & Bolts to Asymptotics","CCF","Algorithms in the Field","09/01/2015","08/06/2015","David Andersen","PA","Carnegie-Mellon University","Standard Grant","Tracy Kimbrel","08/31/2019","$250,000.00","","dga@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7239","012Z","$0.00","This project engages experts in systems and network algorithms from Carnegie Mellon University and Harvard University to improve hashing-based data structures for systems. Hashing is an approach that turns a variable length string into a small, fixed-length value. Hashing provides a short, consistent fingerprint used to identify larger pieces of data, for uses including storing and locating data items quickly and effectively. Hashing provides a key building block for sophisticated approaches to storing, measuring, and managing data. Hashing-based data structures have correspondingly become widely accepted, often key workhorses throughout systems and networking. <br/><br/>This project will create synergies between theory and systems in the area of hashing, with various approaches for lasting broader impact. Prototype code will be released for new algorithms and data structures created in the course of the project. Curricular materials focused on project material will be developed and distributed. The project will offer a wide range of research opportunities at various levels of sophistication for graduate and undergraduate students at both universities. <br/><br/>The team unites expertise with theoretical design and analysis with expertise in systems design and analysis, allowing ideas and insights to flow between the two sides. The work starts from the lowest level of what choice of what hash functions to use, through the design and analysis of general data structures, to the development of applications that utilize hashing-based data structures to provide top performance. Project goals include both improving existing structures such as Bloom filters and cuckoo hash tables in practical systems to developing new structures for related problems such as maintaining small structures for fast function evaluation on key sets and reconciliation of datasets."
"1552497","CAREER: Inference-Driven Data Processing and Acquisition: Scalability, Robustness and Control","CCF","COMM & INFORMATION FOUNDATIONS","02/15/2016","09/14/2018","George Atia","FL","University of Central Florida","Continuing grant","Phillip Regalia","01/31/2021","$425,754.00","","George.Atia@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7797","1045, 7797, 7936","$0.00","Ubiquitous sensing has enabled new services and applications that mark<br/>every aspect of our lives. However, the ensuing data deluge is a brick<br/>wall in face of our abilities to interpret, process and store data. Also, not<br/>all the data collected is informative, nor well-attuned to the tasks we<br/>care to accomplish. This project introduces new approaches for data<br/>processing that hold promise to bring about stunning speedups in the<br/>processing of massive data, and explores principled controlled data<br/>acquisition paradigms for numerous inference problems. The research<br/>activities are expected to advance the theory and practice of data<br/>processing and acquisition in emerging cyber-physical systems for civil<br/>infrastructure, healthcare, energy, and transportation.<br/><br/>The research involves 1) developing, and analyzing the fundamental<br/>limits of, transformative subspace-based approaches to data processing<br/>that are simultaneously scalable and robust to outliers using subspace<br/>pursuit in structure-preserving data sketches, and data-subspace<br/>formulations which are invariant to transformations that preserve the<br/>underlying low-dimensional data structures, 2) exploring controlled data<br/>acquisition in asymptotic regimes of large number of observations with<br/>relaxed notions of optimality to unravel the complete structure of<br/>optimal design and recognize unifying principles for the design of<br/>efficient control policies for a host of controlled inference problems. The<br/>practical implications of the fundamental results are studied in the<br/>context of structural health monitoring for damage detection and<br/>characterization of engineering structures. The educational activities<br/>include establishing a new miniature sensing lab, developing an<br/>introductory course on learning from big data and sequential analysis,<br/>compiling a corpus of intuitive tutorials laying out the core concepts of<br/>research results, and mentoring of senior design projects."
"1618118","SHF: Small: End-To-End Test Data Analytics For Automotive Chip Production Lines","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/10/2016","Li-Chung Wang","CA","University of California-Santa Barbara","Standard Grant","Sankar Basu","05/31/2020","$400,000.00","","licwang@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7798","7923, 7945","$0.00","Using automotive industry as the research driver, this project aims to develop novel data mining solutions to address the challenge of reliable low-power, high quality chip production by improving the effectiveness and reducing the cost of testing them. While research in the data mining community focuses more on developing generic approaches, this project focuses on developing dedicated approaches, optimized for mining test measurement data. The techniques developed in this project can complement and bring synergies to existing data mining research. The technologies developed through this project will be transferred to the industry, providing solutions to overcome the challenges in the automotive chip sector. Furthermore, the research will be integrated with educational activities to produce publications, curriculum materials, tutorials, and software tools for broader impacts to the semiconductor industry. <br/><br/>In production, automotive chip products go through a rather comprehensive test process to assure their quality. End-to-end test data refers to all data collected in this process which comprises multiple stages, starting from product manufacturing all the way to evaluation in an electronic system. Analytics refers to the discovery, interpretation, and utilization of knowledge extracted from the data. This research aims to enable effective and robust analytics in order to improve product quality and reduce production test cost. For effectiveness, novel software tools and methodologies will be designed to automatically incorporate domain knowledge in the analytics. For robustness, new approaches will be developed to determine the meaningfulness of data mining results. From a practical perspective, solutions developed through the research will benefit the semiconductor industry by facilitating the effective use of large-scale data mining for test process optimization. From a scientific perspective, this research will provide a deeper understanding of the limitations with test data analytics and enable its robust implementation. Overall, this project aims to develop the next-generation test data analytics software, thereby enabling application of the research to diverse scenarios encountered in semiconductor chip production test environments."
"1815949","SHF: Small: Lazy Data Structures for Data-Intensive Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","07/16/2018","Yu David Liu","NY","SUNY at Binghamton","Standard Grant","Anindya Banerjee","09/30/2021","$449,827.00","","davidl@cs.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","7798","7923, 7943","$0.00","Developing and optimizing data-intensive applications is a crucial but challenging goal in the Big Data era. This project aims to research and design a novel programming system to improve the performance and assurance of data-intensive applications. The project's novelties are (i) laying a new foundation for programming, optimizing, and reasoning about Big Data systems, and (ii) building a practical software ecosystem to improve the quality of data-intensive applications. The project's impacts are (i) shedding fundamental insight in data-intensive programs, with a broad range of applications from social network analysis to artificial intelligence; (ii) enabling new curriculum development, and bringing underrepresented students to the exciting frontier of data-intensive computing. <br/><br/>The project centers around the idea of data-centric laziness: the operations to be performed over data structures --- such as topological changes or payload queries --- may be delayed and flexibly memoized within the data structure itself in a decentralized manner. The project is carried out in several directions. First, it conducts a foundational study on laziness in the presence of data processing, including a rigorous study on the subtleties in designing a lazy propagation system, a proof of observable equivalence between lazy and eager data processing, a cost-based semantics for capturing lazy behaviors, and a unification of eagerness vs. laziness and data vs. computation. Second, it investigates how parallelism and laziness interact to improve the performance of lazy data structures, through the support of asynchronous data processing, in-data propagation parallelism, and concurrent garbage collection of propagation labels. Third, it bridges the language foundation with practical algorithm design and system building, exploring algorithm-oriented programming abstractions, partition-based out-of-core data processing, just-in-time data structure re-organization, and propagation-aware performance monitoring.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816409","CIF:  NSF-BSF:  Small:  Collaborative Research:  Characterization and Mitigation of Noise in a Live DNA Storage Channel","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/08/2018","Farzad Hassanzadeh","VA","University of Virginia Main Campus","Standard Grant","Phillip Regalia","09/30/2021","$312,749.00","Mete Civelek","ffh8x@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7797","014Z, 7923, 7935","$0.00","Deoxyribonucleic acid (DNA) is emerging as a potential solution to growing data storage needs, due to recent advances in DNA synthesis and sequencing. Compared to silicon-based storage, DNA offers higher density and data longevity. Data-bearing DNA can be stored in non-biological environments (in vitro) or be embedded into the DNA of living organisms such as bacteria or plants (in vivo). In vivo DNA data storage benefits from a natural protective shell and a reliable and cost-efficient way to copy data. Furthermore, the compatibility of in vivo storage with living cells creates a potential for advances in synthetic-biology algorithms that require a mechanism for data storage. The goal of this project is to develop and demonstrate an in vivo DNA data storage system that is resilient to errors arising during sequencing and synthesis and from mutations, while achieving the maximum possible data density.<br/><br/>To achieve its goal, the project relies on the following research thrusts: 1) Characterization of the live DNA channel and 2) Error mitigation and correction. Errors in DNA storage are known to be context-dependent, i.e., they depend on the local structure of the sequence. To enable error control, a mathematical model for the channel of live DNA storage will be formulated and the dependence of error rates on sequence context will be characterized. This project will take advantage of new advances in gene-editing technologies, in particular the CRISPR/Cas system, to characterize the channel through experiments. The second thrust aims to overcome the limitations of the existing error-control techniques, which either do not provide adequate error protection or are highly redundant. The project will develop semiconstrained-coding techniques for limiting the frequency of patterns with high error rates and high-rate error-correction schemes for combating errors. The semiconstrained and error-correction methods will be adapted to the live DNA channel and then implemented to evaluate their performance in practice and to demonstrate their advantages.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822939","SPX: Collaborative research: Scalable Heterogeneous Migrating Threads for Post-Moore Computing","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/27/2018","Peter Kogge","IN","University of Notre Dame","Standard Grant","Vipin Chaudhary","09/30/2021","$524,483.00","","kogge@cse.nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","042Y","026Z","$0.00","The project will advance the state of the art in computer architecture and programming systems for extreme and heterogeneous parallelism. It is clear that the post-Moore' law era will require major disruptions in computing systems. This project will address computer architecture and programming system challenges for this new era, with a focus on approaches that are expected to be scalable in size, cost effectiveness, and usability by retaining some tenets of the von Neumann computing model (unlike more exploratory approaches like biological or quantum computing). By emphasizing data analytics, the work will also benefit a rapidly growing swatch of modern life (commercial, cyber, national security, social networks). A deeper understanding of how such applications can be made more scalable, and responsive enough to handle increasing real-time requirements, should lead to wider impacts across every-day life with significant potential for technology transition. There is also a direct connection to pedagogy and workforce development, since both hardware and software aspects of this proposal can enable a broad range of students to better understand the wider diversity of computing platforms projected in future technology roadmaps. <br/><br/><br/>The SHMT (Scalable Heterogeneous Migrating Thread) model developed in this award will include extensions to the migrating threads and asynchronous task models to support heterogeneity, and extensions to the transaction and actor models to support data coherence. Further, the investigators propose to use data analytic graph problems to evaluate their research, since these applications are both important in practice and are challenging to solve on current systems. Given the expected continued increase in the size, complexity, and dynamic nature of such computations, it is of growing value to understand how to implement them in a manner that can scale to very high levels of concurrency in environments that include high rate streams of both updates and queries. These techniques can also apply to other application classes, such as scientific applications where data is sparse or irregular. The overall objective of this 3-year research project is to advance the foundations of computer architecture and programming systems to address the emerging challenges of scalable parallelism and extreme heterogeneity, with an emphasis on data analytics and solving data coherence, system management, resource allocation, and task scheduling issues. The investigators will leverage their distinct but synergistic expertise in the architecture and programming systems areas by building on, and integrating, their past work on migrating threads and near-memory processing, software support for asynchronous task parallelism for heterogeneous computing, and data analytics. The Center for Research into Novel Computing Hierarchies (CRNCH) at Georgia Tech will provide access to first-of-a-kind alternative systems for use in evaluating the new concepts. Industrial collaborators include Lexis-Nexis Risk Solutions and Kyndi, for whom graph data analytics are central to their business model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822972","SPX: Collaborative Research: Memory Fabric: Data Management for Large-scale Hybrid Memory Systems","CCF","SPX: Scalable Parallelism in t, SOFTWARE & HARDWARE FOUNDATION","10/01/2018","07/27/2018","Ada Gavrilovska","GA","Georgia Tech Research Corporation","Standard Grant","Yuanyuan Yang","09/30/2021","$450,000.00","Greg Eisenhauer","ada@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","042Y, 7798","026Z, 7941, 7942, 9102","$0.00","New large-scale high performance computing systems being developed for the national labs and by US industry, combine heterogeneous memory components, accelerators and accelerator-near memory, and programmable high-performance interconnects. These memory-rich designs are attractive as they provide the compute-near-data capacity needed for improving the time to scientific discovery, and for supporting new classes of latency-sensitive data-intensive applications. However, existing software stacks are not equipped to deal with the heterogeneity and complexity of these machine designs, which impacts application performance and machine efficiency. The Memory Fabric (MF) solution developed in this project provides new abstractions and mechanisms that permit the systems software stacks to gain deeper insight into applications' data usage patterns and requirements, and to coordinate the decisions concerning how data should be distributed across different memories, or exchanged along different interconnection paths. <br/><br/>The Memory Fabric (MF) architecture introduces new data-centric abstractions, memory object and memory object flow, and accompanying memory and communications management methods. The higher-level information captured in the new abstractions empowers the MF runtime to better guide the underlying memory and interconnect management, and to mask the complexities of the underlying memory substrate. Additional benefits are derived from use of near-memory-fabric computation, including via dynamically inserted application-specific codes, which further specialize and accelerate the operations carried out by MF. MF is evaluated using several important application domains, including big data learning and analytics, and traditional high-performance scientific simulations. Its benefits include gains in application performance and resource efficiency, while shielding applications and application developers from the underlying machine details.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1637458","AitF: Collaborative Reserach:  Theory and Implementation of Dynamic Data Structures for the GPU","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS","09/01/2016","05/09/2017","Martin Farach-Colton","NJ","Rutgers University New Brunswick","Standard Grant","Tracy J. Kimbrel","08/31/2020","$365,409.00","","farach@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7239, 7796","7926, 9251","$0.00","Computers organize data in ""data structures,"" which are designed to allow certain operations on data such as looking up all items that match a particular set of criteria, or adding new items to an existing data set.  Computer scientists strive to build data structures that can perform these operations quickly and efficiently.  One way to make data structure operations faster is to use not just one but many processors, operating in parallel, to perform a given operation.  However, many of today's parallel data structures support only a limited set of operations and, notably, do not allow operations that modify these data structures instead of rebuilding an entire structure from scratch when only part of the data is updated.  In this project the PIs bring together expertise in data structures and parallel computing to design, build, and evaluate dynamic data structures that allow update operations.  This work targets the high-performance, highly-parallel graphics processing unit (GPU) and will significantly broaden the class of applications that the GPU can address.  The PIs will release their results as freely-available open-source software and will work with industrial partner NVIDIA to incorporate the research and educational outcomes of this project into NVIDIA's broad educational efforts.<br/><br/>In this project the PIs propose to build dynamic, high-performance data structures for manycore (GPU) computing.  Today's GPU data structures are rarely constructed on the GPU but instead are built on the CPU and copied to the GPU, and today's GPU data structures cannot be updated dynamically on the GPU but instead must be rebuilt from scratch.  This project targets dynamic dictionary data structures with point and range queries, lists, and approximate membership and range query structures.  The PIs will implement these data structures as high-performance, flexible, open-source software and use these data structures to develop a theoretical model, targeted at the GPU, for use by theorists and practitioners in manycore computing.  The project will also focus on numerous cross-cutting issues in data structure design, implementation, modeling, and evaluation that have the potential for significant practical impact on manycore computing."
"1535795","AitF: FULL: Collaborative Research: Better Hashing for Applications:  From Nuts & Bolts to Asymptotics","CCF","Algorithms in the Field","09/01/2015","08/06/2015","Michael Mitzenmacher","MA","Harvard University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$250,000.00","","michaelm@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7239","012Z","$0.00","This project engages experts in systems and network algorithms from Carnegie Mellon University and Harvard University to improve hashing-based data structures for systems.  Hashing is an approach that turns a variable length string into a small, fixed-length value.  Hashing  provides a short, consistent fingerprint used to identify larger pieces of data, for uses including storing and locating data items quickly and effectively.  Hashing provides a key building block for sophisticated approaches to storing, measuring, and managing data. Hashing-based data structures have correspondingly become widely accepted, often key workhorses throughout systems and networking.<br/><br/>This project will create synergies between theory and systems in the area of hashing, with various approaches for lasting broader impact. Prototype code will be released for new algorithms and data structures created in the course of the project.  Curricular materials focused on project material will be developed and distributed.  The project will offer a wide range of research opportunities at various levels of sophistication for graduate and undergraduate students at both universities.<br/><br/>The team unites expertise with theoretical design and analysis with expertise in systems design and analysis, allowing ideas and insights to flow between the two sides.  The work starts from the lowest level of what choice of what hash functions to use, through the design and analysis of general data structures, to the development of applications that utilize hashing-based data structures to provide top performance.  Project goals include both improving existing structures such as Bloom filters and cuckoo hash tables in practical systems to developing new structures for related problems such as maintaining small structures for fast function evaluation on key sets and reconciliation of datasets."
"1718658","CIF:Small:Collaborative Research:Codes for Storage with Queues for Access","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","06/05/2018","Pierce Cantrell","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","08/31/2020","$190,000.00","","p-cantrell@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7797","7923, 7935, 7937","$0.00","Large volumes of data, which are being collected for the purpose of knowledge extraction, have to be reliably, efficiently, and securely stored. Retrieval of large data files from storage has to be fast. Large-scale cloud data storage and distributed file systems have become the backbone of many applications such as web searching, e-commerce, and cluster computing. Cloud services are implemented on top of a distributed storage layer that acts as a middleware to the applications, and also provides the desired content to the users, whose interests range from performing data analytics to watching movies. Users of cloud systems demand that their content and services be readily available and their data be reliably stored. Although there are apparent connections and trade-offs between these two objectives, so far they have been addressed mostly separately. The proposed research will characterize the interplay between reliable data storage and fast data access, and develop methods to jointly optimize these two main objectives of cloud storage. The project team will develop the methodology for design, analysis, and performance evaluation of a broad range of techniques for distributed storage that enable high reliability, robustness, and fast data retrieval. The research team will also develop schemes for storage and distributed computing which will optimize both reliability and the latency of data access. The methodology developed in the course of this project will allow the system operators to make their content and services readily available to users and support delay-sensitive applications ranging from individual video streaming to using online collaborative tools.  This research will minimize the energy requirements of data centers, which have increased massively in recent years. The project will contribute to the broader areas of coding theory, information theory, and queueing theory, and open new ways of cross-pollination.<br/><br/>This project focuses on efficient data access in distributed file systems that employ codes for reliable and efficient storage. Users of cloud systems demand that their content and services be readily available and their data be reliably stored. Although there are apparent connections and trade-offs between these two objectives, thus far they have been addressed mostly separately. This proposal follows findings that analyze how some of today?s solutions for reliable data storage affect the speed of data download under certain access models. This preliminary research has shown that, in some scenarios, the coding schemes used for increasing storage reliability can be further exploited for fast data access, while in others, the coding schemes that seemingly increase data availability actually fail to provide efficient access to popular content (so-called ?hot data?). The proposed research aims to characterize the interplay between reliable data storage and fast data access, and develop methods to jointly optimize these two main objectives of cloud storage. The proposed research will first identify and design schemes for coded-data access and derive (bounds on and estimates of) the expected download time for these schemes. Regardless of which data access scheme is used, the expected download time will depend on realistic service models as well as the distributed system service capacity provisioning and allocation schemes, which are then addressed. The work will also focus on the connections between the proposed research and the areas of efficient distributed computing and reduction in data center energy consumption. Preliminary results indicate that these areas are closely connected and the techniques developed for one area can benefit other areas."
"1718093","AF: Small: Computational Methods for Large-scale Inference of Population History","CCF","COMPUTATIONAL BIOLOGY","09/01/2017","08/02/2017","Yufeng Wu","CT","University of Connecticut","Standard Grant","Mitra Basu","08/31/2020","$405,000.00","","ywu@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7931","7923, 7931","$0.00","Consider several unrelated human individuals from a population. It is a common sense that these individuals are descendants of some common ancestors if tracing backward in time long enough. Indeed, these sampled individuals share a genealogical history that specifies the ancestry of these individuals. This genealogy is very informative, since it can tell, e.g. which individuals are closely related. One potential application of the genealogy is understanding why some individuals are more susceptible to some phenotypic traits (such as diabetes or cancer) than others. It might be the case that individuals sharing a trait are more closely related to each other on the genealogy than the rest of the population. <br/><br/>Genealogy, although useful, cannot be directly observed. The so-called genetic variation provides hints on the underlying genealogy of the sampled individuals. One common type of genetic variations is the single nucleotide polymorphism (SNP). A SNP refers to the genomic position where individuals in a population may have different nucleotides at the position. A moment of thoughts suggests that individuals with the same nucleotide at a SNP tend to be more closely related than individuals with different nucleotides. This may allow one to infer the plausible underlying genealogy from genetic data collected at multiple SNPs. Inference of genealogy from real SNP data is, however, much more complex than this. One main difficulty is caused by meiotic recombination. Without recombination, genealogy can be modeled as a tree (similar to the usual tree of life model that has been extensively studied in biology). Recombination allows one genome to have more than one ancestor and thus violates the basic property of this simple tree model. Recombination essentially breaks down the genome into many small segments, where each segment may originate from different ancestors. That is, genealogical history at different genomic positions may be different. Genealogy with recombination is thus much more complex than that with no recombination.<br/><br/>This project aims to developing effective computational methods for analyzing large-scale population genetic data that has become available during the past several years. The main goals are first accurately inferring the genealogical history of sampled individuals from the genetic data, and then performing inference for several population genetic problems with the inferred genealogy. The successful completion of the proposed research will produce new computational tools and software that may allow population geneticists to better understand the implications of large-scale population genetic data. Potential applications of these tools include, for example, mapping the genomic positions that are associated with complex traits, inferring the population admixture history and finding regions of the genome that are under natural selection.<br/><br/><br/>The intellectual merits of this project are as follows. This project will develop efficient and accurate computational methods for inferring population history from haplotypes based on inferred gene genealogies. Gene genealogy refers to the evolutionary history of extant population haplotypes, and captures the underlying LD information. While gene genealogies are fundamental to population genetics, most existing inference methods don't use gene genealogies explicitly because genealogies are not directly observable. Inferring gene genealogies from haplotypes is just starting to become feasible, due to the latest development in genomic technologies and genealogy inference methods. This project aims to developing effective computational methods for the following two problems. First, new methods for inferring gene genealogies from haplotypes will be developed. Second, new methods for inferring population demographic history (e.g. population admixture) will be developed. Successful completion of the proposed research will produce new efficient and accurate algorithms that are implemented in practical software tools and allow population biologists to infer population history from genome-scale data.<br/><br/>The broader impacts of this project include the following. Developed software tools will be made available freely to the multidisciplinary research community, and are expected to enable novel biological applications in complex population history inference. Research results will be integrated into classroom teaching. The project will ensure broad dissemination of the research results and teaching materials. The proposed educational and outreach activities include training of future researchers with unique interdisciplinary skills."
"1717373","SHF:Small:Cooperative Garbage Collection for Big Data and Server Applications","CCF","CI REUSE, SOFTWARE & HARDWARE FOUNDATION","09/01/2017","07/07/2017","Samuel Guyer","MA","Tufts University","Standard Grant","Anindya Banerjee","08/31/2020","$447,031.00","","sguyer@cs.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","6892, 7798","7433, 7923, 7943, 8004","$0.00","Increasingly, critical parts of our computing infrastructure need to process huge quantities of data, and do so quickly and efficiently. These so-called ""big data"" applications range from familiar Internet search engines to industrial installations, such as factory and farm monitoring. Detailed environmental sensing for agriculture, for example, can generate many gigabytes of data per day for a single farm. The demands of these new applications differ substantially from the conventional programs that have shaped our existing computing systems. As a result, system support does not always serve these applications well, resulting in poor performance or increased resource requirements. The goal of this project is to study big data applications in detail and develop new system-level software to better support them. The project's broader significance and importance are that more data will be able to be processed more quickly and with less computing hardware, reducing costs and improving responsiveness for a wide array of applications.<br/><br/>The technical focus of this project is on improving support for big data applications in the Java virtual machine, specifically, in the garbage collector. The intellectual merits are derived from a three-part approach to the problem. The first involves using the project's GC tracing tool to study the memory patterns of big data and server applications in order to quantify and characterize them. The second part involves designing a set of configurable GC mechanisms, which allow applications to tailor memory management support to their specific needs. The third part is an industrial-strength implementation that is used to evaluate real workloads, and that is available to other researchers and to practitioners.<br/><br/>The project is releasing the developed tools as open source and is building a user community around the tools by ensuring that interested researchers are able to contribute to the codebase. This aspect is of special interest to the software cluster in NSF's Office of Advanced Cyberinfrastructure, which provides co-funding for this award."
"1617727","AF: Small: Collaborative Research: Maintaining order","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","05/20/2016","Jeremy Fineman","DC","Georgetown University","Standard Grant","Rahul Shah","08/31/2019","$239,703.00","","jfineman@cs.georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","7796","7923, 7926","$0.00","This project investigates ""order structures,"" that is, data structures for maintaining total orders on dynamic data sets.  Order structures are applicable in surprisingly diverse settings.  The PIs aim to answer fundamental theoretical questions relating to order structures. The investigation takes place in the context of two high-impact application areas: (1) tools for debugging parallel programs and (2) data structures used in databases and file systems to organize data on disk or SSDs.  As part of the project, the PIs will develop publicly available educational materials and reference implementations on order structures to be incorporated into courses at their institutions and shared openly on the web.<br/><br/>The order structures addressed in this project are order-maintenance data structures, sparse tables, and data structures for incremental topological ordering.  The investigation comprises (1) new algorithms with good worst-case guarantees, (2) algorithms with strong common-case guarantees, i.e., optimized for common input distributions, (3) provably good concurrent algorithms, (4) algorithms that leverage randomization in surprising ways (e.g., to achieve history independence), and (5) robust lower bounds that also apply to the randomized setting. In the context of the target applications, the PIs will design and use order structures to implement race detectors, which uncover determinacy races in parallel programs. The PIs will also use orders structures to build external-memory key-value stores to make databases and file systems run faster."
"1717712","SHF:Small:Data Structures and Transactions for Emerging Nonvolatile Memory","CCF","Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","09/01/2017","07/07/2017","Michael Scott","NY","University of Rochester","Standard Grant","Anindya Banerjee","08/31/2020","$449,937.00","","scott@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7354, 7798","7923, 7943","$0.00","Data that need to be persistent, either for the sake of long-term access or for recovery from system crashes, have traditionally been kept on disk and flash drives, which are very slow.  Within the next few years, much faster persistent memory is expected to be widely available at reasonable cost, raising the possibility that file systems might be replaced, at least in part, by memory that is simply ""always available."" The intellectual merits of the project lie in addressing two key challenges to such ""always available memory"": first, ensuring that the values in memory in the wake of a system crash are always mutually consistent, despite the possibility that traditional caches often pass data to the memory out of order; second, safeguarding the structural integrity of persistent data despite the possibility that buggy programs may erroneously modify arbitrary memory locations.  The project's broader significance and importance lie in the promise of significant improvements in programmability, reliability, and system performance, and in the ability to survive power outages and hardware failures at much lower cost than has previously been possible.  This latter benefit may be of particular importance for ubiquitous sensors in the Internet of Things.<br/><br/>The project builds on prior work by the principal investigator and colleagues, which has developed formal correctness criteria for persistent data, together with automatic methods to guarantee this correctness.  The current project is pursuing three major research thrusts.  First, it is developing a library of reusable, high-performance persistent data structures, with particular emphasis on exploiting high-level semantics to minimize instrumentation overhead, maintaining sufficient information to complete or undo partial operations in the wake of a program or system crash, and formalizing and proving correctness.  Second, the project is developing techniques to compose persistent operations into larger atomic transactions.  This work builds on past experience with hardware and software transactional memory, and encompasses both nonblocking and lock-based approaches.  Particular emphasis is being placed on ""boosting"" the operations of persistent data structures so that they can serve as reversible high-level operations of a transactional system.  Third, the project is developing mechanisms (including user-level daemons, compiler-based sandboxing, fine-grain memory protection, and the use of virtualization hardware) to ensure that persistent data is modified only by trusted library code, thereby safeguarding its structural integrity in the presence of buggy applications."
"1618637","CIF: Small: Collaborative Research: Sparse and Low Rank Methods for Imbalanced and Heterogeneous Data","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","05/08/2018","Rene Vidal","MD","Johns Hopkins University","Standard Grant","Phillip Regalia","06/30/2019","$258,000.00","","rvidal@cis.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7797","7923, 7935, 7936, 9251","$0.00","In recent years, sparse and low-rank modeling techniques have emerged as powerful tools for efficiently processing visual data in non-traditional ways. A particular area of promise for these theories is visual recognition, where object detection and image classification approaches need to be able to deal with the highly diverse appearance of real-world objects. However, existing visual recognition methods generally succeed only in the presence of sufficient amounts of homogeneous and balanced training data that are well matched to the actual test conditions. In practice, when the data are heterogeneous and imbalanced, the performance of existing methods can be much worse than expected. <br/><br/>This project will develop a comprehensive framework for real-world visual recognition based on novel sparse and low-rank modeling techniques, which will be able to deal with imbalanced, heterogeneous and multi-modal data. Imbalanced data will be handled using convex optimization techniques that automatically divide a dataset into common and rare patterns, and select a small set of representatives for the common patterns that are then combined with the rare patterns to form a balanced dataset. Heterogeneous and multi-modal data will be handled using non-convex optimization techniques that learn a latent representation from multiple domains or modalities. Classification and clustering algorithms can be applied to the latent representation. Applications of these methods include image and video-based object recognition, activity recognition, video summarization, and surveillance."
"1801855","CIF: Small: Collaborative Research: Network Event Detection with Multistream Observations","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2017","11/30/2017","Yingbin Liang","OH","Ohio State University","Standard Grant","Phillip Regalia","06/30/2019","$184,277.00","","liang.889@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7797","7923, 7936","$0.00","The goal in network event detection is to detect the existence of a set of nodes over a large network whose observations reflect the occurrence of an unusual event. Existing studies of network event detection have been mainly from two perspectives. The first is data-driven without assuming any underlying statistical model, and is typically applicable to more general data sets, but may not come with performance guarantees. The second perspective is model-driven, with certain statistical distributions (e.g., Gaussian) assumed for the data, and usually comes with performance guarantees, but may be limited to applications where the data fit the model. The goal in this project is to explore a framework for network event detection that unifies a wide range of event detection problems, in which the data are assumed to be governed by some underlying statistical distributions, but is data-driven in the sense that little is assumed a priori about the distributions. The developed detection approaches and statistical tools have a wide range of applications, including fraud detection, clinical trials, medical diagnosis, high-frequency trading, voting irregularity analysis, and network intrusion.<br/><br/>A comprehensive approach to general network event detection problems is developed in this project through the exploration of three thrusts: (i) detection of (unstructured) point events, (ii) detection of graph-based structured events, and (iii) sequential and quickest detection of dynamically evolving graph structures. The performance of the designed tests is characterized in terms of the probability of detection error and the rate at which this error goes to zero. Various fundamental issues are addressed, including non-i.i.d. data streams, as well as the interplay between network size, event size, sample size, and data dimension."
"1703051","SHF: Medium: A Visual Cloud for Virtual Reality Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2017","07/18/2017","Magdalena Balazinska","WA","University of Washington","Standard Grant","Almadena Y. Chtchelkanova","07/31/2021","$916,000.00","Luis Ceze","magda@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7798","7924, 7942, 9102, 9251","$0.00","The ability to collect videos can revolutionize interactions with the world by enabling powerful virtual reality video applications in education, tourism, tele-presence and others. These new applications involve processing and serving 360-degree stereoscopic videos, which require a dramatic improvement in technology to manage and process the massive-scale visual data necessary for truly immersive experiences. Systems that support VR video also represent an excellent educational tool as students can experience scenes in a truly immersive way and hence better convey content. <br/>This project builds a visual cloud that provides seamless access to a new database management system with hardware acceleration and edge computation that enables the efficient and real-time management of massive-scale image data and virtual reality (VR) applications built on top of it. <br/>The project develops a new hardware and software stack for VR data processing with execution in public clouds. The stack includes a new storage manager that significantly increases data ingest and retrieval throughputs for multidimensional array data compared with existing systems, as motivated by the extreme needs of VR applications. The storage manager utilizes novel hardware technologies (non-volatile memory) and provides novel approximate and multi-resolution data storage capabilities. The project also develops a new runtime system for high-throughput and large-scale array processing by developing a new API for expressing VR pipelines as a graph of user-defined functions, a library of specialized implementations of known VR algorithms for different types of hardware (CPU, GPU, FPGA, and 3D XPoint), and associated optimizers and schedulers. Finally, the project develops new techniques to enable real-time VR applications. They include an FPGA-based acceleration platform for real-time VR video processing and algorithms and software components for prefetching and caching VR data close to the viewers and processing that data in the viewing device.  http://visualcloud.cs.washington.edu"
"1618658","CIF: Small: Collaborative Research: Network Event Detection with Multistream Observations","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/30/2016","Venugopal Veeravalli","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","06/30/2019","$280,000.00","","vvv@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7936","$0.00","The goal in network event detection is to detect the existence of a set of nodes over a large network whose observations reflect the occurrence of an unusual event. Existing studies of network event detection have been mainly from two perspectives. The first is data-driven without assuming any underlying statistical model, and is typically applicable to more general data sets, but may not come with performance guarantees. The second perspective is model-driven, with certain statistical distributions (e.g., Gaussian) assumed for the data, and usually comes with performance guarantees, but may be limited to applications where the data fit the model. The goal in this project is to explore a framework for network event detection that unifies a wide range of event detection problems, in which the data are assumed to be governed by some underlying statistical distributions, but is data-driven in the sense that little is assumed a priori about the distributions. The developed detection approaches and statistical tools have a wide range of applications, including fraud detection, clinical trials, medical diagnosis, high-frequency trading, voting irregularity analysis, and network intrusion.<br/><br/>A comprehensive approach to general network event detection problems is developed in this project through the exploration of three thrusts: (i) detection of (unstructured) point events, (ii) detection of graph-based structured events, and (iii) sequential and quickest detection of dynamically evolving graph structures. The performance of the designed tests is characterized in terms of the probability of detection error and the rate at which this error goes to zero. Various fundamental issues are addressed, including non-i.i.d. data streams, as well as the interplay between network size, event size, sample size, and data dimension."
"1527130","CIF: Small: Collaborative Research:Synchronization and Deduplication of Distributed Coded Data: Fundamental Limits and Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2015","06/29/2015","Lara Dolecek","CA","University of California-Los Angeles","Standard Grant","Phillip Regalia","06/30/2019","$150,000.00","","dolecek@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7797","7923, 7935","$0.00","Part 1: Coding for distributed storage systems has garnered significant attention in the past few years due to the rapid development of information technologies and the emergence of Big Data formats that need to be stored and disseminated across large-scale networks. As typical distributed systems need to ensure low-latency data access and store a large number of files over a set of nodes connected through a communication network, it is imperative to develop new distributed coding schemes that protect the systems from undesired component failures. The two key functionalities of codes used in distributed systems, namely the reconstruction of files via access to a subset of the nodes and repair of failed nodes, need to be retained when the files are accessed and processed by the users via symbol/block insertion, deletion, or substitution edits. Deletions frequently arise due to system-level data deduplication: when parts of files are deduplicated or edited, the changes in the information content need to be communicated to the redundant storage nodes with minimum communication cost. Current solutions for synchronizing data that underwent edits assume that data is uncoded and they do not fully exploit the distributed nature of information. Furthermore, they mostly ignore the presence of deduplication protocols. This makes distributed storage architectures inefficient in terms of storage, user access times, and error protection. Hence, the goals of the proposed research program are to develop a new set of protocols and coding schemes that will  support a new generation of versatile and updatable coded distributed storage systems. <br/><br/>Part 2: Building on the preliminary work of the investigators, this proposal aims to set the foundations of the new field of coded synchronization and deduplication, with the goal of deriving fundamental performance limits, developing efficient algorithmic solutions for the two families of problems, and constructing new distributed storage codes that enable synchronization of coded data and coded deduplication. In particular, the proposal addresses the following comprehensive issues: <br/>1) Characterizing the communication rate limits of known and new (un)coded synchronization schemes, trade-offs between deduplication and data repair performance for different structured or encoded data formats and different types of communication channels.<br/>2) Introducing and analyzing the communication rate-distortion (CRD) function for approximate synchronization and deduplication of structured/encoded data, with a special focus on delay-sensitive applications.<br/>3) Developing dynamically updatable synchronization and deduplication algorithms cognizant of the network topology and of different prioritization needs of the users, as encountered in image and video data coding."
"1816913","CIF: Small: Collaborative Research:Leveraging Data Popularity in Distributed Storage Systems via Constrained Design Theory","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/19/2018","Olgica Milenkovic","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","09/30/2021","$250,000.00","","milenkov@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7935, 9102","$0.00","Recent years have witnessed a surge of large-scale distributed storage system implementations and accompanying data analyses and coding methods that enable their reliable, secure and low-delay operation. Nevertheless, in many system studies, important data demand (popularity) features which have a strong bearing on system access control, private information retrieval and computational complexity have been largely overlooked. This can be attributed in part to the fact that most cloud storage facilities employ different storage platforms for hot and cold data, thereby partly addressing problems associated with variable data demands. But even within the hot and cold data categories there exist significant variations in data popularity that create many nontrivial system design challenges.<br/><br/>To address these issues, the proposed research program aims to develop a new family of mathematical objects termed constrained designs and Steiner systems in particular. Designs represent finite collections of subsets of a ground set whose elements satisfy predefined symmetry constraints with respect to set intersections and arrangements. Elements of a design are associated with data chunks, while subsets of elements represent data chunks to be stored on the same disk or server; given their simplicity and rich mathematical structure, designs have been used with great success in many practical distributed storage system platforms. In the presence of nonuniform demands for objects and data files, intersection constraints alone fail to ensure underlying implementation constraints. Consequently, elements have to be equipped with nonnegative popularity values, and the underlying combinatorial designs modified to satisfy additional algebraic and frequency constraints enforced by data popularity values. This new model leads to a unique collection of challenging mathematical problems regarding constructions of weighted and labeled combinatorial designs. Particular problems to be considered include developing designs for balanced server access, private information retrieval in the presence of popularity side information and transversal designs for labeled batch codes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814026","AF: Small: Fundamental Problems in Geometric Data Structures","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","05/14/2018","Timothy Chan","IL","University of Illinois at Urbana-Champaign","Standard Grant","Tracy J. Kimbrel","09/30/2021","$500,000.00","","tmc@illinois.edu    Role: Principal Investigator","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7923, 7929","$0.00","In today's era of big data, we frequently encounter large amounts of data that are geometric, or that may be represented as points or objects in geometric spaces. There is a growing need for the design of efficient data structures that can process and answer queries about such geometric data quickly. This project focuses on some of the most fundamental and central data structuring problems in computational geometry, and explores new ideas to obtain improved results on these longstanding problems. The research project will be integrated with the development of courses for undergraduate and graduate students that incorporate the latest research findings on geometric data structures.<br/><br/>Among the fundamental problems studied are point location and nearest neighbor search. The project will also investigate geometric data structure problems in new settings that arise from modern-day applications, including distance-related problems in high dimensions, distance-related problems concerning unit-disk graphs (frequently used to model ad hoc wireless networks) and other types of geometric graphs, and streaming algorithms that can process high volumes of geometric data quickly with a limited amount of storage space.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815303","SHF: Small: Using Software Defined Cache to Accelerate Index Search for In-memory Applications: Software and Hardware Approaches","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","07/02/2018","Song Jiang","TX","University of Texas at Arlington","Standard Grant","Yuanyuan Yang","09/30/2021","$345,000.00","","song.jiang@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","7798","7923, 7941","$0.00","Memory is one of the most important components in large-scale data centers. As many software systems for big data processing keep their data sets entirely in memory to enable high-performance in-memory computing, memory efficiency becomes critical to application performance. While the applications, such as database systems and big data analytics, often serve as an infrastructure for information processing and providing IT services for millions of people in our society, improvement of their performance via optimization of the memory access is of great importance and impact. As memory access is slow compared with processor and cache speeds, the project eliminates unnecessary memory accesses with a re-designed cache architecture supporting flexible access and efficient management. In addition, this project provides research training to both undergraduate and graduate students, especially under-represented minority students, to prepare them to be future information technology professionals with strong skills in computer architecture and system areas.     <br/><br/>In memory-intensive computing, a significant percentage of memory access is spent on indices for translating user-defined keys into memory addresses for data accessing. However, due to lack of temporal and spatial localities, it can be very difficult to cache the indices and receive high cache-hit ratio. Accordingly, searching of the indexes is often at the memory speed, and searching for a data item may require multiple memory accesses. This project designs a software-defined cache -- an informed use of processor cache where a user program can explicitly specify data items for caching with their defined keys. As a two-phase effort, the project adopts a software approach, in which it is presented as a user-level library managing a look-aside buffer implicitly mapped into the cache, and a hardware approach, in which keys are explicitly hashed into the cache. Both approaches well exploit access locality and perform index search at the cache speed with their respective unique advantages. Accordingly, performance of memory system and memory-intensive applications can be significantly improved.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1514305","AF: Medium: Collaborative Research: Algorithmic Foundations for Trajectory Collection Analysis","CCF","ALGORITHMIC FOUNDATIONS","06/01/2015","05/30/2017","Leonidas Guibas","CA","Stanford University","Continuing grant","Tracy J. Kimbrel","05/31/2019","$400,000.00","","guibas@cs.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","7924, 7929","$0.00","This project engages experts in computational geometry, optimization, and computer vision from Duke and Stanford to develop a theoretical and algorithmic framework for analyzing large collections of trajectory data from sensors or simulations. Trajectories are functions from a time interval to a multi-dimensional space that arise in the description of any system that evolves over time.<br/><br/>Trajectory data is being recorded or inferred from hundreds of millions of sensors nowadays, from traffic monitoring systems and GPS sensors on cell phones to cameras in surveillance systems or those embedded in smart phones, in helmets of soldiers in the field, or in medical devices, as well as from scientific experiments and simulations, such as molecular dynamics computations in biology. Algorithms for trajectory-data analysis can lead to video retrieval systems, activity recognition, facility monitoring and surveillance, medical investigation, traffic navigation aids, military analysis and deployment tools, entertainment, and much more. Many of these application fields intersect areas of national security, as well as domains of broader societal benefit.<br/><br/>This project pursues a transformational approach that combines the geometry of individual trajectories with the information that an entire collection of trajectories provides about its members. Emphasis is on simple and fast algorithms that scale well with size and dimension, can handle uncertainty in the data, and accommodate streams of noisy and non-uniformly sampled measurements.<br/><br/>The investigators have a long track record of collaboration with applied scientists in many disciplines, and will continue to  transfer their new research to these scientific fields through joint publications and research seminars, also in collaboration with several industrial partners. This project will heavily rely on the participation of graduate and undergraduate students. Participating undergraduates will supplement their education with directed projects, software development, and field studies. Data sets used and acquired for this project will be made available to the community through online repositories. Software developed will also be made publicly available.<br/><br/>Understanding trajectory data sets, and extracting meaningful information from them, entails many computational challenges. Part of the problem has to do with the huge scale of the available data, which is constantly growing, but there are several others as well. Trajectory data sets are marred by sensing uncertainty and heterogeneity in their quality, format, and temporal support. At the same time, individual trajectories can have complex shapes, and even small nuances can make big differences in their semantics.<br/><br/>A  major tension in understanding trajectory data is thus between the need to capture the fine details of individual trajectories and the ability to exploit the wisdom of the collection, i.e., to take advantage of the information embedded in a large collection of trajectories but missing in any individual trajectory. This emphasis on the wisdom of the collection is one of the main themes of the project, and leads to a multitude of important problems in computational geometry, combinatorial and numerical optimization, and computer vision. Another theme of  the project is to learn and exploit both continuous and discrete modes of variability in trajectory data.<br/><br/>Deterministic and probabilistic representations will be developed to summarize collections of trajectories that capture commonalities and differences between them, and efficient algorithms will be designed to compute these representations. Based on these summaries, methods will be developed to estimate a trajectory from a given collection, compare trajectories to each other in the context of a collection, and retrieve trajectories from a collection in response to a query. Trajectory collections will also be used to infer information about the environment and the mobile entities involved in these motions."
"1814487","CIF: Small: Information Recovery Under Connectivity and Communication Constraints","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","05/23/2018","Alexander Barg","MD","University of Maryland College Park","Standard Grant","Phillip Regalia","09/30/2021","$499,502.00","","abarg@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7797","7923, 7935","$0.00","Modern-day large-scale distributed storage systems store data on thousands of storage nodes, and failure of individual nodes is everyday reality of the system operation. Companies maintaining storage systems make provisions for node failures, relying on erasure codes to ensure data integrity. While specialized encoding methods developed in recent years involve minimum amount of inter-nodal communication possible for such systems, most current solutions rely on the common assumption of universal connectivity between the nodes. At the same time, in many applications starting with Internet-of Things, connections between the nodes are established based on physical proximity or similar features that affect the ability of the nodes to communicate with each other or with the data collector. With this in mind, this project investigates methods of data recovery in systems with limited connectivity whereby the cost of data repair is governed by the length of the path between the nodes, and therefore depends on the topology of the network.<br/><br/>This project intends to establish fundamental limits of communication complexity of data recovery that account for connectivity properties of the underlying network as well as to construct coding methods that ensure data integrity against node failures, incorrect information, or adversarial action that approach the bounds on the minimum possible amount of communication. As an indicator of the network properties, the research conducted in this project aims to investigate limits of data recovery in random networks and to quantify thresholds between high-probability recovery and the impossibility of recovery in random networks. This project also examines algebraic constructions of codes that correct errors under communication constraints including codes on algebraic curves, and optimal-repair codes for the cooperative repair model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751392","CAREER: Transducer-Centric Parallelization for Scalable Semi-Structured Data Processing","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2018","04/26/2018","Zhijia Zhao","CA","University of California-Riverside","Continuing grant","Almadena Y. Chtchelkanova","04/30/2023","$88,149.00","","zhijia@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","1045, 7941","$0.00","Semi-structured data is the de facto standard for exchanging data over the web and the default data type for many document-based data stores. With its fast growth in volume, it becomes critical to process semi-structured data on parallel processors, which have become ubiquitous and increasingly powerful. However, data-parallel processing of semi-structured data remains a fundamental challenge, due to its inherent nested structure. A partitioning of semi-structured data can easily break the well-formed nature of nested levels, making the data hard to process. To address the challenge, this research proposes to examine the basic computation models used for processing semi-structured data -- pushdown transducers, and designs a transducer-centric parallelization paradigm. This enables automatic generation of data-parallel processing routines for software applications that consume semi-structured data. Because of the fundamental role of semi-structured data, the insights gained from this research will facilitate research advancement beyond program parallelization.<br/><br/>Transducer-centric parallelization consists of four components. The first component examines inherent dependences in pushdown transducer executions and designs a series of basic mechanisms to break them by leveraging their special properties, such as 'finite-state' and 'bounded stack access'. The second and third components focus on improving the parallelization efficiency either by exploiting the transition structures of pushdown transducers and the grammars of semi-structured data, or by adopting an aggressive speculative execution scheme. The last component of this research develops algorithms and software tools to automatically generate parallel pushdown transducers for commonly used processing routines of semi-structured data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1563918","CIF: Medium: Signal representation, sampling and recovery on graphs","CCF","COMM & INFORMATION FOUNDATIONS","05/15/2016","06/08/2018","Jelena Kovacevic","PA","Carnegie-Mellon University","Continuing grant","Phillip Regalia","04/30/2020","$698,000.00","Aarti Singh","jelenak@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7924, 7936, 8089, 9251","$0.00","Datasets that are collected in physical and engineering applications, as well as social, biomolecular, commercial, security, and many other domains, are becoming larger and more complex. In many cases, such data is analyzed manually or using methods that extract only superficial information and can lead to subjective and non-reproducible conclusions. There is thus an urgent need for the development of methodologies that formalize analysis of complex data. Graphs provide a natural formalism to capture complex interactions that govern the structure of the data in many applications. However, a rigorous framework for signal and data processing on graphs has been lacking. This proposal aims to develop the fundamentals of signal representation, sampling and recovery on graphs. <br/><br/>Signal and data processing has been the focus of the principal investigators? work for many years. In this project, the team will develop a mathematically rigorous framework for signal processing on graphs that offers a new paradigm for the analysis of high-dimensional data with complex, non-regular structure. By extending fundamental signal processing concepts such as filtering, Fourier and wavelet analysis to data residing on general graphs, the framework will offer principled solutions to a number of data analysis problems, such as data compression, recovery, localization, detection, and others. Specifically, the team will 1) develop efficient succinct representations for signals on graphs, 2) design efficient strategies that leverage the graph structure for sampling signals on graphs, and 3) develop near-optimal and computationally efficient estimators for recovering graph signals from samples."
"1453112","CAREER: Commonality Exploiting Architectures for Energy Efficiency","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","02/01/2015","09/18/2018","David Wentzlaff","NJ","Princeton University","Continuing grant","Yuanyuan Yang","01/31/2020","$511,176.00","","wentzlaf@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","2878, 7798","1045, 7941","$0.00","An increasing amount of computation is moving into the Cloud and large-scale data centers.  Hosted web-applications, such as social networks, hosted email, and photo sharing, are all examples of applications, which have moved into the Cloud.  In order to control costs and save energy, maximizing the computational efficiency of datacenter computers is of paramount importance.  This research project investigates computational efficiency in datacenter processors by looking for commonality between all of the applications that are executing in the Cloud.  This project explores how to change a microprocessor core to save energy by exploiting the commonality available between applications in the Cloud.  In addition, this project utilizes an integrated education plan that will disseminate information about computer architecture inside of the University, across the New York region, and across the world through the use of a Massively Open Online Course (MOOC).  Accelerating data center and big data applications can have large societal impact by helping humans understand data that has been collected, thus enabling the understanding of social trends and public health challenges. The success of the research project will create more efficient data centers that will enable humanity to answer societal big data questions and have richer Internet experiences.<br/><br/>This project investigates how to create computer architectures that can exploit similarity between programs executing on different cores across the data center.  A software system identifies likely candidates that exhibit commonality.  Once likely commonality has been identified, micro-architectural information is sent between different cores or within a single core to enable the execution of one program to reduce the energy needed to execute a second similar program.  This project investigates what is the best micro-architectural information to share between cores executing common programs and how it is best to send that information between cores.  In particular, this project investigates customizing on-chip networks specialized for sending commonality information and how to modify a manycore processor to harvest common micro-architectural information."
"1750656","CAREER: Algorithm-Centric High Performance Graph Processing","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2018","03/13/2019","Xuehai Qian","CA","University of Southern California","Continuing grant","Almadena Chtchelkanova","01/31/2023","$165,449.00","","xuehai.qian@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","1045, 7941","$0.00","With the advent of big data, large amounts of data are collected from numerous sources, such as social media, sensor feeds, and scientific experiments. Graph analytics has emerged as an important way to understand the relationships between heterogeneous types of data, allowing data analysts to draw valuable insights from patterns in the data for a wide range of applications, including machine learning tasks, natural language processing, anomaly detection, clustering, recommendation, social influence analysis, bioinformatics. Due to the broad applications, the research community tackled graph processing from multiple angles, including distributed, disk-based systems and in-memory graph processing. There are four key problems of today's graph processing research: 1) the gap between programming model and algorithm; 2) the lack of diversity in applications studied; 3) insufficient research on dynamic graphs and graph database; and 4) architectural supports focus only on classical problems. This proposal attempts to advance the graph processing systems by solving these major challenges.<br/><br/>This research proposes a novel approach ALCHEM, algorithm-centric high performance graph processing, which involves the collaborative designs of algorithms, programming model, systems, and architecture. This interdisciplinary research program takes the opportunity to explore or enhance the interactions between different layers, with the emphasis on algorithm efficiency. It contains four research thrusts: (1) Using graph abstraction as a bridge between programming model and algorithm to speed up the convergence; (2) Developing efficient execution model with specialization; (3) Building a graph database as a unified engine for relational and dynamic graph data; (4) Enhancing architecture with novel features to support new graph algorithms (e.g., random walk). The research will trigger close interactions between researchers in theory, system, and architecture. The project will engage women, minorities and undergraduates. Uniquely, it will not only train the students' system building skills, but also strengthen their algorithm understanding. The research outcomes will benefit the society by improving everyday life with better and faster recommendations, enhanced security, and better social relationships."
"1526875","CIF: Small: Collaborative Research:Synchronization and Deduplication of Distributed Coded Data: Fundamental Limits and Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2015","06/29/2015","Olgica Milenkovic","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","06/30/2020","$150,000.00","","milenkov@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7935","$0.00","Part 1: Coding for distributed storage systems has garnered significant attention in the past few years due to the rapid development of information technologies and the emergence of Big Data formats that need to be stored and disseminated across large-scale networks. As typical distributed systems need to ensure low-latency data access and store a large number of files over a set of nodes connected through a communication network, it is imperative to develop new distributed coding schemes that protect the systems from undesired component failures. The two key functionalities of codes used in distributed systems, namely the reconstruction of files via access to a subset of the nodes and repair of failed nodes, need to be retained when the files are accessed and processed by the users via symbol/block insertion, deletion, or substitution edits. Deletions frequently arise due to system-level data deduplication: when parts of files are deduplicated or edited, the changes in the information content need to be communicated to the redundant storage nodes with minimum communication cost. Current solutions for synchronizing data that underwent edits assume that data is uncoded and they do not fully exploit the distributed nature of information. Furthermore, they mostly ignore the presence of deduplication protocols. This makes distributed storage architectures inefficient in terms of storage, user access times, and error protection. Hence, the goals of the proposed research program are to develop a new set of protocols and coding schemes that will  support a new generation of versatile and updatable coded distributed storage systems. <br/><br/>Part 2: Building on the preliminary work of the investigators, this proposal aims to set the foundations of the new field of coded synchronization and deduplication, with the goal of deriving fundamental performance limits, developing efficient algorithmic solutions for the two families of problems, and constructing new distributed storage codes that enable synchronization of coded data and coded deduplication. In particular, the proposal addresses the following comprehensive issues: <br/>1) Characterizing the communication rate limits of known and new (un)coded synchronization schemes, trade-offs between deduplication and data repair performance for different structured or encoded data formats and different types of communication channels.<br/>2) Introducing and analyzing the communication rate-distortion (CRD) function for approximate synchronization and deduplication of structured/encoded data, with a special focus on delay-sensitive applications.<br/>3) Developing dynamically updatable synchronization and deduplication algorithms cognizant of the network topology and of different prioritization needs of the users, as encountered in image and video data coding."
"1564207","SHF: Medium: Collaborative Research: From Volume to Velocity: Big Data Analytics in Near-Realtime","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/21/2016","Tiark Rompf","IN","Purdue University","Standard Grant","Almadena Chtchelkanova","07/31/2020","$332,800.00","","tiark@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","7924, 7942","$0.00","Most existing techniques and systems for data analytics focus exclusively on the volume side of the common definition of Big Data as volume, velocity and variety. In contrast, there are clear indications that the velocity component will become the dominant requirement in the near future, most significantly because of the proliferation of mobile devices across the planet. This is compounded by the fact that the freshest data often contains the most valuable information and that users have grown accustomed to data that is deeply analyzed and processed by sophisticated machine learning (ML) techniques, to enable their ""always on"" experience. In most mobile interactions, for example, the physical locations of one or potentially many users play a role, but the system needs to process the actual locations, not the ones from ten minutes ago. Many similar use cases exist in finance, intelligence and other domains. In all of them, the desires for fresh and for highly processed data are in a fundamental tension, as high quality analysis is computationally expensive and often done in large batches. The intellectual merits of this project are to investigate a combination of new ideas to address this challenge, spanning machine learning algorithms, specialized hardware accelerators, domain-specific languages, and compiler technology. The project's broader significance and importance are to pave the way for new kinds of high-velocity big-data analytics, which have the potential to revolutionize the way that people interact with the world.<br/><br/>The project investigates new incremental ML primitives and new algorithms that can trade off speed with precision, but retain provable guarantees. Novel DSLs (domain-specific languages) make such algorithms and techniques available to application developers, and new compilation techniques map DSL programs to specialized accelerators. In particular, the project shows how through these novel compilation techniques, machine learning algorithms can especially benefit from hardware acceleration with FPGAs. Finally, the project investigates new compilation techniques for end-to-end data path optimizations, including conversion of incoming data from external formats into DSL data structures, and transferring data between network interfaces and FPGA accelerators. Tying these new ideas and techniques together, this project will result in an integrated full-stack solution (spanning algorithms, languages, compilers, and architecture) to the problem of achieving high velocity in big data analytics."
"1452994","CAREER: A Hardware and Software Architecture for Data-Centric Parallel Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","02/15/2015","02/27/2019","Daniel Sanchez Martin","MA","Massachusetts Institute of Technology","Continuing grant","Yuanyuan Yang","01/31/2020","$500,000.00","","sanchez@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7798","1045, 7941","$0.00","Energy efficiency is the key challenge facing computer systems. To improve<br/>performance under a limited energy budget, systems are becoming increasingly<br/>parallel, featuring many smaller and simpler cores, and heterogeneous,<br/>featuring cores specialized for certain tasks. Even with these improvements,<br/>two critical challenges remain. First, without reducing data movement, memory<br/>accesses and communication will dominate energy consumption. Thus, limiting<br/>data movement must become a primary design objective. Second, these systems<br/>will be highly complex, and will need powerful abstractions to shield<br/>programmers from this complexity. Current systems are designed in a<br/>computation-centric way that is a poor match for these challenges. Memory<br/>hierarchies are hardware-managed and opaque to software, which needlessly<br/>increases data movement; and runtimes lack the proper hardware mechanisms and<br/>software policies to manage heterogeneous resources efficiently.<br/><br/>This research project takes a holistic approach to addressing these challenges, by<br/>co-designing an architecture and runtime system that efficiently run dynamic<br/>parallel applications on systems with heterogeneous cores and memories.<br/>Redesigning hardware to be directly exploited by a dynamic runtime enables (a)<br/>many more opportunities to reduce data movement, (b) better usage of<br/>heterogeneous resources, and (c) much faster adaptation to changing application<br/>needs and available resources. Three key components underlie this design.<br/>First, a scalable memory system incorporates combinations of heterogeneous<br/>memory technologies to improve efficiency, and exposes them to software, which<br/>can divide these physical memories into many virtual cache and memory<br/>hierarchies to finely control data placement. Second, specialized programmable<br/>engines orchestrate communication among cores, accelerate intensive runtime<br/>functions such as load balancing, and monitor how tasks use hardware resources<br/>to guide runtime decisions. Third, a hardware-accelerated runtime leverages<br/>this novel architectural support to place data and computation to minimize data<br/>movement, use the most suitable core for each task, and quickly respond to<br/>changing application needs. This runtime targets a high-level programming model<br/>that lets programmers express fine-grained and irregular task, data, and<br/>pipeline parallelism. These techniques build on an analytical design approach<br/>that makes hardware easy to understand and predict, and enables runtimes to<br/>navigate multi-dimensional tradeoffs efficiently.<br/><br/>If successful, this project will make heterogeneous systems more efficient,<br/>more broadly applicable, and easier to program. It will especially benefit<br/>applications with dynamic and fine-grained parallelism, advancing key emerging<br/>domains where these workloads are pervasive, such as graph analytics and online<br/>data-intensive services. In addition, the infrastructure developed as part of<br/>this project will be publicly released, enabling others to build on the results<br/>of this work."
"1629395","XPS: EXPL: Hippogriff: Efficient Heterogeneous Servers for Data Centers and Cloud Services","CCF","Exploiting Parallel&Scalabilty","10/01/2016","09/08/2016","Steven Swanson","CA","University of California-San Diego","Standard Grant","Marilyn McClure","09/30/2019","$300,000.00","","swanson@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8283","","$0.00","The growing importance of artificial intelligence, network services, and cloud storage drives the demand of building powerful computer systems that can perform many operation at once.  Building computers with different kinds of computing processors (i.e., heterogeneous processing) is an effective way to achieve this goal. However, this approach also creates new problems that can negate some of the benefits it provides.  In particular, using different processors for different tasks requires moving data between those processors.  This movement takes time and can cancel out saving heterogeneous processing provides. This project is addressing this problem in heterogeneous computing systems by making the movement of data between different processors more efficient.  This improved efficiency leads directly to benefits for applications of scientific and commercial importance.<br/><br/>Much of the cost of data movement heterogeneous computing systems stems from the entrenched central processing unit (CPU)-centric programming model.  This project is revisiting the design of the application interface, system software and hardware components to remove CPUs and main memory from the critical path of moving data.  The project provides an efficient programming model that allows the system software stack to automatically and efficiently setup the data movements between heterogeneous processors. We are applying the system to large-scale database systems, massive parallel programming systems like Spark and MapReduce as well as scientific computing that power important daily applications and research projects."
"1350563","CAREER: Low-Rank Matrix Modeling for Constrained Reconstruction from Noisy and Sparsely-Sampled Data","CCF","COMM & INFORMATION FOUNDATIONS","04/01/2014","05/07/2018","Justin Haldar","CA","University of Southern California","Continuing grant","Phillip Regalia","03/31/2020","$415,010.00","","jhaldar@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","1045, 7936","$0.00","Sensing systems are found everywhere in the modern world, ranging from the microphones, video cameras, and antennas found in everyday cellular phones, to the radar systems used for astronomy, meteorology, archaeology, and defense applications, to the biomedical imaging systems used to understand the human body and its diseases. Real-world sensing systems must always strike a practical balance between the cost of data acquisition and the quality of the measured data. The costs of acquiring a large amount of high-quality data are prohibitive for many important applications, and place practical limits our ability to explore and understand our bodies, our world, and our universe. This effect is exacerbated as new sensors become increasingly capable of acquiring multidimensional data.<br/><br/>This research is focused on exploring the use of parsimonious low-rank signal models to extract information from incomplete and/or low-quality data.  Specifically, the investigators are developing theory and methods to unify and generalize a range of existing constrained signal reconstruction methods within a framework based on low-rank matrix embeddings.  While the new theory and methods can be applied in general sensing applications, the proposed models are being evaluated in the specific practical context of magnetic resonance imaging (MRI), where they can enable faster MRI experiments and more informative high-dimensional examinations."
"1718194","CCF:Small:Collaborative Research: Taowu: A Heterogeneous Processing-in-Memory for High Performance Scientific Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2017","08/29/2017","Dong Li","CA","University of California - Merced","Standard Grant","Almadena Chtchelkanova","08/31/2020","$282,000.00","Yanbao Ma","dli35@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092598670","CSE","7798","7923, 7942, 9215, 9251","$0.00","Many scientific computing applications in critical areas of research, such as nanotechnology, astrophysics, climate, bioinformatics, and high-energy physics, are becoming more data intensive than ever before. The volume of the data and the pressure on the runtime system capability of supporting data intensive operations substantially increases over the time. This project introduces and optimizes data processing capabilities within memory. The project provides a fundamental change to data management and program optimization, and brings promising performance and energy benefits. The project will significantly advance simulation capabilities of scientific applications, especially those with intensive data processing. <br/> <br/>The goal of the project is to enable high-performance, energy-efficient, and flexible processing-in-memory design, which is adaptive to the irregular, diverse, and changing behaviors among data intensive scientific applications. To achieve the goal, a heterogeneous processing-in-memory design, built up with fixed-function processing-in-memory and general programmable processing-in-memory, is introduced. The project explores a series of critical questions for building emerging processing-in-memory, including heterogeneous processing-in-memory architecture, processing-in-memory programming models, runtime design, and the implications of processing-in-memory on high performance scientific applications. The  project will significantly advance the knowledge to build processing-in-memory, and pave the way to integrate it into the existing and future systems."
"1849622","CRII: SHF: Foundations for Stateful Network Programming","CCF","SOFTWARE & HARDWARE FOUNDATION","03/01/2019","02/20/2019","Jedidiah McClurg","NM","University of New Mexico","Standard Grant","Anindya Banerjee","02/28/2021","$175,000.00","","jrmcclurg@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","7798","7798, 7943, 9150","$0.00","Billions of people use web-based services on a daily basis for a variety of tasks, such as communication, social networking, and navigation. These services rely on correct and efficient functioning of complex networked systems, which are prone to failures that are often caused by human error. Existing technologies such as software-defined networking (SDN) seek to reduce human error by providing centralized network management and programmability via a standard API, and by separating network management (the SDN ""control-plane"") from packet forwarding (the SDN ""data-plane""). While centralization permits a single network controller to have a global view of the network--which simplifies the control logic--it suffers from severe scalability limitations when the size of a network increases. Moreover, modern SDN data-planes feature powerful, decentralized devices that are able to perform computations and update their local state based on packet contents, thereby allowing them to implement functionality traditionally restricted to the SDN control-plane. Such decentralization requires viewing a network program as a distributed system running on network hardware, rather than as a process running on a controller and interacting with switches. This project establishes a new approach for constructing modern network programs, viewed as distributed systems. The project's novelties are (i) a new abstraction for writing network programs that takes distribution into account and ensures that a network program correctly maintains distributed views of global state; and (ii) techniques for ensuring efficiency of network programs: the techniques ensure that network performance is not penalized by maintenance of global state. The project's impacts are (i) contributions to the general understanding of how to properly build distributed systems, and (ii) development of a language and associated tools which eliminate some of the difficulties domain experts face in building these systems.<br/><br/>The key contribution is a thorough investigation of the network data-plane as a platform for executing dynamic, stateful code, resulting in a programming system which allows network functions to be realized in the data-plane, and ensures that network programs are verifiably correct and efficiently implementable. The project has two research thrusts. The first thrust focuses on generalizing and implementing event nets. While previous work introduced the concept of event nets (a Petri-nets-based abstraction) for event-driven programming in the context of SDN, this project enriches the event nets language with functionality needed to make it easily usable in other domains such as wireless sensor networking (WSN), internet of things (IoT), and data-centric programming. The resulting language allows programmers to write, for each of these domains, event-driven network programs that use global data structures to describe network-wide behavior, without having to handle unexpected data races during program execution on the (distributed) switches. Additionally, the project implements a compiler that produces executable code from an extended event net: specifically, the compiler automatically constructs a network program's distributed version that targets modern hardware such as Barefoot Network's Tofino chip. The second research thurst focuses on formalizing event nets. The project investigates an algebraic formalization of the extended event nets language, allowing (mechanized) formal reasoning about dynamic, stateful network programs. Finally, the project explores ways in which event nets can be used as a pedagogical tool in teaching undergraduate classes on Computer Networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815287","SHF: Small: Open-domain, Data-driven Code Synthesis from Natural Language","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","05/22/2018","Graham Neubig","PA","Carnegie-Mellon University","Standard Grant","Sol Greenspan","09/30/2021","$499,726.00","Bogdan Vasilescu","gneubig@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7944","$0.00","One of the major hurdles in programming is turning ideas into code; all programmers, even experts, frequently reach points in a program where they know what they want to do but cannot easily turn it into a concrete implementation. In this case, it is common to turn to the web, e.g. enter a natural language query, search, browse results, copy-and-paste appropriate code, and modify it to the desired shape. However, this process is still time-consuming. This research aims to automate and enhance this process, by creating new data-driven methods for code synthesis from natural language, which allow developers to go directly from natural language description to code. Specifically, this project's goal is to bring code synthesis to the open domain, moving from highly engineered methods that work on only a single programming language or task, to methods that have the flexibility and scalability to answer most of the questions asked by programmers, in many different programming languages. The intellectual merit of this cross-disciplinary project lies in its potential to contribute to software engineering through the examination of developer's interaction with natural language productivity tools, and its potential to contribute to natural language processing through new models to understand procedural texts. This project will have broader impact through the development of tools and data linking together programs and natural language, potential to improve STEM education by lowering the barriers to programming, and training of graduate and undergraduate research assistants who will be able to straddle and act as bridges between the fields of natural language processing and software engineering.<br/><br/>There are three technical pillars to the work. First, it will focus on methods to mine data consisting of natural language and corresponding code at scale, necessary for training. The mining will be performed over existing online data sources, such as community question answering sites (Stack Overflow) and open-source software repositories (GitHub), using machine learning models that consider both content matches and available meta-data, and crowd-sourcing-based verification of the extracted data. Second, the project will develop code synthesis methods that have the flexibility to handle the wide variety of expressions expected across a variety of software projects and developer needs. This will be done by developing models using neural networks, which have recently shown impressive ability to interpret a wide variety of expressions in other natural language processing tasks. We will expand these models to condition on project context, which will ensure handling of the various constraints necessary to create well-formed programs and allow for adaptation to project-specific conventions and needs. Third, the project will develop methods for learning and improving the models from developer behavior, by feeding back corrections to the generated code into the system and learning from the differences between the pre- and post-correction code. These methods will all be integrated into developer support tools that can be used in a development environment, or through an online API. The utility of these methods will be examined in both controlled and in-the-wild studies. Controlled studies will examine the subjective accuracy of the mined data and generated code, as well as the effect of the tools on the efficiency and ease of development, for programmers from novice to expert level. This project will also create and release tools for general consumption, solicit feedback from a wide variety of developers, and examine how developers use the proposed tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1452904","CAREER: Leveraging temporal streams for micro-architectural innovation in data center servers","CCF","SOFTWARE & HARDWARE FOUNDATION","02/15/2015","02/15/2019","Michael Ferdman","NY","SUNY at Stony Brook","Continuing grant","Yuanyuan Yang","01/31/2020","$500,000.00","","mferdman@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7798","1045, 7941","$0.00","As the global user base for online services continues to expand and new services and features are rapidly developed, data centers from which these online cloud services operate experience constant pressure to achieve higher performance and improve their quality of service. However, supporting the adoption of cloud services in all aspects of people?s daily lives requires expanding data centers to an extreme scale, with hundreds of millions of servers and ecologically unthinkable energy bills. This research develops technologies to improve the performance and efficiency of future data centers, targeting higher performance and lower energy costs from each deployed server. As such, it directly contributes to sustainable growth of data centers and online services, while at the same time training world-class experts specialized in tackling the challenges facing future data centers and clouds.<br/><br/>This research leverages a recently-codified phenomenon called ""temporal streams"" to solve a number of long-standing micro-architectural performance bottlenecks facing server systems in the cloud. Many of the performance enhancing techniques developed over the course of the past several decades for the desktop, mobile, and super-computer domains provide limited benefits to server systems, because the size and complexity of a typical cloud workload requires significantly greater meta-data storage capacity than currently available to these techniques. This work re-architects the meta-data storage of speculative structures, leveraging temporal streams to expand their effective capacity. Specifically, this work targets instruction prefetchers, branch predictors, and hardware memorization as case studies to demonstrate the ability of temporal streaming to provide sufficient meta-data storage for these mechanisms when executing cloud workloads."
"1750826","CAREER: Revamping the Memory Systems for Efficient Data Movement","CCF","SOFTWARE & HARDWARE FOUNDATION","05/15/2018","05/02/2018","Xiaochen Guo","PA","Lehigh University","Continuing grant","Yuanyuan Yang","04/30/2023","$97,353.00","","xig515@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","7798","1045, 7798, 7941, 9102","$0.00","Data movement efficiency is currently one of the most challenging impediments to the next performance leap in scientific computing and to the next qualitative improvement in big data analytics. The inefficiency of data movement is rooted in the conventional memory system design, which is based on the assumption that applications have good locality and is guided by the primary goal to overcome the memory latency wall. This design principle leads to a memory hierarchy design that progressively increases access granularity from the top to the bottom level. An access at one level of the memory hierarchy moves a contiguous block of data to another level to benefit from spatial locality. As the number of processor cores increases, however, good memory locality is difficult to achieve by programmer efforts or compiler optimizations alone, due to increasing contentions at all levels of the memory hierarchy and the dynamic nature of execution environments. The average utilization of a memory block is less than twelve percent even for highly optimized code, which results in a waste of energy, bandwidth, and on-chip storage. To improve data movement efficiency, this project seeks to revamp the memory systems to proactively create and redefine locality in hardware.  This research holds the potential to fundamentally improve data movement efficiency through new memory system designs, which can motivate a complete rethinking of programming language, compiler, and run-time system designs as well. This project also seeks to train and mentor graduate and undergraduate students, promoting STEM among women and underrepresented groups, and developing outreach activities that raise awareness of the data movement efficiency problem among future programmers and computer engineers.<br/><br/>The goal of this work is to re-architect the memory systems to improve data movement efficiency by exploiting fine-grain access correlations. The challenge is that tracking fine-grain correlations could require high meta data and control overheads. This work takes an end-to-end approach to share the meta data and control signals among different levels of the memory hierarchy. A new class of memory- and cache-architectures are developed in this research to redefine locality at each level of the memory hierarchy, which improves data movement, cache storage efficiency, and performance without introducing significant overheads. This research also investigates new memory organizations to improve data movement efficiencies in emerging memory systems such as non-volatile memory and near-memory processing systems. This work is evaluated using cycle-accurate architectural simulators and a set of important data-intensive workloads. Data structure- and algorithm- oriented optimizations are explored to allow existing and emerging workloads to take full advantage of the new memory architectures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717120","CIF: SMALL: Metric Representations of Network Data","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/26/2017","Alejandro Ribeiro","PA","University of Pennsylvania","Standard Grant","Phillip Regalia","06/30/2020","$450,000.00","","aribeiro@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7797","7923, 7936","$0.00","Network data, defined as one that encodes relationships between elements, is a pervasive component of modern data analytics. The purpose of this project is to advance our capacity to process and understand network data. The contention is that the central difficulty in analyzing large-scale complex networks comes from lack of structure. This loose nature contrasts with the rigidity of a closely related construction: the metric space. If understanding networks is challenging but understanding metric spaces is not, a route to network analysis is to project networks into metric spaces. This motivates the technical goal of designing methods and algorithms to implement these projections. <br/><br/>The project builds on preliminary results combining a projection axiom (networks that are already metric remain unchanged after projection) and a dissimilarity-reducing axiom (smaller networks have smaller projections) to establish existence and uniqueness results. Further explorations are pursued in four thrusts: (1) Study of projection methods in symmetric networks. (2) Incorporation of asymmetric networks and asymmetric quasimetric spaces. (3) Metric representations derived from triangle inequalities written in dioid algebras; which albeit metric in an abstract sense are very different from regular metrics. (4) Generalizations to high order networks in which dissimilarities are defined for tuples other than binary. Applications to search, network comparison, and diffusion processes, complement the theoretical research. Broader impacts come through industrial partnerships and an aggressive educational agenda that leverages the University of Pennsylvania's institutional commitment to play a leading role in the education of engineers that are to exploit the opportunities afforded by the ever increasing access to data and the ever broadening scope that networks play in our society."
"1421243","SHF: Small: Random Testing for Language Design","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2014","06/13/2014","Benjamin Pierce","PA","University of Pennsylvania","Standard Grant","Anindya Banerjee","08/31/2019","$500,000.00","","bcpierce@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7798","7923, 7943","$0.00","Title: SHF:Small:Random Testing for Language Design<br/><br/>PROPERTY-BASED RANDOM TESTING (PBRT) is a form of black-box testing in which executable partial specifications of a software artifact are used to check its behavior with respect to large numbers of randomly generated test cases. PBRT offers a range of benefits that complement the traditional strengths of full, formal verification; in particular, it (1) allows much more rapid iteration on designs, (2) encourages early focus on stating correct specifications, and (3) supports later proof efforts by allowing invariants to be debugged quickly.  Popularized by the QuickCheck tool in Haskell, PBRT is now widely used in both research and industry.  However, one area where current PBRT methodology has been less successful is where the data values used in testing come with have complex internal structure or intricate invariants.  In particular, this is the case in testing of language designs and related artifacts such as compilers, where the test data are programs. Despite some promising preliminary efforts, random testing has proved difficult to apply to full-scale language designs.  Many of the interesting properties of programming languages are ""conditional,"" leading to a large number of discarded test cases when random testing is applied naively.  This places a premium on the ability to construct good custom generation strategies, but generation strategies for ``interesting programs'' are neither well understood nor well supported by existing tools: better techniques are needed for writing and debugging test-data generators.  <br/><br/>This project aims to significantly advance the state of the art in property-based random testing, with specific applications to testing fundamental properties of language definitions and related artifacts---properties such as type safety, security (e.g., ``secret inputs cannot influence public outputs''), and compiler correctness.  The intellectual merits are: (1) developing new methodology for writing and debugging random generators for complex data, in particular a framework based on generating ``mutants'' of an artifact under test; (2) designing a domain-specific language for writing generators for random test data with complex invariants (3) distributing polished implementations, both as a compatible extension to the standard QuickCheck library and as a native random-testing tool for the Coq proof assistant; and (4) evaluating the usefulness of these tools by applying them to several significant case studies.  The broader impacts of the project are twofold.  First, better understood, more secure language designs will lead to better and more secure software, and hence to fewer bugs and vulnerabilities in everyday applications and in critical infrastructure.  In particular, the project's main case studies aim to show how random testing can improve the design process for new languages with built-in support for guaranteeing fundamental security properties such as confidentiality, integrity, authorization, and access control.  Second, beyond language design, random testing has proven extremely effective for improving software quality.  The envisaged tools will significantly increase the power of random testing by offering new tools for writing and testing random data generators that can be used with QuickCheck, an existing industry-standard platform, and by offering native support for random testing within Coq, a popular specification and verification tool.  Project results will be incorporated into the ""Advanced Programming"" course at Penn (which already emphasizes random testing) and will form the basis for a module on random testing of language properties at the Oregon Programming Languages Summer School."
"1815011","AF: Small: Foundations for Collaborative and Information-Limited Machine Learning","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","10/01/2018","08/28/2018","Avrim Blum","IL","Toyota Technological Institute at Chicago","Standard Grant","Rahul Shah","09/30/2021","$324,884.00","","avrim@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","2878, 7796","075Z, 7923, 7926","$0.00","Machine learning increasingly is being used throughout society, and in a wide range of applications.  Businesses use machine learning systems for decision support, internet sites use machine learning to better interact with users, our personal devices use machine learning to adapt to our needs, and our cars are beginning to use data-trained systems to improve safety.  These applications bring up new opportunities as well as new concerns. Opportunities include the potential for systems to more rapidly learn and adapt through collaboration, and concerns include privacy and the fairness of algorithmically-made decisions.  This project is aimed at developing new foundational understanding of these opportunities and concerns, to help guide the development of more efficient, more adaptive, and fairer, machine learning methods.  This project additionally will support educational workshops on these issues, and more broadly will support the education and training of young scientists on these topics.<br/><br/>Specifically, this project has the following four main thrusts: (1) Collaborative Machine Learning. How can devices with related learning tasks best collaborate to learn efficiently from only a modest amount of data, and how can privacy and related concerns be addressed?  (2) Property Testing and Error Extrapolation.  This thrust aims to develop methods that, from a small amount of labeled data, can reliably estimate how well a given learning algorithm or representation class would perform if given a much larger labeled data sample.  (3) Semi-Supervised Learning. Semi-supervised learning refers to methods that combine labeled and unlabeled data, to learn well even when labeled data is limited. This work aims to develop theoretical foundations for an approach based on explicitly learning regularities within the unlabeled data and then using these to guide how learning is performed over the labeled data. (4)  Fairness in Learning. There has recently been substantial concern about algorithmic decisions (such as whether to offer an applicant a loan) that could unfairly discriminate against certain classes of people. This work aims to develop improved theoretical understanding, tools, and guarantees for tackling these kinds of problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1725663","SPX: Collaborative Research: Multicore to Wide Area Analytics on Streaming Data","CCF","SPX: Scalable Parallelism in t","08/15/2017","08/02/2017","Phillip Gibbons","PA","Carnegie-Mellon University","Standard Grant","Tracy Kimbrel","07/31/2020","$492,000.00","","gibbons@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","042Y","026Z","$0.00","In today's big data era, there is an urgent need for methods that can quickly derive analytical insights from large volumes of data that are continuously generated.  Such streaming data include video, audio, activity logs, and sensor data, and are generated on a massive scale all over the world.  The need for real-time streaming analytics can only be fulfilled with the help of appropriately designed parallel and distributed algorithms. However, parallel and distributed computing systems come in a variety of shapes and sizes, and algorithms should be designed to match the characteristics of the underlying system. This project develops methods for analyzing massive streaming data on computing systems ranging from machines with multiple cores sharing memory to geo-distributed data centers communicating over wide-area networks. The results of this research are expected to improve the efficiency, latency, and throughput of streaming analytics.  Due to the foundational nature of the analytical tasks considered, results of this project will impact disciplines that use large-scale machine learning and graph analytics, including cybersecurity, social network analysis, and transportation. Resulting software will be released as toolkits on stream processing platforms, and deployed in a smart-city camera infrastructure. Synergy between the research goals and the teaching goals of the PIs will lead to new instructional material in existing courses as well as development of new courses in data analytics. Individuals from underrepresented groups will be included as a part of the project. The project will benefit from and strengthen collaborations between academia, industry, and national labs on streaming analytics.<br/> <br/>The first technical thrust of the project is on designing shared memory parallel algorithms for computation on data streams, that can achieve a high throughput and fast convergence for complex analytics tasks. The second thrust is on designing distributed streaming algorithms that can tolerate variable communication delays and adapt to available bandwidth in a wide-area network, through identifying good tradeoffs between freshness of results and volume of communication. These advances will be studied in the context of fundamental graph analytics and machine learning tasks such as subgraph counting, graph connectivity and clustering, matrix factorization, and deep networks. The project will utilize the vast body of theory and techniques developed in the realm of parallel computing in the design of methods for processing streaming data, leading to a toolkit of techniques that can be reused across applications. The project will also lead to advances in sequential streaming and incremental algorithms for certain problems; for instance, problems in machine learning that use iterative convergent methods. Based on the techniques designed, the project will design and build a hierarchical parameter server that operates effectively across the spectrum from multicore machines to data centers to wide-area data sources.<br/>"
"0353079","Collaborative ITR/CSE:  Modular Strategies for Internetwork Monitoring","CCF","INFORMATION TECHNOLOGY RESEARC, ITR MEDIUM (GROUP) GRANTS, , , , , , , , ","09/02/2003","02/12/2019","Robert Nowak","WI","University of Wisconsin-Madison","Continuing grant","Phillip Regalia","08/31/2019","$1,401,556.00","","rdnowak@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1640, 1687, I184, I264, N502, N542, O207, P174, T553, T790","1658, 8237, 9218, 9251, HPCC","$0.00","ABSTRACT<br/>0325390<br/>Robert Nowak<br/>William Marsh Rice University<br/><br/>This project addresses the longstanding and difficult problem of detecting and classifying spatially distributed network anomalies from multiple monitoring sites.  To characterize baseline vs. anomalous behavior of the Internet requires deployment of collaborative data collection, anomaly detection and pattern recognition for complex largescale systems. The project combines the forces of leading researchers in three complementary disciplines: (i) networking and data collection; (ii) statistical data analysis and signal processing; (iii) decentralized decision-making. The research goes well beyond the state-of-the art anomaly detection for centrally administered networks. In particular tools and practical data sharing algorithms are being developed for detecting coordinated intrusions, distributed denial of service attacks, and quality-of-service degradations in decentralized networks such as the Internet.  The project also includes activities with broader impact including: creation of a public network anomaly database, K-12 educational outreach, and university-industry collaborations.<br/>The research approach is based on a modular and distributed monitoring paradigm that is organized into a three level hierarchy: local level measurement of data from servers, routers and switches; intermediate level data analysis and processing of end-to-end traffic measurements, summary statistics and alarms transmitted from the local level; and upper level decision-making and processing of information transmitted from the intermediate level.  This modular structure is scalable to large networks of monitoring sites. However, this structure also imposes constraints on data analysis, which requires development of new approaches. Three approaches are being pursued: distributed spatio-temporal data analysis using wavelets over graphs; event detection and classification using distributed pattern analysis and learning; and multi-site event correlation using discrete event dynamical systems and decentralized stochastic systems.<br/>"
"1552946","CAREER: Deciphering Brain Function Through Dynamic Sparse Signal Processing","CCF","COMM & INFORMATION FOUNDATIONS","02/15/2016","02/11/2019","Behtash Babadi","MD","University of Maryland College Park","Continuing grant","Phillip Regalia","01/31/2021","$381,791.00","","behtash@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7797","1045, 7936, 8089","$0.00","The ability to adapt to changes in the environment and to optimize performance against undesirable stimuli is among the hallmarks of the brain function. Capturing the adaptivity and robustness of brain function in real-time is crucial not only for deciphering its underlying mechanisms, but also for designing neural prostheses and brain-computer interface devices with adaptive and robust performance.  Thanks to the advances in neural data acquisition technology, the process of data collection has been substantially facilitated, resulting in abundant pools of high-dimensional, dynamic, and complex data under various modalities and conditions from the nervous systems of animals and humans. The current modeling paradigm and estimation algorithms, however, face challenges in processing these data due to their ever-growing dimensions. This research addresses these challenges by providing a unified framework to efficiently utilize the abundant pools of data in order to deliver game-changing applications in systems neuroscience.<br/><br/>Converging lines of evidence in theoretical and experimental neuroscience suggest that brain activity is a distributed high-dimensional spatiotemporal process emerging from sparse dynamic structures. From a computational perspective sparsity is a key ingredient in rejecting interfering signals and achieving robustness in neural computation and information representation in the brain.  The main objective therefore is to develop a mathematically principled methodology that captures the dynamicity and sparsity of neural data in a scalable fashion with high accuracy. By focusing on the auditory system as a quintessential instance of sophisticated brain function, this research investigates several fundamental questions in systems neuroscience such as plasticity, attention, and stimulus decoding. The research is integrated with education and outreach activities including high school level hands-on workshops, undergraduate capstone projects, and interdisciplinary course development."
"1652294","CAREER:  In-Situ Compute Memories for Accelerating Data Parallel Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2017","02/11/2019","Reetuparna Das","MI","University of Michigan Ann Arbor","Continuing grant","Almadena Chtchelkanova","01/31/2022","$363,130.00","","reetudas@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","1045, 7942","$0.00","As computing today is dominated by Big Data, there is a strong impetus for specialization for this important domain. Performance of these data-centric applications depends critically on efficient access and processing of data. These applications tend to be highly data-parallel and deal with large amounts. Recent studies show that by the year 2020, data production from individuals and corporations is expected to grow to 73.5 zetabytes, a 4.4 increase from the year 2015. In addition, they tend to expend disproportionately large fraction of time and energy in moving data from storage to compute units, and in instruction processing, when compared to the actual computation. This research seeks to design specialized data-centric computing systems that dramatically reduce these overheads. <br/> <br/>In a general-purpose computing system, the majority of the aggregate die area (over 90%) is devoted for storing and retrieving information at several levels in the memory hierarchy: on-chip caches, main memory (DRAM), and non-volatile memory (NVM). The central vision of this research is to create in-situ compute memories, which re-purpose the elements used in these storage structures and transform them into active computational units. In contrast to prior processing in memory approaches, which augment logic outside the memory arrays, the underpinning principle behind in-situ compute memories is to enable computation in-place within each memory array, without transferring the data in or out of it. Such a transformation could unlock massive data-parallel compute capabilities (up to 100), and reduce energy spent in data movement through various levels of memory hierarchy (up to 20), thereby directly address the needs of data-centric applications. This work develops in-situ compute memory technology, adapts the system software stack and re-designs data-centric applications to take advantage of those memories."
"1855441","CRII: SHF: HPC Solutions to Big NGS Data Compression","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2018","10/22/2018","Fahad Saeed","FL","Florida International University","Standard Grant","Almadena Chtchelkanova","01/31/2020","$7,708.00","","FSAEED@FIU.EDU","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7798","7942, 9251","$0.00","Sequencing of genomes for numerous species including humans has become increasingly affordable due to next generation high-throughput genome sequencing (NGS) technologies. This opens up perspectives for diagnosis and treatment of genetic diseases and is increasingly effective in conducting system biology studies. However, there remain many computational challenges that need to be addressed before these technologies find their way into every day health and human care. One such daunting challenge is the volume of sequencing data which can reach peta-byte level for comprehensive system-biology studies. <br/>Genomic data compression is needed to reduce the storage size, to increase the speed and reduce the cost of I/O bandwidth required for transmission of such data. However, existing genomic compression solutions yield poor performance for Big Genomic Data. Further, the existing state of the art tools require the user to decompress the data before it can be used for further analysis. This project is focused on compression of genomic information and developing a framework which will allow analysis of compressed form of the data. The project develops HPC solutions for fast compression of Big NGS Data sets using ubiquitous architectures such as GPUs and multicore processors. HPC techniques are utilized to compute essential functions such as alignment and mapping using the compressed form of the NGS data. More efficient encoding of the NGS data for better network utilization is also being investigated."
"1453705","CAREER: Flashing Up Data Centers: An Orchestrated Design for Flash-based Distributed Storage Systems","CCF","SOFTWARE & HARDWARE FOUNDATION, EPSCoR Co-Funding","02/01/2015","02/08/2019","Feng Chen","LA","Louisiana State University & Agricultural and Mechanical College","Continuing grant","Yuanyuan Yang","01/31/2020","$540,000.00","","fchen15@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7798, 9150","1045, 7941, 9150","$0.00","A large-scale deployment of flash devices into data centers can greatly improve the overall system performance and reduce the rapidly growing management cost (e.g., power, cooling, staffing, floor space). Despite the technical merits promised, such a grand technical transition fundamentally changes the long-held system design assumption for a disk-based storage and will inevitably bring major critical challenges in a real-world practice. For example, underutilization of flash space would cause huge economic loss; premature device wear-out may result in catastrophic data corruption; unbalanced system could bring severe resource contention; unoptimized applications may not receive anticipated benefits; and many others.<br/> <br/>This project aims to address these challenges. Research will be conducted to develop a cohesive design approach to providing an orchestrated whole-system optimization. By revisiting the entire storage hierarchy, from hardware, operating system, cluster middleware, to applications, the team will redesign the device architecture to enable an organic integration of flash devices as integral elements in a huge flash storage system, create a flash-based distributed storage service with optimized resource utilization and guaranteed data reliability. Furthermore, a set of key data center applications will be enhanced to fully exploit the great potential of the flash technology. As part of this CAREER project, the team will also seek influence to the industry, contribution to curriculum, and outreach to local area under represented students."
"1815073","NSF-BSF: AF: Small: Geometric Realizations and Evolving Data","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","05/14/2018","Michael Goodrich","CA","University of California-Irvine","Standard Grant","Tracy Kimbrel","09/30/2021","$474,392.00","","goodrich@acm.org","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7796","7923, 7929","$0.00","This project involves an integrated study of geometric realizations and evolving data. Geometric realizations are structures that realize relationships combining combinatorial and geometric constraints, and evolving data captures ways in which data changes over time. Of particular interest are algorithmic challenges arising from geometric realizations and evolving data applications in society, including physics, data visualization, and on-line servicing of fast-changing data. A vital component of the project involves the involvement of students in research; hence, this project has the potential of bringing expanded educational and research opportunities for developing the next generation of information technology researchers. In addition, this project involves a collaboration between researchers in the United States and Israel, which is expected to foster further ties between these two countries.<br/><br/>Specific topics of interest in this project include the following:<br/>* Stable-matching Voronoi diagrams, which are planar subdivisions determined by combining geometric constraints determined by distances involving a given set of points and combinatorial constraints based on matching preferences among these points.<br/>* Polyominoes, which are connected cells in an orthogonal lattice. These are often used to model percolation networks in physics.<br/>* Geometric graphs, which are representations of graphs using points for vertices and straight lines for edges.<br/>* Reactive data structures, which are efficient data representations that support data enable and disable operations along with queries.<br/>* Approximate representations, which are data configurations that provide good approximate solutions for data sets that are changing at a rate commensurate with the speed of the algorithm. <br/>For each of these and related topics, the goal of the research is to develop fast and efficient algorithms and data structures, based on exploiting methods from graph drawing, computational geometry, and theory of computation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718220","SHF: Small: Collaborative Research: Programming Tools for Adaptive Data Analysis","CCF","Secure &Trustworthy Cyberspace","08/01/2017","07/07/2017","Marco Gaboardi","NY","SUNY at Buffalo","Standard Grant","Anindya Banerjee","07/31/2020","$224,452.00","","gaboardi@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","8060","7923, 7943","$0.00","False discovery, or overfitting, occurs when an empirical researcher draws a conclusion based on a dataset that does not generalize to new data.  Although there are many statistical methods for preventing false discovery, most are designed for static data analysis, where a dataset is used only once.  However, modern data analysis is adaptive, and often the same datasets are reused for multiple studies by multiple researchers.  Adaptivity has been identified by statisticians as one cause of non-reproducible research, and this project?s broader significance and importance will be to begin addressing this problem.  Specifically, this project will build a prototype programming tool for preventing false discovery arising from adaptive data analysis.  The intellectual merits are to incorporate and extend recent theoretical advances on this problem into a programming framework that allows researchers to analyze datasets adaptively with robust guarantees that overfitting will not occur.<br/><br/>The project builds on a surprising recent connection between differential privacy and false discovery, a robust statistical guarantee that emerged recently to protect the privacy of sensitive data. This line of work shows that when data is analyzed in a differentially private way, then false discoveries cannot occur. Differential privacy is also programmable, and allows complex differentially private algorithms to be built from simple components, so it is an ideal programming framework for adaptive data analysis.  This project is extending existing differentially private programming frameworks to adaptive data analysis.  The PIs are also developing new algorithmic and programming languages tools for adaptive data analysis, and incorporating them into the first prototype system for this application."
"1527249","SHF: Small: Multi-criteria optimization control for temperature constrained energy efficient data center using fuzzy decision making theory","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/27/2015","Jun Wang","FL","University of Central Florida","Standard Grant","Yuanyuan Yang","07/31/2019","$369,092.00","","Jun.Wang@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7798","7923, 7941","$0.00","Recent years have seen many well-recognized energy conservation schemes developed for big data computing infrastructures, which aggregate heavy workloads on either a few chips or devices. While both of these methods reduce energy consumption, they can also elevate temperature levels on long standing IT instruments and ultimately cause them to overheat. As a consequence, the reliability of these devices can be significantly degraded as shown in many recent studies. In the worst cases, they can fail or malfunction. There is an imperative need for developing new power and energy control solutions in the multibillion-dollar industry of big data computing.<br/><br/><br/>This research project is directed towards system-level solutions for temperature constrained data center energy management. It will develop methods and tools for controlling energy consumption in high-performance data centers operating under various temperature constraints by exploring the use of fuzzy decision-making techniques. Most of the existing studies rely on a well-developed relationship model between each criterion including open-loop search and optimization methods and rigid control schemes with fixed temperature constraint assumptions. In contrast, this project pursues solutions from a different angle: given that each criterion is in a non-linear relationship with another in a data center, how can new modeless control techniques that do not rely on fixed constraints work successfully? The project, if successful, will achieve foreseeable societal gains in terms of environmental benefits of energy conservation, potential for commercial impact of reducing costs and increasing operational efficiency in both warehouse-scale computer systems and data centers and reducing the data-center temperature to maintain reliability."
"1718796","CIF:Small:Collaborative Research:Distributed Fog Computing for Non-Convex Big-Data Analytics","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","06/28/2017","Konstantinos Slavakis","NY","SUNY at Buffalo","Standard Grant","Phillip Regalia","08/31/2020","$200,000.00","","kslavaki@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7797","7923, 7935","$0.00","In our data-deluge era, massive chunks of information, perpetually collected by pervasive sensors, are communicated and processed by distributed computational architectures. To address emergent big-data computational issues, this project embarks on an ambitious multidisciplinary research effort that aims at advancing the state-of-the-art in-network/distributed big-data processing via a general algorithmic framework for data analytics over massively distributed data sets. The proposed algorithmic framework enables fully distributed and parallel big-data analytics, for a variety of heterogeneous data sets over a wide range of computational architectures. The developed research directions are beneficial also to domains far beyond big-data analytics, such as signal processing, machine learning, next-generation wireless communications, smart-city and smart-grid networks. Research results are distributed through archival publications, courses, undergraduate research opportunities, tutorials and conference presentations.<br/><br/>The developed scheme relies on a novel convexification/decomposition technique which accommodates a rich class of non-convex, unstructured and stochastic optimization tasks with non-separable objective functions. Algorithms are designed for settings where data are distributed across a large number of multi-core computational nodes, within a network of arbitrary topology with (possibly) time-varying and even random links. This new class of algorithms addresses shortcomings of current (non-parallel and non-distributed) convexification techniques via (i) full control of the degree of parallelism and distribution of the computation/signaling among processors/network nodes, and (ii) by offering a plethora of convex approximants, regularization terms, step-size rules, and communication protocols. Designed for time-varying or even random network topologies, the advocated framework demonstrates also another desirable attribute for distributed computations: resiliency to (random) network failures."
"1850274","CRII: SHF: Expediting Subgraph Matching on GPUs","CCF","SOFTWARE & HARDWARE FOUNDATION","02/15/2019","02/05/2019","Hang Liu","MA","University of Massachusetts Lowell","Standard Grant","Almadena Chtchelkanova","01/31/2021","$175,000.00","","Hang_Liu@uml.edu","600 Suffolk Street","Lowell","MA","018543643","9789344170","CSE","7798","7798, 7942, 8228","$0.00","We are living in an increasingly connected world, where the Big Data movement has resulted in not only more data but, more importantly, more connected data, such as, social networks, knowledge graphs and deep neural networks. Sub-graph isomorphism, which finds sub-graphs of interest from an enormous data graph, is a fundamental tool for an array of critical applications, e.g., cyber-security, criminal detection and health care. In spite of such a great potential, identifying all isomorphic sub-graphs is a challenge in the Big Data context due to high computation complexity and memory consumption. This project addresses this issue, and will benefit both industrial and academic communities, as well as train undergraduate, underrepresented and STEM high school students for high-performance data analytics (HPDA) research (https://www.uml.edu/research/hpda/).<br/><br/>Conventional efforts split the query graph into two disjoint parts for prune and join, which impairs the prune strength of the query graph and results in a large volume of unpromising candidates for the 'join' phase. This project advocates an entire query graph-based prune approach to resolve the computation and memory challenge. In particular, this research consists of two parts: 1) algorithms research -- provenance-aware intersection-based candidate construction -- will greatly reduce the false positives faced by the conventional approaches; 2) systems research, i.e., GPU-enabled massively parallel provenance group intersection, will tackle the bottleneck that is encountered on conventional CPU platforms, through use of Graph Processing Unit (GPU) acceleration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815643","SHF: Small: Energy Saving in Heterogeneous Data Centers","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","07/02/2018","Daniel Wong","CA","University of California-Riverside","Standard Grant","Yuanyuan Yang","09/30/2021","$499,375.00","Laxmi Bhuyan","danwong@ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7923, 7941","$0.00","Many critical online services are turning to cloud infrastructure to meet scalability demands. In order to sustain cloud computing growth, it is necessary to scale the computational capacity, and improve the energy efficiency, of data centers. Modern data centers increasingly integrate accelerators, such as Graphical Processing Units (GPUs), with traditional CPUs to provide unprecedented parallelism and order-of-magnitude improvement to computational throughput. In order to manage software workloads and hardware resources at cloud-scale, data centers are increasingly being virtualized to provide ease of software and hardware management. However, existing energy efficiency techniques are not well-suited for virtualization technology and heterogeneous hardware. This project provides fundamental insights and solutions towards achieving energy-efficient computing for emerging virtualized heterogeneous data centers. This project has wide-reaching benefits for the computational engines behind many workloads of national interest, such as weather forecasting and machine learning. Results of this research are being integrated into the existing undergraduate and graduate courses.<br/><br/>The research consists of three main thrusts: 1) development of heterogeneous benchmarks to evaluate server energy profile and metrics to quantify heterogeneous server energy proportionality in order to investigate the implications of virtualization and heterogeneous architectures, 2) development of software support for container live migration and performance-aware migration strategies to minimize live migration overhead, 3) development of management techniques to maximize energy proportionality of heterogeneous CPU and multi-accelerator systems, such as dynamic load balancing and automatic runtime task migration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1846502","CAREER: The Exocompiler: Decoupling Algorithms from the Organization of Computation and Data","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2019","02/01/2019","Jonathan Ragan-Kelley","CA","University of California-Berkeley","Continuing grant","Anindya Banerjee","06/30/2024","$101,530.00","","jrk@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7798","1045, 7943","$0.00","The performance of many important algorithms is dominated by the way their computations and data are organized for execution on specific hardware. Traditional ways of programming conflate algorithms and their organization such that a straightforward implementation is unacceptably slow. At the same time, neither one can be written or optimized independently, which limits the productivity of programmers and the portability of programs to future hardware. Optimized organizations are often an order of magnitude faster and more complex since they necessarily take a global, rather than per-operation, view of the algorithm to exploit parallelism and locality. This project's novelty is in creating a new kind of programming language in which the algorithm and its organization are decoupled from one another. Programming systems are the tool through which computation is applied to human problems. The project's impact will be transforming the way major classes of software are written, enabling a wider range of people to more productively write new algorithms which are both high-performance and portable to future computing hardware.<br/> <br/>This project explores a programming model that decouples algorithms from their organization, represented explicitly in the language as a ""schedule."" It extends this paradigm from multidimensional arrays to more general computations and data structures, including sparse matrices and graphs, and builds a machine-learning system to automatically find schedules competitive with human experts. To realize this vision, it pursues four major research directions: (a) combining tree search with neural networks in a reinforcement learning system to automatically find expert-quality schedules; (b) broadening the class of expressible computations and schedules to include general loops and program gradients; (c) broadening the class of data structures and schedules with a relational model to process sparse and linked data; and (d) creating a comprehensive performance benchmark suite to serve as our evaluation testbed, and also be shared broadly to foster new research in systems, compilers, and architecture. The resulting language and compiler enables productive high-performance programming and provides a powerful foundation for easily building new domain-specific programming systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1850439","CRII: CIF: Resource Allocation in Data Center Networks: Algorithms, Fundamental Limits and Performance Bounds","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2019","01/31/2019","Siva Theja Maguluri","GA","Georgia Tech Research Corporation","Standard Grant","Phillip Regalia","05/31/2021","$175,000.00","","siva.theja@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797","7797, 7935, 8228","$0.00","Today's era of big data and the proliferation of web apps and mobile apps is powered by enormous computing data centers, consisting of up to hundreds of thousands of computing servers with data distributed over these servers. In order to serve user requests, these servers need to communicate with each other, and this is facilitated by a data center network. Design and operation of these networks becomes more challenging as the size of the data centers increases. Today's data centers are typically operated at very low utilizations in order to meet stringent latency requirements of the users. The focus of this project is to develop analytical techniques to study the delay performance of algorithms that are used to make connections between the servers in a data-center network. This analysis will feed the development novel algorithms that maintain low latency and implementation complexity while improving utilization. The project includes engagement with companies to explore the use of these algorithms, training of graduate students, dissemination of this research through undergraduate and graduate courses, outreach activities to high-school students and involving undergraduate students in research. <br/><br/>This project consists of two main parts. The first develops analytical tools to study the performance of scheduling algorithms for data-center networks. Prior work in this area used methods based on diffusion limits and Brownian approximations, which become unwieldy when studying data-center networks. More recent work demonstrates the power of a much simpler drift-based approach. This project employs the drift method to study the performance of tail latencies, and a novel moment-generating function method will be developed to overcome the limitations of the drift method. In the second part, these tools will be leveraged to develop novel low-complexity algorithms for data-center networks. A key focus in the development of these algorithms is accommodating more realistic traffic patterns beyond the independent and identically distributed random arrivals assumed in the literature.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849876","CRII: CIF:  New Directions in Learning from Data with Faulty Correspondence","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2019","01/31/2019","Martin Slawski","VA","George Mason University","Standard Grant","Phillip Regalia","08/31/2021","$174,942.00","","mslawsk3@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7797","7797, 7935, 8228","$0.00","Contemporary data acquisition and analysis frequently involves the integration of multiple pieces of information about a common set of entities into a single comprehensive data set. In the absence of unique identifiers, merging corresponding fragments of data can be demanding and error-prone. This challenge is encountered in various settings covering applications in engineering such as sensor networks as well as in the work of government data analytics. In this project, it is explored to what extent functional relationships between different data sets can be leveraged to resolve potential ambiguities in the process of data integration. This question is also relevant to data confidentiality in situations in which an adversary tries to disclose sensitive information from anonymized data by using auxiliary data sources.<br/><br/>The objective of the project is the development and statistical analysis of computationally feasible methods to safeguard downstream statistical analysis against errors in data linkage and to restore missing or faulty correspondences. In this context, linear regression in the presence of an unknown permutation is of central interest. One specific direction of research is the use of prior knowledge about the underlying permutation as commonly available in applications with the goal to sidestep computational barriers and to reduce the occurrent of ill-posed problems in statistical estimation. Emphasis will be placed on the characterization of the fundamental limits of recovery. Making advances in this regard will entail the use of tools from various areas including the theory of assignment problems, nonlinear optimization, high-dimensional and robust statistical inference, and random matrix theory. Results of the conducted research can potentially impact related problems such as regression under unknown linear transform, including blind deconvolution, and inference for permutations, as found in ranking, seriation, or graph matching.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1563078","SHF: Medium: Collaborative Research: From Volume to Velocity: Big Data Analytics in Near-Realtime","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/21/2016","Oyekunle Olukotun","CA","Stanford University","Standard Grant","Almadena Chtchelkanova","07/31/2020","$666,665.00","Christopher Re","kunle@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7798","7924, 7942","$0.00","Most existing techniques and systems for data analytics focus exclusively on the volume side of the common definition of Big Data as volume, velocity and variety. In contrast, there are clear indications that the velocity component will become the dominant requirement in the near future, most significantly, because of the proliferation of mobile devices across the planet. This is compounded by the fact that the freshest data often contains the most valuable information and that users have grown accustomed to data that is deeply analyzed and processed by sophisticated machine learning (ML) techniques, to enable their ""always on"" experience. In most mobile interactions, for example, the physical locations of one or potentially many users play a role, but the system needs to process the actual locations, not the ones from ten minutes ago. Many similar use cases exist in finance, intelligence and other domains. In all of them, the desires for fresh and for highly processed data are in a fundamental tension, as high quality analysis is computationally expensive and often done in large batches. The intellectual merits of this project are to investigate a combination of new ideas to address this challenge, spanning machine learning algorithms, specialized hardware accelerators, domain-specific languages, and compiler technology. The project's broader significance and importance are to pave the way for new kinds of high-velocity big-data analytics, which have the potential to revolutionize the way that people interact with the world.<br/><br/>The project investigates new incremental ML primitives and new algorithms that can trade off speed with precision, but retain provable guarantees. Novel DSLs (domain-specific languages) make such algorithms and techniques available to application developers, and new compilation techniques map DSL programs to specialized accelerators. In particular, the project shows how through these novel compilation techniques, machine learning algorithms can especially benefit from hardware acceleration with FPGAs. Finally, the project investigates new compilation techniques for end-to-end data path optimizations, including conversion of incoming data from external formats into DSL data structures, and transferring data between network interfaces and FPGA accelerators. Tying these new ideas and techniques together, this project will result in an integrated full-stack solution (spanning algorithms, languages, compilers, and architecture) to the problem of achieving high velocity in big data analytics."
"1811894","SHF:  Small:  Verified High Performance Data Structure Implementations","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","06/22/2018","Lennart Beringer","NJ","Princeton University","Standard Grant","Anindya Banerjee","09/30/2021","$499,999.00","William Mansky","eberinge@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7798","7923, 7943","$0.00","Computational infrastructures of the 21st-century require data processing engines that receive, store, analyze, and provide massive amounts of data, with large numbers of requests arriving in rapid succession. To achieve the necessary responsiveness, these systems process many requests simultaneously and use sophisticated techniques to ensure that concurrent requests do not interfere with each other. These techniques are highly error-prone, and mistakes in the design or implementation of a system can lead to incorrect responses to queries and the storing of incorrect information. The goal of this project is to develop techniques for proving that existing systems are correctly implemented, and for building new systems that are correct by construction. The project's novelties are the principles used to show that sophisticated concurrent programs produce the correct results, and the application of these principles to real-world high-performance storage systems. The project's impacts are more reliable software for storage systems, including cloud services, web servers, and data warehouses, allowing people and businesses to rely on the systems that store their data online.<br/><br/>The project builds on recent advances in concurrent separation logic and machine-checked program verification, which allow researchers to prove that concurrent programs as written in languages like C correctly implement high-level specifications. In particular, the project examines the effects of relaxed-memory operations, which give higher performance at the cost of complicating the programmer's model of memory behavior. These operations are used in programming patterns such as optimistic concurrency control, which feature in several state-of-the-art database implementations. The project involves scaling up relaxed-memory reasoning to apply to C programs at a realistic scale, and showing how relaxed-memory reasoning at the level of individual operations relates to high-level database correctness properties like snapshot isolation. The upshot of such reasoning is to produce strong mathematical guarantees of application-level correctness for storage systems optimized for multicore architectures, and in the process to develop techniques that can be used to verify other high-performance concurrent software systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1526870","CIF: Small: Online Algorithms for Streaming Structured Big-Data Mining","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2015","09/03/2015","Namrata Vaswani","IA","Iowa State University","Standard Grant","Phillip Regalia","09/30/2019","$442,385.00","","namrata@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7797","7923, 7936, 9150, 9251","$0.00","In today's big data age, a lot of streaming big-data is generated around us. Most of this is either not stored or stored only for short periods of time, e.g., streaming videos or changing social network connections. This project develops novel online algorithms for dimensionality reduction and structural information recovery from incomplete or distorted data.<br/><br/>This research makes several contributions to both the theory and the practice of structure recovery from streaming big-data. (1) The investigator and her team are developing provably correct online algorithms for robust structure (subspace or support) tracking from undersampled, outlier-corrupted or otherwise highly noisy streaming data. While batch approaches for these problems have been well-studied, the online problem is largely open. Our theoretical results are among the first correctness results for the robust subspace tracking (online robust PCA) and robust support tracking problems. Online algorithms are useful because they are faster and need lesser storage compared to most batch techniques. Moreover, our online algorithms remove a key limitation of batch approaches by exploiting certain extra temporal dependency assumptions: they allow significantly more correlated support change compared with batch methods. (2) The investigator is also developing novel provably accurate solutions for the online robust sparse-PCA problem, as well as several other related problems. (3) Finally, this project is helping to produce a well-trained and diverse future workforce. We expect our solutions to significantly transform the state-of-the-art in various big-data analytics applications, e.g., streaming video analytics, mobile video chats, autonomous vehicle or airplane navigation in foggy or rainy environments, anomalous or suspicious behavior detection from dynamic social network connectivity data."
"1822965","SPX: Integrating Persistent Memory in the Cloud","CCF","SPX: Scalable Parallelism in t","10/01/2018","11/29/2018","Samira Khan","VA","University of Virginia Main Campus","Standard Grant","Marilyn McClure","09/30/2021","$969,505.00","Baishakhi Ray, Haiying (Helen) Shen","smk9u@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","042Y","026Z, 9102","$0.00","The massive volume of data and high computing intensity of large-scale applications in the cloud require thousands of machines in big data centers. In addition, there is an increasing demand for faster, energy-efficient, and scalable performance from new data-intensive applications. Unfortunately, as the technology scaling slows down, the semiconductor industry has been facing a major challenge in providing better performance and reducing the power consumption while processing large datasets.  To provide better performance and lower costs for cloud applications that manipulate massive data with tight latency constraints, service providers are moving towards in-memory frameworks to store the working data.  By exploring the roles of emerging memory technologies, this research project has the potential to improve cloud computing performance. The ideas developed in this research will bridge the gap between architecture, systems, and software engineering community and will enable system support and automated tools for adapting applications in the persistent cloud. The project will eventually enable a holistic ""persistent cloud system"" such that the cloud applications can be adapted transparently without significant programmers' effort.<br/><br/>The goal of this work is to enable a persistent cloud system in a holistic manner across the system stack such that the persistent cloud applications can be adapted in the systems without significant programmers? effort. In order to design a persistent cloud system, this work is to provide full stack support from the applications to hardware through three major research directions that need to be addressed to design a full-stack persistent cloud system, (i) lightweight storage layer support for persistent memory systems, (ii) data monitoring and placement based on application characteristics and trade-offs in NVM, and (iii) automated persistency support at the application-level.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822919","SPX: Collaborative Research: Scalable Heterogeneous Migrating Threads for Post-Moore Computing","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/27/2018","Vivek Sarkar","GA","Georgia Tech Research Corporation","Standard Grant","Vipin Chaudhary","09/30/2021","$450,000.00","","vsarkar@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","042Y","026Z","$0.00","The project will advance the state of the art in computer architecture and programming systems for extreme and heterogeneous parallelism. It is clear that the post-Moore' law era will require major disruptions in computing systems. This project will address computer architecture and programming system challenges for this new era, with a focus on approaches that are expected to be scalable in size, cost effectiveness, and usability by retaining some tenets of the von Neumann computing model (unlike more exploratory approaches like biological or quantum computing). By emphasizing data analytics, the work will also benefit a rapidly growing swatch of modern life (commercial, cyber, national security, social networks). A deeper understanding of how such applications can be made more scalable, and responsive enough to handle increasing real-time requirements, should lead to wider impacts across every-day life with significant potential for technology transition. There is also a direct connection to pedagogy and workforce development, since both hardware and software aspects of this proposal can enable a broad range of students to better understand the wider diversity of computing platforms projected in future technology roadmaps. <br/><br/><br/>The SHMT (Scalable Heterogeneous Migrating Thread) model developed in this award will include extensions to the migrating threads and asynchronous task models to support heterogeneity, and extensions to the transaction and actor models to support data coherence. Further, the investigators propose to use data analytic graph problems to evaluate their research, since these applications are both important in practice and are challenging to solve on current systems. Given the expected continued increase in the size, complexity, and dynamic nature of such computations, it is of growing value to understand how to implement them in a manner that can scale to very high levels of concurrency in environments that include high rate streams of both updates and queries. These techniques can also apply to other application classes, such as scientific applications where data is sparse or irregular. The overall objective of this 3-year research project is to advance the foundations of computer architecture and programming systems to address the emerging challenges of scalable parallelism and extreme heterogeneity, with an emphasis on data analytics and solving data coherence, system management, resource allocation, and task scheduling issues. The investigators will leverage their distinct but synergistic expertise in the architecture and programming systems areas by building on, and integrating, their past work on migrating threads and near-memory processing, software support for asynchronous task parallelism for heterogeneous computing, and data analytics. The Center for Research into Novel Computing Hierarchies (CRNCH) at Georgia Tech will provide access to first-of-a-kind alternative systems for use in evaluating the new concepts. Industrial collaborators include Lexis-Nexis Risk Solutions and Kyndi, for whom graph data analytics are central to their business model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822987","SPX: Collaborative Research: Memory Fabric: Data Management for Large-scale Hybrid Memory Systems","CCF","SPX: Scalable Parallelism in t","10/01/2018","07/27/2018","Xiaoyi Lu","OH","Ohio State University","Standard Grant","Yuanyuan Yang","09/30/2021","$450,000.00","","lu.932@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","042Y","026Z, 7941, 9102","$0.00","New large-scale high performance computing systems being developed for the national labs and by US industry, combine heterogeneous memory components, accelerators and accelerator-near memory, and programmable high-performance interconnects. These memory-rich designs are attractive as they provide the compute-near-data capacity needed for improving the time to scientific discovery, and for supporting new classes of latency-sensitive data-intensive applications. However, existing software stacks are not equipped to deal with the heterogeneity and complexity of these machine designs, which impacts application performance and machine efficiency. The Memory Fabric (MF) solution developed in this project provides new abstractions and mechanisms that permit the systems software stacks to gain deeper insight into applications' data usage patterns and requirements, and to coordinate the decisions concerning how data should be distributed across different memories, or exchanged along different interconnection paths. <br/><br/>The Memory Fabric (MF) architecture introduces new data-centric abstractions, memory object and memory object flow, and accompanying memory and communications management methods. The higher-level information captured in the new abstractions empowers the MF runtime to better guide the underlying memory and interconnect management, and to mask the complexities of the underlying memory substrate. Additional benefits are derived from use of near-memory-fabric computation, including via dynamically inserted application-specific codes, which further specialize and accelerate the operations carried out by MF. MF is evaluated using several important application domains, including big data learning and analytics, and traditional high-performance scientific simulations. Its benefits include gains in application performance and resource efficiency, while shielding applications and application developers from the underlying machine details.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1725702","SPX: Collaborative Research: Multicore to Wide Area Analytics on Streaming Data","CCF","SPX: Scalable Parallelism in t","08/15/2017","08/02/2017","Srikanta Tirthapura","IA","Iowa State University","Standard Grant","Tracy J. Kimbrel","07/31/2020","$308,000.00","","snt@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","042Y","026Z","$0.00","In today's big data era, there is an urgent need for methods that can quickly derive analytical insights from large volumes of data that are continuously generated.  Such streaming data include video, audio, activity logs, and sensor data, and are generated on a massive scale all over the world.  The need for real-time streaming analytics can only be fulfilled with the help of appropriately designed parallel and distributed algorithms. However, parallel and distributed computing systems come in a variety of shapes and sizes, and algorithms should be designed to match the characteristics of the underlying system. This project develops methods for analyzing massive streaming data on computing systems ranging from machines with multiple cores sharing memory to geo-distributed data centers communicating over wide-area networks. The results of this research are expected to improve the efficiency, latency, and throughput of streaming analytics.  Due to the foundational nature of the analytical tasks considered, results of this project will impact disciplines that use large-scale machine learning and graph analytics, including cybersecurity, social network analysis, and transportation. Resulting software will be released as toolkits on stream processing platforms, and deployed in a smart-city camera infrastructure. Synergy between the research goals and the teaching goals of the PIs will lead to new instructional material in existing courses as well as development of new courses in data analytics. Individuals from underrepresented groups will be included as a part of the project. The project will benefit from and strengthen collaborations between academia, industry, and national labs on streaming analytics.<br/> <br/>The first technical thrust of the project is on designing shared memory parallel algorithms for computation on data streams, that can achieve a high throughput and fast convergence for complex analytics tasks. The second thrust is on designing distributed streaming algorithms that can tolerate variable communication delays and adapt to available bandwidth in a wide-area network, through identifying good tradeoffs between freshness of results and volume of communication. These advances will be studied in the context of fundamental graph analytics and machine learning tasks such as subgraph counting, graph connectivity and clustering, matrix factorization, and deep networks. The project will utilize the vast body of theory and techniques developed in the realm of parallel computing in the design of methods for processing streaming data, leading to a toolkit of techniques that can be reused across applications. The project will also lead to advances in sequential streaming and incremental algorithms for certain problems; for instance, problems in machine learning that use iterative convergent methods. Based on the techniques designed, the project will design and build a hierarchical parameter server that operates effectively across the spectrum from multicore machines to data centers to wide-area data sources.<br/>"
"1637442","AitF: Collaborative Research: Theory and Implementation of Dynamic Data Structures for the GPU","CCF","Algorithms in the Field","09/01/2016","08/15/2016","John Owens","CA","University of California-Davis","Standard Grant","Tracy J. Kimbrel","08/31/2020","$438,876.00","","jowens@ece.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7239","","$0.00","Computers organize data in ""data structures,"" which are designed to allow certain operations on data such as looking up all items that match a particular set of criteria, or adding new items to an existing data set.  Computer scientists strive to build data structures that can perform these operations quickly and efficiently.  One way to make data structure operations faster is to use not just one but many processors, operating in parallel, to perform a given operation.  However, many of today's parallel data structures support only a limited set of operations and, notably, do not allow operations that modify these data structures instead of rebuilding an entire structure from scratch when only part of the data is updated.  In this project the PIs bring together expertise in data structures and parallel computing to design, build, and evaluate dynamic data structures that allow update operations.  This work targets the high-performance, highly-parallel graphics processing unit (GPU) and will significantly broaden the class of applications that the GPU can address.  The PIs will release their results as freely-available open-source software and will work with industrial partner NVIDIA to incorporate the research and educational outcomes of this project into NVIDIA's broad educational efforts.<br/><br/>In this project the PIs propose to build dynamic, high-performance data structures for manycore (GPU) computing.  Today's GPU data structures are rarely constructed on the GPU but instead are built on the CPU and copied to the GPU, and today's GPU data structures cannot be updated dynamically on the GPU but instead must be rebuilt from scratch.  This project targets dynamic dictionary data structures with point and range queries, lists, and approximate membership and range query structures.  The PIs will implement these data structures as high-performance, flexible, open-source software and use these data structures to develop a theoretical model, targeted at the GPU, for use by theorists and practitioners in manycore computing.  The project will also focus on numerous cross-cutting issues in data structure design, implementation, modeling, and evaluation that have the potential for significant practical impact on manycore computing."
"1566137","CRII: AF: Breaking Barriers for Geometric Data","CCF","","05/01/2016","04/12/2016","Benjamin Raichel","TX","University of Texas at Dallas","Standard Grant","Rahul Shah","10/31/2019","$161,277.00","","Benjamin.Raichel@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","026y","7796, 7929, 8228","$0.00","It is surprising how often geometric abstractions help us deal with understanding large systems:  molecules become balls and sticks, complex fluid or combustion simulations are shown as contours or isosurfaces, and movies become points in a high dimensional space to allow recommendations based on which other points are near one's favorite movies.  Computational Geometry, which develops efficient computer algorithms for problems stated in geometric terms, can thus play a central role in data analytics.  Traditionally, the focus in Computational Geometry was on exact algorithms with guaranteed performance on all possible inputs, including worst-case inputs. <br/><br/>This project recognizes that many practical data analysis tasks do not generate worst-case instances, and seeks to identify structural aspects of given problems that allow existing or new algorithms with better guarantees than the worst-case bounds for realistic cases, often using approximation, probabilistic analysis, parameterized complexity, or output sensitivity.<br/><br/>Understanding the huge volume of data from a combustion simulation run on a super computer gives a 3d example: Contour trees, a data structure used to summarize interactions between density or temperature isosurfaces in a simulation, take more than linear time to compute in the worst case, but by parameterizing on tree shape one can show that trees that are balanced can be computed in linear time.  Machine learning and clustering problems, like recommendation systems, give higher-dimensional examples in which one desires to extract a smaller and lower dimensional representation of the input, while preserving some feature of interest.  A geometric form of this problem is known as extracting a coreset; in the worst case the coreset size can be exponential in the dimension.  On real inputs however, there is often hidden low dimensional structure; rather than designing an algorithm whose running time depends on the worst case coreset size, the running time should adapt to the size required by the given instance.<br/><br/>Advancing non-worst-case analysis techniques helps bridge the gap between theory and practice, as there is often a disconnect between running times predicted by worst-case analysis and those seen on real data sets. The investigator will incorporate non-worst-case analysis techniques into his course curricula, as such techniques are essential yet severely lacking in standard algorithms courses. This project will also be used to support student research at the graduate as well as undergraduate levels on this topic."
"1725420","SPX: Scalable In-Memory Processing Using Spintronics","CCF","SPX: Scalable Parallelism in t","08/15/2017","07/24/2017","Ulya Karpuzcu","MN","University of Minnesota-Twin Cities","Standard Grant","Yuanyuan Yang","07/31/2020","$800,000.00","Jian-Ping Wang, Sachin Sapatnekar","ukarpuzc@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","042Y","026Z","$0.00","The computational demands of modern workloads are influenced by a data-centric view of computing. The traditional model of computing, which brought the data into the compute engine for processing, is falling apart in the era of exploding data volumes as the overheads of data transportation become forbidding. Instead, it is more advantageous to take computing to the data. The objective of this project is to explore the alternative paradigm of bringing computation to the data by developing a novel scalable framework for processing-in-memory (PIM). While traditional CMOS structures are unsuited to this tight integration, emerging spintronic technologies show remarkable versatility in this regard. The proposed approach will develop the notion of the computational RAM (CRAM) to build PIM solutions to solve data-intensive computing problems using spintronics technologies. The project seeks to provide a complete solution across the system stack to the PIM problem under the CRAM platform.The project seeks to advance the state of the art in electronics technology, and potentially has a large impact in a pervasively-electronic society. Technically, its research results are projected to significantly advance the state of the art in large scale memory-centric computing using post-CMOS spintronic technologies, paving the way for new ways to build energy-efficient, scalable integrated systems. A multi-pronged outreach strategy will be pursued to take the results of this effort to a set of core constituencies. Human resource development will be achieved by training of undergraduate and graduate students in post-CMOS methods and novel computing paradigms.<br/><br/>The notion of bringing computation nearer to memory has gained wide currency in the recent past. However, since the regularity of large memory arrays is considered sacrosanct, the most viable solutions proposed so far perform processing near-memory, performing computation at the edge of a large memory array. The proposed CRAM-based approach avoids the substantial overheads of such a method, in bringing data to and from the periphery, and proposes a method for reconfiguring the memory to write the output of a logic operation directly into a memory cell. This project realizes the potential of the CRAM across the system stack by exploring the optimum over a space of choices in technology, logic design, and memory architecture to implement a diverse set of basic computational building blocks; by quantitatively characterizing CRAM-specific multi-granular parallelism; by investigating implications for the eco-system integration; by devising effective methods for CRAM-specific spatio-temporal parallel task scheduling; and by demonstrating how bioinformatics applications and applications featuring irregular, i.e., amorphous parallelism can benefit from CRAM."
"1656905","CRII: AF: Novel Geometric Algorithms for Certain Data Analysis Problems","CCF","CRII CISE Research Initiation","05/15/2017","05/29/2018","Hu Ding","MI","Michigan State University","Standard Grant","Rahul Shah","04/30/2019","$174,328.00","Hu Ding","huding@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","026Y","7796, 7929, 8228","$0.00","We can often see trends or clusters in data by graphing or plotting-- giving geometric form to data.   As data increases in volume and complexity, giving it geometric form and then developing computational geometry algorithms is still a fruitful way to approach data analysis. For example, activity data from a smartphone or fitness tracker can be viewed as a point in thousands of dimensions whose coordinates include all positions, heart rates, etc. from an entire sequence of measurements.  For better privacy, we can share summaries (rough position, duration, etc.) as points in tens of dimensions.  Points from many people can be clustered to identify similar patterns, and patterns matched (with unreliable data identified and discarded) to recognize actions that a digital assistant could take to improve quality of life or health outcomes.   <br/><br/>This project aims to develop a set of advanced data structures and novel geometric algorithms for three fundamental data analysis problems: (1) constrained clustering in high dimensions, (2) geometric matching under certain transformations, and (3) extracting trustworthy information from unreliable data.  The first two problems are both naturally studied by computational geometry, and the third has a novel formulation as a geometric optimization problem in high dimensions.  The goal is to achieve highly efficient and quality guaranteed solutions for each of these problems. The new geometric insights, advanced data structures, and efficient algorithmic techniques introduced by this project will enrich further development in computational geometry and bring fresh ideas to other areas, including machine learning, computer vision, data mining, and bioinformatics.  <br/><br/>This project provides research and educational opportunities in data analysis to both graduate and undergraduate students (including women, minorities, and other underrepresented groups) at Michigan State University.  It also undertakes outreach activities for students in K-12 outreach activities and prepares online materials to benefit more students and teachers. In particular, student evaluations of teacher performance will be one of the data sets used in problem (3), extracting trustworthy information from unreliable data."
"1535565","AitF: FULL: Query Processing with Optimal Communication Cost","CCF","Algorithms in the Field","08/15/2015","08/07/2015","Dan Suciu","WA","University of Washington","Standard Grant","Tracy J. Kimbrel","07/31/2019","$720,000.00","Magdalena Balazinska","suciu@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7239","012Z","$0.00","Big Data analytics is changing traditional query processing in two ways.  The first is a shift from single server or small-scale parallel relational databases to massively distributed architectures, where hundreds or thousands of servers are used during the computation of a single query.  The second is an increased complexity in the queries being issued, from single- or star-joins, to complex graph-like structured queries.  This project develops new algorithms for query processing over large distributed systems, which are optimized for the cost of communication, then implements and evaluates these algorithms in an open-source big data management system and service.<br/><br/>The project studies a new approach to query evaluation that computes the entire query at once, replacing the traditional approach based on a query plan. The theoretical part of this project builds on a new model, called the Massively Parallel Communication model (MPC), where the communication is the only cost. The system development is performed over the Myria big data management system and service.<br/><br/>The Intellectual Merit of the project consists in advancing the state of the art in both the theory and systems approaches to query evaluation in modern, massive-scale shared-nothing clusters.  It develops new, fundamental algorithms for processing queries over massively distributed architectures, with a provably optimal communication cost. The project implements and deploys these algorithms in a system, validating and informing the theoretical model. In particular, the project makes the following contributions: it develops provably optimal, one-round algorithms for skewed data; it studies how and when multiple rounds can be used to further reduce the communication cost; it experiments with these novel algorithms on clusters with up to 1000 worker processes; and it develops a new theoretical model for the communication cost on large shared-nothing architectures with heterogeneous hardware.<br/><br/>The Broader Impact of the project is to contribute to a new architecture for massively parallel query processing, where the traditional multi-step, single-join query evaluation approaches are replaced with novel, single-step, multi-join algorithms. This change has the potential to lead to more efficient big data analytics engines, allowing data analysts to explore large datasets more efficiently. As an immediate application, the project will impact the domain scientists who already use the Myria big data management system and service. All algorithmic discoveries in this project will be implemented in the Myria system, and will significantly improve query performance, allowing domain scientists to conduct more complex analytics and explorations over their data."
"1645599","EAGER: Towards Automated Characterization of the Data-Movement Complexity of Large Scale Analytics Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2016","08/11/2016","Ponnuswamy Sadayappan","OH","Ohio State University","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$300,000.00","Srinivasan Parthasarathy","sadayappan.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7798","7916, 7942","$0.00","We have entered a new era where power/energy limitations have become fundamental drivers of technological trends.  The cost in both time and energy for moving data from off-chip main memory to the processor is significantly higher than the cost of a double-precision floating-point computation.  With future technologies, this ratio will only get worse.  Therefore the characterization of the inherent data movement costs of algorithms is very important, and is particularly critical for large scale data-analytic applications.  However, unlike the well-understood computational complexity of algorithms, the data movement complexity is known only for a small number of algorithms.<br/><br/>Prior techniques for characterizing the data movement complexity of algorithms has either been restricted to subclasses of computations, or has required ad hoc manual reasoning. This project develops a scalable automated tool for analyzing the data movement complexity of arbitrary unstructured computations, expressed as computational directed acyclic graphs (CDAGs). The researchers explore several directions including out-of-core strategies, decomposition/recomposition of graphs, directional component analysis, and empirical function fitting, to address scalability challenges."
"1717314","CIF:Small:Collaborative Research:Codes for Storage with Queues for Access","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","04/16/2018","Emina Soljanin","NJ","Rutgers University New Brunswick","Standard Grant","Phillip Regalia","08/31/2020","$317,228.00","","emina.soljanin@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7797","7923, 7935, 7937, 9102, 9251","$0.00","Large volumes of data, which are being collected for the purpose of knowledge extraction, have to be reliably, efficiently, and securely stored. Retrieval of large data files from storage has to be fast. Large-scale cloud data storage and distributed file systems have become the backbone of many applications such as web searching, e-commerce, and cluster computing. Cloud services are implemented on top of a distributed storage layer that acts as a middleware to the applications, and also provides the desired content to the users, whose interests range from performing data analytics to watching movies. Users of cloud systems demand that their content and services be readily available and their data be reliably stored. Although there are apparent connections and trade-offs between these two objectives, so far they have been addressed mostly separately. The proposed research will characterize the interplay between reliable data storage and fast data access, and develop methods to jointly optimize these two main objectives of cloud storage. The project team will develop the methodology for design, analysis, and performance evaluation of a broad range of techniques for distributed storage that enable high reliability, robustness, and fast data retrieval. The research team will also develop schemes for storage and distributed computing which will optimize both reliability and the latency of data access. The methodology developed in the course of this project will allow the system operators to make their content and services readily available to users and support delay-sensitive applications ranging from individual video streaming to using online collaborative tools.  This research will minimize the energy requirements of data centers, which have increased massively in recent years. The project will contribute to the broader areas of coding theory, information theory, and queueing theory, and open new ways of cross-pollination.<br/><br/>This project focuses on efficient data access in distributed file systems that employ codes for reliable and efficient storage. Users of cloud systems demand that their content and services be readily available and their data be reliably stored. Although there are apparent connections and trade-offs between these two objectives, thus far they have been addressed mostly separately. This proposal follows findings that analyze how some of today?s solutions for reliable data storage affect the speed of data download under certain access models. This preliminary research has shown that, in some scenarios, the coding schemes used for increasing storage reliability can be further exploited for fast data access, while in others, the coding schemes that seemingly increase data availability actually fail to provide efficient access to popular content (so-called ?hot data?). The proposed research aims to characterize the interplay between reliable data storage and fast data access, and develop methods to jointly optimize these two main objectives of cloud storage. The proposed research will first identify and design schemes for coded-data access and derive (bounds on and estimates of) the expected download time for these schemes. Regardless of which data access scheme is used, the expected download time will depend on realistic service models as well as the distributed system service capacity provisioning and allocation schemes, which are then addressed. The work will also focus on the connections between the proposed research and the areas of efficient distributed computing and reduction in data center energy consumption. Preliminary results indicate that these areas are closely connected and the techniques developed for one area can benefit other areas."
"1718549","AF: Small: Learning and Optimization with Strategic Data Sources","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","06/26/2017","Yiling Chen","MA","Harvard University","Standard Grant","Tracy J. Kimbrel","08/31/2020","$450,000.00","","yiling@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7923, 7926, 7932","$0.00","The goal of this research project is to develop new results in machine learning and optimization when training data for machine learning or information about optimization problems is acquired from strategic sources.  We are blessed with unprecedented abilities to connect with people all over the world: buying and selling products, sharing information and experiences, asking and answering questions, collaborating on projects, borrowing and lending money, and exchanging excess resources.  These activities result in rich data that scientists can use to understand human social behavior, generate accurate predictions, find cures for diseases, and make policy recommendations.  Machine learning and optimization traditionally take such data as given, for example treating them as independent samples drawn from some unknown probability distribution.  However, such data are possessed or generated by people in the context of specific rules of interaction.  Hence, what data become available and the quality of available data are results of strategic decisions.  For example, people with sensitive medical conditions may be less willing to reveal their medical data in a survey and freelance workers may not put in a good-faith effort in completing a task.  This strategic aspect of data challenges fundamental assumptions in machine learning and optimization.  The research project takes a holistic view that jointly considers data acquisition with learning and optimization.  It will bring improved benefits in business, government, and societal decision-making processes where machine learning and optimization are widely applicable.  The research project also involves the mentoring of PhD students, innovation in graduate teaching, and engagement of members of underrepresented groups in research.<br/><br/>The PI will pursue a broad research agenda developing a fundamental understanding of how acquiring data from strategic sources affects the objectives of machine learning and optimization.  The first set of goals aims to develop a theory for machine learning when a learning algorithm needs to purchase data from data holders who cannot fabricate their data but each have a private cost associated with revealing their data.  A notion of economic efficiency for machine learning will be established.  The second set of goals will further advance the frontier of machine learning by designing joint elicitation and learning mechanisms when data are acquired from strategic agents but the quality of the contributed data cannot be directly verified.  The third set of goals will develop optimization algorithms with good theoretical guarantees when parameters of an optimization problem may be unknown initially but the algorithm designer can gather information about the parameters from strategic agents."
"1421848","CIF:Small:Collaborative Research:Efficient Codes and their Performance Limits for Distributed Storage Systems","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2014","07/22/2014","P. Vijay Kumar","CA","University of Southern California","Standard Grant","Phillip Regalia","08/31/2019","$162,699.00","","vijayk@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","7923, 7935","$0.00","The explosive growth of data being generated and collected has rekindled interest in efficient means of storing such data.  Large data centers and distributed storage systems have become more widespread, playing an ever-increasing role in our everyday computational tasks.  While a data center should never lose data, disk failures occur on a daily basis as confirmed by the industry statistics. Methods and ideas from error correcting codes developed in this project enable the system to provide better guarantees against data loss as well as to reduce the amount of data that needs to be moved in order to enable recovery of information lost due to disk failures. Another related goal of this project is the reduction of storage overhead needed to support the recovery procedures. These goals are accomplished by relying on algebraic methods of constructing the data encoding procedures as well as on novel algorithms of data exchange and recovery. Overall the research performed in the course of this project contributes to the development of more efficient data management procedures in large-scale distributed storage systems.<br/><br/>This project puts forward new algebraic procedures for data encoding and recovery that enables one to achieve tradeoff between overhead and repair bandwidth based on the concept of local recovery.  The project studies both the case of recovering from a single disk loss, which is the most frequent problem in systems, as well as from the failure of multiple disks, addressing the problem of correcting one erasure as well as multiple erasures in data encoding. New bounds on the distance of codes with the locality requirement derived in this research are attained with new constructions of optimal locally recoverable codes equipped with simple recovery procedures. The project also addresses the problem of simultaneous recovery of data from multiple locations, enhancing data availability in large-scale distributed storage systems which are a key backbone component of the 21st century economy."
"1564355","CIF: Medium: Collaborative Research: Learning in High Dimensions: From Theory to Data and Back","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/29/2016","Alon Orlitsky","CA","University of California-San Diego","Continuing grant","Phillip Regalia","06/30/2020","$297,994.00","","alon@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7797, 7924, 7936","$0.00","Statistical-modeling is the cornerstone of analyzing modern data sets, and using observed data to learn the underlying statistical model is a crucial part of most data analysis tasks. However, with the success of data utilization came a vast increase in its complexity as expressed in complex models, numerous parameters, and high dimensional features. This research project studies problems in learning such high-dimensional models, both in theory and in practice with actual datasets in cutting-edge applications.<br/><br/>Learning high-dimensional models efficiently, both in terms of computation and in terms of the use of the data, is an important challenge. The research characterizes the fundamental limits on the sample and computational complexity of several key distribution learning problems, as well as the associated optimal learning algorithms that achieve the limits. The learning problems underpin important tasks such as clustering, multiple testing of hypothesis and information measure estimation. The new algorithms and new methodologies developed are evaluated and applied on real data from three specific applications: 1) denoising of high throughput transcriptomic data; 2) analysis of omics data for personalized medicine; 3) ecological population studies.  While these applications are useful on their own right, there will also be many other potential applications in fields such as speech recognition, topic modeling, character recognition, neuroscience, etc."
"1563098","CIF: Medium: Collaborative Research: Learning in High Dimensions: From Theory to Data and Back","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","09/13/2018","David Tse","CA","Stanford University","Continuing grant","Phillip Regalia","06/30/2020","$597,629.00","","dntse@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7797","7797, 7924, 7936","$0.00","Statistical-modeling is the cornerstone of analyzing modern data sets, and using observed data to learn the underlying statistical model is a crucial part of most data analysis tasks. However, with the success of data utilization came a vast increase in its complexity as expressed in complex models, numerous parameters, and high dimensional features. This research project studies problems in learning such high-dimensional models, both in theory and in practice with actual datasets in cutting-edge applications. <br/><br/>Learning high-dimensional models efficiently, both in terms of computation and in terms of the use of the data, is an important challenge. The research characterizes the fundamental limits on the sample and computational complexity of several key distribution learning problems, as well as the associated optimal learning algorithms that achieve the limits. The learning problems underpin important tasks such as clustering, multiple testing of hypothesis and information measure estimation. The new algorithms and new methodologies developed are evaluated and applied on real data from three specific applications: 1) denoising of high throughput transcriptomic data; 2) analysis of omics data for personalized medicine; 3) ecological population studies. While these applications are useful on their own right, there will also be many other potential applications in fields such as speech recognition, topic modeling, character recognition, neuroscience, etc."
"1527127","CCF: SHF: Small: Collaborative Research: Domain-specific Reconfigurable Processor for Time-Series Data Mining and Monitoring","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2015","07/18/2016","Abdullah Mueen","NM","University of New Mexico","Standard Grant","Yuanyuan Yang","08/31/2019","$293,497.00","","mueen@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","7798","7923, 7941, 8091, 9150, 9251","$0.00","This proposal will investigate techniques to improve performance and reduce the cost and energy consumption of wearable devices (e.g., Internet-of-Things) that perform real-time medical monitoring. The objective is to demonstrate how to construct programmable integrated circuits that can provide monitoring capabilities while remaining small enough to be convenient and unobtrusive to the wearer. As a motivating example, such a device could detect and predict medical problems of fundamental importance such as pericardial tamponade, a life-threatening emergency where in the pericardium (a sac surrounding the heart) fills with fluid and prevents the heart from pumping blood, leading quickly and directly to death. Broader impacts of this effort include: reducing the cost of medical monitoring and saving lives; introduction of undergraduate students at both institutions to hardware/software co-design for wearable computing through a Freshman Discovery Seminar; inclusion of women and underrepresented minorities in the project; public release of hardware and software developed in the course of this project; and tutorial dissemination targeting the database/data mining and design automation research communities.<br/><br/><br/>The technical approach will involve the creation of application-specific integrated circuit hardware that can be added to embedded processors and microcontrollers to improve the performance of real-time medical monitoring applications. The research is based on the observation that real world data sets often exhibit a significant disparity in the dimensionality (data sampling rate) and cardinality (number of distinct values) of the data; for example, echocardiograms (ECGs) are often sampled at 1,024 Hz and 64-bits, although reducing the dimensionality to 128 Hz and the cardinality to 8-bits suffice for real-time monitoring. The fundamental challenge is that the minimum dimensionality and cardinality may vary from task-to-task, from individual-to-individual, and possibly even from hour-to-hour for a given individual. The Minimum Description Length (MDL) principle will be investigated as a potential solution to find the intrinsic dimensionality and cardinality of the data source, which can reduce data volume and improve detection accuracy by noise reduction. This information will then be leveraged to design domain-specific adaptive architectures that can exploit this reduced data volume to improve throughput and enhance battery lifetime."
"1533795","XPS: EXPL: SDA: Scalable Concurrency Control Techniques for Distributed Systems","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/14/2015","Ananth Grama","IN","Purdue University","Standard Grant","M. Mimi McClure","08/31/2019","$299,990.00","","ayg@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8283","","$0.00","Virtually all distributed computing applications, from transactions on databases to updates on social media platforms, involve concurrent operations on data objects. For these applications, concurrency control mechanisms represent significant performance overheads. These applications typically exhibit strong and persistent patterns in data access. Motivated by the importance of the problem, this project investigates the use of dynamic data- and lock-access patterns in distributed computations to significantly improve the performance of concurrency control mechanisms for scalable systems, specifically, in conventional cloud environments and key-value stores such as BigTable, HBase, and Cassandra. In contrast to conventional techniques that collocate locks with corresponding data items, this project relies on a modular lock service that decouples lock locations from corresponding data objects, and maintains lock state of all data items in a small set of storage nodes. <br/><br/>This design choice motivates a number of questions for this research: (i) where and when should lock states be migrated into the lock service? (ii) when should lock state be repatriated to the data store? (iii) how should the lock service be scaled out? (iv) what are fault-tolerant, low-overhead, deadlock- and livelock-free protocols for these operations? and (v) how can long-lived data access patterns be leveraged in such systems? Building on preliminary results that demonstrate the feasibility and considerable promise of the approach, the project develops algorithms, protocols, analyses, and open-source software, along with comprehensive validation in the context of a diverse set of applications.<br/><br/>The project will result in a novel framework for concurrency control in scalable distributed systems. The concurrency control service has a number of desirable features: (i) modularity -- the service can be instantiated at runtime, with minimal change to underlying data storage organization and access mechanisms; (ii) extensibility ? the service adapts dynamically to load and service requirements; and (iii) high performance through the use of efficient algorithms exploiting data and lock access patterns. These features are achieved through a novel mix of algorithms for lock migration and collocation, statistical models for dynamic lock and data access, protocols for lock state management, associated proofs of correctness and fairness, fault tolerance, performance, and scalability. The concurrency control service is fully validated on private as well as public clouds on a mix of applications drawn from Online Transaction Processing and Machine Learning.<br/><br/>The project directly impacts an important class of cloud-based applications by providing a modular and extensible lock service.  The service relieves burden on the application programmer while providing high performance and elastic throughput. Beyond this, the project includes a number of educational initiatives aimed at undergraduate and graduate education, along with outreach efforts aimed at enhancing representation of minority groups. These include development of instructional material, curricula, organization of and presentations at workshops and summer schools, and recruitment initiatives aimed at students from under-represented groups."
"1409601","SHF: Medium: Collaborative Research: Scalable Algorithms for Spatio-temporal Data Analysis","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2014","05/21/2014","Alok Choudhary","IL","Northwestern University","Standard Grant","Almadena Y. Chtchelkanova","05/31/2019","$709,342.00","Wei-keng Liao, Ankit Agrawal","choudhar@ece.northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7798","7924, 7942","$0.00","Acceleration of computing power of supercomputers along with development and deployment of large instruments such as telescopes, colliders, sensors and devices raises one fundamental question. ""Can the time to insight and knowledge discovery be reduced at the same exponential rate?"" The answer currently is clearly ""NO"", because a critical step that combines analytics, mining and discovering knowledge from the massive datasets has lagged far behind advances in software, simulation and generation of data. Analysis of data requires ""data-driven"" computing and analytics. This entails scalable software for data reduction, approximations, analysis, statistics, and bottom-up discovery. Scalable and parallel analytics software for processing large amount of data is required in order to make a significant leap forward in scientific discoveries. <br/>This project develops innovative, scalable, and sustainable data analytics algorithms to enable analysis and mining of massive data on high-performance parallel computers, which include (1) bottom-up and unsupervised data clustering algorithms that are suitable for spatio-temporal data, massive graph analytics, community computations, and detection of patterns in time-varying graphs, different types of data, and different data characteristics; (2) change detection and anomaly detection in spatio-temporal data; and (3) tracking moving data and cluster dynamics within certain time and space constraints. These parallel algorithms use the massive amount of data generated from scientific applications, such as astrophysics, cosmology simulations, climate modeling, and social networking analysis, for result verification and performance evaluation on modern high-performance parallel computers.<br/>This project directly addresses the critical needs for spatio-temporal data analysis, performance scalability, and programming productivity of large-scale scientific discovery via parallel analytics software for big data. This work will impact applications of enormous societal benefits and scientific importance such as climate understanding, environmental sustainability, astrophysics, biology and medicine by accelerating scientific discoveries. Furthermore, the developed software infrastructure can be used and adopted in commercial applications, such as commerce, social, security, drug discovery, and so on. The source codes are open to the public for all community to adapt, build-upon, customize and contribute to, thereby multiplying its value and usage."
"1629450","XPS:FULL: New Abstractions and Applications for Automata Computing","CCF","Exploiting Parallel&Scalabilty","09/01/2016","04/13/2018","Kevin Skadron","VA","University of Virginia Main Campus","Standard Grant","Matt Mutka","08/31/2020","$875,000.00","Westley Weimer, Ahmed Abbasi, Mircea Stan","skadron@cs.virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8283","","$0.00","As society collects more and more data about the world around us, and digitizes more and more artifacts, ""big data"" promises unparalleled potential, but also poses new and unique computational challenges. Turning data into useful knowledge at or near real-time can have significant impacts, such as enabling timely intervention in healthcare and fast response in cybersecurity. As technology constraints limit CPU performance, researchers and practitioners are increasingly looking to specialized processors to accelerate data analytics. The ability to extract patterns from unstructured data is an especially important task. This research project carries out a cross-stack investigation to evaluate the effectiveness of the automata computing paradigm to accelerate pattern mining of unstructured data. <br/><br/>Specifically, by leveraging the industry's new Automata Processor (developed by Micron Technology), this project is (1) developing benchmark suites of truly diverse automata for performance comparison of real and simulated, existing and future automata engines, (2) developing new tools, including programming languages, systems, and architectural enhancement to make automata computing intuitive and easy to adopt, (3) evaluating automata computing solutions to address real-world big-data applications, and (4) developing a set of educational and community-building activities to maximize the broader impact of the project outcome.  Successful implementation of this project enable new automata-based abstractions to shed light on the performance of AP technology for various applications, such as pattern mining. This project will build the the intellectual foundations to support and catalyze research, education, training, and adoption of automata-based solutions to address big-data challenges in industry, government, and society."
"1302435","CIF: Medium: Collaborative Research: New Approaches to Robustness in High-Dimensions","CCF","COMM & INFORMATION FOUNDATIONS, SIGNAL PROCESSING","07/01/2013","08/27/2015","Sujay Sanghavi","TX","University of Texas at Austin","Continuing grant","Phillip Regalia","06/30/2019","$695,369.00","Constantine Caramanis","sanghavi@mail.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7797, 7936","7924, 7936","$0.00","Rapid development of large-scale data collection technology has<br/>ignited research into high-dimensional machine learning.  For<br/>instance, the problem of designing recommender systems, such as those<br/>used by Amazon, Netflix and other on-line companies, involves<br/>analyzing large matrices that describe users' behavior in past<br/>situations.  In sociology, researchers are interested in fitting<br/>networks to large-scale data sets, involving hundreds or thousands of<br/>individuals.  In medical imaging, the goal is to reconstruct<br/>complicated phenomena (e.g., brain images; videos of a beating heart)<br/>based on a minimal number of incomplete and possibly corrupted<br/>measurements.  Motivated by such applications, the goal of this<br/>research is to develop and analyze models and algorithms for<br/>extracting relevant structure from such high-dimensional data sets in<br/>a robust and scalable fashion.<br/><br/><br/>The research leverages tools from convex optimization, signal<br/>processing, and robust statistics.  It consists of three main thrusts:<br/>(1) Model restrictiveness: Successful methods for high-dimensional<br/>data exploit low-dimensional structure; however, many real-world<br/>problems fall outside the scope of existing models.  This proposal<br/>significantly extends the basic set-up by allowing for multiple<br/>structures, leading to computationally efficient algorithms while<br/>eliminating negative effects of model mismatch.  (2) Non-ideal data:<br/>Missing data are prevalent in real-world problems, and can cause major<br/>breakdowns in standard algorithms for high-dimensional data. The<br/>second thrust devises relaxations and greedy approaches for these<br/>non-convex problems.  (3) Arbitrary Outliers: Gross errors can arise<br/>for various reasons, including fault-prone sensors and manipulative<br/>agents.  The third thrust proposes efficient and randomized algorithms<br/>to address arbitrary outliers."
"1254218","CAREER: Structured Learning of Distribution Spaces","CCF","COMM & INFORMATION FOUNDATIONS, SIGNAL PROCESSING","04/01/2013","04/20/2017","Raviv Raich","OR","Oregon State University","Continuing grant","Phillip Regalia","03/31/2019","$468,077.00","","raich@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7797, 7936","1045, 7936","$0.00","A rapid acceleration in both volume and complexity of public domain and scientific data presents new and exciting challenges. This project aims to develop a theoretical framework for structured learning of distribution spaces and study tools for identifying and utilizing probabilistic structure in high-dimensional large volume data. This project lies within the intersection of multiple disciplines: signal processing, pattern recognition, machine learning, probability and statistics, and thus will foster collaboration among these disciplines. The application of the proposed framework to data-driven medical diagnosis and ecological research will further the impact of this project beyond the realm of computational data analysis. Additionally, this research sets a goal to enrich the quality of education for both undergraduate and graduate students, through exciting integration of research, application, and new curriculum.<br/><br/>The research framework consists of geometrically-constrained probabilistic modeling and efficient optimization approaches for inference of multiple instance data. The project sets forth the following tasks i) confidence-constrained joint estimation of multiple discrete probability models, ii) joint learning of multiple distribution based geometrically-constrained maximum-entropy models, and iii) direct application of the developed framework to the analysis of clinical flow cytometry data for medical diagnosis and in-situ bioacoustics data for ecological research."
"1763918","SHF: Compute Caches: Opportunistic Parallelism in General Purpose Processors at Extreme Scale","CCF","INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CCF","10/01/2018","09/04/2018","Reetuparna Das","MI","University of Michigan Ann Arbor","Standard Grant","Almadena Y. Chtchelkanova","09/30/2019","$300,000.00","Dennis Sylvester, David Blaauw","reetudas@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1640, 2878","7924, 7942","$0.00","Computer designers have traditionally separated the role of storage and compute units. Memories and caches stored data. Processors' logic units computed them. It is not obvious that this separation is needed. A human brain does not separate the two so distinctly. This project addresses this fundamental question regarding the role of caches. Caches are used in almost all modern processors. They occupy a large fraction (over 70%) of the computer chip area. Latest Intel's server class Xeon processor, for instance, devotes several tens of megabytes just for its last-level cache. By avoiding movement of data in and out of memory arrays, this project will demonstrate the efficiency of compute caches, across a broad range of data-intensive applications that span several domains: cognitive computing, data analytics, security and graphs, and save vast amounts of energy spent in shuffling data between compute and memory units in modern computing systems. <br/><br/>This project will develop novel SRAM array designs for supporting a rich set of operation types and address various architectural challenges that arise in integrating highly parallel compute caches with a general-purpose host processor. Until today, caches have  served only as an intermediate low-latency storage unit. This project directly challenges this conventional design, and imposes a dual responsibility on caches: store and compute data. The key advantage of this approach is that it allows data stored across hundreds of memory arrays in caches to be operated on concurrently. The end result is that memory arrays morph into massive vector compute units that are potentially one to two orders of magnitude wider than a modern graphics processors (GPUs) vector units. This project considers vertically integrated solutions that cut across the computing stack: circuits, architecture, compilers, to applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1525372","SHF: Small: Memory Persistency: programming paradigms for byte-addressable, non-volatile memories","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","08/30/2016","Thomas Wenisch","MI","University of Michigan Ann Arbor","Continuing grant","Yuanyuan Yang","07/31/2019","$499,996.00","Peter Chen","twenisch@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","7923, 7941","$0.00","For over a decade, researchers in industry and academia have sought new techniques to store data so vast amounts of data can be stored inexpensively and accessed quickly.  Today, computer systems typically have three kinds of data storage: rotating disks for cheap-but-slow, long-term, bulk data storage; main memory for fast, but expensive access to data applications are actively using; and FLASH, which behaves much like rotating disks, but offers somewhat faster access at a higher price per bit.  However, we are now at the cusp of availability of new storage technologies that offer performance close to that of main memory, but can store data durably?that is, when the computer is powered off?at a cost per bit between Flash and main memory.  These new non-volatile memory devices add a new layer to the computer system data storage hierarchy between memory and FLASH.   Within a decade, systems incorporating these new non-volatile memories will become widely available.  Indeed, as cost and manufacturing processes improve, such memories may become the preferred storage for small devices in the ``Internet of Things'' and become critical to performance and recoverability in cloud systems. <br/><br/>These new memories open up the possibility to explore new formats and structures for storing data across computer system power failures.  Unlike disk and FLASH, data in non-volatile memories can be accessed at very fine granularity?individual data items (e.g., bytes)?rather than coarse-grained blocks of data.  This fine-grain access enables the development of data structures that reliably store data even if power fails unexpectedly, but with greater performance and simplicity than data storage on disk or FLASH.  Existing computer main memory systems have not been designed with durability across power failures in mind, since the data is lost when power fails, and therefore do not try to maintain any specific order in which data is written to memory.  However, controlling this order of writing data is critical to allowing a system to recover and continue operation if power fails while writes are in progress.  This research project will develop new ways for programmers to efficiently and easily design high-performance data structures that can recover from unexpected failures by carefully controlling the order in which data is recorded.  The research will be performed in close collaboration with industry, and integrated with the principal investigators? course offerings in operating systems and parallel computer architecture."
"1617618","AF: Small: Collaborative Research: Maintaining Order","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","05/20/2016","Michael Bender","NY","SUNY at Stony Brook","Standard Grant","Rahul Shah","08/31/2019","$210,296.00","","bender@cs.sunysb.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7796","7923, 7926","$0.00","This project investigates ""order structures,"" that is, data structures for maintaining total orders on dynamic data sets.  Order structures are applicable in surprisingly diverse settings.  The PIs aim to answer fundamental theoretical questions relating to order structures. The investigation takes place in the context of two high-impact application areas: (1) tools for debugging parallel programs and (2) data structures used in databases and file systems to organize data on disk or SSDs.  As part of the project, the PIs will develop publicly available educational materials and reference implementations on order structures to be incorporated into courses at their institutions and shared openly on the web.<br/><br/>The order structures addressed in this project are order-maintenance data structures, sparse tables, and data structures for incremental topological ordering.  The investigation comprises (1) new algorithms with good worst-case guarantees, (2) algorithms with strong common-case guarantees, i.e., optimized for common input distributions, (3) provably good concurrent algorithms, (4) algorithms that leverage randomization in surprising ways (e.g., to achieve history independence), and (5) robust lower bounds that also apply to the randomized setting. In the context of the target applications, the PIs will design and use order structures to implement race detectors, which uncover determinacy races in parallel programs. The PIs will also use orders structures to build external-memory key-value stores to make databases and file systems run faster."
"1533767","XPS: FULL: CCA: Cymric: A Flexible Processor-Near-Memory System Architecture","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/12/2015","Hyesoon Kim","GA","Georgia Tech Research Corporation","Standard Grant","Yuanyuan Yang","08/31/2019","$750,000.00","Sudhakar Yalamanchili, Saibal Mukhopadhyay","hyesoon@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8283","","$0.00","Emerging software applications in data analytics, graph analysis, and machine learning must process increasing volumes of data. Such data-intensive applications stress the communication and storage parts of a computer system more than the computation parts. Yet most of today?s systems are ?compute-centric? in the sense that their designs focus on improving the efficiency of computation. Computing near data or processing-near-memory can significantly reduce data communication between memory and processor providing potential for significant improvement in energy-efficiency of next generation computing systems.  <br/><br/>This project investigates and addresses the challenges facing the design of the next generation of memory-centric processors ? specifically processing-near-memory (PNM) architectures.  The project will characterize the energy and time behavior of future applications and use this understanding to assess how to best architect new systems that combine memory and compute in close proximity. Specifically, the activities will address i) programming models, ii) processing-near-memory architectures, and iii) power & thermal managements with the development of cross-cutting data movement, compiler and microarchitectural optimizations. The results of this work can influence the design of future enterprise and high performance computing systems. The major principles, insights, and outcomes of this project will be integrated as modules into mainstream courses in Computer Architecture. The investigators will also continue their participation in Institute outreach programs that have the goal of promoting participation of undergraduate, minority, and female students in engineering research and higher education."
"1816922","AF: Small: Relaxed Distributed Data Structures: Implementations and Applications","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","10/01/2018","08/27/2018","Jennifer Welch","TX","Texas A&M Engineering Experiment Station","Standard Grant","Rahul Shah","09/30/2021","$396,999.00","","welch@cse.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","2878, 7796","7923, 7926, 7934, 9102","$0.00","Society has become increasing reliant on distributed computing systems, from search engines to mobile telephony to electronic commerce to social media, and the future is likely to bring autonomous vehicles and more.  Yet distributed systems are notoriously hard to design so that they are correct, let alone efficient.  One way to construct distributed applications that are easier to verify as correct is to use shared memory for inter-process communication instead of more low-level techniques, as that contributes to better structured code.  Although shared memory is a convenient abstraction, it is not generally provided in large-scale distributed systems; instead, the processes keep individual copies of the data and communicate by sending messages to keep the copies consistent.  This project will contribute to making shared memory applications more reliable and efficient by developing and analyzing shared data abstractions that have relaxed semantics and thus can exhibit a trade-off between performance and specification.  In addition to training graduate students, the project will include a focus on involving domestic undergraduate students, especially women and under-represented minorities, in research through summer as well as academic-year experiences, with the goal of increasing the numbers that attend graduate school in computing related fields.  <br/><br/>Strongly consistent implementations of shared objects with strict semantics are provably expensive, fueling interest in relaxations. The objectives of the project are: to find optimally efficient algorithms to implement shared objects, focusing on relaxing specifications of both data types and consistency conditions; to understand the relationships between relaxing a type and relaxing a condition; and, to characterize applications that can exploit the relaxations.  Current performance analyses of shared object implementations in message-passing systems have numerous gaps: upper and lower bounds are not tight, some classes of operations are not considered, other metrics have not been studied, and mostly only overly-pessimistic worst-case analyses are known.  The project will focus on tight amortized analyses of algorithms for relaxed data types and seek a complete characterization.  Currently, relaxation of consistency conditions and relaxation of data type specifications have been considered independently; the project will seek to understand the relationships and trade offs between them to ease the task of the programmer.  Distributed systems in which processors enter and leave dynamically, such as peer-to-peer networks, data centers, and social networks, are typically asynchronous and crash-prone.  Characterizing churn patterns that allow implementations of relaxed shared objects would make it easier to determine if a particular situation can support them.  Many opportunities remain for characterizing classes of applications that can exploit relaxed data structures and/or relaxed consistency conditions; this would show which circumstances can benefit from savings obtained from relaxation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1715777","CCF-BSF: AF: Small: Collaborative Research: The Dictionary Problem Considered","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","06/19/2017","Martin Farach-Colton","NJ","Rutgers University New Brunswick","Standard Grant","Tracy J. Kimbrel","08/31/2020","$250,000.00","","farach@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7796","7923, 7926","$0.00","The performance of databases, file systems, and other data-storage systems depends on how the data is organized on disk.  The data-organization parts of these systems are called dictionaries.  Even though dictionaries have been studied for decades, some very basic questions remain unanswered. This grant will support research to address several of these questions. This research has the potential to improve performance of data-storage systems and applications based on these systems in many environments, including large-scale data analysis and high-performance computing.  Dictionaries are so fundamental that theoretical advances in dictionaries are quickly picked up by practitioners, and thus this research has strong potential for tech transfer.<br/> <br/>Specifically, the project will tackle foundational problems in dictionaries using techniques in write-optimization pioneered by the PIs. How are systems of dictionaries different than individual dictionaries?  How can dictionaries take advantage of variations in the rate of queries and updates?  How can one comprehensively compare dictionary data structures?  How can one bring the benefits of write-optimization to dictionaries on more complex data types?  Write-optimized dictionaries have fundamentally changed the landscape of databases, and this research will improve our understanding of the complexity of operations on such systems."
"1716252","CCF-BSF: AF: Small: Collaborative Research: The Dictionary Problem Considered","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","06/19/2017","Michael Bender","NY","SUNY at Stony Brook","Standard Grant","Tracy J. Kimbrel","08/31/2020","$250,000.00","Rob Johnson","bender@cs.sunysb.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7796","7923, 7926","$0.00","The performance of databases, file systems, and other data-storage systems depends on how the data is organized on disk.  The data-organization parts of these systems are called dictionaries.  Even though dictionaries have been studied for decades, some very basic questions remain unanswered. This grant will support research to address several of these questions. This research has the potential to improve performance of data-storage systems and applications based on these systems in many environments, including large-scale data analysis and high-performance computing.  Dictionaries are so fundamental that theoretical advances in dictionaries are quickly picked up by practitioners, and thus this research has strong potential for tech transfer.<br/> <br/>Specifically, the project will tackle foundational problems in dictionaries using techniques in write-optimization pioneered by the PIs. How are systems of dictionaries different than individual dictionaries?  How can dictionaries take advantage of variations in the rate of queries and updates?  How can one comprehensively compare dictionary data structures?  How can one bring the benefits of write-optimization to dictionaries on more complex data types?  Write-optimized dictionaries have fundamentally changed the landscape of databases, and this research will improve our understanding of the complexity of operations on such systems."
"1718380","AF: Small: Integrated Knowledge Discovery and Analysis Using Sum-of-Squares Proofs","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/30/2017","Brendan Juba","MO","Washington University","Standard Grant","Rahul Shah","08/31/2020","$439,997.00","","bjuba@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7796","7923, 7926, 7927, 9150","$0.00","Developing approaches to subjects as diverse as advertising, bioinformatics, counterterrorism, fraud detection, politics, sociology, and so on are based on data analysis. This project will develop new algorithms for data analysis with strong guarantees of their correctness and efficiency. Tools based on these algorithms will be usable as-is, without expert knowledge, in innovative data-driven applications far removed from academia. The project will also involve the training of students in advanced techniques for data analysis.<br/><br/>The algorithms considered in this project involve automated reasoning with an algebraic logic known as ""sum-of-squares."" Automated reasoning requires a delicate trade-off between expressiveness and simplicity, to facilitate reasoning that is both fast and effective. Sum-of-squares is capable of expressing much statistical reasoning, and yet is sufficiently simple to allow the design of tractable algorithms for reasoning. This project will consider how these algorithms can be used to reason about an overall distribution or population from a sample of data drawn from it. The main aim of the project is to develop efficient algorithms that guarantee that all of the relevant statistical facts are discovered during data analysis. The project will further develop these algorithms to solve problems in domains such as Computer Vision and Natural Language Processing.<br/><br/>In addition to the development of domain-specific algorithms, the project will consider sum-of-squares reasoning with high-degree polynomials. Although such reasoning in the standard sense is intractable, the project aims to simulate reasoning with such high-degree expressions with the aid of the sample of data from the distribution to be reasoned about. The project will also investigate the expressive power of sum-of-squares with such high-degree expressions. Specifically, the project will investigate whether or not such reasoning can simulate other logics such as resolution (or vice-versa), and will further investigate the extent of its ability to basic capture statistical notions."
"1823559","FoMR: Improving Microprocessor IPC for Data Center Workloads","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/22/2018","Heiner Litz","CA","University of California-Santa Cruz","Standard Grant","Yuanyuan Yang","09/30/2021","$173,000.00","","hlitz@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","2878","021Z, 2878, 7798, 7941","$0.00","More and more companies and institutions are moving their products and services into the cloud, increasing the number of compute cycles spent in data centers significantly. The explosive growth of such cloud computation has delivered great value to society at the cost of increasing the environmental footprint on our world. This project can improve the energy efficiency of data center systems through the design and optimization of cloud application-specific microprocessors. The research will be infused into the undergraduate curriculum at University of California Santa Cruz, educating the next generation of students on designing energy efficient, data center specific architecture and systems.<br/><br/>This project will develop machine learning techniques to predict datacenter application behavior and apply them to processor microarchitecture. In particular, a new cross-layer approach will be developed to improve the instructions-per-cycle (IPC) performance of data center workload-optimized processors. It will explore and develop solutions for a number of challenges including offline learning of data center application behavior, leveraging machine learning models for instruction prefetching and branch prediction, and building resource and power efficient hardware to leverage the machine learning models at execution time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813434","SHF: Small: Bitstream Processing","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/23/2018","Mikko Lipasti","WI","University of Wisconsin-Madison","Standard Grant","Yuanyuan Yang","09/30/2021","$449,997.00","","mikko@engr.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","2878","2878, 7923, 7941","$0.00","Embedded computing systems are becoming very common in today's world, ranging from wearable devices, to in-home smart speakers, to autonomous appliances and vehicles. Many of the computational tasks that these systems implement require very high performance hardware for tasks like audio voice recognition, visual object recognition, path optimization, and autonomous control.  Historically, such high-performance hardware relied on binary fixed-point algorithms deployed on low-power microcontrollers or digital signal processors. However, the sensing and control interfaces themselves do not use binary number representations, but instead use bitstreams, which encode numeric input and output values using the density of ones over time. Conventional computing substrates requires conversion of both inputs and outputs to interface with physical systems that utilize bitstreams.  This project is developing novel, biologically-inspired approaches for directly operating on data represented in the native bitstream format. Compute hardware that directly operates on these bitstreams can be seamlessly integrated into systems that sense the real world, process sensory data, and issue control commands based on the processed data as well as learned actions based on rewards from the environment. The capability of these new approaches will be demonstrated through two experimental platforms: a very low power speech recognition system that operates on bitstream audio data, and an autonomous airborne vehicle that learns to navigate its environment. This research has broad industry- and economy-wide impact since it will lead to the discovery and realization of novel, powerful, and energy-efficient approaches for implementing power- and energy-constrained embedded computing systems.<br/><br/>This research advocates development of novel, biologically-inspired approaches for processing data represented as bitstreams. Bitstreams, which encode numeric values using density of ones (unary) or ones and zeroes (binary) are a natural representation for data sensed from the environment (input) as well as robotic control (output), and can be inexpensively generated using low-cost, yet accurate, sigma-delta modulators. The initial phase of this research project focuses on developing the theoretical and algorithmic underpinnings for visual, auditory, and inertial sensory processing, including feature extraction, bandpass filtering, perspective and coordinate transforms, linear optimization, and memory formation, which are grounded in principles from the speech processing, computer vision, spiking neural networks, reinforcement learning, and signal processing domains. The novel sensory processing capabilities are then deployed in two experimental platforms: first, an ultra-low power acoustic model for speech recognition that can demonstrate the suitability of bitstream processing for feature extraction and sequence learning via long short-term memory. Next, bitstream sensory processing technology is coupled directly to a control system that enables an unmanned aerial vehicle to navigate in a controlled indoor environment while learning, with increasing efficiency, to identify and target sources of rewards. Both demonstration platforms rely on concepts from biological spiking neural networks, stochastic computing for arithmetic operations, as well as oversampled sigma-delta modulation theory for data representation and signal processing tasks, and provide unprecedented levels of efficiency in terms of energy consumption, compute density, and autonomous operation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1528181","CCF: SHF: Small: Collaborative Research: Domain-specific Reconfigurable Processor for Time-Series Data Mining and Monitoring","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2015","08/04/2018","Philip Brisk","CA","University of California-Riverside","Standard Grant","Yuanyuan Yang","08/31/2019","$327,247.00","","philip@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7923, 7941, 8091, 9251","$0.00","This proposal will investigate techniques to improve performance and reduce the cost and energy consumption of wearable devices (e.g., Internet-of-Things) that perform real-time medical monitoring. The objective is to demonstrate how to construct programmable integrated circuits that can provide monitoring capabilities while remaining small enough to be convenient and unobtrusive to the wearer. As a motivating example, such a device could detect and predict medical problems of fundamental importance such as pericardial tamponade, a life-threatening emergency where in the pericardium (a sac surrounding the heart) fills with fluid and prevents the heart from pumping blood, leading quickly and directly to death. Broader impacts of this effort include: reducing the cost of medical monitoring and saving lives; introduction of undergraduate students at both institutions to hardware/software co-design for wearable computing through a Freshman Discovery Seminar; inclusion of women and underrepresented minorities in the project; public release of hardware and software developed in the course of this project; and tutorial dissemination targeting the database/data mining and design automation research communities.<br/><br/><br/>The technical approach will involve the creation of application-specific integrated circuit hardware that can be added to embedded processors and microcontrollers to improve the performance of real-time medical monitoring applications. The research is based on the observation that real world data sets often exhibit a significant disparity in the dimensionality (data sampling rate) and cardinality (number of distinct values) of the data; for example, echocardiograms (ECGs) are often sampled at 1,024 Hz and 64-bits, although reducing the dimensionality to 128 Hz and the cardinality to 8-bits suffice for real-time monitoring. The fundamental challenge is that the minimum dimensionality and cardinality may vary from task-to-task, from individual-to-individual, and possibly even from hour-to-hour for a given individual. The Minimum Description Length (MDL) principle will be investigated as a potential solution to find the intrinsic dimensionality and cardinality of the data source, which can reduce data volume and improve detection accuracy by noise reduction. This information will then be leveraged to design domain-specific adaptive architectures that can exploit this reduced data volume to improve throughput and enhance battery lifetime."
"1527074","CIF: Small: Collaborative Research:Towards more Secure Systems: Uniformization for Secrecy","CCF","COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace","10/01/2015","09/22/2015","Matthieu Bloch","GA","Georgia Tech Research Corporation","Standard Grant","Phillip Regalia","09/30/2019","$230,000.00","","matthieu@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797, 8060","7434, 7923, 7935","$0.00","As communication networks play an increasingly vital role in our society, ensuring the confidentiality of information transmission and storage has become an increasing concern. New classes of networks, such as heterogeneous wireless networks and distributed storage networks, are emerging, in which the deployment of off-the-shelf cryptographic solutions faces several limitations. These include, for instance, the overhead of key management to maintain synchronized private and public keys; the challenge of distributing keys within the stringent delay constraints imposed by clients; and the exposure to large-scale attacks that were once deemed infeasible. Consequently, several solutions from network coding to physical-layer security have been developed to improve the confidentiality of data over modern wireless networks. Many key-independent and keyless schemes have been developed to provide additional robustness against attacks and more flexibility in the network operation. These schemes have in common that they exploit coded data as keys and move away from the traditional paradigm in which data is a commodity and in which information packets must be transmitted and protected independently. Instead, these schemes envision that information packets could be mixed in a controlled manner so as to introduce an intrinsic level of security against adversaries.<br/><br/>Most existing secrecy results hinge on the crucial assumption that data is uniformly distributed and independent from a packet to another. Unfortunately, recent results have shown that even optimal data compression algorithms  uniformize the statistics of coded data only in a very weak sense. Consequently, several security guarantees established in the literature may well collapse in the absence of any proven robustness with respect to data uniformity assumptions. The objective of this project is to develop a better understanding of how the statistical properties of passwords and data influence the security of secrecy systems. The project investigates several interrelated research tasks: (i) the analysis of the fundamental limits of data-compression techniques with improved uniformity properties; (ii) the design of low-complexity codes for uniformization; (iii) the application of such algorithms in cloud storage systems. Additional activities include an outreach effort to high-school students."
"1813330","CIF: Small: A Novel Paradigm of Information Extraction in Big Data Problems","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","08/09/2018","Xiangrong Yin","KY","University of Kentucky Research Foundation","Standard Grant","Phillip Regalia","09/30/2020","$281,871.00","","yinxiangrong@uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7797","7923, 7935, 9150","$0.00","This project considers problems related to the increasing volume of big data characterized by ultra-large sample size and ultra-high dimensionality. Such volumes introduce many new and unique challenges to scientists in both computation and statistical inference. Finding and isolating the important information in big data is an extremely difficult challenge. In response to such challenges, the project will develop a novel system that can provide quality analysis of big data. With the investigator's experience in statistical research of theory, methodology, and applications, the project will provide an excellent opportunity for both undergraduate and graduate students to participate in cutting-edge statistical applications and methodology development, and thereby prepare them well for their future careers.<br/> <br/>By localizing the data, this project develops a system with a coherent collection of novel techniques for estimation, computation, asymptotic studies, and statistical inference overcoming the new challenges for big data. The project will lead to new research directions in sufficient dimension reduction (SDR) and sufficient variable selection (SVS), producing new big data mining tools applicable in a wide range of scientific fields. This work is a major step towards improving the understanding of SDR and SVS, including their advantages and how to overcome their disadvantages to suit for the challenge of big data analysis. This project will also provide scientists a new platform to develop more flexible and efficient methods. The investigator will establish theoretical properties of the proposed estimators and develop new scalable computation algorithms for ultra-size and ultra-high dimensional data with an open-access R package to disseminate the knowledge to the scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718088","SHF: Small: Collaborative Research: Programming Tools for Adaptive Data Analysis","CCF","ALGORITHMIC FOUNDATIONS, SOFTWARE & HARDWARE FOUNDATION","08/01/2017","07/07/2017","Jonathan Ullman","MA","Northeastern University","Standard Grant","Anindya Banerjee","07/31/2020","$224,409.00","","jullman@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7796, 7798","7923, 7926, 7943","$0.00","False discovery, or overfitting, occurs when an empirical researcher draws a conclusion based on a dataset that does not generalize to new data.  Although there are many statistical methods for preventing false discovery, most are designed for static data analysis, where a dataset is used only once.  However, modern data analysis is adaptive, and often the same datasets are reused for multiple studies by multiple researchers.  Adaptivity has been identified by statisticians as one cause of non-reproducible research, and this project?s broader significance and importance will be to begin addressing this problem.  Specifically, this project will build a prototype programming tool for preventing false discovery arising from adaptive data analysis.  The intellectual merits are to incorporate and extend recent theoretical advances on this problem into a programming framework that allows researchers to analyze datasets adaptively with robust guarantees that overfitting will not occur.<br/><br/>The project builds on a surprising recent connection between differential privacy and false discovery, a robust statistical guarantee that emerged recently to protect the privacy of sensitive data. This line of work shows that when data is analyzed in a differentially private way, then false discoveries cannot occur. Differential privacy is also programmable, and allows complex differentially private algorithms to be built from simple components, so it is an ideal programming framework for adaptive data analysis.  This project is extending existing differentially private programming frameworks to adaptive data analysis.  The PIs are also developing new algorithmic and programming languages tools for adaptive data analysis, and incorporating them into the first prototype system for this application."
"1817634","CIF: Small: Collaborative Research:Synchronization and Deduplication of Distributed Coded Data: Fundamental Limits and Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","12/13/2017","Salim El Rouayheb","NJ","Rutgers University New Brunswick","Standard Grant","Phillip Regalia","06/30/2019","$87,338.00","","salim.elrouayheb@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7797","7923, 7935, 9251","$0.00","Part 1: Coding for distributed storage systems has garnered significant attention in the past few years due to the rapid development of information technologies and the emergence of Big Data formats that need to be stored and disseminated across large-scale networks. As typical distributed systems need to ensure low-latency data access and store a large number of files over a set of nodes connected through a communication network, it is imperative to develop new distributed coding schemes that protect the systems from undesired component failures. The two key functionalities of codes used in distributed systems, namely the reconstruction of files via access to a subset of the nodes and repair of failed nodes, need to be retained when the files are accessed and processed by the users via symbol/block insertion, deletion, or substitution edits. Deletions frequently arise due to system-level data deduplication: when parts of files are deduplicated or edited, the changes in the information content need to be communicated to the redundant storage nodes with minimum communication cost. Current solutions for synchronizing data that underwent edits assume that data is uncoded and they do not fully exploit the distributed nature of information. Furthermore, they mostly ignore the presence of deduplication protocols. This makes distributed storage architectures inefficient in terms of storage, user access times, and error protection. Hence, the goals of the proposed research program are to develop a new set of protocols and coding schemes that will  support a new generation of versatile and updatable coded distributed storage systems. <br/><br/>Part 2: Building on the preliminary work of the investigators, this proposal aims to set the foundations of the new field of coded synchronization and deduplication, with the goal of deriving fundamental performance limits, developing efficient algorithmic solutions for the two families of problems, and constructing new distributed storage codes that enable synchronization of coded data and coded deduplication. In particular, the proposal addresses the following comprehensive issues: <br/>1) Characterizing the communication rate limits of known and new (un)coded synchronization schemes, trade-offs between deduplication and data repair performance for different structured or encoded data formats and different types of communication channels.<br/>2) Introducing and analyzing the communication rate-distortion (CRD) function for approximate synchronization and deduplication of structured/encoded data, with a special focus on delay-sensitive applications.<br/>3) Developing dynamically updatable synchronization and deduplication algorithms cognizant of the network topology and of different prioritization needs of the users, as encountered in image and video data coding."
"1349285","EAGER: Cost- and Energy-Aware Query Processing in Mobile Clouds","CCF","INFO INTEGRATION & INFORMATICS","09/01/2013","06/29/2018","Le Gruenwald","OK","University of Oklahoma Norman Campus","Standard Grant","Almadena Y. Chtchelkanova","08/31/2019","$200,000.00","","ggruenwald@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","7364","7364, 7916, 9150","$0.00","Innovative mobile technologies offer interesting opportunities in many domains, such as health care, transportation, and commerce. They enable distant monitoring and permit consideration of parameters such as patient's and physician's mobility. This makes it possible to develop novel applications, such as mobile health services for telemedicine and assisted ambient living (particularly in rural areas) and mobile traffic services. Nevertheless, the amount of data to be generated and queried is very large and diverse and is collected from multiple sources.  The combination of big data and mobility leads to a major challenge: how to efficiently process queries from a myriad of mobile devices on a large amount of data, especially when the data are to be stored in a novel data management system supplied by several cloud providers with possibly different pricing models?   To solve this challenge, this project develops novel mobile cloud data management architectures and novel query processing algorithms that leverage mobile users' storage and computation power and take mobile users' mobility, disconnection, energy limitation, and cloud service providers' pricing models into consideration in order to improve query response time, while reducing the amount of money that must be paid to the cloud service providers. The research is evaluated using both real and synthetic datasets by means of prototyping.<br/><br/>The project is an international collaboration effort between the University of Oklahoma (OU) and Blaise Pascal University in France.  For research, both universities participate in the design, prototype and evaluation of the architectures and algorithms.  For education, via Skype, OU provides lectures on mobile and big data management for the Big Data Management course at Blaise Pascal University, while Blaise Pascal University provides lectures on cloud data management for the Advanced Database Management course at OU. The students in both courses participate in testing the constructed prototype as a part of their class assignments.  The project makes important impacts not only on research but also on education as it provides training for graduate and undergraduate students in the areas of critical national needs: cloud and mobile database management systems, big data and high-end computing. The developed architectures, algorithms, prototype, datasets and performance evaluation results are made available to the public at the Website: http://cs.ou.edu/~database."
"1350954","CAREER: Exploiting low-dimensional structure in data for more effective, efficient and interactive machine intelligence","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2014","07/13/2018","Christopher Rozell","GA","Georgia Tech Research Corporation","Continuing grant","Phillip Regalia","06/30/2019","$474,839.00","","crozell@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797","1045, 7495, 7936","$0.00","The rapid increase in sensor data is revolutionizing many areas of technology, defense, and scientific discovery.  Fortunately, despite data being high-dimensional, various aspects of the data can frequently be characterized as having low-dimensional geometric structure. This research project dramatically improves machine intelligence by exploiting this  geometric structure for more effective, efficient and interactive data analysis systems.  Complementary to these technical objectives, this project also aims to engage, recruit, and educate a diverse collection of students to STEM careers by  developing novel curricular and outreach materials that illustrate how mathematics can be used in information systems.  The potential benefits of this project are wide ranging in areas where data plays a fundamental role.<br/><br/>Improving machine intelligence requires understanding how to best exploit the underlying low-dimensional structure in data for a given type of task, and this project is guided by three research objectives toward this goal.  The first objective enhances machine effectiveness by exploiting the fact that multiple observations of the same phenomenon are often related by movement along a manifold, with a particular focus on the canonical computer vision problem of invariant object recognition.  The second objective seeks to improve computational efficiency by developing dimensionality reduction techniques for manifold-modeled data that preserves information about nonlinear feature-space mappings.  The third objective seeks to leverage interactivity to fully ""close the loop"" between between humans and machines while learning low-dimensional information from a human expert (an extension of the active learning paradigm).  The project also pursues two educational objectives, including developing curriculum modules for pre-college outreach and integrating neural systems content into the ECE curriculum to illustrate the connections between quantitative methods and intelligent systems."
"1617884","CIF: Small: Fundamental Analysis and Design of Repair-Efficient Cloud Storage Systems: a Linear Perspective","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/10/2016","Soheil Mohajer","MN","University of Minnesota-Twin Cities","Standard Grant","Phillip Regalia","05/31/2019","$499,618.00","","soheil@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7797","7923, 7935","$0.00","Cloud storage systems are increasingly being adopted by a wide spectrum of data intensive applications such as web search, cloud computing, distributed file sharing, and various types of data networks with high access demand, including social networks, health and medical databases, and banking system. A cloud storage system consists of several data centers and computer servers that are connected through Internet. While individual components of the system are subject to several types of failure, reliability, in the sense of data availability, and ability of regenerating failed storage nodes are two key features to design cloud storage systems. As the size of stored data and the number of users accessing the data continues to increase, designing robust, efficient, and scalable systems is a challenging and yet critically important problem. This research will develop a solid theoretical foundation for design and analysis of efficient cloud storage systems. <br/><br/>On the technical side, this research will focus on (1) studying the tradeoff between storage overhead and repair requirements in cloud storage systems, and (2) development of efficient coding schemes with guarantees on data recovery and failed node regeneration. While characterizing the general information-theoretic tradeoff appears to be intractable, progress can be made by limiting the problem to the class of practically relevant and low-complexity linear codes, and exploiting the duality between linear codes and multi-dimensional subspaces over finite fields. An optimization problem is formulated for this question, where an intelligent interpretation of the optimum solution provides insightful guidelines to devise code construction mechanisms. Lastly, a unified framework is proposed for code construction, to not only encompass several ad-hoc designs, but also generalize construction to all optimum operating points of the system."
"1533768","XPS: FULL: DSD: A Parallel Tensor Infrastructure (ParTI!) for Data Analysis","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/28/2015","Richard Vuduc","GA","Georgia Tech Research Corporation","Standard Grant","Aidong Zhang","08/31/2019","$750,000.00","Jimeng Sun","richie@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8283","","$0.00","This project concerns efficient parallel algorithms and software for<br/>emerging and future data analysis and mining applications, based on an<br/>emerging class of techniques known as tensor networks. Tensors, which<br/>are higher-dimensional generalizations of matrices, are finding<br/>applications in signal and image processing, computer vision,<br/>healthcare analytics, and neuroscience, to name just a few. Yet<br/>despite this demand, there is no comprehensive, high-performance<br/>software infrastructure targeting server systems that may have many<br/>parallel processors. Thus, the overarching research goal of this<br/>project is to design the first such infrastructure. The resulting<br/>prototype will be an open-source package, called the Parallel Tensor<br/>Infrastructure, or ParTI! The broader impact of the ParTI! project is<br/>to make the use of tensors, in a variety of data processing domains,<br/>much easier to do and more widespread.<br/><br/>The ParTI! project will focus specifically on algorithmic and software<br/>support for sparse tensors on single-node multi- and many-core<br/>accelerated platforms. The technical approach relies on a specific way<br/>of representing tensors, referred to as tensor networks.  A tensor<br/>network is an efficient approach for representing the structure of a<br/>high-order tensor or tensor factorization. It can used by the data<br/>analyst as a simple, high-level way to express the specific structure<br/>or relationships he or she seeks in the data that the tensor<br/>represents. However, a tensor network is not just a tool for the<br/>analyst; it is also an abstract intermediate form, from which it is<br/>possible to derive algorithms, express and manage parallelism, and<br/>semi-automatically generate tensor processing software. This insight,<br/>combined with well-known data layout and communication-avoiding<br/>parallelization techniques from high-performance sparse linear<br/>algebra, is what will enable a ParTI! for tensor-based data<br/>analysis. The project will show the utility of this approach by<br/>evaluating the ParTI! prototype on real data sets and systems, through<br/>collaborations with government research laboratory and industry<br/>partners.<br/><br/>For further information see the project web site at: parti-project.org"
"1453104","CAREER:  Scaling Source Separation to Big Audio Data","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2015","07/18/2018","Paris Smaragdis","IL","University of Illinois at Urbana-Champaign","Continuing grant","Phillip Regalia","07/31/2020","$414,525.00","","paris@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","1045, 7936","$0.00","The world we live in is composed out of mixed signals. It is practically impossible to easily obtain a clean recording of speech, music, environmental, mechanical, underwater, or biomedical sounds. This, in turn, complicates any further processing due to the presence of unwanted elements (e.g. background noise in speech recognition). We traditionally address this issue by using source separation and denoising methods that allow us to extract only a desired signal from a mixture. Unfortunately such methods do not scale at ?big data? levels, which means that most of the audio data we gather today remains at a state unsuitable for automatic content analysis or further processing. This project addresses the use of source separation methods when confronted with very large data sets. It considers use of modern data analysis methods to efficiently process large amounts of data, and also the effects of training on large signal corpora in order to improve source separation performance. The ultimate goals are to improve source separation performance by leveraging large signal collections, and to enable large-scale signal analysis by making modern source separation algorithms more efficient.<br/><br/>In order to enable processing at such large scales, this project takes advantage of recent developments in manifold structure analysis, deflation methods for spectral decompositions, hashing strategies, and quantization. Using a quantized manifold representation, large signal data sets can be approximated using a compact and efficiently accessible structure. Such representations can then be used as priors to a source separation algorithm and help guide it to extract signals that match them. Applying such models on large data to perform source separation is further accelerated by making use of a deflation method. Instead of performing the textbook (and computationally intensive) model matching process, this project uses a greedy approach to quickly extract target components while bypassing many unnecessary calculations. Finally, given the latest research on unifying multiple models of source separation, this project considers the application of such concepts to multiple data analysis models at once (such as HMM models, continuous dynamical systems, etc.), thereby becoming relevant to a wide range of signals and mixing situations."
"1650913","CAREER: Quick Detection for Streaming Data Over Dynamic Networks","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","07/01/2017","07/17/2018","Yao Xie","GA","Georgia Tech Research Corporation","Continuing grant","Phillip Regalia","06/30/2022","$280,913.00","","yao.xie@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","2878, 7797","1045, 7797, 7935, 7936","$0.00","Streaming data over networks have become ubiquitous in today?s world. A fundamental question is how to detect change-points (over time and space) from network streaming data as quickly as possible. This arises from a wide range of applications including geophysical exploration, social network surveillance, power network monitoring, multi-sensor systems for smart cities, as well as cyber security. Currently, not much is known about how to model these data, how to design an algorithm through a rigorous theoretical framework, how to implement algorithms efficiently online, and how fast we can detect the change with false alarms under control. The proposed research will address these fundamental theoretical and algorithmic questions. The efforts will lead not only to novel technological advances but also help with a much wider interdisciplinary audience in related fields.<br/> <br/>The overarching research objective of this project is to develop a modeling and algorithmic framework with theoretical performance guarantees for sequential change-point detection over networks. This bridges the fundamental gap between the statistical and computational approaches. Regarding modeling, the proposed work aims to capture complex dependence of network streaming data and exploit the structure of changes in the network setting. Regarding algorithm design, the goals include efficient online implementation, scalability to high dimensionality, and adaptiveness to data dynamics. Regarding theory, the goals are to establish optimality and to characterize the fundamental performance tradeoff between false alarms and detection delay. The proposed research will build on recent progress in modeling complex network data such as network point processes and correlation networks, algorithmic development such as sequential optimization, sketching, community detection, and subspace tracking, as well as theoretical advances in studying tail probabilities and extremal value theory."
"1719205","CIF:Small:Collaborative Research:Distributed Fog Computing for Non-Convex Big-Data Analytics","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","06/28/2017","Gesualdo Scutari","IN","Purdue University","Standard Grant","Phillip Regalia","08/31/2020","$270,000.00","","gscutari@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7923, 7935","$0.00","In our data-deluge era, massive chunks of information, perpetually collected by pervasive sensors, are communicated and processed by distributed computational architectures. To address emergent big-data computational issues, this project embarks on an ambitious multidisciplinary research effort that aims at advancing the state-of-the-art in-network/distributed big-data processing via a general algorithmic framework for data analytics over massively distributed data sets. The proposed algorithmic framework enables fully distributed and parallel big-data analytics, for a variety of heterogeneous data sets over a wide range of computational architectures. The developed research directions are beneficial also to domains far beyond big-data analytics, such as signal processing, machine learning, next-generation wireless communications, smart-city and smart-grid networks. Research results are distributed through archival publications, courses, undergraduate research opportunities, tutorials and conference presentations.<br/><br/>The developed scheme relies on a novel convexification/decomposition technique which accommodates a rich class of non-convex, unstructured and stochastic optimization tasks with non-separable objective functions. Algorithms are designed for settings where data are distributed across a large number of multi-core computational nodes, within a network of arbitrary topology with (possibly) time-varying and even random links. This new class of algorithms addresses shortcomings of current (non-parallel and non-distributed) convexification techniques via (i) full control of the degree of parallelism and distribution of the computation/signaling among processors/network nodes, and (ii) by offering a plethora of convex approximants, regularization terms, step-size rules, and communication protocols. Designed for time-varying or even random network topologies, the advocated framework demonstrates also another desirable attribute for distributed computations: resiliency to (random) network failures."
"1619189","CIF: Small: Effective bounds for distributed storage and data access","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/10/2016","Iwan Duursma","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","05/31/2019","$306,107.00","","duursma@math.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7935","$0.00","Today's data centers rely on advanced methods to efficiently store increasing volumes of data. To protect against loss of data, data is stored redundantly on multiple disks. At the large scale of clusters of thousands of disks, simple replication of data is inefficient and not an option. To address the challenge of efficient, reliable and secure storage of data at a large scale, the project uses various combinations of algebraic and combinatorial methods. New constructions are given for the efficient recovery of data in case of disk failure. New methods are introduced to optimize bounds for storage capacity. New secure schemes are developed to ensure that information can only be obtained from the combined data of multiple disks. The research uses a novel algebraic approach to fundamental aspects of data storage and data access. Undergraduate and graduate students will be involved, working on projects with both a theoretical and a computational component.<br/><br/>Algorithms for data storage encode and divide data over several disks. The encoding challenge is to optimize the allocation of storage space between primary data and repair data. The optimization is analyzed for the general case in the setting of entropy inequalities and for the linear case in the setting of rank inequalities for matroids. The main focus is on three aspects. 1) Outer bounds: Optimize the use of available storage space under various combinations of constraints. 2) Multiple-access: Use coordinated encoding of data from different sources to add error-correction without sacrificing storage capacity. 3) Small alphabets: Using a novel approach, analyze nontraditional coset schemes that are defined over smaller fields."
"1553086","CAREER: The optimal use of data","CCF","COMM & INFORMATION FOUNDATIONS","02/15/2016","04/24/2018","John Duchi","CA","Stanford University","Continuing grant","Phillip Regalia","01/31/2021","$290,023.00","","jduchi@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7797","1045, 7936","$0.00","Modern techniques for data gathering?arising from medicine and bioinformatics, internet applications such as web-search, physics and astronomy, mobile data gathering platforms?have yielded an explosion in the mass and diversity of data. Concurrently, statistics, decision theory, and machine learning have successfully laid a groundwork for answering questions about our world based on analysis of this data. As more information is collected, classical approaches for inference and learning are insufficient, as additional concerns arise?computational resources, privacy considerations, storage limitations, network communication constraints? outside of statistical accuracy. This prompts a basic question: how can multiple criteria be balanced while maintaining statistical performance?<br/><br/>To bring statistics and machine learning into closer contact with other desiderata, this research involves the development of procedures that trade between scarce resources in principled and optimal ways. Such trade-offs have been difficult to characterize, as current tools for providing fundamental limits (such as information theory in communication) do not connect disparate areas. Three concrete sub-areas serve as bases for this research. The investigators study the interplay of computing with learning, estimation, and optimization by connecting notions of computation?such as memory accesses or synchronization in distributed systems?to data analysis tasks. Second, the research investigates adaptive and robust procedures?and associated statistical costs?that will become more important given increasingly long-tailed and messy data. Thirdly, the investigators study privacy in estimation, using information and decision-theoretic tools to characterize the tensions between statistical accuracy and sensitive data disclosures. Combined, these lay the groundwork for a theory on the use of data in the face of constraints, along with a functional and practical understanding of procedures that balance scarce resources against statistical accuracy."
"1563710","CIF: NeTS: Medium: Collaborative Research: Unifying Data Synchronization","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/19/2018","Michael Mitzenmacher","MA","Harvard University","Continuing grant","Phillip Regalia","06/30/2020","$400,000.00","","michaelm@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7797","7924, 7935","$0.00","This project addresses the technical means of maintaining and sharing the huge amounts of diverse data that are being generated from the computing devices that increasingly envelop everyday lives, specifically by improving synchronization technology.  The goal is to synthesize various approaches that have been developed, often in an ad hoc manner and in various fields, into a general, holistic solution that includes benefits from new analyses and solutions.  The approach is fundamental in nature, and its effectiveness shows promise for impact on a diverse set of technologies, including mobile and cloud computing, digital currency, security, and biological sequencing.  Part of the work involves outreach to younger students, such as initial efforts at setting up a novel computing-based summer program for high school students.<br/><br/>The project focuses on four research thrusts.  The first involves an analysis, comparison, and improvement (where possible) of core point-to-point synchronization primitives from the existing literature.  The second thrust generalizes these primitives to higher-dimensional data, such as files, images, video, databases, and graphs.  The third research thrust involves developing approaches for synchronizing data from many synchronizing parties communicating through a variety of channels.  Finally, the project aims to implement a general data synchronization engine based on the research results, together with an evaluation of various applications."
"1563753","CIF: NeTS: Medium: Collaborative Research: Unifying Data Synchronization","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/19/2018","Ari Trachtenberg","MA","Trustees of Boston University","Continuing grant","Phillip Regalia","06/30/2020","$829,963.00","David Starobinski","trachten@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7797","7924, 7935, 9251","$0.00","This project addresses the technical means of maintaining and sharing the huge amounts of diverse data that are being generated from the computing devices that increasingly envelop everyday lives, specifically by improving synchronization technology.  The goal is to synthesize various approaches that have been developed, often in an ad hoc manner and in various fields, into a general, holistic solution that includes benefits from new analyses and solutions.  The approach is fundamental in nature, and its effectiveness shows promise for impact on a diverse set of technologies, including mobile and cloud computing, digital currency, security, and biological sequencing.  Part of the work involves outreach to younger students, such as initial efforts at setting up a novel computing-based summer program for high school students.<br/><br/>The project focuses on four research thrusts.  The first involves an analysis, comparison, and improvement (where possible) of core point-to-point synchronization primitives from the existing literature.  The second thrust generalizes these primitives to higher-dimensional data, such as files, images, video, databases, and graphs.  The third research thrust involves developing approaches for synchronizing data from many synchronizing parties communicating through a variety of channels.  Finally, the project aims to implement a general data synchronization engine based on the research results, together with an evaluation of various applications."
"1829525","CCF:Small:Collaborative Research: Taowu: A Heterogeneous Processing-in-Memory for High Performance Scientific Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","01/01/2018","02/27/2018","Jishen Zhao","CA","University of California-San Diego","Standard Grant","Almadena Y. Chtchelkanova","08/31/2020","$184,245.00","","jzhao@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7798","7923, 7942","$0.00","Many scientific computing applications in critical areas of research, such as nanotechnology, astrophysics, climate, bioinformatics, and high-energy physics, are becoming more data intensive than ever before. The volume of the data and the pressure on the runtime system capability of supporting data intensive operations substantially increases over the time. This project introduces and optimizes data processing capabilities within memory. The project provides a fundamental change to data management and program optimization, and brings promising performance and energy benefits. The project will significantly advance simulation capabilities of scientific applications, especially those with intensive data processing. <br/> <br/>The goal of the project is to enable high-performance, energy-efficient, and flexible processing-in-memory design, which is adaptive to the irregular, diverse, and changing behaviors among data intensive scientific applications. To achieve the goal, a heterogeneous processing-in-memory design, built up with fixed-function processing-in-memory and general programmable processing-in-memory, is introduced. The project explores a series of critical questions for building emerging processing-in-memory, including heterogeneous processing-in-memory architecture, processing-in-memory programming models, runtime design, and the implications of processing-in-memory on high performance scientific applications. The  project will significantly advance the knowledge to build processing-in-memory, and pave the way to integrate it into the existing and future systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1619303","SHF: Small: Automated Analysis and User Feedback on Data Movement Bottlenecks in Programs","CCF","SOFTWARE & HARDWARE FOUNDATION, EPSCoR Co-Funding","07/01/2016","06/27/2016","Jagannathan Ramanujam","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Almadena Y. Chtchelkanova","06/30/2019","$400,000.00","","jxr@ece.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7798, 9150","7923, 7942, 9150","$0.00","The cost of data movement through the memory hierarchy is very high in current computer systems, relative to the cost of performing arithmetic operations, both in terms of time and energy.  It is expected to become even more dominant in future computer systems. Therefore optimizing data access costs will become increasingly critical in the coming years. This has great impact on algorithm design and implementation on such systems.  A characterization of data access complexity is only known today for a few classes of algorithms, such as dense linear algebra, and has required the use of problem-specific analysis techniques.  This proposal presents novel ideas for developing automated tools and techniques to characterize the inherent data access complexity of arbitrary algorithms. An automatable general approach for data access complexity of programs will have significant impact on deriving highly effective algorithms and their implementations, understanding the impact of future architectures on algorithm design, and on advances in compilers aimed at improving data access costs of programs.<br/><br/>The proposal develops novel ideas to address a fundamental problem of increasing importance -- developing tools and techniques for characterization of the inherent data access complexity of algorithms. The work develops the following: (i) the first unifying approach for developing both upper and lower bounds for the inherent data access complexity of a CDAG under a common underlying theoretical model; (ii) an approach to remove a fundamental current limitation in the use of the popular reuse distance analysis metric; (iii) a scalable and versatile dynamic analysis infrastructure of use to application developers, compiler writers and architecture designers."
"1814888","SHF: Small: Communication-Efficient Distributed Algorithms for Machine Learning","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2018","07/17/2018","Maryam Mehri Dehnavi","NJ","Rutgers University New Brunswick","Standard Grant","Almadena Y. Chtchelkanova","06/30/2021","$464,412.00","MERT GURBUZBALABAN, Maryam Mehri Dehnavi","maryam.mehri@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7798","7923, 7942, 9102","$0.00","Advances in sensing and processing technologies, communication capabilities and smart devices have enabled deployment of systems where a massive amount of data is collected and then processed in order to make decisions. The platforms that process this vast amount of data differ depending on the application. Among these, data centers are powerful platforms with vast computational resources where the collected data can be distributed over multiple processors that are all connected through a high-bandwidth network. Data can also be generated and processed in multi-agent systems which are made up of multiple interacting computational units (such as smart devices connected through wireless internet) with limited resources in terms of storage, power, computation, and communication capabilities. Data communication costs, which include the bandwidth and latency, often dominate floating point operation costs thus the performance of optimization algorithms when operating on large data sets is bounded by data communication for both multi-agent systems and data centers. This project proposes novel communication-efficient methods for a class of distributed optimization problems arising in large-scale data analysis and machine learning. The methods and techniques developed under the scope of this project contribute to the efficiency, practical performance and to the mathematical foundations of distributed optimization algorithms. The project is also developing a high-performance software framework that allows the dissemination of efficient domain-specific software and benchmarks.<br/><br/>The project has three goals: the first goal is to improve the communication efficiency of existing algorithms for solving distributed optimization problems in the context of multi-agent systems, through a distributed algorithm for improving the total number of communications required in consensus iterations. The approach is based on leveraging the notion of the effective resistance of a link to identify bottleneck edges for communication purposes, and modifying the classical consensus averaging by taking effective resistances into account. The second goal is to develop communication-avoiding algorithms for data centers, through a framework that allows for reduction in communication by a tunable amount while keeping the arithmetic costs and bandwidth costs the same for a number of applications and existing algorithms. The third goal is to improve communication for hybrid systems which interpolate between multi-agents systems and data centers in terms of communication structure, using a framework that generates algorithm- and architecture-aware codes for reducing communication over these hybrid platforms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1719160","SHF:SMALL: MECAR: Memory-Centric Architecture to Bridge the Gap Between Computing and Memory","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2017","07/07/2017","Yuan Xie","CA","University of California-Santa Barbara","Standard Grant","Yuanyuan Yang","06/30/2020","$450,000.00","","yuanxie@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7798","7923, 7941","$0.00","Traditional computer systems usually follow the so-called classic Von Neumann architecture, with separated processing units (such as CPUs and GPUs) to do computing and memory units for data storage.The increasing gap between the computing of processor and the memory has created the memory wall problem in which the memory subsystem is becomingthe bottleneck of the entire computing system.As technology scales, data movement between the processing units (PUs) and the memory is becoming one of the most critical performance and energy bottlenecks in various computer systems, ranging from cloud servers to end-user devices.As we enter the era of big data, many emerging data-intensive workloads become pervasive and mandate very high bandwidth and heavy data movement between the computing units and the memory.The fundamental goal of this project is to advance the trend of bridging the gap between computing and memory, with an application-driven approach.<br/><br/>By leveraging the PI's prior extensive research on 3D-stacked memory and non-volatile memory architecture, the PI proposes to focus on (1) designing memory-centric processing unit (PU) architecture with massive GB on-chip/on-package memory integrated with computing units; (2) investigating new processing-in-memory(PIM) memory architecture designs with both DRAM and emerging NVM; (3) and co-design and co-optimization of both memory-centric PU architecture and NDC/PIM memory architecture, with the emerging data-intensive applications such as neural computing and graph analytics as application driver to guide the architecture optimization.The success of this researchwill have enormous economic and social benefits as broader impact. The research willprovide the design guidelines for enabling future computing systems beyond thestate-of-the-art, ranging from high performance exascale computing to low power mobile systems. Consequently, it will enhance nearly every digitaldevice available today from consumer to enterprise electronics. It canalso spawn new applications involving the computation on the exascaleof data, e.g. data mining, machine learning, bio-informatics, etc. It is expected that thisproject will serve as a catalyst to accelerate the adoption of data-intensive and memory-centric technologies in future computer systems and applicationsfrom architecture and system design perspectives.The PI has extensive industrial tieswithsummer internships planned, which will be invaluable for broadening the knowledge and skills of the students.The PI will also strive to educate a broad audience on the emerging technologies through regular and online classes. Publication/lecture notes will be released on public websites to promote the broader dissemination of scientific knowledge."
"1526162","SHF: Small: Designing Expandable and Cost-Effective Server-Centric Interconnects for Data Centers","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","11/28/2017","Yuanyuan Yang","NY","SUNY at Stony Brook","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$460,000.00","","yuanyuan.yang@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7798","7923, 7941","$0.00","In modern data centers, the performance of data center networks (DCNs) plays a critical role. Currently, DCNs are mainly divided into two categories: switch-centric networks and server-centric networks. In a switch-centric network, switches are responsible for a variety of tasks such as routing and addressing, while servers are only used to send and receive packets in the network. In a server-centric network, the computational intensive tasks like routing are put on the servers, which act not only as end hosts, but also as relay nodes for each other. A significant advantage of server-centric networks is that network hardware cost can be drastically reduced, as inexpensive commodity switches are sufficient given that most complex network tasks have been shifted to servers where computation resources are abundant. Moreover, since servers are much more programmable than switches, server-centric networks can accelerate the process of network innovation. Motivated by the importance and opportunities of deploying server-centric networks in DCNs, this research systematically investigates the fundamental and challenging issues towards building cost-efficient server-centric DCNs with guaranteed performance.<br/><br/><br/>The objective of this research is to design novel, extendable and cost-effective server-centric interconnects and associated routing algorithms for data centers. This research explores the unique novel features and techniques in data centers to build cost-efficient interconnects with dual-port servers or any fixed number of NIC ports. More specifically, the research focuses on following closely coupled issues: (1) construct cost-efficient DCN interconnects using commercial off-the-shelf (COTS) switches and dual-port servers that are available in current markets; (2) efficient routing algorithms for unicast, broadcast and multicast that can sufficiently take advantage of the novel interconnects; (3) employ network virtualization technology to offer a more flexible solution to both industry and academic research; (4) construct cost-efficient DCN interconnects using servers with any fixed number of NIC ports that meet the trend of future servers; (5) conduct a comprehensive performance evaluation through extensive simulations and implementation of proposed schemes in a realistic network prototype. The proposed research combines theoretical analysis, algorithm design, network optimization, simulation and prototyping techniques to provide a comprehensive working solution that enables high performance next generation DCNs. This research hopes to impact fundamental design principles of server-centric DCNs. The outcome of this research has the potential to boost the performance of DCNs while keeping the cost low, and facilitate numerous cloud computing applications currently hosted in data centers. A project goal is to train graduate students and promote the participation of female engineering students. The important findings of this project are to be disseminated to the research community by way of conferences, journals and web site access."
"1717731","SHF: Small: A Versatile, Adaptive Network Architecture for Combating Traffic Heterogeneity in Data Centers","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2017","07/07/2017","Yuanyuan Yang","NY","SUNY at Stony Brook","Standard Grant","Sankar Basu","06/30/2020","$450,000.00","","yuanyuan.yang@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7798","7923, 7941, 9102","$0.00","Driven by technology advances, massive data centers consisting of tens or even hundreds of thousands servers have been built as infrastructures by large online service providers, in which the performance of data center networks plays a critical role. Traffic in data center networks (DCNs) generally exhibits tremendous heterogeneous characteristics, at different locations in the network, among various applications, between distinct companys implementations, and from time to time. On the other hand, today's data center networks still lack sufficient flexibility to handle such huge heterogeneity of traffic in an efficient way. Meanwhile, as virtualization techniques undergo a profound development, cloud applications do not directly run on physical hardware or operating systems. Instead, they are assigned on a simulated operating system layer which is virtualized from a substrate network. Thus, a single infrastructure provider can support numerous virtual machines/networks for different users while keeping each user with a vision that the entire infrastructure is uniquely occupied by the user. To meet the challenges, this research designs a versatile network architecture oriented to the ever-increasing performance demands and heterogeneous traffic characteristics of today's data centers. The architecture provides an adaptive full-stack paradigm which includes interconnect topologies, addressing and routing schemes, and virtualization algorithms. <br/><br/>More specifically, the research focuses on several closely coupled issues: (1) design a novel, cost-efficient DCN architecture that can dynamically and systematically handle traffic heterogeneity in data centers; (2) design the corresponding routing algorithms for the architecture under unicast and multicast traffic models; (3) develop efficient network virtualization algorithms in the architecture, including virtual network embedding, virtual network function placement and virtualization over geographically distributed DCNs; (4) conduct a comprehensive performance evaluation through extensive simulations and implementation of the schemes in a realistic network prototype. The research combines theoretical analysis, algorithm design, network optimization, simulation and prototyping techniques to provide a comprehensive working solution that enables high performance next generation DCNs. This research is expected have a profound impact on fundamental design principles of future DCNs. The outcome of this research will not only greatly boost the performance of DCN flexibility and energy efficiency while keeping low cost, but also facilitate numerous cloud computing applications currently hosted in data centers. As cloud computing is penetrating into all aspects of the society, this research will have a broader impact on the society and help change the world."
"1814298","CIF: Small: Collaborative Research: Leveraging Data Popularity in Distributed Storage Systems via Constrained Design Theory","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/19/2018","Charles Colbourn","AZ","Arizona State University","Standard Grant","Phillip Regalia","09/30/2021","$249,475.00","","Charles.Colbourn@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7797","7923, 7935, 9102","$0.00","Recent years have witnessed a surge of large-scale distributed storage system implementations and accompanying data analyses and coding methods that enable their reliable, secure and low-delay operation. Nevertheless, in many system studies, important data demand (popularity) features which have a strong bearing on system access control, private information retrieval and computational complexity have been largely overlooked. This can be attributed in part to the fact that most cloud storage facilities employ different storage platforms for hot and cold data, thereby partly addressing problems associated with variable data demands. But even within the hot and cold data categories there exist significant variations in data popularity that create many nontrivial system design challenges.<br/><br/>To address these issues, the proposed research program aims to develop a new family of mathematical objects termed constrained designs and Steiner systems in particular. Designs represent finite collections of subsets of a ground set whose elements satisfy predefined symmetry constraints with respect to set intersections and arrangements. Elements of a design are associated with data chunks, while subsets of elements represent data chunks to be stored on the same disk or server; given their simplicity and rich mathematical structure, designs have been used with great success in many practical distributed storage system platforms. In the presence of nonuniform demands for objects and data files, intersection constraints alone fail to ensure underlying implementation constraints. Consequently, elements have to be equipped with nonnegative popularity values, and the underlying combinatorial designs modified to satisfy additional algebraic and frequency constraints enforced by data popularity values. This new model leads to a unique collection of challenging mathematical problems regarding constructions of weighted and labeled combinatorial designs. Particular problems to be considered include developing designs for balanced server access, private information retrieval in the presence of popularity side information and transversal designs for labeled batch codes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815467","SHF: Small: Parallel Algorithms and Architectures Enabling Extreme-scale Graph Analytics for Biocomputing Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/22/2018","Anantharaman Kalyanaraman","WA","Washington State University","Standard Grant","Almadena Y. Chtchelkanova","06/30/2021","$509,676.00","Partha Pande","ananth@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7798","7923, 7942, 9251","$0.00","Graph-theoretic modeling of biological data has a rich history of delivering foundational scientific knowledge and breakthrough discoveries. As data sets continue to explode both in size and complexity, the combination of graph analytics and scalable (parallel) computing has a critical role to play in shaping the future of data-driven discovery in many biological applications including national health. Yet, implementing such graph computations at scale continues to be a daunting challenge despite the growing availability of high-end parallel architectures. The goal of this project is to design efficient parallel algorithms and architectures that would enable extreme scaling of graph computations in biological applications. Other project activities integrate and leverage upon the research outcomes of this project, while preparing the next generation scientific workforce. The project is also leading to the development of curricular modules in parallel algorithms and applications, and related hardware design, and conference tutorials for broader outreach.<br/><br/>The project is focused on developing core techniques in two problem spaces: i) performing graph analytics at scale for a host of generic graph operations that find prevalent use-cases in biological applications and also in many other data-driven domains; and ii) performing graph construction at scale using biological raw data.  Taken together, the proposed effort embodies a systematic and holistic approach to enhance the reach and impact of parallel computing on large-scale graph applications and, in the process, usher in new generic data-driven design techniques and paradigms into parallel applications design. While the emphasis will be on biological applications, as a space for drawing scientific motivation and to demonstrate utility through validation and testing, it is expected that many of the developed techniques will extend beyond this realm and impact a broader class of applications that need extreme-scale processing of graphs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813370","SHF: Small: Energy-Efficient and Reliable Communication with Silicon Photonics for Terascale Datacenters-on-Chip","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","06/12/2018","Sudeep Pasricha","CO","Colorado State University","Standard Grant","Sankar Basu","09/30/2021","$450,000.00","Mahdi Nikdast","sudeep@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7798","7923, 7945","$0.00","Electronic processing chips are at the heart of the digital intelligence that has been the driving force for groundbreaking technological advances across the medical, consumer, industrial, networking, aerospace, automotive, and defense application domains. In recent years, there has been a growing trend in these application domains of massive data generation and consumption, which puts immense pressure on the networks at the chip-scale that must now transfer very high volumes of data in much shorter durations of time than ever before. Traditional electrical networks at the chip-scale are breaking down under this pressure, which is catastrophic as it prevents the development of the next generation of high-performance digital intelligence that can transform society and improve lives. Fortunately, silicon photonics has emerged as an exciting technological panacea that can replace slow electrical links with much faster light-speed transfers. While communication over long optical fibers (e.g., several miles) is quite common today, the nano-integration of silicon photonics technology with electronic chips is a new paradigm and presents enormous challenges that have yet to be addressed. This project will involve transformative research to overcome these fundamental challenges, and pave the way for realizing future photonics-based electronic chips that are miniature in size, but with the same computing power as a large datacenter computing facility today. Close collaborations with industrial partners at HP Enterprise and Lumerical will aid in the rapid adoption of the outcomes. Moreover, by exposing K-12, undergraduate, and graduate students to the diverse aspects of emerging technologies, devices, circuits, architectures, and algorithms, the project will contribute to an agile high-tech workforce that will maintain continued US leadership in technological innovation.<br/><br/>The principal contribution of this project will be a new framework that will push the boundaries of achieving ultra-low energy and high reliability data transfers with silicon photonics at the chip-scale. This framework consists of three major thrusts that are closely related and will be addressed in a highly integrated manner: (1) Characterize behavior of silicon photonics devices and explore new device configurations based on device fabrication at Applied NanoTools Inc., to enable the selection of energy-efficient and low-cost devices; (2) Design new circuits with silicon photonics devices to overcome noise, increase bandwidth, and reduce power dissipation during communication; and (3) Create new silicon photonics-based network architectures and tools for their optimization, to realize ultra-low energy and fault-resilience solutions for transferring data between processing cores at the chip-scale. Beyond these three thrusts, the framework will exploit cross-layer insights across the device, circuit, and architecture layers, and devise optimizations that span across two or more of these layers. The innovations at the individual layers together with optimizations across layers will achieve more aggressive energy savings and higher reliability chip-scale communication than what is possible today. This outcome will usher in a new era of ultra-high performance computing where light speed data transfers within electronic chips can work together with optics-based data transfers external to the chip, to overcome communication energy and performance bottlenecks at all levels. Such a development will enable lower-cost supercomputing and cloud datacenters, making computing more affordable for scientists and more ubiquitous in everyday life, to transform our lives in innumerable ways.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816965","CIF: NSF-BSF: Small: Collaborative Research: Characterization and Mitigation of Noise in a Live DNA Storage Channel","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/08/2018","Jehoshua Bruck","CA","California Institute of Technology","Standard Grant","Phillip Regalia","09/30/2021","$187,250.00","","bruck@paradise.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7797","014Z, 7923, 7935","$0.00","Deoxyribonucleic acid (DNA) is emerging as a potential solution to growing data storage needs, due to recent advances in DNA synthesis and sequencing. Compared to silicon-based storage, DNA offers higher density and data longevity. Data-bearing DNA can be stored in non-biological environments (in vitro) or be embedded into the DNA of living organisms such as bacteria or plants (in vivo). In vivo DNA data storage benefits from a natural protective shell and a reliable and cost-efficient way to copy data. Furthermore, the compatibility of in vivo storage with living cells creates a potential for advances in synthetic-biology algorithms that require a mechanism for data storage. The goal of this project is to develop and demonstrate an in vivo DNA data storage system that is resilient to errors arising during sequencing and synthesis and from mutations, while achieving the maximum possible data density.<br/><br/>To achieve its goal, the project relies on the following research thrusts: 1) Characterization of the live DNA channel and 2) Error mitigation and correction. Errors in DNA storage are known to be context-dependent, i.e., they depend on the local structure of the sequence. To enable error control, a mathematical model for the channel of live DNA storage will be formulated and the dependence of error rates on sequence context will be characterized. This project will take advantage of new advances in gene-editing technologies, in particular the CRISPR/Cas system, to characterize the channel through experiments. The second thrust aims to overcome the limitations of the existing error-control techniques, which either do not provide adequate error protection or are highly redundant. The project will develop semiconstrained-coding techniques for limiting the frequency of patterns with high error rates and high-rate error-correction schemes for combating errors. The semiconstrained and error-correction methods will be adapted to the live DNA channel and then implemented to evaluate their performance in practice and to demonstrate their advantages.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1160904","Synthesis and Analysis of Heap Data Structures","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2012","03/15/2018","Alexander Aiken","CA","Stanford University","Continuing grant","Anindya Banerjee","04/30/2019","$1,012,139.00","","aiken@cs.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7798","7924, 7943","$0.00","Almost all software today is developed in two stages. First, a human writes a program and subsequently various automated tools process that program to produce the actual computer code that is the software application. Over time, researchers have found ways to shift more of the burden of developing software from the programmer to the automated tools, which has gradually made software development more and more productive. One area of software development that has not changed for a couple of decades is the way in which the structure of data is programmed. The organization of data is still described in relatively low-level terms as links between individual objects in memory, which is too low-level for the tools to reason successfully about. As a result, programmers must spend significant time hand-coding and tuning their data structures, and there are significant missed opportunities for improvements to program performance, correctness, and security.<br/><br/>We propose to try a new approach to this problem, consisting of two parts: we will synthesize data structures from high-level relational specifications.  We will also use inference techniques to automatically analyze the program?s use of the relations. In a relational style of programming explicit pointers are eliminated, removing one of the biggest impediments to automatic reasoning. We believe recent advances in the analysis of control flow, destructive updates, and low-level indexing operations make it feasible to capture most other aspects of programs.<br/><br/>If successful, programs, including concurrent programs, will be written at a higher level and be more easily retargeted to new situations because the data representations are not fixed in advance, and tools will be able to do a more reliable, efficient, and scalable job of optimizing resources and verifying properties of programs. As part of the project, we will seek to demonstrate these benefits by conducting studies of our techniques applied to realistic challenge applications."
"1513816","AF: Medium: Collaborative Research: Algorithmic Foundations for Trajectory Collection Analysis","CCF","ALGORITHMIC FOUNDATIONS","06/01/2015","05/30/2017","Pankaj Agarwal","NC","Duke University","Continuing grant","Tracy J. Kimbrel","05/31/2019","$807,999.00","Carlo Tomasi","pankaj@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796","7924, 7929, 9251","$0.00","This project engages experts in computational geometry, optimization, and computer vision from Duke and Stanford to develop a theoretical and algorithmic framework for analyzing large collections of trajectory data from sensors or simulations. Trajectories are functions from a time interval to a multi-dimensional space that arise in the description of any system that evolves over time.<br/><br/>Trajectory data is being recorded or inferred from hundreds of millions of sensors nowadays, from traffic monitoring systems and GPS sensors on cell phones to cameras in surveillance systems or those embedded in smart phones, in helmets of soldiers in the field, or in medical devices, as well as from scientific experiments and simulations, such as molecular dynamics computations in biology. Algorithms for trajectory-data analysis can lead to video retrieval systems, activity recognition, facility monitoring and surveillance, medical investigation, traffic navigation aids, military analysis and deployment tools, entertainment, and much more. Many of these application fields intersect areas of national security, as well as domains of broader societal benefit.<br/><br/>This project pursues a transformational approach that combines the geometry of individual trajectories with the information that an entire collection of trajectories provides about its members. Emphasis is on simple and fast algorithms that scale well with size and dimension, can handle uncertainty in the data, and accommodate streams of noisy and non-uniformly sampled measurements.<br/><br/>The investigators have a long track record of collaboration with applied scientists in many disciplines, and will continue to  transfer their new research to these scientific fields through joint publications and research seminars, also in collaboration with several industrial partners. This project will heavily rely on the participation of graduate and undergraduate students. Participating undergraduates will supplement their education with directed projects, software development, and field studies. Data sets used and acquired for this project will be made available to the community through online repositories. Software developed will also be made publicly available.<br/><br/>Understanding trajectory data sets, and extracting meaningful information from them, entails many computational challenges. Part of the problem has to do with the huge scale of the available data, which is constantly growing, but there are several others as well. Trajectory data sets are marred by sensing uncertainty and heterogeneity in their quality, format, and temporal support. At the same time, individual trajectories can have complex shapes, and even small nuances can make big differences in their semantics.<br/><br/>A  major tension in understanding trajectory data is thus between the need to capture the fine details of individual trajectories and the ability to exploit the wisdom of the collection, i.e., to take advantage of the information embedded in a large collection of trajectories but missing in any individual trajectory. This emphasis on the wisdom of the collection is one of the main themes of the project, and leads to a multitude of important problems in computational geometry, combinatorial and numerical optimization, and computer vision. Another theme of  the project is to learn and exploit both continuous and discrete modes of variability in trajectory data.<br/><br/>Deterministic and probabilistic representations will be developed to summarize collections of trajectories that capture commonalities and differences between them, and efficient algorithms will be designed to compute these representations. Based on these summaries, methods will be developed to estimate a trajectory from a given collection, compare trajectories to each other in the context of a collection, and retrieve trajectories from a collection in response to a query. Trajectory collections will also be used to infer information about the environment and the mobile entities involved in these motions."
"1813049","AF: Small: Robust and Secure Learning","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","05/24/2018","Gregory Valiant","CA","Stanford University","Standard Grant","Rahul Shah","09/30/2021","$500,000.00","","gvaliant@cs.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","7923, 7926","$0.00","Machine learning (ML) systems play an increasingly central role in society--from ubiquitous speech recognition systems, to navigation systems, product recommendation systems, and deployed learning systems across manufacturing, industry, and healthcare.  The near future, with complex computer vision systems, self-driving cars, and ML driven medical care and patient monitoring, promises a nearly pervasive presence of ML systems in our society.  Despite promising performance in idealized settings, current ML systems are often brittle--they are sensitive to slight changes in the input data, and often have weaknesses that can be easily exploited by a malicious adversary.  Resolving these current shortcomings is a necessary step in ensuring the stability, safety, and security of a society that relies heavily on machine learning.  The central goal of this project is to develop learning algorithms that are robust, and secure.  These go beyond the traditional goal of developing learning algorithms that achieve high accuracy, and address the broad need for reliability and safety in critical deployed systems.  As an extension of the research component of the project, the investigator will continue education and outreach efforts.  These include disseminating the research publications and code produced by this project, continuing to develop new courses and teaching materials on data-centric algorithms, machine learning, and related topics, and organizing a semi-annual forum for the exchange of ideas between industry and academia.    <br/><br/>The research core of this project addresses the lack of robustness of current learning and optimization algorithms. This lack of robustness takes the following two distinct forms. First, current algorithms are sensitive to changes in even a very small portion of the data-set on which they are trained.  Second, even when trained on legitimate data, the learned models are often susceptible to ""adversarial examples"" in the sense that for the vast majority of data points--even data points in the training set--a small adversarial perturbation of the data point in question will result in the model outputting a completely different label.  The presence of these two types of fragility in current learning systems raises the possibility of vulnerabilities to two new sorts of security threats: 1) the threat that a portion of the training data is either extremely biased and unreliable, or worse--that it has been generated by an adversary whose goal is to mislead the machine learning system, and 2) the threat that deployed machine learning systems can be tricked via minute but carefully generated adversarial modifications in their test points--modifications that are essentially invisible to humans. The project seeks to address these two critical weaknesses of current systems, by : 1) developing new algorithms that are robust to the presence of significant fractions of arbitrary -- including adversarial -- data, which can be applied to a number of fundamental estimation, machine learning, and optimization tasks, and 2) developing a rigorous understanding of why certain training algorithms yield models that are inherently vulnerable to adversarial examples, and develop tools for reducing this vulnerability.  Additionally, this project investigates the computational, and information theoretic aspects of robust and secure learning, including developing an understanding of any potential trade-offs, for example between the amount of training data and computation time, and robustness or security of the resulting trained model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1421823","SHF: Small: Introducing Next Generation I/O Accelerator","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2014","07/09/2014","Qing Yang","RI","University of Rhode Island","Standard Grant","Yuanyuan Yang","07/31/2019","$479,998.00","","qyang@ele.uri.edu","RESEARCH OFFICE","KINGSTON","RI","028811967","4018742635","CSE","7798","7923, 7941, 9150","$0.00","Big data applications demand high speed, reliable, and energy  <br/>efficient data storage systems. Traditional storage architectures have  <br/>fundamental limitations because of legacy systems that have centered  <br/>on spinning hard disk drives. With rapid advances in nonvolatile  <br/>memory technologies such as NAND-gate flash, phase change memory, Memristor, and  <br/>magnetic RAM, a great opportunity arises for revolutionizing storage  <br/>architectures. The objective of this research is to start a paradigm  <br/>shift in storage architecture to meet the increasing demand of big  <br/>data applications. It is envisioned that future storage systems will  <br/>have machine intelligence that learns, analyzes, predicts, and  <br/>controls the system at runtime dynamically. A novel accelerator  <br/>architecture is introduced with machine intelligence to enable high  <br/>speed processing of storage data operations that are critical to high  <br/>performance computing in general and big data computing in particular.<br/><br/>   The newly introduced I/O accelerator, residing either in a  <br/>many-core CPU chip or on a storage controller board, enables  <br/>sufficiently accurate predictions for effective optimization of  <br/>storage I/Os. With new architecture features, the proposed I/O  <br/>accelerator can carry out complicated I/O tasks in the speed  <br/>comparable to the emerging nonvolatile memories, which is critical to  <br/>I/O performance because it no longer operates in milliseconds as  <br/>spinning disks do. The project will explore and implement the I/O  <br/>accelerator that can effectively deal with the complexity and high  <br/>dimensionality of factors related to diverse storage technologies, a  <br/>large variation of application workloads, different  <br/>reliability/availability requirements, and power consumptions of  <br/>various storage components. The result is a new heterogeneous storage  <br/>architecture that is optimized for future computing infrastructure.  <br/>With the accelerator as an enabler, comprehensive methodology will be  <br/>investigated that proactively learns system behavior to anticipate  <br/>long-term trends and to respond quickly to fast changing I/O events.  <br/>The new architecture is believed to be the first of the kind providing  <br/>dynamic optimizations by means of 1) intelligent data placements and  <br/>replacements across heterogeneous devices, 2) optimal resource  <br/>allocation and provisioning to applications' workloads, 3) effective  <br/>data deduplication based on content locality, and 4) smart policy  <br/>decision on data protection and recovery adaptive to different data  <br/>types. Furthermore, the new accelerator enables fast in-situ data  <br/>analytics in active storage systems. <br/><br/>This research project is expected  to have the following broader impacts: <br/>1) In today's cloud computing and big data  <br/>applications, servers generate a large amount of I/Os that can take  <br/>full advantage of the new storage architecture. 2) The new accelerator  <br/>can be incorporated into many core CPUs as a specialized core for  <br/>future heterogeneous processors. 3) The new storage architecture will  <br/>speed up the adoption of emerging storage class memories. 4) The new  <br/>methodology will stimulate more research in applying machine learning  <br/>to storage systems. 5) The new CPU-and-data centric Computer Engineering curriculum will train  <br/>both graduate and undergraduate students for real world needs. 6) The  <br/>outreach program will continue the success stories of prior NSF  <br/>projects to help the economic development of the state of Rhode Island and the  <br/>nation."
"1815633","SHF: Small:Verifying Complex Concurrent Data Structures with Flow Interfaces","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","05/14/2018","Thomas Wies","NY","New York University","Standard Grant","Nina Amla","09/30/2021","$498,496.00","","tw47@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7798","7923, 8206","$0.00","Among the most critical components of today's cyber-infrastructure are concurrent data structures that coordinate work between sub-computations. These software components are notoriously difficult to implement correctly. While formal verification tools can guarantee the reliability of software, there remains a gap between the highly complex concurrent data structures found in real systems and the relatively simple ones amenable to today's tools. This project aims to close this gap. The project maintains a repository of software tools and benchmarks that is publicly available under open source licenses. The educational objectives include involvement of undergraduate students and the dissemination of course materials created for this project. These activities are supplemented by the investigator's continued involvement in outreach programs for high-school students.<br/> <br/>Concurrent separation logic has helped to simplify formal correctness proofs for concurrent data structures. However, a recurring problem in such proofs is that data structure abstractions that work well for sequential software are much harder to reason about in a concurrent setting. The project takes a radically different approach to data structure abstraction that leads to a new semantic model of separation logic and can describe the complex concurrent data structures found in practice. The obtained abstractions admit proof rules that generalize over a wide variety of data structures. This gives rise to novel proof modularization techniques where abstract algorithm templates are proved correct once and for all and can then be refined to concrete data structure implementations, significantly reducing the proof effort compared to the state of the art.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815983","SHF: Small: Rigorous Synthesis and Verification of Decisions Using Data-Driven Models","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","05/14/2018","Sriram Sankaranarayanan","CO","University of Colorado at Boulder","Standard Grant","Nina Amla","05/31/2021","$499,570.00","David Bortz","srirams@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7798","7923, 8206","$0.00","This project develops data-driven mathematical models to synthesize and verify control/decision-support algorithms for autonomous systems that operate under a variety of environments, involving various users. The research investigates the inference of models from data and reasoning over these models to provide guarantees for the overall system.  The project focuses on artificial pancreas systems that deliver insulin to people with type-1 diabetes. Such systems must adapt to the specific physiological and behavioral patterns for each individual user. To this end, the project infers data-driven models for specific individuals to verify and synthesize personalized control/decision support algorithms. Additionally, on-board data collected from unmanned aerial vehicles under windy conditions are used to check for collision avoidance in real-time.  The project reaches out to high school students through workshops on data analysis, and to the general public through curated wikipedia articles that explain the principles underlying artificial pancreas systems<br/><br/>The project investigates systematic inference of data-driven models with uncertainty quantification and model selection. The resulting models are incorporated into verification approaches such as model checking and invariant synthesis to provide probabilistic correctness guarantees for key temporal properties. The evaluation is carried out using clinical trial datasets for artificial pancreas systems and on-board flight data for ground and unmanned aerial vehicles. The project formally defines, identifies and eliminates harmful biases in the data, to assure the integrity of the verification and synthesis results.  The project develop assurance cases that support the use of data-driven models in the formal verification and synthesis process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1629201","XPS: EXPL: Exploring the Design Space of Augmented Memory Controllers with Native Support for In-Memory Data Storage","CCF","Exploiting Parallel&Scalabilty","07/01/2016","06/28/2016","Tong Zhang","NY","Rensselaer Polytechnic Institute","Standard Grant","Yuanyuan Yang","06/30/2019","$299,621.00","","tzhang@ecse.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8283","","$0.00","As the cornerstone of the global information technology infrastructure, large-scale parallel processing platforms host an ever-increasing amount of real-time in-memory computing applications (e.g., data/graph analytics, transaction processing, machine learning, and business intelligence). As a result, large-scale distributed in-memory data storage has become a very important component in large-scale parallel processing platforms. Nevertheless, conventional realization of in-memory data storage tends to occupy a large amount of memory capacity and consume substantial CPU cycles. This makes in-memory data storage subject to a significant cost overhead in terms of both memory and CPU resources. How well this cost challenge can be addressed largely determines the overall system performance and efficiency of future large-scale parallel processing platforms. This project will have significant impact on the research community and the industry, while providing interdisciplinary training of graduate and undergraduate students, and draw broad participation of students of different levels and backgrounds in collaborative research and education.<br/><br/>This project proposes to improve the cost effectiveness of in-memory data storage by enhancing the function and data processing capability of the hardware memory controller. In particular, the in-memory filesystem and memory controller will explicitly cooperate together across the software/hardware layers and share the responsibility for optimizing the implementation of in-memory data storage. Such a cross-layer design framework enables the use of memory footprint reduction techniques to reduce memory resource cost without incurring CPU overhead. Moreover, the memory controller will integrate customized hardware engines that can carry out certain storage-oriented data processing tasks. Those customized storage data processing engines in the memory controller can be leveraged to directly reduce the memory and CPU resources overhead in the realization of in-memory data storage. An FPGA-based platform will be implemented to carry out experiments to further empirically validate the feasibility and potential effectiveness of the developed design solutions."
"1755880","CRII: SHF: Bespoke Data Representation Synthesis via Contextual Data Refinement","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2018","01/05/2018","Benjamin Delaware","IN","Purdue University","Standard Grant","Anindya Banerjee","07/31/2020","$163,212.00","","bendy@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","7943, 7944, 8228","$0.00","Nearly every modern programming language provides some mechanism for hiding the implementation details of reusable components behind some abstract interface. This interface acts as a contract, enforced by the language, that benefits both the developers and clients of such components: protecting the developers? design decisions from clients and enabling clients to safely swap in different implementations of the same interface. Recent advances in program synthesis have shown how custom implementations can be automatically built from high-level specifications of a client?s requirements, exploiting this contract to ensure that synthesized components satisfy the desired requirements. Existing approaches to language-enforced abstraction approaches can be too restrictive in this setting, however, as they require the synthesized implementation to work for any client. This disallows any implementations whose correctness are dependent on a particular client?s usage. The goal of this project is to relax this condition, enabling the synthesis of custom implementations that are tailored to a particular client while still providing the same strong abstraction guarantees that programmers expect from their programming languages. The intellectual merits are the development of a refined notion of modularity in programming languages, advancing the state of the art in the synthesis of correct, performant code. The project's broader significance and importance are the development of an approach that allows programmers to program against high-level abstractions without paying a performance penalty.<br/><br/>The project advances the state of the art in both the theoretical foundations of data abstraction and the development of verified software. The vehicle for the work's theoretical contributions is a formalization of a core calculus for data refinement. This calculus is used to reformulate the well-established notion of data refinement for abstract data types (ADTs) to incorporate information about a specific client's usage of an interface. A key component is the development of the metatheory proofs establishing that the standard property of representation independence under data refinement is preserved. This approach is used to improve the existing Fiat deductive synthesis framework, enabling clients to derive verified ADT implementations that are tailored to their particular usage. The augmented system is evaluated via the synthesis of custom implementations of the popular Haskell bytestring library for two open-source Haskell programs, using an existing derivation of a performant bytestring implementation in Fiat as a starting point.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749981","CAREER: Coded Caching for Wireless Content Delivery Networks: Challenges and Opportunities","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2018","01/05/2018","Soheil Mohajer","MN","University of Minnesota-Twin Cities","Continuing grant","Phillip Regalia","06/30/2023","$192,160.00","","soheil@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7797","1045, 7935, 9251","$0.00","The continuously increasing demand for broadband data has produced overwhelming network traffic and, despite steady improvements in wireless communication technology, data rates continue to fall short of the exponential growth in demand. As such, opportunistic transmission strategies based on network characteristics, demand profile, and content type, can help meet expectations. The major contributor to this traffic explosion is broadband video, characterized as being repeatedly requested by multiple users and having highly variant temporal traffic. Such properties provide an opportunity for caching the data at local storage units closer to users during the off-peak hours of the network, to reduce network traffic at peak hours. Recent developments in coded caching offer a promising solution for high data rates. This research will study challenges and benefits of employing caching in practical communication networks, and pursue a solid theoretical foundation for adopting caching as a universal resource in data delivery networks. <br/><br/>This project will target the fundamental theory and practical aspects of caching in data delivery networks by addressing several practical concerns. While caching gain degrades in real networks with time-varying channels, asynchronous and delay sensitive requests, and without a central coordinator, coding techniques will be used to improve the gain of caching. In addition, a rate-distortion theoretic framework will be developed to characterize the fundamental trade-off between cache size, delivery rate, and reconstruction quality, alongside efficient coding schemes to achieve this tradeoff. Following the success of multi-antenna communication systems, the interaction between caching gain and spatial diversity will be studied. A successful completion of this part will lead to an optimum resource (cache size, power, and rate) allocation as well as transmission scheduling in non-homogeneous Multi-Input-Multi-Output (MIMO) networks. Finally, software to simulate caching techniques for a wide range of networks and applications, and supporting both research and education, will be developed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718432","AF:Small:Extreme Streaming Problems","CCF","ALGORITHMIC FOUNDATIONS","07/15/2017","07/14/2017","Shanmugavelayu Muthukrishnan","NJ","Rutgers University New Brunswick","Standard Grant","Rahul Shah","06/30/2020","$499,088.00","","muthu@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7796","7923, 7926","$0.00","The need for modern streaming systems to collect and analyze human activities is enormous, in businesses, government, academia, and society. Businesses can improve their operations and production; governments can improve the participation and satisfaction of citizens; society at large can be more sustainable or safe; etc. However, systems that collect and analyze such streams have enormous challenges of scale. At the highest level, this proposal combines a research, education and outreach plan to address these challenges. <br/><br/>This research focusses on developing new algorithmic methods and theoretical understanding of modern streaming problems.  There is an extensive theory of streaming algorithms for single streams, or to a limited extent to distributed streams, for one (or few) high cardinality dimensional data and simple frequency based analyses. But there are large gaps in creating streaming algorithms for ""extreme"" needs in modern data streaming applications, where the dimension of data that is collected is large, multiple streams of collected in distributed vantage points, there is a need to find anomalies in high dimensional spaces, and analyses that are needed are sophisticated including machine learning and other real time decision making tasks.  This research develops the algorithmic theory of these extreme streaming problems. In particular, the project develops the algorithmic foundations for using large scale distributed streaming systems and tradeoff quality, certainty, CPU, memory and communication needed to do extreme, streaming, sophisticated analyses. Since modern streaming systems power businesses and deal with behavioral data on users, this work has broad societal impact. The project significantly improves the state of the art in algorithms for modern streaming systems. By providing new, rich algorithmic approaches, the project inspires practitioners in academia and industry to conceive more impactful applications, which are infeasible given the current algorithmic tools.<br/><br/>The research program both enables and benefits from an education and outreach program that enhances curriculum, fosters training women and underrepresented minorities. To enable technology transfer the project involves practitioners in streaming systems, for field-testing the methods whenever possible. All publishable results are disseminated in respected academic journals, conferences, and workshops. All code and data sets developed in this work are made openly available to the community via the MassDAL site that already has code that is used by the community for classical streaming problems.<br/><br/>Classical streaming algorithms use space sublinear, typically polylogarithmic in the input parameters. When extended to distributed systems, often the focus is on sublinear communication. The research program, here, builds on this algorithmic theory of past 15 years and addresses the modern, extreme needs of streaming applications. The project extends the theory to: many dimensions with large attributes, using far fewer resources in memory, computing and communication; emerging, pipeline models of streaming; more sophisticated analyses from local privacy to deep learning type vector embeddings; etc.  The research program addresses fundamental problems. In more detail:<br/><br/>(a)  Extant streaming algorithms work for one or few dimensions of data of high cardinality. Modern streaming systems collect logs of human activity and have 100s of dimensions, 10s or more of them have very high cardinality. Can one identify the key problems for these domains and develop an algorithmic theory? The PI has identified an effective approach based on graphical modeling of the relationship between the dimensions, and believes this nugget can yield an effective theory.<br/>(b) Extant streaming algorithms use polylogarithmic words of memory per analysis when they are considered successful (and lower bounds point to problems for which sublinear space is not sufficient). Modern streaming systems run several orders of magnitude of such analyses, for example, one analysis for each of the millions of users. The project has identified an approach of ""frugal"" streaming, where algorithms use a small constant number of words, and develops a theory of frugal streaming algorithms, and their limitations.<br/>(c) Modern data stream systems allow pipelining, with the stream (modifiable or not) passing through stages, either at individual sites, or across the sites. The project abstracts and develops algorithmic theory of streaming problems with pipelined streaming systems. Preliminary results indicate that this allows algorithms that use time sublinear in the sublinear space used by the solutions, and there is a rich class of path problems that can be solved in these models which are impossible in classical streaming.<br/>(d) Modern systems need sophisticated streaming analyses. For example, streaming systems that collect usage data from users need private methods, and combining local differential privacy with streaming methods is an exciting direction; as another example, systems that analyze usage data might rely on embedding data into vectors with semantics, like word2vec and related deep learning methods. These methods need to work with polylogarithmic space for streaming. As another example, rich classes of graph problems are solvable in property testing framework with sublinear samples, can such classes be solved in streaming models too? The project highlights specific research challenges involved in developing streaming algorithms, and develops algorithms with provable performance guarantees on the tradeoff of resources used, approximation ratio, and probability of success. The project empirically evaluates them when possible.<br/><br/>One cannot take known statements of problems and hope to solve them using streaming algorithms. One needs to modify the problems a bit to be amenable to streaming. In classical streaming, ""heavy hitters"" and ""few terms"" properties helped achieve that. In similar vein, the project identifies certain natural phenomena which helps formulate versions of problems for which extreme streaming solutions can be developed. Contours of this are already seen in using graphical models that limit interactions between dimensions to circumvent high dimensional high cardinality cases, or reusing counters in frugal streaming or sampling the sketch data structure in privacy problems and pipelined streaming, or using the randomness in stream order. This may eventually lead to algorithmic and empirical insights. Overall vision of the project is to provide a principled perspective for design and analysis of streaming algorithms with extreme needs."
"1439057","XPS: FULL: DSD: End-to-end Acceleration of Genomic Workflows on Emerging Heterogeneous Supercomputers","CCF","Exploiting Parallel&Scalabilty","09/01/2014","03/15/2019","Kamesh Madduri","PA","Pennsylvania State Univ University Park","Standard Grant","Anindya Banerjee","02/29/2020","$849,984.00","Padma Raghavan, Mahmut Kandemir, Paul Medvedev","madduri@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8283","","$0.00","The proposed research harnesses parallelism to accelerate the<br/>pervasive bioinformatics workflow of detecting genetic variations.<br/>This workflow determines the genetic variants present in an<br/>individual, given DNA sequencing data. The variant detection workflow<br/>is an integral part of current genomic data analysis, and several<br/>studies have linked genetic variants to diseases. Typical instances<br/>of this workflow currently take several hours to multiple days to<br/>complete with state-of-the-art software, and current algorithms and<br/>software are unable to exploit and benefit from even modest levels of<br/>hardware parallelism. Most prior approaches to parallelization and<br/>performance tuning of genomic data analysis pipelines have targeted<br/>computation, I/O, or network data transfer bottlenecks in isolation,<br/>and consequently, are limited in the overall performance improvement<br/>they can achieve. This project targets end-to-end acceleration<br/>methodologies and uses emerging heterogeneous supercomputers to<br/>reduce workflow time-to-completion.<br/><br/>The project focuses on holistic methodologies to accelerate multiple<br/>components within the genetic variant detection workflow. It explores<br/>lightweight data reorganizations at multiple granularities to enhance<br/>locality, investigates compute-, communication-, and I/O task<br/>cotuning, locality-aware load-balancing, and coordinated resource<br/>partitioning to exploit high-performance computing platforms. A key<br/>goal of the proposed research is to design domain-specific<br/>optimizations targeting the massive parallelism and scalability<br/>potential of current heterogeneous supercomputers, so that the<br/>developed techniques can be easily transferred and applied to <br/>dedicated academic cluster and commercial computational environments.<br/>Outreach efforts target undergraduate students through recruiting<br/>workshops and attract them to interdisciplinary graduate programs.<br/>Curriculum development activities emphasize cross-layer parallelism.<br/><br/>For further information, see project web site at <br/>http://sites.psu.edu/XPSGenomics"
"1717515","SHF:  Small:  Collaborative Research:  PEGASUS:  ProgrEss GuAranteeS for Universal tranSactions","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2017","11/29/2018","Damian Dechev","FL","University of Central Florida","Standard Grant","Anindya Banerjee","07/31/2020","$232,000.00","","dechev@cs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7798","7923, 7943, 9150, 9251","$0.00","Software scalability and reliability on multi-core systems is a crucial and urgent issue that needs to be addressed. Transactional processing is a promising programming model for developing parallel applications across many domains, including scientific and application software, that simplifies the development of correct concurrent software. This project will develop transactional techniques that overcome many current limitations, and it will explore the optimization of these techniques on systems with support for transactions. The intellectual merits are the development of novel techniques for constructing transactional data structures on systems with and without hardware support for transactions. The project will create a first approach for transactional processing with progress guarantees. The project's broader significance and importance are to make accessible to the industry a set of prototype data structures and programming techniques that furthers the reliability and performance of software on current and future multi-core systems. The project will also develop critical human resources in systems programming.  <br/> <br/>The project will lead to a number of fundamental and practical outcomes. Transactions have been recognized as a promising alternative to lock-based systems. However, transactions as currently implemented in software or hardware have different drawbacks, with the absence of a progress guarantee being the most fundamental. To overcome this problem, this work further develops the notion of transactional data structures that are based on lock-free techniques, thereby guaranteeing progress. The project will also extend the applicability of commutable transactions to various data structure types including commonly used linked and contiguous memory data structures. The project harnesses transactional hardware to accelerate the execution of common cases and demonstrates the applicability by prototyping  a set of important containers. Finally, this project will evaluate the outcome against"
"1350914","CAREER: Privacy-Guaranteed Distributed Interactions in Critical Infrastructure Networks","CCF","COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace","01/15/2014","02/06/2018","Lalitha Sankar","AZ","Arizona State University","Continuing grant","Phillip Regalia","12/31/2019","$463,000.00","","lalithasankar@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7797, 8060","1045, 7935, 7937, 9251","$0.00","Information sharing between operators (agents) in critical infrastructure systems such as the Smart Grid is fundamental to reliable and sustained operation. The contention, however, between sharing data for system stability and reliability (utility) and withholding data  for competitive advantage (privacy) has stymied data sharing in such systems, sometimes with catastrophic consequences. This motivates a data sharing framework that addresses the competitive interests and information leakage concerns of agents and enables timely and controlled information exchange.<br/><br/>This research develops a foundational approach to privacy-guaranteed information sharing among distributed self-interested agents in complex systems using information theory and game theory. This multidisciplinary project focuses on four mutually related challenges in multi-agent network abstractions of the Smart Grid: (1) characterization of the fundamental limits of distributed interaction with privacy constraints; (2) operational and practical significance of information-theoretic privacy measures; (3) formalizing the cost of privacy and the role of trust and repeated interactions for cooperation; and a direct application of these results via (4) distributed algorithms and protocols for privacy-guaranteed data sharing in the Smart Grid. The research has the broader implication of enabling information sharing in a variety of complex networks with strict privacy requirements including electronic healthcare and water distribution systems, and also engenders academic and industry collaborations in power systems. This research project incorporates carefully tailored outreach efforts including privacy awareness for middle- and high-school students, and active engagement of undergraduate and graduate students, especially females, in research."
"1526205","SHF: Small:  Green Parallel Language Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","06/15/2015","Yu David Liu","NY","SUNY at Binghamton","Standard Grant","Anindya Banerjee","06/30/2019","$484,547.00","","davidl@cs.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","7798","7923, 7943","$0.00","Title: SHF:Small:Green Parallel Language Systems<br/><br/>The broad spectrum of modern parallel computing platforms are faced with the growing concern about energy consumption. This project explores the frontier at the converging point of parallel computing and energy-efficient computing. The intellectual merits of the proposal are to study energy efficiency of parallel computing through novel programming language and compiler technologies, and produce a set of energy-optimizing compilers, language runtimes, and programming models for parallel systems. With parallel platforms pervasively deployed in the modern computing world, the project's broader significance and importance are as diverse as reducing the operational cost of data centers, improving system reliability of mission-critical systems, extending the battery life of handheld devices, and promoting the environmental sustainability of our society. <br/><br/>The unifying theme of the project is to exploit program structures and run-time semantic information for optimizing energy consumption of multi-threaded programs on parallel architectures. Concretely, the project consists of three interconnected directions. (i) energy-efficient thread management, that is, optimizing energy consumption through coordinating threads based on their static and dynamic dependencies. (ii) energy-efficient data management, that is, promoting energy proportionality through analyzing data-intensive programs in a setting where data rates, data processing schedules, and CPU frequencies may interplay in complex fashions. (iii) energy-efficient synchronization management, that is, harvesting different thread synchronization patterns to achieve pattern-specific energy optimization. Together, the three directions constitute a unique and important dimension of energy optimization for parallel computing platforms."
"1733798","AitF: Collaborative Research: Topological Algorithms for 3D/4D Cardiac Images: Understanding Complex and Dynamic Structures","CCF","Algorithms in the Field","09/15/2017","09/05/2017","Yusu Wang","OH","Ohio State University","Standard Grant","Rahul Shah","08/31/2020","$273,032.00","","yusu@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7239","9102","$0.00","The interiors of ventricles of a human heart are spanned by a fine net of muscle fibers that are difficult to resolve, even in high resolution CT images.  An accurate account of these structures, however, could improve diagnosis of cardiac disease, evaluation of cardiac function, assessment of stroke risk, and simulation of cardiac blood flow. Topology is the branch of abstract mathematics that deals with connections; this project uses the theory of persistent homology to identify crucial topological handles that can be useful for accurate reconstruction and analysis of the complex cardiac dynamics from these CT images. The outcome of the project will not only advance our understanding of cardiac function, but also generate novel computational topology methods that are more efficient and effective for practical applications. This project not only bridges the gap between the theory of computational topology and the practical problem of cardiac image analysis, but also trains the next generation of researchers and educators to do so by a carefully integrated education plan. The PIs will engage undergraduate students, high school students, women and other underrepresented students in their proposed research.<br/><br/>The goal of this project is to develop a topological approach to unveil the intrinsic structures from complex and dynamic 3D/4D cardiac data, and furthermore, to provide principled tools to quantitatively analyze these structures. The PIs will create new computational topology methodologies and algorithms to extract rich information from the intrinsic structure of cardiac data. They will develop novel methodologies to extract localized topological features and to track them based on their spatial and temporal coherence. They also plan to design new algorithms to untangle ambiguous and uncertain situations for tracking structures through time sequence data. The resulting techniques and software will be validated on cardiac CT data to produce quantitative assessments of accuracy and to characterize the advantages and limitations of these approaches. Domain experts will validate the quality of the approaches via scientific hypotheses and data exploration. The methods to be developed are general and will impact other scientific fields where intrinsic complex and dynamic structures exist."
"1749241","CIF: Small: Collaborative Research: Inference of Information Measures on Large Alphabets: Fundamental Limits, Fast Algorithims, and Applications","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2016","08/31/2017","Yihong Wu","CT","Yale University","Standard Grant","Phillip Regalia","08/31/2019","$206,012.00","","yihong.wu@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7797","7923, 7935","$0.00","A key task in information theory is to characterize fundamental performance limits in compression, communication, and more general operational problems involving the storage, transmission and processing of information. Such characterizations are usually in terms of information measures, among the most fundamental of which are the Shannon entropy and the mutual information. In addition to their prominent operational roles in the traditional realms of information theory, information measures have found numerous applications in many statistical modeling and machine learning tasks. Various modern data-analytic applications deal with data sets naturally viewed as samples from a probability distribution over a large domain. Due to the typically large alphabet size and resource constraints, the practitioner contends with the difficulty of undersampling in applications ranging from corpus linguistics to neuroscience. One of the main goals of this project is the development of a general theory based on a new set of mathematical tools that will facilitate the construction and analysis of optimal estimation of information measures on large alphabets. The other major facet of this project is the incorporation of the new theoretical methodologies into machine learning algorithms, thereby significantly impacting current real-world learning practices.  Successful completion of this project will result in enabling technologies and practical schemes - in applications ranging from analysis of neural response data to learning graphical models - that are provably much closer to attaining the fundamental performance limits than existing ones. The findings of this project will enrich existing big data-analytic curricula.  A new course dedicated to high-dimensional statistical inference that addresses estimation for large-alphabet data in depth will be created and offered. Workshops on the themes and findings of this project will be organized and held at Stanford and UIUC. <br/><br/>A comprehensive approximation-theoretic approach to estimating functionals of distributions on large alphabets will be developed via computationally efficient procedures based on best polynomial approximation, with provable essential optimality guarantees. Rooted in the high-dimensional statistics literature, our key observation is that while estimating the distribution itself requires the sample size to scale linearly with the alphabet size, it is possible to accurately estimate functionals of the distribution, such as entropy or mutual information, with sub-linear sample complexity. This requires going beyond the conventional wisdom by developing more sophisticated approaches than maximal likelihood (?plug-in?) estimation. The other major facet of this project is translating the new theoretical methodologies into highly scalable and efficient machine learning algorithms, thereby significantly impacting current real-world learning practices and significantly boosting the performance in several of the most prevalent machine learning applications, such as learning graphical models, that rely on mutual information estimation."
"1642280","EAGER: Developing scalable benchmark mini-apps for graph engine comparison","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","05/16/2017","Peter Kogge","IN","University of Notre Dame","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$307,369.00","Douglas Thain, Nitesh Chawla","kogge@cse.nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7798","7916, 7942, 9251","$0.00","The last decade has seen the growth of extremely large, unstructured, and dynamic data sets, loosely termed Big Data. However, there is a growing desire to extract not just specific properties of collections of such facts, but also relationships between the underlying entities in that data. Examples come from a broad swatch of modern life: bioinformatics, financial, recommendation systems, cyber and national security, and social networks. Graphs have emerged as a valuable and productive paradigm for expressing such problems, where a graph is a collection of a set of objects (vertices) where some pairs of objects are connected by links (edges) that represent some relation between the two. <br/>In the last decade there has been an explosion in support for graphs, with widely differing execution models and targeted applicability. Although numerous graph benchmarks have been proposed, only one has had a rigorous accumulation of performance data from multiple platforms (www.graph500.org). Computation is over a whole static graph, whereas the real world sees applications where update data is streaming into large persistent graphs, and very many small targeted queries may be in progress at once.<br/>Given the expected productivity increase of using a graph programming paradigm over conventional programming, especially for parallel systems, it is of growing importance to have common mini-apps that can be used for cross-paradigm comparisons. Also, given the continued increase in graph sizes, it is important to understand how the underlying graph engines scale both in the size and type of the target graphs and in the amount and mix of parallelism and concurrency they can support.<br/>This project addresses this need. In collaboration with commercial and government research labs, the primary objective is on defining a set of mini-apps that reflect complex real-world applications more sophisticated than today's simple benchmarks, converting these mini-apps to the existing major graph packages, and then running them on a wide range of parallel systems. <br/>The wider impact can be significant. Identification of relevant mini-apps and how they perform across different systems will provide insight into both how to write more complete graph applications in more scalable ways, and which aspects of which programming systems and platforms are best suited. It is also expected that not all mini-apps will be expressible in all the current paradigms, providing insight to the developers of those paradigms on expressibility issues. Given the relative infancy of such graph packages such insight now can radically improve their applicability to real applications in the future."
"1763674","SHF: Medium: Collaborative Research: Program Analytics: Using Trace Data for Localization, Explanation and Synthesis","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2018","05/03/2018","Westley Weimer","MI","University of Michigan Ann Arbor","Continuing grant","Sol J. Greenspan","05/31/2022","$150,382.00","","weimerw@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","7798, 7924, 7944","$0.00","Formal program analyses have long held out the promise of lowering the cost of<br/>creating, maintaining and evolving programs. However, many crucial analysis<br/>tasks, such as localizing the sources of errors or suggesting code repairs, are<br/>inherently ambiguous: there is no unique right answer. This ambiguity<br/>fundamentally restricts the wider adoption of formal tools by limiting users to<br/>those with enough expertise to effectively use such ambiguous results. The key<br/>insight is that data-driven machine-learning approaches, which have proved<br/>successful in other domains, can be applied to the data traces generated by<br/>programmers as they carry out development tasks. This research addresses the<br/>challenge of ambiguity by extending classical program analysis into modern<br/>program analytics. This extension enhances classical symbolic methods with<br/>modern data-driven approaches to collectively learn from fine-grained traces of<br/>programmers interacting with compilers or analysis tools to iteratively modify<br/>and fix software.<br/><br/>The research systematically develops program analytics by pursuing research<br/>along two dimensions: language domains and programming tasks. First, it studies<br/>different language domains, from dynamically typed languages (Python), to<br/>statically typed functional languages with contract systems (Haskell), to<br/>interactive proof assistants (Coq). Second, it targets different programming<br/>tasks, from localizing errors like null-dereferences, assertions or other<br/>dynamic type failures, to static type errors, to completing or fixing code to<br/>eliminate an error or to obtain some desired functionality. This approach takes<br/>advantage of a suite of new approaches that harness recent advances in<br/>statistical machine learning and fine-grained, domain specific programmer<br/>interactions. These advantages allow the research to address the fundamental<br/>problem of ambiguity in classical program analysis. This has potential to<br/>transform software development by yielding a new generation of program analysis<br/>tools that are efficient, applicable, and automatically customizable (e.g., to a<br/>particular company, project, group or even individual).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817037","SHF:Small: Tensor-Based Algorithm and Hardware Co-Optimization for Neural Network Architecture","CCF","SPECIAL PROJECTS - CCF","10/01/2018","06/19/2018","Zheng Zhang","CA","University of California-Santa Barbara","Standard Grant","Sankar Basu","09/30/2021","$499,998.00","Yuan Xie","zhengzhang@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","2878","075Z, 7923, 7945","$0.00","Machine learning plays an important role in our daily life, including medical data analysis, finance, autonomous driving and computer vision. Many popular machine learning models are both data-intensive and computationally expensive. In order to address this challenge, algorithm and architecture co-design and co-optimizations are required to achieve better performance and energy efficiency. This project aims to develop a more powerful learning architecture by simultaneously optimizing the neural network algorithm and its hardware implementation. The project will have a broad impact. The AI applications will gain a significant performance boost if the computational and storage cost of its algorithm can be reduced significantly.  It will improve many applications such as data mining, medical imaging analysis, computational biology, and financial analysis.  The project will greatly enrich the undergraduate and graduate course curriculum and attract graduate and undergraduate students to participate in this project and related workshops. Finally, this project will develop new AI, VLSI and computer architecture workforce with solid background in several areas including computational math, machine learning, and hardware design.<br/><br/>Tensors are a generalization of vectors and matrices, and they are promising tools to represent and numerically process high-dimensional data arrays. Leveraging the high effectiveness of tensor computation in big-data analysis, this project will investigate three specific topics towards designing high-performance and energy-efficient machine learning hardware. First, theoretically sound and novel tensor numerical algorithms will be developed to significantly reduce the training and inference cost of deep learning. Second, the algorithm framework will be optimized on existing hardware (e.g., GPU and FPGA) to achieve better performance, and to examine the main challenges when running on hardware platforms. Finally, emerging design technologies (e.g., 3-D process-in-memory) will be investigated to design specific hardware libraries to perform fundamental tensor computation and to further boost the performance and energy efficiency of the whole machine learning architecture.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750716","CAREER: Faster and Smaller Sketches for Bigger Data","CCF","ALGORITHMIC FOUNDATIONS","02/01/2018","01/08/2018","Huy Nguyen","MA","Northeastern University","Continuing grant","Rahul Shah","01/31/2023","$99,067.00","","hu.nguyen@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7796","1045, 7926","$0.00","The advent of new sensing and tracking technologies and expansive use of social networks detailing every walk of life have generated enormous new datasets. The difficulty in dealing with new datasets arises from not only the sheer volume but also the speed required for the analysis and the complex and heterogeneous nature of the data. Underlying these challenges is the need for suitable representations of the data that facilitate efficient computation and are sufficiently compact for storage and communication. This project aims to address fundamental gaps in our understanding of these representations (so-called sketches) and develops both new data representations and new algorithms for massive datasets in a holistic fashion. The project builds on techniques from a wide variety of areas including mathematical analysis, information theory, coding theory, combinatorics, and optimization, and enriches the deep connections among them. Undergraduate and graduate students will be trained and equipped with technical tools to work in these areas. The PI and the students involved in the project will also distill new findings into general audience surveys and give talks at workshops in different technical areas for broadest possible dissemination of information.<br/><br/>This project aims to study sketching algorithms by focusing on three main thrusts:(a) Study time complexity of sketches in streaming algorithms in both upper and lower bounds.(b) Develop new forms of sketches for distributed environments. The project focuses on sketching for submodular functions, a popular model for machine learning, computer vision, economics, etc. Problems in these applications are modeled as submodular maximization subject to various types of constraints. (c) Study space complexity of linear sketches in sparse recovery with respect to different recovery guarantees."
"1617665","SHF:Small: Reducing Test Time and Improving Diagnosis for Increasingly Dense ICs","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/10/2016","Nur Touba","TX","University of Texas at Austin","Standard Grant","Sankar Basu","05/31/2020","$350,000.00","","touba@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7923, 7945","$0.00","The amount of data required to test and diagnose integrated circuits continues to grow dramatically as technology scales, not only because of larger designs, but also the need for additional tests to target new defects. The density of ICs continues to grow faster than the test data bandwidth between automatic test equipment (ATE) and the chip-under-test, which is constrained by the number of channels on the tester and pins on the chip. This project involves developing new approaches for compression and extraction of data that are fundamentally different from existing techniques and capable of more efficiently reducing the amount of data and significantly improving test time and diagnosis accuracy.  The project will provide advanced training to both graduate and undergraduate students, including those from underrepresented groups, in the latest design and test technologies.<br/><br/><br/>Several new research directions for improving compression and extraction of test data will be investigated in this project.  A fundamentally new approach for extracting information from linear signatures using symbolic canceling will be studied for finding error locations to aid in diagnosis.  By combining information from the signature together with structural information from the circuit, much greater diagnostic precision is possible. Improvements in test compression will be pursued through new concepts including (i) bandwidth sharing between test stimulus decompression and output response compaction, (ii) combining linear and non-linear encoding, (iii) low fan-out response compaction architectures, and (iv) dynamic extraction from linear compactors.  The new techniques and architectures developed in this project will be implemented and their performance evaluated."
"1618706","SHF: Small: Accelerating Graph Traversal on GPUs","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/10/2016","H. Howie Huang","DC","George Washington University","Standard Grant","Almadena Chtchelkanova","05/31/2020","$449,929.00","","howie@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","7798","7923, 7942","$0.00","Graph algorithms are crucial to a wide range of big data applications from social network analysis, biological network analysis and simulation, to cybersecurity. Analyzing such graphs has important practical usages, e.g., providing targeted recommendations for e-commerce, as well as identifying person of interest and suspicious behaviors in social networks. Graph-based approaches rely heavily on a number of traversal methods, e.g., breadth-first search serves as one of the most significant building blocks for many graph algorithms. This project advances the state-of-the-art in graph traversal by achieving exceptional performance on both a single graphics processing unit (GPU) and large GPU-based supercomputers. New techniques developed in this project enable many graph applications to run efficiently in data centers, bringing enormous benefits to a number of scientific domains. New educational opportunities for undergraduate and graduate students are also enabled by this project.<br/><br/>Although GPUs provide both massive parallelism and high memory bandwidth ideal for running graph algorithms, unleashing their full power to achieve high-performance, highly scalable graph traversal remains extremely challenging. The main obstacles are highly unbalanced workload and irregular data access, both inherent to data-driven graph algorithms. To address this challenge, this project develops a set of novel techniques that not only remove several performance bottlenecks, but also optimize data access, communication, and task management for graph traversal on GPUs."
"1733843","AitF: Collaborative Research: Topological Algorithms for 3D/4D Cardiac Images: Understanding Complex and Dynamic Structures","CCF","Algorithms in the Field","09/15/2017","09/05/2017","Dimitris Metaxas","NJ","Rutgers University New Brunswick","Standard Grant","Rahul Shah","08/31/2020","$265,981.00","","dnm@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7239","","$0.00","The interiors of ventricles of a human heart are spanned by a fine net of muscle fibers that are difficult to resolve, even in high resolution CT images.  An accurate account of these structures, however, could improve diagnosis of cardiac disease, evaluation of cardiac function, assessment of stroke risk, and simulation of cardiac blood flow. Topology is the branch of abstract mathematics that deals with connections; this project uses the theory of persistent homology to identify crucial topological handles that can be useful for accurate reconstruction and analysis of the complex cardiac dynamics from these CT images. The outcome of the project will not only advance our understanding of cardiac function, but also generate novel computational topology methods that are more efficient and effective for practical applications. This project not only bridges the gap between the theory of computational topology and the practical problem of cardiac image analysis, but also trains the next generation of researchers and educators to do so by a carefully integrated education plan. The PIs will engage undergraduate students, high school students, women and other underrepresented students in their proposed research.<br/><br/>The goal of this project is to develop a topological approach to unveil the intrinsic structures from complex and dynamic 3D/4D cardiac data, and furthermore, to provide principled tools to quantitatively analyze these structures. The PIs will create new computational topology methodologies and algorithms to extract rich information from the intrinsic structure of cardiac data. They will develop novel methodologies to extract localized topological features and to track them based on their spatial and temporal coherence. They also plan to design new algorithms to untangle ambiguous and uncertain situations for tracking structures through time sequence data. The resulting techniques and software will be validated on cardiac CT data to produce quantitative assessments of accuracy and to characterize the advantages and limitations of these approaches. Domain experts will validate the quality of the approaches via scientific hypotheses and data exploration. The methods to be developed are general and will impact other scientific fields where intrinsic complex and dynamic structures exist."
"1629559","XPS: FULL: Broad-Purpose, Aggressively Asynchronous and Theoretically Sound Parallel Large-scale Machine Learning","CCF","Exploiting Parallel&Scalabilty","09/01/2016","02/06/2018","Eric Xing","PA","Carnegie-Mellon University","Standard Grant","Aidong Zhang","08/31/2020","$625,379.00","Garth Gibson","epxing@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8283","","$0.00","Many artificial intelligence (AI) applications such as image understanding and natural language processing rely on Machine Learning (ML) methods to automatically extract valuable knowledge from Big Data (Big Learning). Efficient ML requires not only expertise in advanced mathematical models and algorithms, but also experiences with large computer clusters where issues such as machine failures, memory/network bottlenecks, inter-machine latencies must be properly handled through complex system programming. Such demand on ""dual skill"" often prevents democratizing large-scale AI to wide user communities, and necessitates a new framework that bridges ML and the distributed computing environment of a cluster with a single-machine-like simple interface, allowing ML practitioners to be agnostic about the backend details, and able to quickly prototype or deploy ML programs on clusters. Solutions to such a need remain rare. In this project the PIs develop a new general purpose framework for ML on distributed systems, offering highly efficient and theoretically justified protocols (e.g. communication, scheduling, and partitioning functions) to orchestrate a heterogeneous computer cluster to become programmable and act like a single big computer, and execute distributed ML programs correctly and at a speed orders of magnitude faster than current systems such as Hadoop and Spark. With this new framework, data scientists will be able to conduct ML analytics with complex models on massive data without the need for dedicated engineering and infrastructure teams, allowing Big Learning more readily accessible to society.<br/> <br/>Specifically, over a four year span, the proposed research focuses on three technical aims: (1) Building a System Framework for Big Learning, by developing a new architecture that supports both data- and model-parallel execution of large ML programs, using intelligent scheduler, parameter server, and consistency controller that are configurable to provide flexible options for model/data parallelization, synchronization schemes, load balance, fault tolerance, and multi-instance tenancy; (2) Building a Multi-Level-Abstraction Programming Interface, which supports easy parallel programming of both basic and advanced ML algorithms for large-scale applications; and (3)Conducting theoretical analysis of distributed ML algorithms on the proposed system, based on unique insights such as block consistency and error-tolerance under bounded synchronism. The goal is to develop a system framework to achieve general, automatic, and effective parallelization of ML programs."
"1725322","SPX: Collaborative Research: Mongo Graph Machine (MGM): A Flash-Based Appliance for Large Graph Analytics","CCF","SPX: Scalable Parallelism in t","10/01/2017","09/12/2017","Keshav Pingali","TX","University of Texas at Austin","Standard Grant","Marilyn McClure","09/30/2020","$279,935.00","","pingali@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","042Y","026Z","$0.00","We live in the age of big data. In many problem domains such as data-mining, machine learning, scientific computing, and the study of social networks, the data deals with relationships between pairs of entities, and is represented by a data structure called a graph. Graphs of interest today may have hundreds of billions of entities, and trillions of relationships between these entities. Large-scale graph processing is typically done in data-centers which are huge clusters of power hungry computers. The proposed Mongo Graph Machine (MGM) project will explore a different solution known as out-of-core processing. In this system, graphs will be stored in flash memory, which is much cheaper, denser and cooler than DRAMs, and processed using a combination of specialized circuits called FPGAs in tandem with a conventional processor. A programming system will be developed to hide this complexity from the end-user. The resulting system will be small enough to fit under a desk and dramatically more energy-efficient while providing powerful graph processing capability.<br/><br/>The MGM project will address the problem of storing and processing extreme-scale graphs by using in-storage acceleration based on NAND flash chips with an attached FPGA. A single machine can accommodate 1 TB to 16 TBs of flash memory using current NAND technology. This configuration provides the flash storage necessary to store very large graphs and the computational power necessary to saturate the bandwidth of the flash. To address the programming problem for this architecture, the project will develop compiler technology and FPGA accelerators that will permit developers to write applications in the high-level programming model, leaving it to the system to exploit parallelism and optimize memory accesses for the access characteristics of flash storage. The software system will be based on the Galois system, which has been shown to scale to hundreds of processors on large shared-memory machines."
"1855760","AitF: Collaborative Research: Topological Algorithms for 3D/4D Cardiac Images: Understanding Complex and Dynamic Structures","CCF","Algorithms in the Field","08/27/2018","10/22/2018","Chao Chen","NY","SUNY at Stony Brook","Standard Grant","Rahul Shah","08/31/2020","$240,658.00","","chao.chen.1@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7239","","$0.00","The interiors of ventricles of a human heart are spanned by a fine net of muscle fibers that are difficult to resolve, even in high resolution CT images.  An accurate account of these structures, however, could improve diagnosis of cardiac disease, evaluation of cardiac function, assessment of stroke risk, and simulation of cardiac blood flow. Topology is the branch of abstract mathematics that deals with connections; this project uses the theory of persistent homology to identify crucial topological handles that can be useful for accurate reconstruction and analysis of the complex cardiac dynamics from these CT images. The outcome of the project will not only advance our understanding of cardiac function, but also generate novel computational topology methods that are more efficient and effective for practical applications. This project not only bridges the gap between the theory of computational topology and the practical problem of cardiac image analysis, but also trains the next generation of researchers and educators to do so by a carefully integrated education plan. The PIs will engage undergraduate students, high school students, women and other underrepresented students in their proposed research.<br/><br/>The goal of this project is to develop a topological approach to unveil the intrinsic structures from complex and dynamic 3D/4D cardiac data, and furthermore, to provide principled tools to quantitatively analyze these structures. The PIs will create new computational topology methodologies and algorithms to extract rich information from the intrinsic structure of cardiac data. They will develop novel methodologies to extract localized topological features and to track them based on their spatial and temporal coherence. They also plan to design new algorithms to untangle ambiguous and uncertain situations for tracking structures through time sequence data. The resulting techniques and software will be validated on cardiac CT data to produce quantitative assessments of accuracy and to characterize the advantages and limitations of these approaches. Domain experts will validate the quality of the approaches via scientific hypotheses and data exploration. The methods to be developed are general and will impact other scientific fields where intrinsic complex and dynamic structures exist."
"1811729","AF: Small: RUI: New Directions in Kolmogorov Complexity and Network Information Theory","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","05/21/2018","Marius Zimand","MD","Towson University","Standard Grant","Tracy J. Kimbrel","09/30/2021","$236,222.00","","mzimand@towson.edu","8000 York Road","Towson","MD","212520001","4107042236","CSE","7796","7923, 7926, 7927, 9229","$0.00","Data communication in systems with multiple senders and receivers raises difficult problems caused by the possibility of complex data correlation patterns, network topologies, and interaction scenarios. Traditionally, these problems have been tackled using tools from Information Theory (IT), which assumes that the data has been produced according to a certain generative model.  In practice, most of the time the generative model is not known. Even if it is known, it is often more complicated than the models typically used in theoretical studies. This project will use tools from Algorithmic Information Theory (AIT, also known as Kolmogorov complexity), which equates the information in a piece of data with its minimal description length.  The advantage is that the new approach does not rely on any model, and consequently the results obtained this way are valid in more general circumstances. The problems that will be studied are of interest to computer scientists, electrical engineers, and mathematicians. The project will promote a deep and dynamic exchange of ideas between these communities. This project will produce new insights in Kolmogorov complexity and communication complexity. These areas have applications in computational complexity, machine learning, constructive combinatorics, and other fields. The results will likely have an impact in many of these areas. The project will allow undergraduate and graduate students to participate in research activities that have a strong theoretical flavor and the promise of real-world applications.<br/><br/>The project is timely and realistic because it has recently become apparent that some interesting questions (for instance, source coding of non-ergodic sources), which cannot be approached with tools based on Shannon entropy, can be solved in the AIT framework. In the other direction, there has been recent progress in some classical problems in Kolmogorov complexity inspired from results and techniques from IT. The project will: (1) isolate, understand, and develop certain aspects of Kolmogorov complexity that are particularly relevant for data communication; (2) use the newly-developed tools to make progress in outstanding problems in network communication such as channel coding, network coding, interactive protocols, and others; and (3) use the insights from the communication setting to advance the theory of Kolmogorov complexity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640081","E2CDA: Type I: EXtremely Energy Efficient Collective ELectronics (EXCEL)","CCF","Energy Efficient Computing: fr, SOFTWARE & HARDWARE FOUNDATION","09/01/2016","07/29/2018","Suman Datta","IN","University of Notre Dame","Continuing grant","Sankar Basu","08/31/2019","$2,715,899.00","Arijit Raychowdhury, Supratik Guha, Zoltan Toroczkai, Xiaobo Hu","sdatta@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","015Y, 7798","7798, 7945","$0.00","With billions of smart and connected devices, ""data deluge"" is a reality with more than eight zettabytes of data generated in last year alone. The primary focus of this multi-disciplinary research effort is to develop a new paradigm of computing titled Extremely energy efficient Collective Electronics (EXCEL) to enable hardware accelerated data-analytics that can extract information from unlabeled and unstructured data. This research is expected to uncover fundamentally new ways of harnessing coupled dynamical systems for solving computationally hard problems in an energy efficient way. With innovations in novel materials and devices, chip-scale dynamical system implementation, architectural changes and critical benchmarking, EXCEL will lay the foundation for a new non von-Neumann computing paradigm to achieve orders of magnitude improvement in computational energy efficiency. The outreach activities are prioritized around educating future generations of students to adapt to the forthcoming evolution and revolution in information processing systems. The research project is structured to benefit from strong engagement with industry, which will facilitate technology transfer in the future. The participating PIs are committed to developing course modules for both undergraduate and graduate students in the areas of emerging nanotechnologies, unconventional computing, machine learning and computational medicine.<br/><br/>The EXCEL project lays the foundation for a radically different approach to energy efficient information processing by leveraging emergent phenomena in novel devices and dynamics of coupled systems to execute optimization, learning and inference tasks in a collective, cooperative and scalable way. The intellectual foundation of EXCEL rests on the utilization of local information content embedded in the spatio-temporal dynamics of coupled networks (oscillatory and spiking) to perform computation in a massively parallel way. The collective computing paradigm is a timely departure from the traditional model of von-Neumann computing which relies on batch, discrete time, iterative updates (lacking temporal locality) and shared states (lacking spatial locality). EXCEL focuses on developing a complexity theoretic foundation in analog computing. This includes exploration of both continuous and discrete optimization problems as well as stochastic machines for on-line learning. The EXCEL researchers will actively pursue physical hardware demonstration and quantify their advantages over Boolean computers in solving computationally hard problems that are finding ever expanding applications in high-performance data centers, real-time cyber-physical systems and computational medicine."
"1725303","SPX: Collaborative Research: Mongo Graph Machine (MGM): A Flash-Based Appliance for Large Graph Analytics","CCF","SPX: Scalable Parallelism in t","10/01/2017","09/12/2017","Professor Arvind","MA","Massachusetts Institute of Technology","Standard Grant","M. Mimi McClure","09/30/2020","$520,001.00","","arvind@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","042Y","026Z","$0.00","We live in the age of big data. In many problem domains such as data-mining, machine learning, scientific computing, and the study of social networks, the data deals with relationships between pairs of entities, and is represented by a data structure called a graph. Graphs of interest today may have hundreds of billions of entities, and trillions of relationships between these entities. Large-scale graph processing is typically done in data-centers which are huge clusters of power hungry computers. The proposed Mongo Graph Machine (MGM) project will explore a different solution known as out-of-core processing. In this system, graphs will be stored in flash memory, which is much cheaper, denser and cooler than DRAMs, and processed using a combination of specialized circuits called FPGAs in tandem with a conventional processor. A programming system will be developed to hide this complexity from the end-user. The resulting system will be small enough to fit under a desk and dramatically more energy-efficient while providing powerful graph processing capability.<br/><br/>The MGM project will address the problem of storing and processing extreme-scale graphs by using in-storage acceleration based on NAND flash chips with an attached FPGA. A single machine can accommodate 1 TB to 16 TBs of flash memory using current NAND technology. This configuration provides the flash storage necessary to store very large graphs and the computational power necessary to saturate the bandwidth of the flash. To address the programming problem for this architecture, the project will develop compiler technology and FPGA accelerators that will permit developers to write applications in the high-level programming model, leaving it to the system to exploit parallelism and optimize memory accesses for the access characteristics of flash storage. The software system will be based on the Galois system, which has been shown to scale to hundreds of processors on large shared-memory machines."
"1637536","AitF: Efficient Memory Management via Randomized, Streaming, and Online Algorithms","CCF","Algorithms in the Field","09/01/2016","08/24/2016","Andrew McGregor","MA","University of Massachusetts Amherst","Standard Grant","Rahul Shah","08/31/2020","$500,000.00","Emery Berger","mcgregor@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7239","","$0.00","Memory management is an essential component of computer systems ranging from small low-power and mobile devices up to large data centers. Memory is necessary for any non-trivial computing process that needs to be performed in order to store the input data and the state of the computation. The goal of efficient memory management is to allocate the available memory to the different processes that need to be performed in such a way that the speed of the subsequent computation is maximized; the energy used by the system is minimized; and the available memory hardware is fully exploited. This project is focused on improving existing memory management approaches and could lead to significant improvements in the computer infrastructure used in a broad range of applications. The project will also train students, both through curriculum development and direct involvement in the research, in the application of algorithm design to the field of computer systems.<br/><br/>In this project, we focus on designing and analyzing new randomized algorithms for various problems that arise in the context of memory management. These include both lightweight data stream or ""sketch-based"" algorithms that are fast and use limited memory, and online algorithms that need to commit to decisions about how to best to use a small amount of available memory without knowing what data or operations on this data will be relevant in the future. We will also develop a new randomized technique for compacting memory even for languages such as C and C++ that use explicit memory management and where objects cannot be relocated. Our approach should mitigate the risk of potentially catastrophic fragmentation and thereby improve memory utilization and performance."
"1524246","AF: Small: Communication and Resource Tradeoffs","CCF","ALGORITHMIC FOUNDATIONS","08/15/2015","08/11/2015","Paul Beame","WA","University of Washington","Standard Grant","Tracy J. Kimbrel","07/31/2019","$400,000.00","","beame@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","7923, 7927","$0.00","This project will focus on several problems in computational complexity involving tradeoffs among computational resources. These include an analysis of tradeoffs between the communication load, processors, and rounds of communication required to do data analysis on massively parallel systems, an analysis of tradeoffs between the computation time and memory (space) needed to analyze basic statistical properties of data, and an analysis of the role of nondeterminism versus determinism in tradeoffs between time and space.  The project will also focus on counting problems for functions computed by simple classes of circuits.<br/><br/>A large portion of computations today is being done on a massively parallel basis in cloud data centers. Though there are programming constructs that are convenient for using this computing power, there is only a limited understanding of the extent to which these constructs are using this parallel computing architecture efficiently. The project work on communication load for massively parallel systems focuses on developing a broad understanding of this question, ideally developing more efficient algorithms in the process. Efficient algorithms to use these data centers are critical since the energy requirements of data centers are an increasingly significant portion of the total use of the electric grid.  <br/><br/>The part of this project on finding tradeoffs between time and space required to solve computational problems involves advancing the fundamental understanding of how to use these resources most efficiently. It also will examine a special case involving time-bounded computation of an open question of much wider general interest in computational complexity: How much easier is it to check a proof of an answer to a problem than to determine whether that answer is yes or no? Finally, the portion of the project on the analysis of counting problems is motivated both by fundamental questions about what simple circuits can compute and by potential application to efficient reasoning about uncertain environments."
"1748200","EAGER: A Standard Enabled Workflow for Synthetic Biology","CCF","COMPUTATIONAL BIOLOGY","08/15/2017","05/03/2018","Chris Myers","UT","University of Utah","Standard Grant","Mitra Basu","07/31/2019","$166,000.00","","myers@ece.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7931","7916, 7946, 9251","$0.00","A synthetic biology workflow is composed of data repositories that provide information about genetic parts, sequence-level design tools to compose these parts into circuits, and system-level design tools to construct and analyze models of complete designs. Data standards enable the ready exchange of information within such a workflow. They allow repositories and tools to be connected from a diversity of sources. This workflow will leverage several data standards developed for systems and synthetic biology. This workflow will have a broader impact on several diverse communities of synthetic biology users.  This project also will work with journals, especially ACS Synthetic Biology, to integrate this workflow into their data archival process, and it will provide training videos for authors to minimize the impact on them. Ultimately, this workflow should play a significant role in enabling synthetic biology to become a truly reproducible scientific endeavor. <br/><br/>This standard enabled workflow will leverage a variety of data standards.  These standards include the Synthetic Biology Open Language (SBOL), which can be utilized to describe and visualize genetic designs, the Systems Biology Markup Language (SBML), which can be used to create computational models, the Simulation Experiment Description Markup Language (SED-ML), which can describe simulation experiments, and the COMBINE Archive, which can combine these files into a single exchangeable document. Using these standards, a synthetic biology workflow will be constructed that includes multiple repositories, such as the Newcastle SynBioHub and the JBEI ICE Repository, and multiple software tools such as Benchling, SBOLDesigner, iBioSim, and Cello, among others. The intellectual merit of this project includes the following activities:<br/>--Integration and refinement of genetic part datasets from a variety of libraries, including, among others, the iGEM registry, MIT Cello transcriptional circuit library, and the Newcastle BacillOndex part library. <br/>--Improve SynBioHub's connections with remote repositories, such as Benchling, JBEI ICE, and SBOLme, as further sources for genetic design information.<br/>--Connection of software to form a synthetic biology workflow, including, among others, SBOLDesigner, iBioSim, and SynBioHub.<br/>--Application and validation of this workflow using as a driving example the genetic circuit design and computational modeling of communication circuits for multicellular spatiotemporal patterning."
"1704715","SHF: Medium: Hierarchical Tuning of Floating-Point Computations","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2017","04/25/2018","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Nina Amla","07/31/2020","$1,224,000.00","Hari Sundar, Mary Hall, Zvonimir Rakamaric","ganesh@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798","7798, 7924, 8206, 9102, 9251","$0.00","The project implements methods to improve the resource-efficiency of numerical computations on a variety of computing machines, ranging from supercomputers to mobile devices, by adaptively reducing data-precision based on the needs of the application. Efficiently adapting data-precision permits these machines to run larger computations and also improves the overall performance (including energy consumption) made possible by a combination of reduced computational burden as well as reduced data movement. The intellectual merits of this project are to research and develop the key steps to understand the nature of applications and computing systems, and to suitably minimize computing demands through targeted data-precision adjustment. Broader impacts of the work include training graduate students and releasing tools that the community can employ in future hardware and software product, to help minimize overall energy consumption and improve performance.<br/><br/>Unused precision in floating-point computations ends up wasting allocated space in caches, and also causes unnecessary data movement. The technical parts of the project are to identify as well as pursue opportunities for optimally allocating precision, and to efficiently implement such allocation methods in actual codes.  In addition to developing new algorithms to tune precision assisted by automated learning methods, the project develops symbolic analysis methods to serve novel roles in floating-point instruction selection and optimization in the form of superoptimizers. These tools will be released to a community of researchers interested in working toward exascale computing, and deploying applications in safety-critical devices. This work represents a synergistic combination of the investigator's skills ranging through high performance computing, formal methods, and compiler technologies."
"1651825","CAREER: Geometric Techniques for Big Data Medical Imaging","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","03/01/2017","Mehmet Akcakaya","MN","University of Minnesota-Twin Cities","Continuing grant","Phillip Regalia","06/30/2022","$194,445.00","","akcakaya@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7797","1045, 7936","$0.00","Medical imaging has benefited greatly from advances in signal and image processing, which have enabled better data acquisition, superior reconstruction and improved analysis of massive amounts of imaging data. With improving resolutions and the push for comprehensive diagnosis, medical imaging faces new challenges; including bigger data sizes, longer scan durations, and susceptibility to artifacts, such as patient motion. Hence, it is imperative that these large-scale datasets are processed and analyzed efficiently in the presence of systematic and physiological imperfections, along with constraints on diagnostic ability and patient throughput.<br/><br/>This project builds a cross-disciplinary research framework to provide theoretical, algorithmic and application developments based on geometric methods to characterize the limits and to improve the state of medical imaging reconstruction and analysis. This research comprises three complementary thrusts: rate-distortion characterization of learning algorithms; theoretical guarantees and algorithms for phase retrieval of low-dimensional models; and optimization strategies on low-dimensional manifolds for a class of parameter estimation problems. Each of these thrusts is complemented with applications in medical imaging, with tremendous potential for translational impact in the US healthcare system, including improved diagnosis and throughput in health-care applications. Broader educational impacts of this project result from integration of the research to graduate and undergraduate curriculum; outreach to the local community and to under-privileged K-12 students."
"1149756","CAREER:  Modern Numerical Matrix Methods for Network and Graph Computations","CCF","ALGORITHMIC FOUNDATIONS, NUM, SYMBOL, & ALGEBRA COMPUT","05/01/2012","05/04/2016","David Gleich","IN","Purdue University","Continuing grant","Balasubramanian Kalyanasundaram","04/30/2019","$568,943.00","","dgleich@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7796, 7933","1045, 7933, 9251","$0.00","Connected data is a hallmark of the Internet age.  We now have an unprecedented ability to collect information i) on social relationships from websites like Facebook; ii) on connections between ideas from hyperlinked repositories such as Wikipedia; iii) on links between scientific fields from online cross-referenced citation databases; iv) on interactions between proteins in biology; and v) even on the connections in the human brain.  Network computations, such as finding the most important people, ideas, papers, or the most important connections, help refine these raw collections of information into meaningful summaries.  Consequently, making these computations fast and efficient will help produce scientific insights from the growing plethora of data available.<br/><br/>A highly successful paradigm for stating network computations is as the solution of a matrix problem.  For instance, such an approach was the heart of Google's celebrated PageRank algorithm for finding the most important pages on the web.  Modern connected data, however, is so large that it has eclipsed the ability of even the best algorithms from the 20th century to cope. Interesting network computations have become more complicated as well. The investigator will study a new class of algorithms to compute nonlinear functions of matrices, such as the matrix exponential. The matrix exponential has many uses; for example, it underlies many new computations designed to identify the most important relationships in neural networks. Standard techniques for the matrix exponential involve examining all of the connections at each step (and there could be hundreds or thousands of steps), only to highlight a few pieces of information.  A more recent paradigm, called local computations, only utilizes the connections from a few  entities (in the matrix, they only look at a few rows or columns) at a time. The goal of this research is to design new algorithms for the matrix exponential and other functions of matrices in the local computations paradigm.  These new algorithms will be able to operate on the world's largest networks quickly (ideally in seconds or minutes), and help application specialists study their data in new ways. Three driving applications will be ranking and voting, link prediction, and brain networks.  The investigation will also include the study of higher-order connections in networks that give rise to three or four dimensional matrices -- commonly called tensors.  <br/><br/>All of the software developed for this research will be made available in a software package for local computations of matrix functions.  The investigator will present tutorials on this software package to ensure that researchers across many disciplines can utilize the outcome of this research. To ensure that this research reaches students across many disciplines, the investigator will develop a graduate course on the use of matrix methods for network computations. Finally, given the growing importance of network data, the investigator will develop a module for high school students to show how solving systems of equations, part of the core high school curriculum, can be used to analyze information networks."
"1717963","SHF:Small: Rooting Out Data- and Control-Flow Anomalies in Event-Based Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/10/2017","Nenad Medvidovic","CA","University of Southern California","Standard Grant","Sol J. Greenspan","07/31/2020","$500,000.00","","neno@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","7923, 7944","$0.00","Distributed event-based (DEB) software systems are widespread, spanning diverse domains such as user interfaces, financial markets, logistics, and mobile applications. Unlike traditional software systems, in DEB systems software components do not directly interact but rely on brokers to transfer data and notifications of different events. Consequently, components in DEB systems are highly decoupled, which yields scalable, easy-to-evolve applications. However, this flexibility and scalability comes at a price: It is difficult to have an accurate insight into the structure and functionality of a DEB system, which makes it difficult to ensure a DEB system's desired behavior and absence of security vulnerabilities. Many techniques have been developed for dealing with analogous issues in traditional software systems. However, those techniques provide inadequate and/or misleading information when applied to DEB systems. This project is providing a suite of tools for ensuring software correctness, reliability, and security that are specifically targeted at DEB systems.<br/> <br/>The project does so by developing novel program analysis, runtime monitoring, and visualization techniques that account for the implicit invocation, concurrency, and ambiguous interfaces inherent in DEB systems. The project's specific focus is on (1) data-flow anomalies that cause correctness and reliability issues and (2) data- and control-flow anomalies that cause security problems. The broader impact of this research is direct: It provides to engineers in a wide range of important software development domains analysis and visualization techniques that are comparable to techniques available in traditional domains. On one hand, the flexibility of DEB systems bodes well for their continued adoption and expansion. On the other hand, that adoption is impeded by obstacles in understanding, analyzing, debugging, evolving, and securing DEB systems. This research presents a significant step in the direction of providing the necessary remedies and helping to realize the full potential of DEB systems."
"1717159","AF:Small:Reeb Spaces and Parameterized Clustering","CCF","ALGORITHMIC FOUNDATIONS","08/15/2017","08/11/2017","Amit Patel","CO","Colorado State University","Standard Grant","Rahul Shah","07/31/2020","$291,284.00","","akpatel@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7796","7923, 7929","$0.00","In these times, scientists are not the only ones struggling to grasp the overall picture from a flood of specific, detailed, high-dimensional data.  Understanding a high-dimensional data set often means understanding a lower-dimensional space that generates it, as, say, the important part of the image of someone playing a body-controlled video game is the few angles of their arm and leg joints, and not the many pixels coming from a camera and depth sensor.  Topology is the branch of mathematics that looks at connections rather than specific points or values; two popular tools in topological data analysis, hierarchical clustering and Reeb graphs, are based on path-connectivity.  This project generalizes both, developing generalizations to parameterized hierarchical clustering and Reeb spaces.  There is an exciting interplay between the practical questions of data analysis and the theoretical and mathematical tools to answer them -- each deepens the other.  This project will train students through this interplay. <br/><br/>The PI will develop algorithmic foundations for Reeb spaces and theoretical foundations for parameterized hierarchical clustering. The Reeb space of a continuous map summarizes the path-connectivity of its fibers. The Reeb graph, which is the Reeb space of a map to the real line and for which there are fast algorithms to compute, is now a commonly used tool in topological data analysis. The PI will focus on developing practical algorithms for computing the Reeb space of a map to higher dimensional Euclidean spaces. When the domain of the continuous map is equipped with a metric, the path-connected components of each fiber assembles into a (Vietoris-Rips) hierarchical cluster. Thus the map gives rise to a family of hierarchical clusters parameterized by the co-domain of the map. The PI will use sheaf theory to develop a theoretical foundation for parameterized hierarchical clustering."
"1717635","SHF:   Small:   Collaborative Research:  PEGASUS:  ProgrEss GuAranteeS for Universal tranSactions","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2017","04/23/2018","Peter Pirkelbauer","AL","University of Alabama at Birmingham","Standard Grant","Anindya Banerjee","07/31/2020","$249,780.00","","pirkelbauer@uab.edu","AB 1170","Birmingham","AL","352940001","2059345266","CSE","7798","7798, 7923, 7943, 9150, 9251","$0.00","Software scalability and reliability on multi-core systems is a crucial and urgent issue that needs to be addressed. Transactional processing is a promising programming model for developing parallel applications across many domains, including scientific and application software, that simplifies the development of correct concurrent software. This project will develop transactional techniques that overcome many current limitations, and it will explore the optimization of these techniques on systems with support for transactions. The intellectual merits are the development of novel techniques for constructing transactional data structures on systems with and without hardware support for transactions. The project will create a first approach for transactional processing with progress guarantees. The project's broader significance and importance are to make accessible to the industry a set of prototype data structures and programming techniques that furthers the reliability and performance of software on current and future multi-core systems. The project will also develop critical human resources in systems programming.  <br/> <br/>The project will lead to a number of fundamental and practical outcomes. Transactions have been recognized as a promising alternative to lock-based systems. However, transactions as currently implemented in software or hardware have different drawbacks, with the absence of a progress guarantee being the most fundamental. To overcome this problem, this work further develops the notion of transactional data structures that are based on lock-free techniques, thereby guaranteeing progress. The project will also extend the applicability of commutable transactions to various data structure types including commonly used linked and contiguous memory data structures. The project harnesses transactional hardware to accelerate the execution of common cases and demonstrates the applicability by prototyping  a set of important containers. Finally, this project will evaluate the outcome against"
"1615845","AF: Small: Geometric Clustering and Covering: New Directions","CCF","ALGORITHMIC FOUNDATIONS","08/01/2016","07/08/2016","Kasturi Varadarajan","IA","University of Iowa","Standard Grant","Rahul Shah","07/31/2019","$399,732.00","","kasturi-varadarajan@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7796","7923, 7929, 9150","$0.00","Clustering, which collects nearby data together in groups, is a key operation in data analysis, but one whose computation is surprisingly difficult due to the many choices for dividing data into groups. <br/>This project investigates some fundamental algorithmic problems and new directions in the areas of geometric clustering and covering. Progress on the problems considered in this project will not only enhance knowledge in the PI's core research field, and that of his graduate students, but also yield new techniques, ideas, and points of view that will be useful beyond the core area. <br/><br/>One avenue for such impact is the training of graduate students who will work on this project. By the time they successfully complete their dissertations, these students develop a deep understanding of how technical knowledge may (or may not) influence real world problem solving, an appreciation for the difficulty of obtaining reliable new knowledge, and a sense of the excitement of the process of discovery. This experience informs their work, whether they end up in academia or industry.<br/><br/><br/>In geometric clustering, the goal is to partition data, viewed as a set of points, into groups based on similarity. Geometric clustering can help infer useful patterns from data, but can also help in planning infrastructure installation, such as base station placement in a cellular network. In geometric covering, we wish to cover a set of points by the smallest possible number of a given set of objects; such problems arise in the context of sensor networks. The clustering and covering problems studied in this project are viewed as optimization problems. These problems are typically  NP-complete, which essentially means that it is not possible to solve them exactly using an algorithm with guaranteed efficiency. The PI will examine efficient algorithms that solve the problems approximately, and study the best approximation that can be provably guaranteed. The PI expects that the project will contribute exciting new ideas and techniques at the intersection of the fields of Approximation Algorithms and Computational Geometry."
"1816936","SHF: Small: SMT Reasoning for Tensors and Data","CCF","SPECIAL PROJECTS - CCF","10/01/2018","06/21/2018","Dejan Jovanovic","CA","SRI International","Standard Grant","Nina Amla","09/30/2021","$499,452.00","","dejan@csl.sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","6508592651","CSE","2878","075Z, 7923, 8206","$0.00","Our society is increasingly more dependent on computing systems that include components built from tensor models and rely on them to make critical decisions, like for example airplane controllers built from MATLAB code, or neural networks in self-driving cars. The surge in size and complexity of these systems has made their development, testing, and verification extremely costly and time consuming. To address this challenge, this project develops novel automated reasoning techniques that can support the development cycle of tensor-based systems, at scale, from design to verification, with the potential to increase both performance and reliability of the resulting systems, while broadening the applicability of symbolic reasoning to new domains.<br/><br/>The goal of the project is to develop new automated reasoning and symbolic optimization techniques needed for reasoning about complex systems that rely on tensors and data as the underlying model of<br/>computation. The current generation of automated reasoning techniques focuses solely on scalar domains, and they are inadequate for reasoning about large tensor systems with data. This project develops new reasoning techniques, in the context of satisfiability modulo theories, that can operate at the level of tensors. The key novel ideas explored in this project include high-level modeling and representation of tensor models, data-aware reasoning and optimization techniques for both linear and non-linear tensor models, and combination of numerical optimization and symbolic reasoning techniques. The project integrates the new techniques into machine learning and software analysis tools and evaluates their effectiveness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817242","SHF: Small: Differential Policy Verification and Repair for Access Control in the Cloud","CCF","SOFTWARE & HARDWARE FOUNDATION, Secure &Trustworthy Cyberspace","07/01/2018","06/04/2018","Tevfik Bultan","CA","University of California-Santa Barbara","Standard Grant","Nina Amla","06/30/2021","$499,992.00","","bultan@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7798, 8060","7923, 8206","$0.00","Due to ubiquitous use of software services, protecting the confidentiality of private information stored in compute clouds is becoming an increasingly critical problem.  Users frequently trust sensitive personal information such as financial or medical records to software services.  Protection of such private data is of paramount importance for users. In order to prevent disastrous data breaches (which do happen), software developers must accurately specify who can access the data resources and in what ways. Although access control specification languages and libraries provide mechanisms for protecting confidentiality of information in software systems, without automated techniques that can assist developers in writing and checking access control policies, complex policy specifications are likely to have errors that lead to unintended and unauthorized access to data, possibly with disastrous consequences. This project will expose graduate and undergraduate students to security and analysis problems in cloud-based systems through a variety of educational activities including courses, seminars and individual research mentoring.<br/><br/>This project develops automated techniques that help software developers in protecting users' data and preventing dangerous exposure of<br/>private information.  In particular, the investigators develop automated techniques that help software developers in identifying and eliminating errors in access control policies by converting policy specifications to logical constraints and identifying inconsistencies among policies using constraint solving techniques. The project also investigates methods to automatically generating inputs that demonstrate inconsistencies among policies, and automatically generating repairs that remove the inconsistencies among the policies. Within the scope of this project the investigators develop and use novel constraint solving techniques for policy analysis, particularly focusing on analysis of complex numeric and text manipulation operations that are error-prone.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751040","CAREER: Fundamental Algorithms for Data-Limited Problems","CCF","ALGORITHMIC FOUNDATIONS","08/01/2018","01/29/2018","Eric Price","TX","University of Texas at Austin","Continuing grant","Tracy J. Kimbrel","07/31/2023","$94,979.00","","ecprice@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7796","1045, 7926","$0.00","From medical imaging to astronomy, scientific hypothesis testing to data analysis, computers are used in a wide variety of areas where computation is cheaper than data collection.  Such situations call for algorithms that are not only fast, but also data efficient. This project considers sub-linear algorithms for fundamental computational problems of interest in both theory and practice. It focuses on two basic questions: how many samples, or noisy observations from a signal, does it take to accurately reconstruct the signal, and how many samples from an object does it take to estimate a property of the object?<br/><br/>The PI will investigate ways to leverage knowledge of signal structure into improved signal reconstruction.  An example signal structure is the property of having a sparse Fourier transform; this has been well studied in the discrete setting, but is still poorly understood in the more realistic continuous setting. Another signal structure is that given by generative models built with deep convolutional neural networks; these have produced remarkably accurate models of images in recent years.  This project will use such models to estimate images more accurately from fewer measurements.  The PI will also investigate problems in distribution testing and graph sampling, with a goal of translating techniques from the distribution testing literature into the statistical hypothesis testing framework.  The PI will incorporate research into teaching, and mentor students at levels ranging from high school to graduate school.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1350670","CAREER: Sketching Algorithms for Massive Data","CCF","ALGORITHMIC FOUNDATIONS","05/01/2014","01/22/2014","Jelani Nelson","MA","Harvard University","Standard Grant","Rahul Shah","04/30/2019","$512,818.00","","minilek@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","1045, 7926","$0.00","A sketch of a massive dataset is some compression of it which still allows for answering, sometimes only approximately, some pre-specified types of queries about the data. For many query types of interest, it turns out that sketches exist that provide exponentially smaller compressions. This feature has made sketching methods pervasive in coping with recent trends in data explosion to reduce both communication bandwidth and required storage capacity. Sketching has also been applied to obtain algorithmic speedup for certain high-dimensional problems such as nearest neighbor search, clustering, and low-rank approximation for large matrices, as well as to enable more efficient signal acquisition in a field that has come to be known as compressed sensing. This research plans to further the state of knowledge concerning three intertwined subtopics of sketching: streaming, dimensionality reduction, and compressed sensing.<br/><br/>A fundamental question the PI will investigate is whether one can design sketches that are moderately ""universal"", in that the same sketch can be used to answer many different types of queries. Dimensionality reduction has been successfully used to circumvent the so-called ""curse of dimensionality"" in many problems, where the best known algorithms have running times that scale poorly with dimension. This research plans to study the tradeoffs between approximation quality, number of vectors in the data set, and target dimension, and to close gaps between known upper and lower bounds.  Compressed sensing has found applications in a diverse range of areas, such as magnetic resonance imaging and photography. This research plans to investigate more efficient compressed sensing schemes for providing various types of approximate recovery guarantees."
"1763814","SHF: Medium: Collaborative Research: Program Analytics: Using Trace Data for Localization, Explanation and Synthesis","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2018","05/03/2018","Ranjit Jhala","CA","University of California-San Diego","Continuing grant","Sol J. Greenspan","05/31/2022","$424,785.00","Kamalika Chaudhuri, Sorin Lerner","jhala@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7798","7798, 7924, 7944","$0.00","Formal program analyses have long held out the promise of lowering the cost of<br/>creating, maintaining and evolving programs. However, many crucial analysis<br/>tasks, such as localizing the sources of errors or suggesting code repairs, are<br/>inherently ambiguous: there is no unique right answer. This ambiguity<br/>fundamentally restricts the wider adoption of formal tools by limiting users to<br/>those with enough expertise to effectively use such ambiguous results. The key<br/>insight is that data-driven machine-learning approaches, which have proved<br/>successful in other domains, can be applied to the data traces generated by<br/>programmers as they carry out development tasks. This research addresses the<br/>challenge of ambiguity by extending classical program analysis into modern<br/>program analytics. This extension enhances classical symbolic methods with<br/>modern data-driven approaches to collectively learn from fine-grained traces of<br/>programmers interacting with compilers or analysis tools to iteratively modify<br/>and fix software.<br/><br/>The research systematically develops program analytics by pursuing research<br/>along two dimensions: language domains and programming tasks. First, it studies<br/>different language domains, from dynamically typed languages (Python), to<br/>statically typed functional languages with contract systems (Haskell), to<br/>interactive proof assistants (Coq). Second, it targets different programming<br/>tasks, from localizing errors like null-dereferences, assertions or other<br/>dynamic type failures, to static type errors, to completing or fixing code to<br/>eliminate an error or to obtain some desired functionality. This approach takes<br/>advantage of a suite of new approaches that harness recent advances in<br/>statistical machine learning and fine-grained, domain specific programmer<br/>interactions. These advantages allow the research to address the fundamental<br/>problem of ambiguity in classical program analysis. This has potential to<br/>transform software development by yielding a new generation of program analysis<br/>tools that are efficient, applicable, and automatically customizable (e.g., to a<br/>particular company, project, group or even individual).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717207","AF:  Small:  Collaborative Research:  Distributed Quasi-Newton Methods for Nonsmooth Optimization","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","03/22/2019","Nikolaos Freris","NY","New York University","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2020","$163,160.00","Nasir Memon","nf47@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7796","7923, 7933","$0.00","Optimization, which finds the inputs to a mathematical function that produce the minimum output, is a workhorse algorithm behind many of the advances in smart devices or applications in the cloud. As data gets larger and more distributed, new ideas are needed to maintain the speed and accuracy of optimization. Operator splitting, which expresses the function to minimize as the sum of two convex functions, one of which is smooth and the other non-differentiable, is an idea that has produced to new first-order optimization methods.  This project explores operator splitting with second-order optimization methods, which have faster convergence to the minimum.  The focus is on large, distributed, and streaming data sets, so that the resulting general-purpose numerical solvers and embedded systems implementations can support optimization in cyberphysical systems and the Internet-of-Things.   The project has as priority the active engagement and training of students and researchers, with specific emphasis on the inclusion of women and under-represented minority groups. This project not only involves collaboration across three top-tier American universities, but also with European research institute, KU Leuven. <br/><br/>In specific, this research project seeks to interpret existing methods for structured convex optimization (such as the celebrated ADMM algorithm) as gradient methods applied to specific functions arising from the original problem formulation, and  interpret of operator-splitting techniques as fixed point iterations for appropriately selected operators.  A key theoretical foundation is the introduction of new envelope functions (smooth upper approximations possessing the same sets of solutions) that can be used as merit functions for variable-metric backtracking line-search. To conclude, a principal focus of the project is to design distributed asynchronous methods applicable to large-scale multi-agent cyberphysical systems that involve big data and impose stringent real-time constraints for decision-making. In this purview, the goal is to deliver methods that will outperform current state-of-the-art in terms of (a) speed of computations, (b) scalability with big data sizes, (c) robustness to various types of uncertainty, and, most topically, (d) distributed asynchronous implementation over networks in real-time. The merits will be illustrated in the context of applications in signal processing, control, machine learning and robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1714334","SHF:Small:Scalable Spiking Neural Network Enabled by Probabilistic and Non-Volatile Synapses","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2017","08/30/2017","Lawrence Pileggi","PA","Carnegie-Mellon University","Standard Grant","Sankar Basu","08/31/2020","$450,000.00","Samuel Pagliarini","pileggi@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7945, 8089","$0.00","Scaling of integrated circuit (IC) technology for the past few decades has enabled remarkable advancement of computing speed and power efficiency, as evidenced by the cellphone computing that we hold in our hands today that would have corresponded to room-size super computers not long ago. But as we reach fundamental physical limits for scaling to smaller feature sizes at nanometer scale, massive amounts of data can be stored, and super-fast circuits can process the data, such that now the overall system performance is limited by the bottleneck that forms with transferring the data between the memory and the processor. For this reason, alternative computation models, particularly those based on brain-inspired (neuromorphic) computation, have recently resurged as a potential new computing paradigm for certain classes of computing applications and problems. This requires advancements in devices, circuits and computing architectures, along with creation of the education platform that will allow future engineers and computer scientists to advance and exploit them.<br/><br/>At the core of this proposed work is the use of a novel magnetic device that is combined with traditional integrated circuit technology to enable a scalable and power efficient neuromorphic computer chip. The design will be optimized to handle ""big data"" problems that require extremely power efficient implementations, such as real-time processing of a video stream for a medical imaging application. Such implementations are challenging for various reasons, most notably the storing of values for a large number of artificial synapse weights, and efficient methods to compute random numbers that are needed to make artificial neuron spiking decisions. Carnegie Mellon researchers are focused on addressing these two key challenges in the proposed work."
"1535977","AitF: Full: Collaborative Research: Graph-theoretic algorithms to improve phylogenomic analyses","CCF","Algorithms in the Field","09/01/2015","08/12/2015","Tandy Warnow","IL","University of Illinois at Urbana-Champaign","Standard Grant","Tracy Kimbrel","08/31/2020","$360,000.00","Chandra Chekuri","warnow@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7239","012Z","$0.00","Understanding the history of life on earth ? how species evolved from their common ancestor ? is a major goal of biological research. These evolutionary trees are very hard to construct with high accuracy, because nearly all of the most accurate approaches require the solution to computationally hard optimization problems. Furthermore, research has shown that the evolutionary tree for a single gene can be different from the evolutionary tree for the species, and current methods do not provide adequate accuracy on genome-scale data. As a result, large evolutionary trees, covering big portions of ?The Tree of Life?, are very difficult to compute with high accuracy. This project will develop methods that can enable highly accurate species tree estimation. The key approach is the development of novel divide-and-conquer strategies, whereby a dataset is divided into overlapping subsets, species trees are constructed on the subsets, and then the subset species trees are merged together into a tree on the full dataset. These approaches will be combined with powerful statistical estimation methods, to potentially transform the capability of evolutionary biologists to analyze their data. This project will also provide open source software for the new methods that are developed, and provide training in the use of the software to biologists at national meetings. The project will also contribute to interdisciplinary training for two doctoral students, one at Illinois and one at Berkeley, and course materials for computational biology will be made available online.<br/> <br/>Understanding evolution, and how it has operated on species and on genes, is a major part of biological data analysis. Statistical estimation approaches often provide the best accuracy, but cannot scale to dataset sizes that are required for modern biology. In addition, species tree estimation is challenged by the heterogeneity of evolutionary trees across the genome, and no current methods are able to provide highly accurate species trees for genome-scale data. These challenges make it essential that new methods be developed in order to make highly accurate large-scale evolutionary tree estimation possible under these complex evolutionary scenarios.  This project will develop novel algorithmic strategies to address three key problems: supertree estimation, species tree estimation in the presence of gene tree heterogeneity, and scaling statistical methods to large datasets. In addition to developing graph-theoretic algorithms, the project team will establish mathematical guarantees for these methods using chordal graph theory and probabilistic analysis, under stochastic models of gene and sequence evolution."
"1849757","CRII: CIF: Distributed Computing with Privacy and Security Considerations","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2019","02/05/2019","Martina Cardone","MN","University of Minnesota-Twin Cities","Standard Grant","Phillip Regalia","05/31/2021","$175,000.00","","cardo089@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7797","7797, 7935, 8228, 9102","$0.00","Modern datasets have reached such sizes and complexities as to make them difficult to analyze and process using traditional centralized approaches. Distributed computing enables large-scale data processing, by distributing the effort of a computing task over such datasets across a network of machines that operate in parallel. With the proliferation of sensitive data that needs to be processed, and the ever-growing computational power of adversarial parties, ensuring data privacy and security of the computing process represents a critical aspect for the design of modern distributed computing systems. This project aims at developing a theoretical foundation to investigate the performance of distributed computing in the presence of machines that are not trusted or well-protected. This project will promote undergraduate and graduate research, and the outcomes of the proposed research will be integrated into education.<br/><br/>This project considers two different distributed computing scenarios and seeks their theoretical performance limits. In the first scenario, the dataset contains sensitive information (such as clinical/genome information) and, when distributed, has to remain confidential from the machines. An optimization framework is used which seeks to quantify the trade-off between the performance of the computing task and the level of data privacy that can be guaranteed. In the second scenario, the machines may have been compromised and hence may introduce false information to the system. The main focus is on the design of distributed computing techniques that are robust to different message-tampering attacks and allow one to efficiently identify the attacked machines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1719017","CIF: Small: Info-Clustering: An Information-Theoretic Framework for Data Clustering","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/22/2017","Tie Liu","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","06/30/2020","$439,315.00","","tieliu@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7797","7923, 7935","$0.00","Clustering refers to a procedure that groups similar objects together while separating dissimilar ones apart. This simple idea has a wide range of applications in different areas of scientific research. From the mathematical viewpoint, the problem of clustering is quite unique in that it attempts to discover unknown patterns of data without a clear knowledge of the ground truth. Instead of jumping to a specific algorithm or a dataset (which is a common practice in the literature), this research aims to lay a rigorous theoretical ground, upon which many meaningful and practical implementations can be developed subsequently. This research is accompanied by the investigator's continuing effort in curriculum development, involving undergraduate and graduate students in research, and broadening the participation of women and underrepresented minorities in engineering.<br/><br/>To achieve the aforementioned goal, the investigator plans to take an information-theoretic view of the data clustering problem by modeling each object to be clustered as a piece of information. A key advantage of this information-theoretic view is that now the similarity among multiple objects can be naturally measured by the amount of shared information. This is precisely where information theory, with the accumulation of over 70 years of active research, can be most useful. The main agendas of this research are to understand: 1) what clustering algorithms can be derived from the proposed info-clustering framework by leveraging the large body of literature on multivariate dependency modeling including graphical models and parameter families; 2) whether the proposed info-clustering framework can be leveraged to make some progress on the long-standing open problem of subset feature selection in statistics and machine learning."
"1717391","AF:  Small:  Collaborative Research:  Distributed Quasi-Newton Methods for Nonsmooth Optimization","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/30/2017","Angelia Nedich","AZ","Arizona State University","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2020","$199,840.00","","Angelia.Nedich@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7796","7923, 7933, 9102","$0.00","Optimization, which finds the inputs to a mathematical function that produce the minimum output, is a workhorse algorithm behind many of the advances in smart devices or applications in the cloud. As data gets larger and more distributed, new ideas are needed to maintain the speed and accuracy of optimization. Operator splitting, which expresses the function to minimize as the sum of two convex functions, one of which is smooth and the other non-differentiable, is an idea that has produced to new first-order optimization methods.  This project explores operator splitting with second-order optimization methods, which have faster convergence to the minimum.  The focus is on large, distributed, and streaming data sets, so that the resulting general-purpose numerical solvers and embedded systems implementations can support optimization in cyberphysical systems and the Internet-of-Things.   The project has as priority the active engagement and training of students and researchers, with specific emphasis on the inclusion of women and under-represented minority groups. This project not only involves collaboration across three top-tier American universities, but also with European research institute, KU Leuven. <br/><br/>In specific, this research project seeks to interpret existing methods for structured convex optimization (such as the celebrated ADMM algorithm) as gradient methods applied to specific functions arising from the original problem formulation, and  interpret of operator-splitting techniques as fixed point iterations for appropriately selected operators.  A key theoretical foundation is the introduction of new envelope functions (smooth upper approximations possessing the same sets of solutions) that can be used as merit functions for variable-metric backtracking line-search. To conclude, a principal focus of the project is to design distributed asynchronous methods applicable to large-scale multi-agent cyberphysical systems that involve big data and impose stringent real-time constraints for decision-making. In this purview, the goal is to deliver methods that will outperform current state-of-the-art in terms of (a) speed of computations, (b) scalability with big data sizes, (c) robustness to various types of uncertainty, and, most topically, (d) distributed asynchronous implementation over networks in real-time. The merits will be illustrated in the context of applications in signal processing, control, machine learning and robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1714305","CIF:Small:Information-theoretic and Computational Thresholds in Statistical Learning","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/28/2017","Andrea Montanari","CA","Stanford University","Standard Grant","Phillip Regalia","06/30/2020","$450,000.00","","montanari@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7797","7923, 7936","$0.00","Advanced algorithms are an increasingly powerful tool to extract information from vast amount of data<br/>that are gathered over the Internet, by smartphones, sensors networks, or high-throughput scientific studies.<br/>As these methods become ubiquitous, it is crucial to understand their full potential. What kind of<br/>information can we hope to extract from a certain type of data? Viceversa, how much data should<br/>we accumulate in order to be able to infer a certain piece of information? What is the bottleneck that prevents us from extracting more information? These questions have been studied within classical statistics, but modern applications pose entirely new challenges and classical concepts are only partially useful.<br/>In particular, computational resources become a crucial bottleneck for modern datasets. In many cases,<br/>although the data contain in principle the information of interest, finding it is a needle-in-haystack problem, and cannot be done on human timescales. This project aims at characterizing these fundamental limitations in several central problems, and develop algorithms that can achieve those limits.<br/><br/>Both information theory and complexity theory fall short of capturing the fundamental limitations to statistical learning tasks. This project follows a different approach which aims at analyzing broad classes of algorithms, and draw connections between their behavior. More precisely, the project considers three such classes that essentially encompass most algorithms used nowadays: empirical risk minimization;<br/>semidefinite programming hierarchies; and local algorithms. The focus is on two concrete statistical estimation problems that are relevant for a number of applications: group synchronization on graphs; non-linear high-dimensional regression and classification.  In these and analogous problems, the behavior of seemingly different types of algorithms is often surprisingly similar.  Understanding the origin of this similarity and its implications is a key focus of this research."
"1540657","BSF: 2014324: Streaming Algorithms for Fundamental Computations in Numerical Linear Algebra","CCF","SPECIAL PROJECTS - CCF","09/01/2015","08/06/2015","Michael Mahoney","CA","International Computer Science Institute","Standard Grant","Tracy J. Kimbrel","08/31/2019","$40,000.00","","mmahoney@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","2878","2878","$0.00","Streaming algorithms that use every input datum once (single-pass) or scan the input a small number of times (multiple passes) are gaining importance due to the increasing volumes of data that are available for business, scientific, and security applications. Performing large-scale data analysis and machine learning often requires addressing numerical linear algebra primitives, such as least squares regression, singular value decompositions, least absolute deviations regression, and canonical correlation analysis.  In this proposal, the PIs aim to improve significantly the theory and practice of streaming algorithms for these fundamental linear algebra kernels.  The new algorithms will provide faster and more accurate kernels for the ubiquitous big data applications, reducing resource use (hardware and energy) of machine learning applications, and will make security applications that rely critically on accuracy provably trustworthy.  In addition, they will enable improved exploitation of data in physical, chemical, and biomedical applications.<br/><br/>The computations that will be considered are performed either using inexact incremental single-pass algorithms, or by expensive multi-pass algorithms. Although existing inexact algorithms often work well enough in practice, the worst-case behavior of applications relying on these building blocks has not been characterized. This is especially troubling in the security and anomaly-detection areas, where a malicious party could conceivably exploit such inexactness. The PIs will develop a set of provably-accurate single-pass algorithms for numerical linear algebra. They will also explore alternative algorithmic routes, mainly multi-pass randomized algorithms, both for the core problems (least squares regression regression and singular value decomposition) and for the more challenging ones (least absolute deviations regression and canonical correlations). They will characterize the accuracy/performance tradeoffs associated with these computations, where performance refers mostly to the number of passes but also to the total computational effort (including communication). The PIs will carry out this investigation using benchmarks from significant applications, as well as theoretical lower bounds on single-pass algorithms."
"1652140","CAREER:  Algorithmic Foundations and Modern Applications for Program Synthesis","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2017","07/24/2017","Aws Albarghouthi","WI","University of Wisconsin-Madison","Continuing grant","Nina Amla","06/30/2022","$197,443.00","","aws@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7798","1045, 8206","$0.00","Almost every aspect of our world has been touched by automation through software systems. Being able to efficiently construct complex software provides a competitive advantage to businesses, streamlines our bureaucratic systems, and improves our personal day-to-day lives. But building software remains a cumbersome, error-prone process. This project simplifies the software development process through techniques that automatically synthesize desired programs. At a foundational level, the project develops modern algorithmic techniques for efficiently and automatically constructing software systems, thus advancing the field of program synthesis. The particular emphasis of this project is on automated synthesis of data-analysis applications. Specifically, the project develops tools for end users to automatically construct data analyses that can run on cloud infrastructure, without any necessary knowledge of programming systems. Thus, the project has the potential to revolutionize and democratize data analytics. The project strongly connects research with education and outreach: by involving women and underrepresented minority undergraduate students in research, incorporating program synthesis ideas in modern curricula, and developing inclusive synthesis-themed hackathons.<br/><br/>This project expands the range of program synthesis applications to data-parallel programs that can run on cloud infrastructure. To enable efficient synthesis, the project contributes a portfolio of novel algorithmic techniques. Specifically, the project (1) develops and utilizes the notion of relational specifications to guide synthesis algorithms with semantic knowledge; (2) develops techniques for learning relational specifications of arbitrary APIs; and (3) develops a suite of techniques for efficiently synthesizing programs that are deterministic in the presence of reorderings."
"1525609","SHF: Small: Improving Memory Performance on Fused Architectures through Compiler and Runtime Innovations","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/27/2015","Xipeng Shen","NC","North Carolina State University","Standard Grant","Yuanyuan Yang","07/31/2019","$470,000.00","Frank Mueller","xshen5@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7923, 7941","$0.00","During the past decade, accelerators such as Graphics Processing Units (GPUs) have entered the area of general-purpose computing. They are now widely used for achieving high performance in scientific simulation, business analytics, image processing, and many other application domains. Their effectiveness however has been largely constrained by narrow and slow interconnections to multicore CPUs. Instead of such disjoint memories for multicore CPUs on one side and GPUs on the other, contemporary architectures are adopting an integrated design: Conventional CPUs and co-accelerators are integrated on the same die with access to the same memory. The integration provides new opportunities for synergistic execution on CPUs and GPUs, but can also intensify resource contention within the memory hierarchy. The implications yet remain to be understood.<br/><br/>This project aims to systematically explore the new challenges and opportunities of the integration, especially for compilers and runtime systems in governing program transformations and maintaining them at runtime for data locality, task partitioning and scheduling. The PIs propose to advance the state of the art by promoting synergistic execution in support of data sharing while creating spheres of isolation between CPU and GPU execution to mitigate resource contention of non-shared data. The proposed techniques include a set of novel compiler transformations, concurrent program control and data abstractions, and systems mechanisms that foster<br/>sharing and reduce cross-boundary contention depending on memory access patterns with respect to shared hardware resources.  This synergy between compiler techniques and the runtime system has the potential to significantly improve performance and power guarantees for co-scheduling program fragments on fused architectures."
"1755773","CRII: CIF: Model-based Compression of Biological Sequences","CCF","COMM & INFORMATION FOUNDATIONS","03/15/2018","03/07/2018","Farzad Hassanzadeh","VA","University of Virginia Main Campus","Standard Grant","Phillip Regalia","02/29/2020","$175,000.00","","ffh8x@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7797","7935, 8228","$0.00","With the increasingly widespread use of high-throughput genome sequencing, the amount of biological sequence data is growing at a rate much faster than the decrease in the cost of storage media. To avoid saturating available storage capacity, such data must be compressed at a high ratio. Biological sequences are created over the course of evolution by mutation processes, including substitution, insertion, deletion, and duplication. While these processes shape the statistical properties of genomic sequences and play a critical role in determining which compression approaches will provide improved performance, they are not taken into account by current methods. The goal of this project is to provide a principled approach to biological data compression by developing and leveraging mutation models that approximate the generation process of genomic sequences.<br/><br/>The main research thrusts of the project are: 1) determining the fundamental limits of the compressibility of biological sequences; and 2) developing and evaluating encoding and decoding algorithms that approach these limits. Identifying the limits of compression relies on developing combinatorial and stochastic string-editing models that represent sequence generation through genomic mutations. These models are then studied from an information-theoretic point of view to determine their combinatorial and stochastic capacities, thus providing bounds on the compressibility of genomic sequences. The second thrust leverages the statistical properties arising from mutation models, such as repeat structures, to develop efficient compression tools. In addition to improving compression methods, the success of these research directions will enhance our understanding of complex sequence generation processes, enable the generation of faithful synthetic data, and facilitate the quantitative study of the role of mutations in generating novel biological functions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618303","SHF: Small: Collaborative Research: Coupling Computation and Communication in FPGA-Enhanced Clouds and Clusters","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","04/16/2018","Martin Herbordt","MA","Trustees of Boston University","Standard Grant","Almadena Chtchelkanova","05/31/2020","$240,992.00","","herbordt@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7798","7798, 7923, 7942, 9251","$0.00","The introduction of Field Programmable Gate Arrays (FPGAs) to accelerate clusters of servers in datacenters and clouds provides a great, immediate opportunity to leverage a new technology in high-end computing. With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore?s Law world. Since the hardware adapts to the application higher efficiency can be achieved, and since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Large-scale communication can consequently proceed with both higher bandwidth, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. The proposed design allows for useful processing while data is in flight in the network resulting in reduced software overhead in parallel middleware and reduced network congestion. The key tenets of the research are to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially software overhead. <br/>The research project, FC5 (an FPGA framework for coupling communication and computation in clouds and clusters) has several thrusts. First, hardware support for FC5 and investigation of methods of configurability in FC5 to reduce communication latency and support computing in the network are studied. A second outcome is a prototype version of the Open MPI open source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations. Third, proof-of-concept versions of multiple FC5 software models, including direct hardware access, a transparent MPI-in-OpenCL, and an API-based mechanism that exposes essential functionality. Finally, because FC5 is evolving rapidly with major new announcements expected imminently, continued refinement is essential. At least two model applications, Molecular Dynamics and Map-Reduce, will be used as test cases. <br/>With the continued consolidation of computing services into the cloud, the potential broader impact is to increase both the scale and availability of parallel applications. The broad range of uses of cloud and cluster computing for commercial, government, and academic applications means that acceleration offered will have a widespread impact applicable across many sectors. The growing acceptance of high performance computing in industry (e.g., fast machine learning) is one particular potential commercial sector that will be enhanced by this project."
"1718080","SHF: Small: Approximate-Computing Enabled Robust 3D NAND Flash Memories","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/11/2017","Jun Yang","PA","University of Pittsburgh","Standard Grant","Yuanyuan Yang","07/31/2020","$400,000.00","Youtao Zhang","juy9@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","7798","7923, 7941, 9102","$0.00","Traditional NAND flash has been prevalently used as the storage for essentially all computer systems. Increasing the NAND flash capacity, to meet the requirement of today's ever increasing data volumes, has been enabled by technology scaling. However, further scaling has come to a screeching halt due to degraded performance and reliability in planar NAND cells. 3D NAND flash memory design thus opens a new  direction for continuous density increase, as capacity can be multiplied by stacking more cells in vertical direction without further shrinking the cell sizes. Nonetheless, 3D NAND also faces new challenges in reliability due to its new architecture. Using strong error correction mechanisms is limited by potential high power consumption, implementation area and long operation latency. On the other hand, there is a trend in relaxing data accuracy during computing and data stores as many of today's applications are tolerant to a small amount of errors.<br/> <br/>This project aims to leverage such approximate storage of data to improve the reliability, performance and endurance of 3D NAND flash memories. The research will first construct reliability models to guide architecture designs. Then, mechanisms for achieving approximate storage through tolerating errors and approximate programming will be developed and evaluated. Finally, new techniques will be developed to exploit approximation for mitigating disturbance, cell-to-cell interference, retention loss, and for improving performance as well as endurance of a 3D flash memory. The success of this project will fuel the continuous scaling of the mainstream storage technology. It will also enable pervasive adoption of 3D NAND flash memories for embedded systems, personal computers and clouds, which will have a tremendous drive for the innovations in storage industry. Trained students will become future taskforce to continue technology innovations in computing."
"1821431","SHF: Small: Collaborative Research: Coupling Computation and Communication in FPGA-Enhanced Clouds and Clusters","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","10/01/2017","08/09/2018","Anthony Skjellum","TN","University of Tennessee Chattanooga","Standard Grant","Almadena Chtchelkanova","05/31/2020","$202,192.00","","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","2878, 7798","7923, 7924, 7942, 9251","$0.00","The introduction of Field Programmable Gate Arrays (FPGAs) to accelerate clusters of servers in datacenters and clouds provides a great, immediate opportunity to leverage a new technology in high-end computing. With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore?s Law world. Since the hardware adapts to the application higher efficiency can be achieved, and since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Large-scale communication can consequently proceed with both higher bandwidth, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. The proposed design allows for useful processing while data is in flight in the network resulting in reduced software overhead in parallel middleware and reduced network congestion. The key tenets of the research are to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially software overhead. <br/>The research project, FC5 (an FPGA framework for coupling communication and computation in clouds and clusters) has several thrusts. First, hardware support for FC5 and investigation of methods of configurability in FC5 to reduce communication latency and support computing in the network are studied. A second outcome is a prototype version of the Open MPI open source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations. Third, proof-of-concept versions of multiple FC5 software models, including direct hardware access, a transparent MPI-in-OpenCL, and an API-based mechanism that exposes essential functionality. Finally, because FC5 is evolving rapidly with major new announcements expected imminently, continued refinement is essential. At least two model applications, Molecular Dynamics and Map-Reduce, will be used as test cases. <br/>With the continued consolidation of computing services into the cloud, the potential broader impact is to increase both the scale and availability of parallel applications. The broad range of uses of cloud and cluster computing for commercial, government, and academic applications means that acceleration offered will have a widespread impact applicable across many sectors. The growing acceptance of high performance computing in industry (e.g., fast machine learning) is one particular potential commercial sector that will be enhanced by this project."
"1514275","CIF: Medium: Fundamental Properties of Millimeter Wave Networks: Signal, Interference, and Connectivity","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2015","05/31/2017","Jeffrey Andrews","TX","University of Texas at Austin","Continuing grant","Phillip Regalia","05/31/2020","$999,923.00","Robert Heath, Francois Baccelli","jandrews@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7797","7924, 7935","$0.00","Cellular communication networks have become one of society's most important and complex technologies. Mobile data usage has been approximately doubling every year since about 2008, which corresponds to a 1,000 times increase over a decade.  To meet this insatiable demand, much more bandwidth is required for cellular systems.  This requires going to (much) higher frequencies, since there is very little unused spectrum below about 10 GHz, especially at peak times in urban areas.  Significant amounts of lightly used spectrum is available in the ""millimeter wave"" (mmW) spectrum, defined here as being above 25 GHz.  There are numerous fundamental technical challenges in mobile data communication above 25 GHz, and the goal of this project is to explore these fundamental challenges and build new tools to overcome them.  The research undertaken should considerably impact the telecommunications industry and society as a whole, by enabling an entirely new paradigm for cellular communication networks.  In addition to theoretical contributions, the investigators will pursue an aggressive technology transition plan through their numerous government and industry partners. <br/><br/>This project will develop new mathematical and analysis tools that uncover the potential of mmW cellular networks. The research agenda is built on the belief that the wealth of tools developed for lower frequency systems are insufficient to capture key mmW signal propagation features, specifically high directionality and blockage.  The research is structured as three inter-related thrusts, each with several proposed research tasks: mmW signal strength, mmW interference, and the mmW network connectivity. The research tasks are unified around several technical themes that cut across all three thrusts: (1) modeling mmW networks in 3D, including the obstacles for which statistical blocking models will be developed and validated with real building data; (2) accounting for signal and interference correlation in performance analysis, which will be significant in mmW due to their main randomizing factors being blocking and beam alignment rather than fading and shadowing; (3) accounting for and addressing the possibly severe effects of mobility on beam alignment and network connectivity. <br/><br/>The developed theories will be used to devise useful models, parametrized by real building data to facilitate fast performance evaluation. Features of practical mmW transceivers like beam adaptation, mobility, and interference cancellation will be included and used to study key design tradeoffs. The new mathematical framework developed in this proposal will allow transparent and comprehensive performance analysis of mmW cellular systems, and enable the development and fair comparison of new communication techniques."
"1763307","AF: Medium: Collaborative Research: Foundations of Fair Data Analysis","CCF","SPECIAL PROJECTS - CCF","07/01/2018","06/12/2018","AARON ROTH","PA","University of Pennsylvania","Continuing grant","Tracy Kimbrel","06/30/2022","$439,621.00","Rakesh Vohra, Sampath Kannan","aaroth@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","2878","075Z, 7924, 7926","$0.00","Machine learning algorithms increasingly make or inform critical decisions that affect peoples' every day lives.  For instance, algorithms make decisions pertaining to hiring, college admissions, credit card and mortgage approvals, sentencing and parole of the incarcerated, first-responder deployment, and what advertisements and search results a user sees on the internet.  An attractive feature is that these algorithms can efficiently process large amounts of data in making these decisions, thus hopefully improving economic and social efficiency.  Because such decisions are so consequential, their fairness has become a matter of increasing concern.  It has been argued that automation, by removing the human element, guarantees fairness, but this is not so -- several empirical studies have demonstrated that automation is no panacea.  Further, the reasons for unfairness and discrimination can be complex and non-obvious.  This project will study the frictions that may cause unfairness in algorithmic decision making, and the costs of mitigating unfairness -- that is, quantitative trade-offs between fairness and other desiderata, including accuracy, computational efficiency, and economic efficiency.<br/><br/>Specifically, this project will study frictions to fairness arising from several factors.  There may not be sufficient data about minority populations.  There can be feedback loops arising from the fact that observations can only be made on an individual if a risky action is taken, e.g., the person is granted a loan, or hired.  Decision makers can be myopic, choosing to maximize short-term gains rather than exploring riskier options that may pay off in the long run.  Economic frictions include self-confirming equilibria---differing subjective perceptions of opportunities leading to choices by individuals and communities which sustain those perceptions, and competition among classifiers (for example, credit agencies) leading to less accurate qualifiers in equilibrium.  Finally, the problem of finding fair and accurate classifiers can be computationally intractable.  This project will seek ways to mitigate the unfairness arising from these frictions.  It will study the cost of incentivizing myopic agents to explore and examine the short-term costs of such incentives, and their long-term impact on fairness.  It will also seek to design computationally tractable classifiers that achieve provably good approximations for fairness and accuracy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755656","CRII: CIF: Crowdsourcing-aware Learning","CCF","COMM & INFORMATION FOUNDATIONS","04/01/2018","12/18/2017","Nihar Shah","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","03/31/2020","$174,934.00","","nihars@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7935, 8228","$0.00","Machine learning has significantly advanced the state of the art in a variety of applications. These successes have required massive labeled datasets for training machine learning algorithms. The collection of these labeled datasets usually involves human annotation. For instance, the training labels for supervised learning algorithms are often obtained through ""crowdsourcing"" where people label the data over the Internet in exchange for monetary incentives. Most learning algorithms, however, are agnostic of this human-labeling process. This project designs improved learning algorithms by incorporating the ""human"" aspect of the data collection process in the machine learning objective.<br/><br/>In more detail, this project considers supervised binary classification tasks where the labels for the training data are obtained from people. The research involves design of learning algorithms that jointly consider the human collection process -- including the interfaces and incentives available to the human labelers -- and the overall learning objective. Theoretical guarantees of optimality are derived and compared with guarantees for algorithms which are agnostic of the human component. The algorithms and guarantees are based on models of human behavior from psychology, such as permutation-based models, that allow for maximal accuracy while making minimal assumptions on how the human labelers behave. The theoretical results are corroborated with practical implementations (open sourced) and real-world experiments (data freely available online).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718633","SHF:Small:Neuromorphic Architectures for On-line Learning","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/10/2017","Tarek Taha","OH","University of Dayton","Standard Grant","Sankar Basu","07/31/2020","$439,998.00","Guru Subramanyam","ttaha1@udayton.edu","300 COLLEGE PARK AVE","DAYTON","OH","454690104","9372292919","CSE","7798","7923, 7945","$0.00","With the increasingly large volumes of data being generated in all fields, it is difficult to draw meaningful understanding from the information. Deep learning is a collection of new algorithms that have been developed recently to make it easier to understand large volumes of data. These algorithms typically have two phases of operation: training and inference. In the training phase, the algorithms learn how to interpret data, while in the inference phase the trained algorithms process new data based on what they learned earlier. Training generally requires high power computing. This project will develop novel computing systems for training that require low power consumption. This makes them suitable for portable systems, and hence could enable the design of significantly smarter products that learn continuously from their environment and are able to better interact with the environment. The proposed work includes outreach to K-12 students and also training of undergraduate, graduate, and minority students. <br/><br/>The novel computing systems to be developed will employ memristor circuits to accelerate the training phase of deep learning algorithms. Memristors are nanoscale resistive memory devices. The PIs will develop and characterize the memristors and then design deep learning circuits for training based on the characterized memristor devices. The PIs will also design computing systems based on the training circuits to be developed. These computing systems will have applications in a broad range of fields, including low power consumer products and high power clusters of computers."
"1421193","SHF: SMALL: NONSTANDARD COMPUTATIONAL MODELS OF LINEAR LOGIC","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2014","05/11/2016","Stephan Zdancewic","PA","University of Pennsylvania","Standard Grant","Anindya Banerjee","08/31/2019","$458,000.00","","stevez@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7798","7923, 7943, 9251","$0.00","TItle: SHF: Small: Nonstandard Computational Models of Linear Logic<br/><br/>Much of the interesting software being developed today relies on mathematical underpinnings that can best be expressed in terms of linear algebra (e.g. large scale matrices or graph data) and statistics (e.g. machine learning algorithms or ""big data"" analysis). Current programming languages aren't especially suited to working with such kinds of data, and so provide little built-in support to help scientists and software developers.  Conversely, many powerful mathematical techniques have been developed in the contexts of linear algebra and statistics, but those techniques have not been applicable to problems in programming language semantics.  This research project seeks to develop a theoretical foundation that connects the seemingly disparate topics of programming languages and these mathematical domains.<br/><br/>The technical approach taken in this work is to develop ""nonstandard"" models of linear logic, which is an expressive and low-level framework for understanding program semantics.  The intellectual merits are found in developing novel connections between well-established, but distinct, mathematical domains, connecting proof theory and program semantics to representations in vector spaces and categories of probability measures.  The broader impacts of this work are best understood through its potential long-term applications, which include: smooth integration of programming language constructs for working with numerical data (like Matlab) with support for higher-order functions and abstract datatypes; new techniques for proof search based on numerical methods; and, better programming languages for expressing machine learning or probabilistic algorithm."
"1840860","CIF: EAGER: Statistical Inference and Decision-Making With Sequential Samples","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2018","07/21/2018","Osman Yagan","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","07/31/2019","$100,527.00","Gauri Joshi","oyagan@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7916, 7935","$0.00","The modern world is rich with diverse sources of data that provide invaluable insights into underlying random phenomena. The data, however, generally provide only indirect or imprecise information about the latent phenomena due to measurement limitations or privacy protections. This project develops efficient algorithms to use sequential samples to infer a hidden random phenomenon and use this knowledge to make decisions. Outcomes of the project will improve the efficiency and accuracy of data-driven decision-making and inference in a wide range of applications such as marketing and recommendation systems, cloud computing, manufacturing, and health care. The investigators will publish the research outcomes to broad academic and professional audiences and incorporate them into teaching curricula via graduate and undergraduate courses.<br/><br/>The framework studied in this project consists of a hidden random variable (or, a random vector) that can be indirectly sampled by choosing one of several measurement mechanisms (referred to as arms). Upon choosing one of the arms, an arbitrary function of a realization of the hidden random variable is observed, instead of a direct sample. Within this framework, the investigators pursue problems including i) maximizing the reward obtained by sampling different arms in a correlated multi-armed bandit setting; and ii) estimating the probability distribution of the hidden random variable using minimum number of samples. These research thrusts will be studied with three main goals: 1) understanding the fundamental limits of the problem via bounds on the cumulative regret, and the error in the estimated distribution; 2) designing efficient sampling algorithms that meet the fundamental limits; and 3) validating the proposed algorithms on real-world datasets. The project deviates from the classic multi-armed bandit framework due to the correlation between arms and from the classic statistical inference due to the sequential and multi-fidelity nature of the data generation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1518732","SHF:Large:Collaborative Research: Inferring Software Specifications from Open Source Repositories by Leveraging Data and Collective Community Expertise","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","06/17/2015","Vasant Honavar","PA","Pennsylvania State Univ University Park","Standard Grant","Sol Greenspan","06/30/2019","$319,511.00","","vhonavar@ist.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7798","7925, 7944","$0.00","Today individuals, society, and the nation critically depend on software to manage critical infrastructures for power, banking and finance, air traffic control, telecommunication, transportation, national defense, and healthcare. Specifications are critical for communicating the intended behavior of software systems to software developers and users and to make it possible for automated tools to verify whether a given piece of software indeed behaves as intended. Safety critical applications have traditionally enjoyed the benefits of such specifications, but at a great cost.  Because producing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available, such specifications are largely unavailable. The lack of specifications for core libraries and widely used frameworks makes specifying applications that use them even more difficult. The absence of precise, comprehensible, and efficiently verifiable specifications is a major hurdle to developing software systems that are reliable, secure, and easy to maintain and reuse. <br/><br/>This project brings together an interdisciplinary team of researchers with complementary expertise in formal methods, software engineering, machine learning and big data analytics to develop automated or semi-automated methods for inferring the specifications from code. The resulting methods and tools combine analytics over large open source code repositories to augment and improve upon specifications by program analysis-based specification inference through synergistic advances across both these areas. <br/><br/>The broader impacts of the project include: transformative advances in specification inference and synthesis, with the potential to dramatically reduce, the cost of developing and maintaining high assurance software; enhanced interdisciplinary expertise at the intersection of formal methods software engineering, and big data analytics; Contributions to research-based training of a cadre of scientists and engineers with expertise in high assurance software."
"1552784","CAREER: experimental design and model reduction in systems biology","CCF","ALGORITHMIC FOUNDATIONS, COMPUTATIONAL BIOLOGY","01/01/2016","02/11/2019","Peng Qiu","GA","Georgia Tech Research Corporation","Continuing grant","Mitra Basu","12/31/2020","$348,196.00","","peng.qiu@bme.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796, 7931","1045, 7931","$0.00","Mathematical modeling is a crucial tool for understanding behaviors of complex biological systems. In systems biology, mathematical models are often highly complex, accounting for interactions among genes and proteins in the system. In contrast, the amount of experimental data is almost always limited. This information gap between complex model and limited data presents challenges to model analysis, and obscures the insights into the key controlling mechanisms underlying complex biological systems. Two intuitive strategies to close the information gap are to obtain more data and to simplify the model. To obtain more data, experimental design is needed to answer the following question: among all possible new experiments, which one will bring in the maximal amount of new information? To reduce the model, one needs to answer the question: among all possible simplifications (e.g., remove or combine parameters or variables), which one is the most appropriate? This project aims to develop novel computational algorithms to answer both questions.  <br/><br/>Experimental design is a question that biologists consider on a daily basis. Model reduction answers a fundamental biology question of how to identify key mechanisms underlying complex biological systems. Rigorous computational algorithms for these two questions are expected to greatly benefit experimental studies of complex biological systems, provide insights that are complementary to biologists' expert intuitions, and bring new knowledge to the field, which is exactly the research focus of this project. The proposed research will also lead to educational activities, such as development of computational tutorials and workshops for biologists, development of novel interdisciplinary courses at both undergraduate and graduate levels, as well as research supervision and community outreach to high-school, undergraduate and graduate students. <br/><br/>Although experimental design and model reduction are two quite different problems, this project will develop a unified computational framework and geometric interpretation to tackle both problems in a systematic way. The key idea is to consider a mathematical model as a manifold, and use its geometric features to guide experimental designs and model reduction. The proposed research is organized into two thrusts. Thrust 1 aims to develop computational algorithms to identify experiments that minimizes parameter uncertainty, establish geometric interpretations. The proposed algorithms will be implemented in open-source software, which will be offered to researcher from diverse disciplines and will also serve as education tools. Thrust 2 aims to develop model reduction algorithms based on manifold learning, explore their ability to identify mechanisms for the adaptation behavior, and apply the proposed algorithms to study protein signaling pathways and cell differentiation processes. The research results will be disseminated to the broad audience via peer reviewed publications, conference, seminars, and workshop tutorials."
"1718195","CIF:Small: A Tensor-based Framework for Reliable Radio Cartography","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2017","06/28/2017","Nazanin Rahnavard","FL","University of Central Florida","Standard Grant","Phillip Regalia","07/31/2020","$395,283.00","","nazanin@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7797","7923, 7935, 7936, 9102","$0.00","The goal of this project is to devise an efficient framework for achieving radio frequency (RF) spectrum awareness through generating reliable and dynamic radio environment maps (REMs). This is a notable step towards the proliferation of realistic cognitive radio networks (CRNs), which enable spectrum sharing to alleviate the problem of spectrum scarcity. This efficient use of spectrum empowers ever-increasing applications and services with a great impact on national health, welfare, public safety, and economic growth. In addition, the developed data analysis tools provide a distinctive solution to high-dimensional signal sampling and processing, enabling further development in a wide range of applications, such as big data, Internet of Things, and wireless sensor networks, which can promote social and economic progress. This project integrates research and education through new course development and revisions, and involving underrepresented minorities, graduate, and undergraduate students in research.<br/><br/>This research bridges the gap between the theoretical research in tensor data analysis and wireless communications and networking to facilitate spectrum awareness. The investigators will develop a tensor-based framework to generate dynamic and reliable REMs that include RF signal power distribution over space, time and frequency. Tensor-based analysis facilitates the integration of the inherent properties and data structures that exist in CRNs as well as the prior knowledge of the behavior of the primary network. Bayesian and structure-based tensor decomposition is investigated, for which existence and uniqueness conditions are studied. The outcome of the decomposition is employed to obtain a model-based interpolation of sensor readings and to generate the REM. Corresponding to an REM, a reliability map will be created and utilized for optimal joint spectrum sensor and channel selection."
"1563750","CSR: Medium: Collaborative Research: Wizard: Exploiting Disk Performance Signatures for Cost-Effective Management of Large-Scale Storage Systems","CCF","Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","08/15/2016","05/15/2017","Song Fu","TX","University of North Texas","Standard Grant","Yuanyuan Yang","07/31/2020","$316,000.00","","song.fu@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","7354, 7798","7924, 7941, 9251","$0.00","The tremendous advances in low-cost, high-capacity magnetic hard disk drives, flash-based solid state drives and non-volatile memory have been among the key factors supporting big data applications and various computing-storage services that the modern society deeply relies on. However, storage drives are reported to be the most commonly replaced hardware components because of failures. This causes service downtime and even data loss, costing enterprises multi-trillion dollars per year. Existing disk failure management approaches are mostly reactive and incur high overheads; they do not provide a cost-effective solution to managing large-scale production storage systems. The goal of this project is to achieve a deep understanding of the reliability of the real-world storage systems, and to develop a cost-effective data and storage resource management system for reliability enhancement.<br/> <br/>The investigators' approach to building reliable, large-scale storage systems is carefully designed to support storage health monitoring, modeling, prediction and proactive recovery in a systematic fashion. In particular, they first categorize and model storage failures to derive disk performance signatures and explore disk performance signatures to forecast occurrences of disk failures. They then characterize I/O workload dependency in disk performance degradation and integrate the performance signatures of heterogeneous disk devices to effectively reconfigure and manage storage resources. Furthermore, the project will provide easy-to-use APIs for storage users and developers to employ the developed tools and techniques for proactive data rescue and preventive disk reliability enhancement. Finally, the project provides excellent opportunities for training graduate students, especially minority and female students, and for developing new curriculum materials on reliable storage systems."
"1552131","CAREER:Information-Theoretic Foundations of Community Detection and Graphical Channels","CCF","COMM & INFORMATION FOUNDATIONS","02/15/2016","03/02/2018","Emmanuel Abbe","NJ","Princeton University","Continuing grant","Phillip Regalia","01/31/2021","$393,233.00","","eabbe@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7797","1045, 7935","$0.00","The main goal of this project is to establish the fundamental limits of community detection. In virtually all applications dealing with networks and large data sets, one wishes to extract sub-groups of data points that are similar, i.e., communities. While community detection techniques are expanding daily with practical successes, relatively less attention has been paid to the fundamental limits, and consequently to where current algorithms stand. By establishing the fundamental limits of community detection, this project offers a novel take on community detection algorithms, and expands information theory in a prominent area where it can naturally flourish. The project will work with real data sets from social and biological networks. In particular, it develops a new initiative to extract communities in Hi-C genomic data, contributing to unveil the 3D folding structure of DNA.<br/><br/>The project will focus in particular on the stochastic block model, a canonical model for community detection. The investigator's recent work leverages information theory to provide the first necessary and sufficient conditions for exact recovery in the stochastic block model, and an efficient algorithm achieving the limit. This opens the door to a new perspective on community detection, which is developed in this project by casting community detection as unorthodox error-control coding problems. In this context, new types of f-divergences are expected to play a key role, analogous to the Kullback-Leibler divergence in Shannon's channel coding theorem, while other weaker recovery requirements may rely on unorthodox broadcasting problems, graph entropic inequalities, and information-estimation problems. This makes the study of community detection a rich area connecting information theory, machine learning and networks; less focused on ergodic results; and more interlaced with graph theory and spectral analysis. In particular, this project will show how these problems, as well as more general low-rank approximation problems, can be studied under the novel and unifying theme of graphical channels."
"1617286","CIF: Small: Collaborative Research: Analytics on Edge-labeled Hypergraphs: Limits to De-anonymization","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/28/2016","Prateek Mittal","NJ","Princeton University","Standard Grant","Phillip Regalia","06/30/2019","$250,000.00","","pmittal@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7797","7797, 7923, 7935","$0.00","Data analytics is a rapidly growing field, aided by the availability of huge amounts of data and significant computing power. The immense potential of data analytics to provide benefits to the society in application areas such as health, economics, and finance, is reliant on the fundamental and urgent challenge of protecting privacy of users. In this project, new theoretical paradigms and approaches to address privacy vulnerability of users in network environments in presence of big data are studied. The vulnerability results from the indigenous structural dependencies in the network as well as the presence of exogenous auxiliary information outside of the network that permits deanonymization of the users. This project has transformative potential to impact a broad class of applications where user privacy is critical. The project?s inherently inter-disciplinary nature and real-world technological potential complements the investigators? on-going efforts to engage more students (especially women and minorities) to study topics at the intersection of application and quantitative reasoning in the STEM disciplines. <br/><br/>The research is divided into three thrusts: (1) Development of information-theoretic converses for deanonymization problem in random edge-labeled hyper-graphs for adversaries with access to correlated information sources. Such converses enable deriving necessary conditions under which the adversary cannot deanonymize the system, no matter how much computational power or storage is available. (2) Research practical achievable schemes: Besides tight (but not necessarily efficient) achievable schemes required for calibrating the converses, the design of practical deanonymization algorithms to quantify how much attackers can learn when the released datasets do not meet the necessary conditions of the converse, are explored. (3) Real-world evaluations: The performance of the algorithms and their practical applicability are evaluated on real world datasets."
"1253918","CAREER: Harnessing Interference Structure in Networks","CCF","COMM & INFORMATION FOUNDATIONS, COMM & INFORMATION THEORY","02/01/2013","04/12/2017","Bobak Nazer","MA","Trustees of Boston University","Continuing grant","Phillip Regalia","01/31/2020","$496,102.00","","bobak@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7797, 7935","1045, 7935","$0.00","Wireless networks are the fabric of the mobile Internet. High-speed, ubiquitous wireless access is increasingly an enabling technology for important applications ranging from communication to commerce, medicine, and education. It is thus critical to create a pathway for sustainable wireless network growth in terms of the number of users and their data rates. A major challenge facing this effort is interference: since the wireless channel is a shared medium, simultaneous transmissions interfere with one another. This phenomenon is conventionally viewed as harmful by theorists and practitioners alike, and modern wireless protocols are designed to avoid interference. Within this paradigm, the wireless channel is a fixed resource with a finite capacity, meaning that as more users join a network, the maximum data rate per user plummets. However, interfering signals are not just additional noise; rather, they represent data from other users and possess considerable structure which can be exploited.<br/><br/>This research pursues a novel approach to multi-user communication based on harnessing the algebraic structure of interference. The key insight is that it is possible for a receiver to first decode linear combinations of all transmitted codewords and only afterwards solve for its desired codeword in the digital domain. In many important network scenarios, this powerful technique yields significantly higher data rates compared to conventional approaches. This project leverages this technique to uncover the fundamental capacity limits of canonical wireless networks subject to interference as well as establish architectural principles for coding strategies that can efficiently approach these limits. It draws upon modern tools from combinatorial optimization to solve the resource allocation problems that emerge when this technique is applied to complex wireless networks, such as distributed multiple antenna systems. More broadly, this research lays the foundation for an algebraic network information theory to tackle challenging problems in decentralized information processing, compression, and decoding. This project incorporates several outreach efforts including interactive presentations on cellular communication for high school students, tutorials, and workshop organization."
"1563728","CSR: Medium: Collaborative Research: Wizard: Exploiting Disk Performance Signatures for Cost-Effective Management of Large-Scale Storage Systems","CCF","Computer Systems Research (CSR","08/15/2016","08/05/2016","Weisong Shi","MI","Wayne State University","Standard Grant","Yuanyuan Yang","07/31/2020","$400,000.00","Patrick Gossman","weisong@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","7354","7924","$0.00","The tremendous advances in low-cost, high-capacity magnetic hard disk drives, flash-based solid state drives and non-volatile memory have been among the key factors supporting big data applications and various computing-storage services that the modern society deeply relies on. However, storage drives are reported to be the most commonly replaced hardware components because of failures. This causes service downtime and even data loss, costing enterprises multi-trillion dollars per year. Existing disk failure management approaches are mostly reactive and incur high overheads; they do not provide a cost-effective solution to managing large-scale production storage systems. The goal of this project is to achieve a deep understanding of the reliability of the real-world storage systems, and to develop a cost-effective data and storage resource management system for reliability enhancement.<br/> <br/>The investigators' approach to building reliable, large-scale storage systems is carefully designed to support storage health monitoring, modeling, prediction and proactive recovery in a systematic fashion. In particular, they first categorize and model storage failures to derive disk performance signatures and explore disk performance signatures to forecast occurrences of disk failures. They then characterize I/O workload dependency in disk performance degradation and integrate the performance signatures of heterogeneous disk devices to effectively reconfigure and manage storage resources. Furthermore, the project will provide easy-to-use APIs for storage users and developers to employ the developed tools and techniques for proactive data rescue and preventive disk reliability enhancement. Finally, the project provides excellent opportunities for training graduate students, especially minority and female students, and for developing new curriculum materials on reliable storage systems."
"1745331","Hawaiian Workshop on Parallel Algorithms and Data Structures","CCF","SPECIAL PROJECTS - CCF","09/01/2017","07/30/2017","Nodari Sitchinava","HI","University of Hawaii","Standard Grant","Tracy Kimbrel","08/31/2019","$40,346.00","","nodari@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","CSE","2878","7556, 7934, 9150","$0.00","This award will support a workshop on the topic of parallel algorithms and data structures to be held at the University of Hawaii at Manoa on December 4-8, 2017.  This workshop will bring together researchers in parallel algorithms and from broader algorithms community to present recent advances in modeling modern parallel systems and to perform research solving open problems in parallel algorithms and data structures. Students will be exposed to the research environment and will have an opportunity to engage in research. The University of Hawaii is a Native Hawaiian-serving institution, and the workshop will promote participation by underrepresented groups in computing."
"1535989","AitF: Full: Collaborative Research: Graph-theoretic algorithms to improve phylogenomic analyses","CCF","Algorithms in the Field","09/01/2015","08/12/2015","Satish Rao","CA","University of California-Berkeley","Standard Grant","Tracy J. Kimbrel","08/31/2019","$359,997.00","","satishr@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7239","012Z","$0.00","Understanding the history of life on earth ? how species evolved from their common ancestor ? is a major goal of biological research. These evolutionary trees are very hard to construct with high accuracy, because nearly all of the most accurate approaches require the solution to computationally hard optimization problems. Furthermore, research has shown that the evolutionary tree for a single gene can be different from the evolutionary tree for the species, and current methods do not provide adequate accuracy on genome-scale data. As a result, large evolutionary trees, covering big portions of ?The Tree of Life?, are very difficult to compute with high accuracy. This project will develop methods that can enable highly accurate species tree estimation. The key approach is the development of novel divide-and-conquer strategies, whereby a dataset is divided into overlapping subsets, species trees are constructed on the subsets, and then the subset species trees are merged together into a tree on the full dataset. These approaches will be combined with powerful statistical estimation methods, to potentially transform the capability of evolutionary biologists to analyze their data. This project will also provide open source software for the new methods that are developed, and provide training in the use of the software to biologists at national meetings. The project will also contribute to interdisciplinary training for two doctoral students, one at Illinois and one at Berkeley, and course materials for computational biology will be made available online.<br/> <br/>Understanding evolution, and how it has operated on species and on genes, is a major part of biological data analysis. Statistical estimation approaches often provide the best accuracy, but cannot scale to dataset sizes that are required for modern biology. In addition, species tree estimation is challenged by the heterogeneity of evolutionary trees across the genome, and no current methods are able to provide highly accurate species trees for genome-scale data. These challenges make it essential that new methods be developed in order to make highly accurate large-scale evolutionary tree estimation possible under these complex evolutionary scenarios.  This project will develop novel algorithmic strategies to address three key problems: supertree estimation, species tree estimation in the presence of gene tree heterogeneity, and scaling statistical methods to large datasets. In addition to developing graph-theoretic algorithms, the project team will establish mathematical guarantees for these methods using chordal graph theory and probabilistic analysis, under stochastic models of gene and sequence evolution."
"1619216","CIF: Small: Collaborative Research: Analytics on Edge-labeled Hypergraphs: Limits to De-anonymization","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/28/2016","Negar Kiyavash","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","06/30/2019","$250,000.00","","negar.kiyavash@gatech.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7797, 7923, 7935","$0.00","Data analytics is a rapidly growing field, aided by the availability of huge amounts of data and significant computing power. The immense potential of data analytics to provide benefits to the society in application areas such as health, economics, and finance, is reliant on the fundamental and urgent challenge of protecting privacy of users. In this project, new theoretical paradigms and approaches to address privacy vulnerability of users in network environments in presence of big data are studied. The vulnerability results from the indigenous structural dependencies in the network as well as the presence of exogenous auxiliary information outside of the network that permits deanonymization of the users. This project has transformative potential to impact a broad class of applications where user privacy is critical. The project?s inherently inter-disciplinary nature and real-world technological potential complements the investigators? on-going efforts to engage more students (especially women and minorities) to study topics at the intersection of application and quantitative reasoning in the STEM disciplines. <br/><br/>The research is divided into three thrusts: (1) Development of information-theoretic converses for deanonymization problem in random edge-labeled hyper-graphs for adversaries with access to correlated information sources. Such converses enable deriving necessary conditions under which the adversary cannot deanonymize the system, no matter how much computational power or storage is available. (2) Research practical achievable schemes: Besides tight (but not necessarily efficient) achievable schemes required for calibrating the converses, the design of practical deanonymization algorithms to quantify how much attackers can learn when the released datasets do not meet the necessary conditions of the converse, are explored. (3) Real-world evaluations: The performance of the algorithms and their practical applicability are evaluated on real world datasets."
"1854737","SHF: Small: Collaborative Research: LDPD-Net: A Framework for Accelerated Architectures for Low-Density Permuted-Diagonal Deep Neural Networks","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2018","10/25/2018","Bo Yuan","NJ","Rutgers University New Brunswick","Standard Grant","Sankar Basu","09/30/2021","$224,997.00","","bo.yuan@soe.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7798","7923, 7945","$0.00","Deep learning has emerged as an important form of machine-learning where multiple layers of neural networks can learn the system function from available input-output data. Deep learning has outperformed traditional machine-learning algorithms based on feature engineering in fields such as image recognition, healthcare, and autonomous vehicles. These are widely used in cloud computing where large amount of computational resources are available. Deep neural networks are typically trained using graphic processing units (GPUs) or tensor processing units (TPUs). The training time and energy consumption grow with the complexity of the neural network. This project attempts to impose sparsity and regularity as constraints on the structure of the deep neural networks to reduce complexity and energy consumption by orders of magnitude, possibly at the expense of a slight degradation in the performance. The impacts lie in the formulation of a new family of structures for neural networks referred to as Low-Density Permuted Diagonal Network or LDPD-Net. The approach will enable the deployment of deep neural networks in energy-constrained and resource-constrained embedded platforms for inference tasks, including, but not limited to, unmanned vehicles/aerial systems, personalized healthcare, wearable and implantable devices, and mobile intelligent systems. In addition, the design methodology/techniques developed in this project can facilitate investigation of efficient computing of other matrix/tensor-based big data processing and analysis approaches. These approaches may also find applications in data-driven neuroscience and data-driven signal processing. In addition to graduate students, the project will involve undergraduates via senior design projects and research experiences for undergraduates. The results of the project will be disseminated to the broader community by publications, presentations, talks at various industries and other academic institutions. <br/><br/>The main barriers to wide adoption of deep learning networks include computational resource constraints and energy consumption constraints. These barriers can be relaxed by imposing sparsity and regularity among different layers of the deep neural network. The proposed low-density permuted-diagonal (LDPD) network can lead to orders of magnitude reduction in computation complexity, storage space and energy consumption. The LDPD-Net will not be retrained by first training a regular network and then only retaining the weights corresponding to the LDPD-Net. Instead, the proposed network will be trained from scratch. The proposed LDPD-Net can enable scaling of the network for a specified computational platform. The proposed research has three thrusts: 1) develop novel resource-constrained and energy-constrained inference and training systems;  2) develop novel efficient hardware architectures that can fully exploit the advantages of the LDPD-Net to achieve high performance; and 3) perform novel software and hardware co-design and co-optimization to explore the design space of the LDPD-Net. Using these, the efficacy of the proposed LDPD-net will be validated and evaluated, via software implementations on high-performance systems, low-power embedded systems, and a hardware prototype on FPGA development boards.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717154","AF:   Small:   Collaborative Research:   Distributed Quasi-Newton Methods for Nonsmooth Optimization","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/30/2017","Petros Voulgaris","IL","University of Illinois at Urbana-Champaign","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2020","$137,000.00","","voulgari@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7796","7923, 7933","$0.00","Optimization, which finds the inputs to a mathematical function that produce the minimum output, is a workhorse algorithm behind many of the advances in smart devices or applications in the cloud. As data gets larger and more distributed, new ideas are needed to maintain the speed and accuracy of optimization. Operator splitting, which expresses the function to minimize as the sum of two convex functions, one of which is smooth and the other non-differentiable, is an idea that has produced to new first-order optimization methods.  This project explores operator splitting with second-order optimization methods, which have faster convergence to the minimum.  The focus is on large, distributed, and streaming data sets, so that the resulting general-purpose numerical solvers and embedded systems implementations can support optimization in cyberphysical systems and the Internet-of-Things.   The project has as priority the active engagement and training of students and researchers, with specific emphasis on the inclusion of women and under-represented minority groups. This project not only involves collaboration across three top-tier American universities, but also with European research institute, KU Leuven. <br/><br/>In specific, this research project seeks to interpret existing methods for structured convex optimization (such as the celebrated ADMM algorithm) as gradient methods applied to specific functions arising from the original problem formulation, and  interpret of operator-splitting techniques as fixed point iterations for appropriately selected operators.  A key theoretical foundation is the introduction of new envelope functions (smooth upper approximations possessing the same sets of solutions) that can be used as merit functions for variable-metric backtracking line-search. To conclude, a principal focus of the project is to design distributed asynchronous methods applicable to large-scale multi-agent cyberphysical systems that involve big data and impose stringent real-time constraints for decision-making. In this purview, the goal is to deliver methods that will outperform current state-of-the-art in terms of (a) speed of computations, (b) scalability with big data sizes, (c) robustness to various types of uncertainty, and, most topically, (d) distributed asynchronous implementation over networks in real-time. The merits will be illustrated in the context of applications in signal processing, control, machine learning and robotics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823546","FoMR: Using Machine Learning to Design Next Generation Caches and Data Prefetchers","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/09/2018","Calvin Lin","TX","University of Texas at Austin","Standard Grant","Yuanyuan Yang","09/30/2021","$224,956.00","","lin@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","2878","2878, 7798, 7941","$0.00","While the general public typically focuses on processor clock speed as a measure of microprocessor performance, it is often the memory system that limits overall system performance, particularly for today's data-hungry uses of computing.  This project explores a transformative approach to designing computer hardware, particularly the memory system:  Rather than rely solely on human insight and intuition, this approach adapts and leverages machine learning techniques to explore larger and richer design spaces in a more thorough and systematic manner.  This approach enables hardware designers to consider more complex design factors than are currently possible.  <br/><br/>This project consists of two phases.  The first phase seeks to dramatically improve traditional memory system components, such as cache replacement policies and data prefetchers, which have been heavily studied but are now ripe for innovation with the use of machine learning. The second phase considers complicating factors such as criticality, the interaction among these components, and the use of new memory technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832309","CIF: Small: Collaborative Research: Fundamental Limits of Information Systems: A Computational Approach","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2017","04/26/2018","Chao Tian","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","07/31/2019","$191,300.00","","chao.tian@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7797","7797, 7923, 7935, 9150","$0.00","Information theory provides a general framework in which the fundamental limits of information systems (wired and wireless networks, cryptographic systems, data compression and storage systems, and others) can be meaningfully investigated. The investigation of such limits is traditionally conducted analytically, requiring in many cases substantial ingenuity. As modern information systems become more and more complex, such an approach becomes rather unwieldy. The main goal of this project is to build upon initial successes in applying a computational approach to characterize the fundamental limits of certain distributed storage systems, and to further understand what it takes for the proposed computational approach to be successful in the broad setting of big-data infrastructure.<br/><br/>The proposed computational approach is built upon the vantage point that the overall process of identifying and proving fundamental limits can be alternatively viewed as an optimization problem under the specific constraints of the system at hand and the general constraints of information measures. The difficulty lies in the fact that such optimization programs are usually very large for any practically relevant problems, and numerical results are difficult to interpret. This project pursues three major thrusts: 1) finding more efficient problem representations using symmetry and other constraints; 2) improving the optimization algorithms using domain knowledge; and 3) finding intelligent interpretations of the computed results through duality to facilitate engineering design. The results are expected to have measurable impacts on the following two disciplines: 1) Big-data infrastructure: The ongoing big-data movement calls for information systems with diverse reliability and functionality requirements. By identifying the fundamental limits of such systems, accurate performance evaluations and meaningful design guidelines may be assessed; and 2) Information theory: The proposed study will bridge the disciplines of computational optimization and information theory, and instill more computerized intelligence into information theory research."
"1527396","CIF: Small: Task-Cognizant Sparse Sensing for Inference","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2016","07/21/2016","Zhi Tian","VA","George Mason University","Standard Grant","Phillip Regalia","07/31/2019","$400,000.00","","ztian1@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7797","7797, 7923, 7936","$0.00","As big data applications continue to grow in size and number, it is of crucial importance to deal only with measurements that are informative for a specific inference task in order to limit the required sensing cost, as well as the related costs of storing or communicating the data. To this end, this project develops a new paradigm of compressive sensing for random processes, where the signal compression and reconstruction strategies are designed to be task-cognizant, hinging on useful signal statistics rather than the original random signals. This research leads to major sensing energy savings, and can benefit a plethora of energy-efficient sensing applications such as location-aware services, weather monitoring, spectrum monitoring, and radio astronomy.<br/><br/>The main goal of this project is to significantly reduce the cost of sensing as well as the related storage and communication requirements by offering innovative sensing approaches tailored to the inference task of interest. Different from existing compressive sampling, the proposed research leverages the inherent structures of signal statistical information in order to enable reliable inference from sparsely sampled data, rather than solely relying on signal sparsity to enable compression. As such, a new approach to sparse sensing is introduced for random processes, which reveals the fundamental limits of compression in relation to the degrees of freedom inherent to the underlying statistical structure, even in the absence of signal sparsity. Efficient inference techniques and deterministic compressive sampler designs are put forth to affect major savings in the sensing costs while achieving the desired inference quality. These basic results open up opportunities for efficient handling of data-intensive sensing applications in which inference from random processes is of foremost importance."
"1422134","AF: Small: Algorithms in Phylogenetics","CCF","INFORMATION TECHNOLOGY RESEARC, CROSS-EF ACTIVITIES, Systematics & Biodiversity Sci, ALGORITHMIC FOUNDATIONS","08/15/2014","08/14/2014","David Fernandez-Baca","IA","Iowa State University","Standard Grant","Mitra Basu","07/31/2019","$400,000.00","","fernande@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","1640, 7275, 7374, 7796","7923, 7931, 8750, 9150","$0.00","Phylogenetic trees, also known as phylogenies, represent the evolutionary history of sets of species.  The construction of such trees is an attempt to understand the origin of life.  Phylogenies have several practical uses.  For instance, they offer biologists a tool to predict gene function, by comparing and leveraging information among species related by evolution.  They also help to track changes in rapidly developing organisms such as viruses or cancer cells.  <br/><br/>Building phylogenetic trees from gene data poses major computational problems.  Every known formulation of the task leads to an intractable (NP-hard) problem.  Further, data is scarce: not every gene has been sequenced for all species, and not every species has the same set of genes.  Thus, to build comprehensive phylogenies, we must piece together information from different genes.  Complicating matters even further, gene evolution does not always conform to the history of speciation.  In fact, due to gene duplication and loss, and horizontal gene transfer, it is not rare for gene phylogenies to be in conflict with each other.<br/><br/>The proposed research addresses the mathematical and algorithmic problems that arise from data scarcity and conflict in phylogenetics.  Among the interrelated themes we will explore are the detection of conflict, the properties of dissimilarity measures between trees, and the identification of common patterns in collections of trees.  A portion of our work will center on the supertree problem, where the goal is to amalgamate a collection of trees for partially overlapping sets of species into a single phylogeny -a supertree- for the combined set of species.  We will investigate the properties a supertree method can and cannot satisfy, and develop mathematical characterizations of the solvable instances of important classes of supertree problems.  We will use the latter to identify parameterizations under which certain problems become tractable.  Our methods for mining phylogenetic databases for common patterns will enable biologists to identify well-supported evolutionary relationships among species and to spot ""rogue taxa""; i.e., species whose presence radically alters the result of a phylogenetic analysis. We will also investigate supertree construction and data mining for multi-labeled trees (trees where each species can occur multiple times), which are now commonplace in phylogenetic databases."
"1514357","AF: Medium: A High Performance Computing Foundation to Whole-Genome Prediction","CCF","ALGORITHMIC FOUNDATIONS, COMPUTATIONAL BIOLOGY","07/01/2015","07/25/2018","Jinbo Bi","CT","University of Connecticut","Continuing grant","Mitra Basu","06/30/2019","$750,000.00","Sanguthevar Rajasekaran","jinbo.bi@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7796, 7931","7924, 7931","$0.00","The premise of personalized medicine is based on prediction of an individual's genetic risk to disease. Modern animal and plant breeding programs select individuals or lines based on genotypic information which circumvents the costly process of progeny testing, leading to greater efficiency. In these scientific areas, the ability to translate genotypic information into a quantitative prediction of the risk to disease or breeding targets is a matter of utmost importance. To address the technical barriers in the prediction using a whole-genome sample of genetic markers, there is urgent need for new statistical models and high performance computing foundations that allow the concurrent use of millions of genetic markers and a large variety of variables describing a disease (or a breeding target). This project proposes to solve several such barriers by an integrative approach combining and developing techniques for data reduction, parallel computing and Bayesian inference. This interdisciplinary project provides educational opportunities for graduate and undergraduate students to get first-hand research experience in computational aspects of genomics data analysis.<br/><br/>This project aims to understand how genome-wide markers help to predict not-yet-specified phenotypes of individuals and how the total genetic contribution can be better estimated for a phenotype. The primary goals of the proposed research are to develop: (1) parallel algorithms to reduce data that comprises millions of genetic markers into lower dimensions; (2) sparse predictive modeling with correction for the uneven tagging issue due to linkage disequilibrium; (3) fast algorithms for multi-locus mapping problems; and (4) collaborative prediction methods to jointly predict multiple phenotypes. The proposed solutions will be tested in the analysis of large-scale biological data, including a dairy cattle database collected by US Department of Agriculture and a dataset aggregated from multiple genetic studies of human diseases. This project will yield user-friendly software tools that can be broadly deployed to biological research areas that study genetics of complex phenotypes. The validated methods and software will be disseminated through the PI's laboratory website."
"1717877","SHF:Small: Optimization of Parallel and Shared Cache Memory using the Footprint Theory","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","04/20/2018","Chen Ding","NY","University of Rochester","Standard Grant","Almadena Y. Chtchelkanova","07/31/2020","$472,262.00","","cding@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7798","7923, 7942, 9251","$0.00","Multicore processors bring a tremendous increase in computing power to personal, scientific, and business computing platforms. Most on-chip memory in these processors is devoted to shared cache, making it a major factor in performance.  A common practice is to improve cache performance by building a path of improvements through testing.  However, testing may take too many steps, it may not converge to a stable solution, and most importantly for large scale parallel programs, the solution is far from optimal since testing covers only a minuscule fraction of all possibilities.  <br/>Instead of testing, this research provides software developers and hardware engineers new tools based on modeling.  The research builds on the past NSF supported research which has developed the footprint theory for sequential applications. This work solves the new problems of data sharing and extends the locality theory to optimize the parallel code. Parallel systems require complex models, and this complexity is addressed by composable models to solve large scale problems with large scale modeling. The new tools include statistical models of both program and machine characteristics.  Program models include profiling analysis that derive the data and cache sharing in all thread combinations and data placements, in cache of all sizes. Cache models analyze the effect of many hardware designs including associativity, exclusivity, coherence, cache-way partitioning, and transient loads/stores. Combined use of these models enables parallel program optimization and improves thread and data placement."
"1816833","SHF:SMALL:Collaborative Research: Exploring Nonvolatility of Emerging Memory Technologies for Architecture Design","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2018","07/16/2018","Yuan Xie","CA","University of California-Santa Barbara","Standard Grant","Yuanyuan Yang","07/31/2021","$249,000.00","","yuanxie@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7798","7923, 7942","$0.00","In modern computers, by combining the speed of traditional cache technology, the density of traditional main memory technology, and the non-volatility of flash memory, a new class of emerging byte-addressable nonvolatile memories (NVMs) have great potential to be used as the universal memories of the future. Such memory types include technologies such as phase-change memory, spin-transfer-torque magnetoresistive memory, and resistive memory. As these emerging memory technologies mature, it is important for computer architects to understand their pros and cons in a comprehensive manner in order to improve the performance, power, and reliability of future computer systems incorporating these systems which will be used in various application domains. Yet, most of previous research on NVM architecture is focused only on the performance, power, and density benefits and how to overcome challenges, such as write overhead and wearout issues. The non-volatility characteristic of NVM technologies is not fully explored. Therefore, this project examines how to exploit the non-volatility characteristic that distinguishes the emerging NVM technologies from traditional memory technologies, and investigate new memory architecture design with novel applications.<br/><br/>The goal of this project is to advance the memory architecture design of various types of computer systems with a full exploration of the non-volatility characteristic of NVM technologies across architecture, system, and application levels. To this end, the project explores the design space of various types of computer systems, ranging from severs to embedded systems.  In particular, the project identifies and addresses design issues in nonvolatile cache architecture, re-architects main memory structure to leverage the non-volatility characteristic to improve system performance and energy consumption, supports persistent memory systems in various use cases with emerging NVM technologies, and studies near-data-computing techniques applied for these NVM technologies. The successful outcome of this research is expected to provide the design guidelines for enabling both large capacity and fast-bandwidth nonvolatile memory/storage, which are beyond the present state-of-the-art. Consequently, the research will spawn new applications involving the computation on the exascale of data, e.g., data mining, machine learning, visual or auditory sensory data recognition, bio-informatics, etc.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817077","SHF: Small: Collaborative Research: Exploring Nonvolatility of Emerging Memory Technologies for Architecture Design","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2018","07/16/2018","Jishen Zhao","CA","University of California-San Diego","Standard Grant","Yuanyuan Yang","07/31/2021","$250,861.00","","jzhao@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7798","7923, 7942, 9102","$0.00","In modern computers, by combining the speed of traditional cache technology, the density of traditional main memory technology, and the non-volatility of flash memory, a new class of emerging byte-addressable nonvolatile memories (NVMs) have great potential to be used as the universal memories of the future. Such memory types include technologies such as phase-change memory, spin-transfer-torque magnetoresistive memory, and resistive memory. As these emerging memory technologies mature, it is important for computer architects to understand their pros and cons in a comprehensive manner in order to improve the performance, power, and reliability of future computer systems incorporating these systems which will be used in various application domains. Yet, most of previous research on NVM architecture is focused only on the performance, power, and density benefits and how to overcome challenges, such as write overhead and wearout issues. The non-volatility characteristic of NVM technologies is not fully explored. Therefore, this project examines how to exploit the non-volatility characteristic that distinguishes the emerging NVM technologies from traditional memory technologies, and investigate new memory architecture design with novel applications.<br/><br/>The goal of this project is to advance the memory architecture design of various types of computer systems with a full exploration of the non-volatility characteristic of NVM technologies across architecture, system, and application levels. To this end, the project explores the design space of various types of computer systems, ranging from severs to embedded systems.  In particular, the project identifies and addresses design issues in nonvolatile cache architecture, re-architects main memory structure to leverage the non-volatility characteristic to improve system performance and energy consumption, supports persistent memory systems in various use cases with emerging NVM technologies, and studies near-data-computing techniques applied for these NVM technologies. The successful outcome of this research is expected to provide the design guidelines for enabling both large capacity and fast-bandwidth nonvolatile memory/storage, which are beyond the present state-of-the-art. Consequently, the research will spawn new applications involving the computation on the exascale of data, e.g., data mining, machine learning, visual or auditory sensory data recognition, bio-informatics, etc.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717602","CIF:Small:High Performance Memories that Integrate Coding and Computer Architecture","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","08/30/2017","Arthur Calderbank","NC","Duke University","Standard Grant","Phillip Regalia","08/31/2020","$470,000.00","Daniel Sorin","robert.calderbank@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7797","7923, 7935","$0.00","Part 1:<br/><br/>Researchers continue to develope new memory and storage technologies for computer systems, including improvements to Flash memory in solid-state drives, as well as novel technologies that have not yet reached the market.  These technologies offer new features---including greater storage density, or the ability to retain their data even without power---yet they also introduce new challenges, such as reduced lifetimes or the possibility that writing data into one part of the memory could disrupt values in other parts. Historically, imperfections in memory technologies have been overcome by encoding the information such that disrupted or lost data can be recovered.  However, existing techniques do not suffice for the newly emerging memory technologies that have different imperfections and divergent usage models.<br/>  <br/>This project is a collaboration between researchers in information theory (who study how to encode information for different purposes) and researchers in computer architecture (who study how to design computer systems, including memory and storage).  The architects will identify exciting technologies and the problems they introduce, and the information theorists will develop new ways of encoding information to overcome these challenges.  Whereas much research in coding is theoretical and lacks connection to the reality of computer systems, the collaboration with computer architects---who focus on implementation issues and costs---will ground the coding work and enhance its impact.<br/><br/>To foster more interdisciplinary work in this research area, the research team will present half-day tutorials at conferences, including a coding tutorial at an architecture conference, and an architecture tutorial at an information theory conference. The investigators will continue to work with undergraduate research assistants, including students from an established summer outreach program at Duke?s Pratt School of Engineering. The investigators will also continue to recruit female research assistants alongside research assistants from under-represented populations; both investigators have extensive track records in this area and are committed to cultivating diversity in the computing research community.<br/><br/>Part 2:<br/><br/>The proposed research program seeks to improve the lifetime and fault tolerance of existing and emerging storage technologies---specifically Flash, 3D DRAM, and racetrack memory---by developing practical coding techniques that can be implemented in real-world systems. These storage technologies feature increased storage density, yet suffer the particular challenge of limited lifetime, that might otherwise limit their potential in microarchitectures.<br/><br/>Historically, improvements in memory and storage have been due not only to advances in the memory technologies themselves, but also to innovations by computer architects who design memory and storage systems and by coding theorists who devise codes for data storage. The proposed research program is a collaboration between a coding theory group led by PI Calderbank and a computer architecture group led by co-PI Sorin. In their preliminary work, architecture problems have driven development of new theory, and advances in coding theory have driven development of new systems that can take advantage of them.<br/><br/>The research thrusts can be classified by the memory technology, and all include specific challenges as well as more open-ended research directions: Specific objectives include<br/><br/>1. Development of new coding solutions for racetrack memory that employ delimiter bits to identify single shift errors and practically implementable codes to correct the error, once identified. Introduction of coding across multiple racetracks (spatial diversity) to correct multiple shift errors.<br/><br/>2. Development of virtual multilevel Flash cells that connect computer architecture with coding theoretic innovations, and evaluation of the tradeoff between host-visible capacity and lifetime.<br/><br/>3. Development of new coding solutions that protect stacked DRAM designs from failures in bits, rows, banks, channels, dies and through silicon vias. Development of fundamental limits on how redundancy trades off against read/write latency, bandwidth requirements, and energy consumption in stacked designs."
"1518776","SHF:Large:Collaborative Research: Inferring Software Specifications from Open Source Repositories by Leveraging Data and Collective Community Expertise","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","06/17/2015","Robert Dyer","OH","Bowling Green State University","Standard Grant","Sol J. Greenspan","06/30/2019","$214,843.00","","rdyer@bgsu.edu","302 Hayes Hall","Bowling Green","OH","434030230","4193722481","CSE","7798","7925, 7944","$0.00","Today individuals, society, and the nation critically depend on software to manage critical infrastructures for power, banking and finance, air traffic control, telecommunication, transportation, national defense, and healthcare. Specifications are critical for communicating the intended behavior of software systems to software developers and users and to make it possible for automated tools to verify whether a given piece of software indeed behaves as intended. Safety critical applications have traditionally enjoyed the benefits of such specifications, but at a great cost.  Because producing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available, such specifications are largely unavailable. The lack of specifications for core libraries and widely used frameworks makes specifying applications that use them even more difficult. The absence of precise, comprehensible, and efficiently verifiable specifications is a major hurdle to developing software systems that are reliable, secure, and easy to maintain and reuse. <br/><br/>This project brings together an interdisciplinary team of researchers with complementary expertise in formal methods, software engineering, machine learning and big data analytics to develop automated or semi-automated methods for inferring the specifications from code. The resulting methods and tools combine analytics over large open source code repositories to augment and improve upon specifications by program analysis-based specification inference through synergistic advances across both these areas. <br/><br/>The broader impacts of the project include: transformative advances in specification inference and synthesis, with the potential to dramatically reduce, the cost of developing and maintaining high assurance software; enhanced interdisciplinary expertise at the intersection of formal methods software engineering, and big data analytics; Contributions to research-based training of a cadre of scientists and engineers with expertise in high assurance software."
"1518897","SHF: Large:Collaborative Research: Inferring Software Specifications from Open Source Repositories by Leveraging Data and Collective Community Expertise","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","06/17/2015","Hridesh Rajan","IA","Iowa State University","Standard Grant","Sol J. Greenspan","06/30/2019","$750,125.00","Tien Nguyen","hridesh@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7798","7925, 7944, 9150","$0.00","Today individuals, society, and the nation critically depend on software to manage critical infrastructures for power, banking and finance, air traffic control, telecommunication, transportation, national defense, and healthcare. Specifications are critical for communicating the intended behavior of software systems to software developers and users and to make it possible for automated tools to verify whether a given piece of software indeed behaves as intended. Safety critical applications have traditionally enjoyed the benefits of such specifications, but at a great cost.  Because producing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available, such specifications are largely unavailable. The lack of specifications for core libraries and widely used frameworks makes specifying applications that use them even more difficult. The absence of precise, comprehensible, and efficiently verifiable specifications is a major hurdle to developing software systems that are reliable, secure, and easy to maintain and reuse. <br/><br/>This project brings together an interdisciplinary team of researchers with complementary expertise in formal methods, software engineering, machine learning and big data analytics to develop automated or semi-automated methods for inferring the specifications from code. The resulting methods and tools combine analytics over large open source code repositories to augment and improve upon specifications by program analysis-based specification inference through synergistic advances across both these areas. <br/><br/>The broader impacts of the project include: transformative advances in specification inference and synthesis, with the potential to dramatically reduce, the cost of developing and maintaining high assurance software; enhanced interdisciplinary expertise at the intersection of formal methods software engineering, and big data analytics; Contributions to research-based training of a cadre of scientists and engineers with expertise in high assurance software."
"1816850","SHF: Small: Lightweight Virtualization Driven Elastic Memory Management and Cluster Scheduling","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/21/2018","Xiaobo Zhou","CO","University of Colorado at Colorado Springs","Standard Grant","Yuanyuan Yang","06/30/2021","$376,000.00","","xzhou@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","7798","7923, 7941, 9251","$0.00","Data-centers are evolving to host heterogeneous workloads on shared clusters to reduce the operational cost and achieve high resource utilization. However, it is challenging to schedule heterogeneous workloads with diverse resource requirements and performance constraints on heterogeneous hardware. Data parallel processing often suffers from interference and significant memory pressure, resulting in excessive garbage collection and out-of-memory errors that harm application performance and reliability. Cluster memory management and scheduling is still inefficient, leading to low utilization and poor multi-service support. Existing approaches either focus on application awareness or operating system awareness, thus are not well positioned to address the semantic gap between application run-times and the operating system. This project aims to improve application performance and cluster efficiency via lightweight virtualization-enabled elastic memory management and cluster scheduling. It combines system experimentation with rigorous design and analyses to improve performance and efficiency, and tackle memory pressure of data-parallel processing. Developed system software will be open-sourced, providing opportunities to foster a large ecosystem that spans system software providers and customers. <br/><br/>Enabled by lightweight containers, cluster scheduling and the underlying operating system can cooperate synergistically, such that, the dynamic resource demand of an application can be exposed to the operating system, and the cluster memory manager and scheduler can be assisted with rich run-time information retrieved from performance counters and operating system.  Towards this end, the project aims to devise a distributed memory manager for data-parallel programs that can survive from memory pressure and enable elastic cluster memory management with architecture-aware container placement, design a cooperative paging to improve performance of memory swapping by extending the current virtual memory reclaim mechanism in Linux kernel, enable memory over-commitment for elastic cluster scheduling with a new service that can detect and exploit the over-commitment opportunities, and design a multi-queue based distributed task scheduler to manage performance interference and hardware heterogeneity. The contributions include a library of developed mechanisms and open-source system software at cluster and kernel levels that can significantly improve cluster utilization and application performance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814759","SHF: Small: Collaborative Research: LDPD-Net: A Framework for Accelerated Architectures for Low-Density Permuted-Diagonal Deep Neural Networks","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","06/22/2018","Keshab Parhi","MN","University of Minnesota-Twin Cities","Standard Grant","Sankar Basu","09/30/2021","$275,000.00","","parhi@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","075Z, 7923, 7945","$0.00","Deep learning has emerged as an important form of machine-learning where multiple layers of neural networks can learn the system function from available input-output data. Deep learning has outperformed traditional machine-learning algorithms based on feature engineering in fields such as image recognition, healthcare, and autonomous vehicles. These are widely used in cloud computing where large amount of computational resources are available. Deep neural networks are typically trained using graphic processing units (GPUs) or tensor processing units (TPUs). The training time and energy consumption grow with the complexity of the neural network. This project attempts to impose sparsity and regularity as constraints on the structure of the deep neural networks to reduce complexity and energy consumption by orders of magnitude, possibly at the expense of a slight degradation in the performance. The impacts lie in the formulation of a new family of structures for neural networks referred to as Low-Density Permuted Diagonal Network or LDPD-Net. The approach will enable the deployment of deep neural networks in energy-constrained and resource-constrained embedded platforms for inference tasks, including, but not limited to, unmanned vehicles/aerial systems, personalized healthcare, wearable and implantable devices, and mobile intelligent systems. In addition, the design methodology/techniques developed in this project can facilitate investigation of efficient computing of other matrix/tensor-based big data processing and analysis approaches. These approaches may also find applications in data-driven neuroscience and data-driven signal processing. In addition to graduate students, the project will involve undergraduates via senior design projects and research experiences for undergraduates. The results of the project will be disseminated to the broader community by publications, presentations, talks at various industries and other academic institutions. <br/><br/>The main barriers to wide adoption of deep learning networks include computational resource constraints and energy consumption constraints. These barriers can be relaxed by imposing sparsity and regularity among different layers of the deep neural network. The proposed low-density permuted-diagonal (LDPD) network can lead to orders of magnitude reduction in computation complexity, storage space and energy consumption. The LDPD-Net will not be retrained by first training a regular network and then only retaining the weights corresponding to the LDPD-Net. Instead, the proposed network will be trained from scratch. The proposed LDPD-Net can enable scaling of the network for a specified computational platform. The proposed research has three thrusts: 1) develop novel resource-constrained and energy-constrained inference and training systems;  2) develop novel efficient hardware architectures that can fully exploit the advantages of the LDPD-Net to achieve high performance; and 3) perform novel software and hardware co-design and co-optimization to explore the design space of the LDPD-Net. Using these, the efficacy of the proposed LDPD-net will be validated and evaluated, via software implementations on high-performance systems, low-power embedded systems, and a hardware prototype on FPGA development boards.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763349","AF: Medium: Collaborative Research: Foundations of Fair Data Analysis","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","07/01/2018","06/12/2018","Mallesh Pai","TX","William Marsh Rice University","Continuing grant","Tracy J. Kimbrel","06/30/2022","$63,168.00","","Mallesh.Pai@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","2878, 7796","075Z, 7924, 7926, 7932","$0.00","Machine learning algorithms increasingly make or inform critical decisions that affect peoples' every day lives. For instance, algorithms make decisions pertaining to hiring, college admissions, credit card and mortgage approvals, sentencing and parole of the incarcerated, first-responder deployment, and what advertisements and search results a user sees on the internet. An attractive feature is that these algorithms can efficiently process large amounts of data in making these decisions, thus hopefully improving economic and social efficiency. Because such decisions are so consequential, their fairness has become a matter of increasing concern. It has been argued that automation, by removing the human element, guarantees fairness, but this is not so -- several empirical studies have demonstrated that automation is no panacea. Further, the reasons for unfairness and discrimination can be complex and non-obvious. This project will study the frictions that may cause unfairness in algorithmic decision making, and the costs of mitigating unfairness -- that is, quantitative trade-offs between fairness and other desiderata, including accuracy, computational efficiency, and economic efficiency. <br/><br/>Specifically, this project will study frictions to fairness arising from several factors. There may not be sufficient data about minority populations. There can be feedback loops arising from the fact that observations can only be made on an individual if a risky action is taken, e.g., the person is granted a loan, or hired. Decision makers can be myopic, choosing to maximize short-term gains rather than exploring riskier options that may pay off in the long run. Economic frictions include self-confirming equilibria---differing subjective perceptions of opportunities leading to choices by individuals and communities which sustain those perceptions, and competition among classifiers (for example, credit agencies) leading to less accurate qualifiers in equilibrium. Finally, the problem of finding fair and accurate classifiers can be computationally intractable. This project will seek ways to mitigate the unfairness arising from these frictions. It will study the cost of incentivizing myopic agents to explore and examine the short-term costs of such incentives, and their long-term impact on fairness. It will also seek to design computationally tractable classifiers that achieve provably good approximations for fairness and accuracy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1539585","CyberSEES Type 2: Achieving Clean Power System Flexibility: Sensing, Modeling, and Optimal Control","CCF","CyberSEES","09/01/2015","08/31/2015","Duncan Callaway","CA","University of California-Berkeley","Standard Grant","Rahul Shah","08/31/2020","$1,199,914.00","Daniel Kammen, Eric Brewer","dcal@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8211","8208, 8231","$0.00","Growing renewable generation capacity is leading to unprecedented variability and uncertainty in power system operations, and a lack of power system flexibility threatens further increases in global sustainability in electricity sectors across the globe.  This project seeks to aggregate and control demand-side resources to provide this flexibility through a distributed control framework that is robust to failures in cyber-infrastructure, and is scalable across institutional dynamics, energy markets, and levels of power system development infrastructure.  Research objectives in this proposal will advance current applications and theories in computing, modeling and control, and the effectiveness and scalability of this work will be rigorously demonstrated through a field trial in Nicaragua.   This project will result in new tools to facilitate sustainable energy systems throughout the globe, an open data platform that strengthens the ability of global researchers to develop global solutions to challenges underling the integration of renewable sources, and advances in education to equip the workforce with new tools to tackle sustainability challenges.   <br/><br/>The envisioned system employs a centralized computing infrastructure to produce optimal control laws for individual thermostatically controlled loads.  The control laws can operate with or without reliable cyber-infrastructure in real time; in the absence of cyber-infrastructure, control can be based on time of use and local frequency measurements only.   Control laws will be published by a central server and periodically accessed by local intelligence at each load.  Challenges to this work include creating computing infrastructure that can actively distribute control laws and validate participation, modeling and control methods that are effective on real-world loads, and identifying control strategies with effective incentive mechanisms for consumers to participate.  At the conclusion of the project the investigators will have: (1) built a framework by which an innovative, cloud-based data management platform called Mezuri can robustly track the provenance and processing of all data and disseminate control laws; (2) developed hierarchical statistical models for state estimation and prediction of large fleets of thermally controlled loads that directly include the effects of human behavior, and adapt models to require as few observations as possible; and (3) devised new stochastic optimal control formulations that account for human activity and accommodate both system- and local-level objectives.  Taken together, these contributions build a new framework for control of demand-side resources that is globally scalable in a short time horizon."
"1254041","CAREER: An Information-Theoretic Approach to Communication-Constrained Statistical Learning","CCF","COMM & INFORMATION FOUNDATIONS, COMM & INFORMATION THEORY","02/01/2013","02/03/2017","Maxim Raginsky","IL","University of Illinois at Urbana-Champaign","Continuing grant","Phillip Regalia","01/31/2020","$518,435.00","","maxim@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797, 7935","1045, 7935","$0.00","This project aims to develop an information-theoretic approach to communication-constrained statistical learning problems involving multiple learning agents located at the nodes of a large network. This approach will build on the recently introduced coordination paradigm within network information theory, which looks at multiterminal problems in terms of optimal use of communication resources in order to establish some desired statistical correlations between the nodes of a network. The main theoretical goal is to explicitly identify the effect of bandwidth limitations, losses, delays, and lack of central coordination on the performance of statistical learning algorithms over networks. The project will systematically explore the fundamental limits of learning in multiterminal settings and design efficiently implementable and robust coding/decoding schemes. The theory developed under this project will be a novel synthesis of probabilistic techniques from machine learning (such as empirical process theory) and of multiterminal information theory (such as distributed lossy source coding).<br/><br/>As a broader impact, this project will provide key enabling technologies for large-scale, distributed applications of machine learning in such domains as smart grids, health-care informatics, transportation networks, and cybersecurity. Statistical machine learning is emerging as a dominant paradigm for making accurate predictions on the basis of empirical observations in the presence of significant model uncertainty. Most of the research activity in this field, however, has taken place in isolation from the realities of complex networks and all the attendant limitations on information transmission and processing: it is frequently assumed that the data needed for learning are available instantly, with arbitrary precision, and at a single location. However, given the fact that most data fed to machine learning algorithms are increasingly generated, exchanged, stored and processed over large-scale networks, there is a pressing need to dispense with this assumption and thus take network effects into consideration. The theory and the algorithms developed as part of this project will ensure that the relevant data are delivered over the network to the right decision-makers, while securing accurate decisions made on the basis of the received information. The research component of the project is tightly integrated with an education and outreach plan, including development and teaching of new courses on machine learning aimed specifically at engineering students."
"1539368","CyberSEES:  Type 1:  Collaborative Research:  High-Performance Image Classification and Search Supporting Large-Scale Seafloor Biodiversity and Habitat Surveys","CCF","CyberSEES","09/01/2015","08/26/2015","Malcolm Stokes","CA","University of California-San Diego Scripps Inst of Oceanography","Standard Grant","Rahul Shah","08/31/2019","$48,214.00","Grant Deane, James Leichter","dstokes@mpl.ucsd.edu","8602 La Jolla Shores Dr","LA JOLLA","CA","920930210","8585341293","CSE","8211","8207, 8231","$0.00","Seafloor ecosystems are complex environments populated by a great diversity of organisms. Unfortunately, these ecosystems are increasingly threatened by direct and indirect human activities, including changes in land-use practices, coastal runoff, energy and mineral extraction, and fishing pressure. Developing effective sustainability policies to deal with these ecosystem threats requires that we first understand seafloor communities as they are today, and then track how they change over time as human activities shift and sustainability policies are modified. Recent advances in high-resolution underwater imaging offer new ways to do this. Survey ships can zigzag back and forth above a threatened region, towing a submerged camera system that repeatedly snaps pictures of the seafloor. This produces an enormous and valuable image set that captures the current state of a seafloor ecosystem. Surveys like this have been done for many threatened regions, and more are in progress. Substantial challenges remain to process these image sets. A useful characterization of a seafloor habitat requires knowing which specific types of corals, sponges, starfish, and so forth are present, how many there are, and how they are distributed throughout a region. But with each survey image set containing hundreds of thousands or millions of images, manual processing is impractical. Instead of an army of experts examining these images, computer software can scan each image and automatically recognize the color and texture of different seafloor species. Experimental classification software like this exists today in research laboratories, but the software is slow. To be useful for huge image sets, this software must be revised and optimized to run on the latest high-performance supercomputers. This is the focus of the project, which will yield new optimized classification software that can quickly sweep through enormous image sets to classify and count the species present and provide essential information about the health and biodiversity of threatened seafloor ecosystems, or any other ecosystem with a suitable image set. Then, when surveys are repeated for the same region every few years, this processing can reveal important trends that document the health of a region and the impact of new sustainability policies that aim to mitigate continuing threats to these communities.<br/><br/>This project leverages prior work prototyping seafloor image classification algorithms. These algorithms divide survey images into small tiles, then characterize each tile with a high-dimensionality feature vector that includes metrics on the colors and textures present in the tile, along with water temperature, salinity, and depth data collected by the survey apparatus at the moment the image was captured. Colors in the feature vector are chosen based upon a quantized hue histogram of the tile, while textures are characterized by luminance Discrete-Cosine-Transform (DCT) coefficients. A tile's feature vector is then compared against stored feature vectors for known species within a large classification library. A probability-based selection using a set of nearest-neighbor matches from the library yields a best guess for the species depicted in the image tile. This process is repeated tile after tile, image after image throughout an image survey. Classification performance is strongly a function of the classification library size and the dimensionality of feature vectors used for image tiles and library entries. This project's approach to improve classification performance uses a customized k-d-tree search data structure for the classification library, along with domain knowledge to guide and tune the classification process. The project begins with new methods to cull the tree, prior to classification, by using broad survey characteristics, such as the geographic region covered, water temperature and salinity, the sea bottom type from acoustic data, and so forth. Additional techniques optimize the construction and matching of feature vectors by using survey and library metrics to cull and weigh vector components (such as contextual color gamut and texture detail reduction, principal component analysis to combine and weigh features), reduce the nearest-neighbor set size by using k-d tree metrics on library diversity, restructure the k-d tree to improve common case search and cache performance, and parallelize the search for efficient classification across multiple threads, cores, processors, and nodes in a large compute cluster. Together these new methods are expected to substantially increase classification performance and enable efficient processing for the latest large survey image sets."
"1321163","SHF:Small:Fine-Grain Many-Core Processor Arrays for Efficient Enterprise Computing","CCF","COMPUTER ARCHITECTURE","10/01/2013","09/15/2018","Bevan Baas","CA","University of California-Davis","Standard Grant","Yuanyuan Yang","09/30/2019","$400,000.00","","bbaas@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7941","7923, 7941","$0.00","Data centers worldwide use approximately 30 billion watts of electricity<br/>with data centers in the United States using approximately 8 to 10<br/>billion watts. Power consumption of datacenter systems is expected to<br/>continue to grow both absolutely and as a percentage of national<br/>consumption. The goal of the proposed research is to develop massively<br/>parallel processor arrays that work in conjunction with traditional<br/>enterprise-class processors to compute datacenter workloads with vastly<br/>greater efficiency. The proposed research may lead to new applications<br/>and capabilities that were previously constrained by power dissipation<br/>or processing throughput.<br/><br/>Project participants will propose, model, develop, and characterize<br/>novel fine-grain many-core architectures, VLSI chip designs, and<br/>application algorithms for a processor array serving as a co-processor<br/>or functional unit. The proposed programmable fine-grained many-core<br/>processor array contains no algorithm-specific hardware and is of a core<br/>granularity that is very lightly explored in prior work. The array will<br/>operate inside or near, and in co-ordination with a host<br/>enterprise-class processor and compute key computational kernels often<br/>with similar or higher performance than the host processor, but with<br/>orders of magnitude higher energy efficiency. During kernel computation,<br/>the host processor could attend to other tasks or enter a low power<br/>state. The targeted workloads include sorting, regular expression based<br/>pattern matching, encryption, data compression, video encoding and<br/>decoding, and other enterprise workloads of high impact that are<br/>discovered during the course of the research."
"1816512","SHF: Small: Fault Model Evaluation and Discovery","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","05/16/2018","Ronald Blanton","PA","Carnegie-Mellon University","Standard Grant","Sankar Basu","05/31/2021","$446,330.00","","blanton@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7945, 9102","$0.00","Integrated electronic circuits (ICs) today are ubiquitous in that they affect every aspect of our daily lives, ranging from use within all forms of mobile devices (smart phones, laptops, etc.) to all modes of transportation (cars, aircraft, etc.). Advances in electronics also form the foundation for the recent and forthcoming advances in machine learning, artificial intelligence, autonomous vehicles, etc.  The research work stemming from the project will aid the continued advancement of the electronic semiconductor industry by helping leading companies ensure that electronics work properly and reliably over long time periods. Additionally, the research conducted will include undergraduate student researchers, and the results will be disseminated to the research community, and incorporated into post-secondary curriculum.<br/><br/>The research will specifically examine how electronic failures are modeled, and for any shortcomings discovered, new models that are more accurate will be developed. While it is widely known that model effectiveness greatly depends on the IC and its fabrication, it is not known which models will be proven ineffective beforehand. In addition, especially for cutting-edge technologies, it is expected that there also will be a need for new, more accurate fault models for state-of-the-art ICs. Therefore, the objective of this research is to develop a comprehensive data-driven methodology for the continuous evaluation and development of fault models. Data from failed ICs will be analyzed to understand the likelihood and types of failures. For the expected mismatch between existing models and measured data, new models will be proposed and evaluated. Successfully meeting the objective of this research would will enable IC manufacturers to produce more reliable ICs at much lower cost.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1418976","CIF: Small: Sparsity and Scarcity in High-Dimensional Point Processes","CCF","SIGNAL PROCESSING","09/01/2013","08/03/2018","Rebecca Willett","WI","University of Wisconsin-Madison","Standard Grant","Phillip Regalia","06/30/2019","$386,331.00","","willett@uchicago.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7936","7923, 7936","$0.00","A wide variety of important applications rely upon our ability to quickly and accurately understand the physical world using a meager supply of event-based data. Such data arise when indirect observations of a physical phenomenon are collected by measuring discrete events (such as photons hitting a detector, sequence motifs appearing in a genome, packets traveling through an Internet router, neurons firing, or people interacting in a social network). The challenge here is to use extremely small numbers of random events to perform inference on the underlying high-dimensional phenomenon (e.g., the distribution of tissue in the body or the distribution of traffic in a network). In this case, conventional models of sensing and noise do not apply, and robust inference requires the development of both novel theoretical analyses and new computational methods.<br/><br/>Point processes model random processes in which a realization consists of a collection of isolated events distributed across space or time. This research program is aimed at the development of new theory and methods for exploiting low-dimensional or sparse models of high-dimensional signal structure using scarce point process realizations. The theoretical results facilitate characterization of fundamental performance limits, such as bounds on the error of physically realizable models of inverse problems in photon-limited imaging and the performance gap between online and batch processing of streaming data. Furthermore, the methods themselves are practical and resource-efficient in a broad range of contexts, and being used by astronomers, microscopists, social scientists, and geneticists.<br/>Underlying these methods are techniques at the intersection of statistical signal processing, learning theory, sparse coding, nonlinear approximation theory, optical engineering, and optimization theory."
"1763793","SHF:Medium:Collaborative Research:Fine-Grain Multithreading through Hardware/Software Co-Design","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/18/2018","Jean-Luc Gaudiot","CA","University of California-Irvine","Continuing grant","Yuanyuan Yang","06/30/2021","$337,805.00","","gaudiot@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7798","7924, 7941","$0.00","The supercomputing landscape has fundamentally changed in the past fifteen years. Chips have evolved from single-thread, single-core to multi-threaded, many-core chips. Even mainstream high-performance chips offer close to 100 hardware threads.  At the same time, accelerators, featuring hundreds or even thousands of hardware threads, have allowed scientists to obtain major performance speedups for certain classes of scientific kernels, thanks to their inherent massively parallel nature. From the software side, programming languages can provide a way to create various types of parallelism, from traditional data-parallel constructs to fine-grain, data-driven ones: directives have been added to leverage instruction-level parallelism (ILP), thus allowing the programmer to identify when the code is vectorizable; accelerator-friendly directives allow code to execute on GPUs or the Intel Xeon Phi; finally, new keywords enable the programmer to express task-dependent parallelism. In order to evaluate the hardware-software trade-offs, the investigators plan to design and develop an abstract machine model for scalable parallel and distributed computing, designing and implementing hardware-assisted mechanisms to realize it. Through a broad dissemination of the research findings and tools to the community via conferences and publications, seminars, and a dedicated website, this research has the potential to foster new directions in holistic and comprehensive solutions important to humanity. In addition, the investigators have recently co-founded a Special Technical Community (Parallel Models & Systems) of the IEEE Computer Society with the specific purpose of fostering research and education in the domain across US and the world.<br/><br/>This project seeks to develop an asynchronous fine-grain event-driven program execution model, Codelet Abstract Machine model (CAM), for thread management in parallel and distributed systems. The research tasks include three major extensions to a dataflow codelet model, implementing CAM by a hardware/software co-design approach and evaluating it using a set of benchmarks and applications. The proposed FPGA-based prototype is built in combination with general-purpose multicore chips and compiler and runtime system currently under development are designed be part of the system to allow high-level programmers to exploit the resulting system targeted to applications ranging from traditional HPC, parallel graph processing, as well as big data frameworks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717134","AF:Small:Tight Topology Dependent bounds on Distributed Communication","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/30/2017","Atri Rudra","NY","SUNY at Buffalo","Standard Grant","Rahul Shah","08/31/2020","$450,000.00","Michael Langberg, Shi Li","atri@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7796","7923, 7926, 7934","$0.00","The ever increasing dependence of society on cloud computing has resulted in a renewed focus on technical problems that arise from solving distributed computational tasks in data centers. The project will address fundamental theoretical challenges in distributed computation in data centers with increasingly complex and fluid interconnections that manage increasingly ambitious computational tasks. The challenges addressed follow from the societal use of cloud computing to perform significant portions of the current and future, personal and institutional day to day computational tasks; and the need for energy and time efficient data centers to process such tasks.  The project will aim to determine the fundamental limitations on communication (which typically accounts for bulk of time and energy expended) in the context of distributed cloud computing under the outlined challenges. This project will bring together researchers from disparate fields. The PIs will participate in K-12 outreach activities and in addition will engage undergraduates in research, especially those from under-represented groups.<br/><br/>The project will consider a general scenario where the underlying topology is given by a graph G = (V,E). Each node in V has a processor, and a subset of k processors (called terminals) have inputs. The terminals want to compute a function f on the k inputs in a setting where all the processors in V cooperate. Each edge e in E corresponds to a private point-to-point communication channel and all communication has to happen on one of these edges. The general goal is to compute f while minimizing the total number of rounds or total communication. The project will study lower and upper bounds on both the number of rounds and total communication needed to compute f. Special focus will be given to functions f that arise in practice from database queries as well as streaming computation. The project has the potential of increasing collaboration between researchers in communication complexity, distributed computing, network design algorithms, network coding and databases."
"1703489","AF: Medium: Collaborative Research: Sequential and Parallel Algorithms for Approximate Sequence Matching with Applications to Computational Biology","CCF","COMPUTATIONAL BIOLOGY","07/01/2017","06/05/2018","Sharma Thankachan","FL","University of Central Florida","Standard Grant","Mitra Basu","06/30/2020","$306,000.00","","sharma.thankachan@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7931","7924, 7931, 9251","$0.00","Sequence matching problems are central to the field of genomics, both in analyzing naturally occurring sequences such as genomes and in analyzing data from sequencing instruments. Often, methods that can accommodate a small number of differences within the matching regions suffice in practice. Such methods, described as alignment-free or approximate sequence matching methods, have typically relied on heuristics. This project work is advancing the field by creating a mathematical framework and solving multiple approximate sequence matching problems with provably efficient run-time guarantees. Project work is also supporting the development of practical heuristics inspired and supported by the mathematical framework, development of parallel methods for solving large-scale problems on high performance parallel computers, and studying the impact of these methods on important applications. Project results are made available through open source software for use by practitioners. Results from this research will be incorporated into courses taught by the PIs, and disseminated more broadly through book chapters and tutorials and accompanying slides. The project will support research scientist and Ph.D. students in interdisciplinary training for launching them into productive careers focused on important problems of current relevance. Undergraduate participation is planned through course projects.<br/><br/>Project work builds upon recent progress in alignment-free genome comparison methods, and exploits the controlled error characteristics of data generated by high-throughput sequencers, and the many bioinformatics applications enabled by them. Project objectives include developing a robust algorithmic framework for designing newer alignment-free methods based on approximate substring composition, and developing sequential and parallel algorithms for pairwise approximate sequence matching among large sequence data sets. The goal is to develop algorithms that are asymptotically superior to quadratic alignment-based approaches, and achieve good practical performance either directly or through further development of practical heuristic that rely on the underlying theory. The developed techniques will be further investigated in the context of important applications such as read error correction, genome mapping, and assembly. Though conducted in the context of computational biology, some of the methods are potentially applicable to other areas such as text processing and information retrieval. Broader research community will be impacted through release of software modules and project work in important application areas."
"1423615","AF: Small: Collaborative Research: Reconfiguration Algorithms","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","08/07/2014","Csaba Toth","CA","The University Corporation, Northridge","Standard Grant","Tracy Kimbrel","08/31/2019","$180,709.00","","csaba.toth@csun.edu","18111 Nordhoff Street","Northridge","CA","913308309","8186771403","CSE","7796","7923, 7929","$0.00","Computational geometry uses graphs, polygons, and arrangements to model physical objects and phenomena. Some of the objects are inherently flexible (e.g., proteins and robotic arms), others are stationary but require adjustments on a regular basis (e.g., communication and transportation networks). The focus of this project is on reconfigurations. The goal is to describe, understand, and control the combinatorial, geometric and topological changes in geometric configurations.<br/> <br/>This research project will develop new algorithms and data structures for modifying geometric configurations in three areas: (1) Optimization problems for in the configuration space of geometric objects, including graph augmentation, variations of the classical TSP tour problem, and network design for multiple criteria. (2) Reconfiguration through discrete moves, where current challenges include designing efficient data structures to support shortest path computation in the configuration space, approximating the diameter and radius of configuration spaces, and deciding whether reconfiguration is possible. (3) Modeling continuous motion, which includes motion planning algorithms and corresponding dynamic data structures for bar-and-joint frameworks, hinged polygons, and disk arrangements, motivated by applications in protein folding. A unified approach to discrete and continuous reconfiguration problems allows breaking down complex systems into elementary operations, which in turn leads to more efficient computational tools.<br/> <br/>The collaboration between faculty members and students from two universities ensures a high quality of training and opens new opportunities for all participating students."
"1850743","CRII: CIF: Learning Hidden Structures in Networks: Fundamental Limits and Efficient Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2018","12/11/2018","Jiaming Xu","NC","Duke University","Standard Grant","Phillip Regalia","03/31/2020","$174,351.00","","jx77@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7797","7935, 8228","$0.00","The problem of learning hidden structures in network data arises in many contemporary applications such as recommender systems, network privacy, and genomics. This problem intersects high-dimensional statistical inference and large-scale network analysis. On the one hand, classical statistical paradigm focuses on optimal statistical performance of learning algorithms, while it neglects computational complexity that becomes increasingly critical as network data gets big. On the other hand, computer scientists mostly focus on computationally efficient approximation algorithms for worst-case instances, while they neglect the underlying statistical structures that can be potentially exploited to improve the algorithm performance. This research combines both statistical and computational perspectives to characterize fundamental performance limits and develop efficient optimal algorithms for learning hidden structures in networks. <br/><br/>This research has two interrelated thrusts: graphon estimation and graph matching. Graphon estimation learns the underlying generating mechanism of a network from a single snapshot of the network. Graph matching matches the vertex sets of two graphs with correlated edges to minimize the number of adjacency disagreements. This research involves: (1) developing efficient algorithms for graphon estimation and graph matching by exploiting insights from spectral graph theory, convex relaxations, and statistical physics; (2) deriving sharp statistical performance limits by drawing tools from information theory, convex duality theory, and random matrix theory; (3) identifying fundamental computational barriers which limit computationally efficient procedures from attaining the optimal statistical performance. The investigator also deploys the algorithms developed in this research to real network data, leading to fast and accurate algorithms for preference elicitation in recommender systems, de-anonymization in social networks, and DNA assembly in genomics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618536","SHF: SMALL: Collaborative Research: Improving Reliability of In-Memory Storage","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/25/2016","Yifeng Zhu","ME","University of Maine","Standard Grant","Yuanyuan Yang","07/31/2019","$213,575.00","","zhu@eece.maine.edu","5717 Corbett Hall","ORONO","ME","044695717","2075811484","CSE","7798","7923, 7941, 9150","$0.00","Emerging nonvolatile memory (NVM) technologies, such as PCM, STT-RAM, and memristors, provide not only byte-addressability, low-latency reads and writes comparable to DRAM, but also persistent writes and potentially large storage capacity like an SSD. These advantages make NVM likely to be next-generation fast persistent storage for massive data, referred to as in-memory storage. Yet, NVM-based storage has two challenges: (1) Memory cells have limited write endurance (i.e., the total number of program/erase cycles per cell); (2) NVM has to remain in a consistent state in the event of a system crash or power loss. The goal of this project is to develop an efficient in-memory storage framework that addresses these two challenges. This project involves undergraduate and graduate students. All software artifacts and tools will be made available to the wider research community. The work has broader industrial and economic impact since it will help improve the reliability of data storage systems for data centers and HPC applications. <br/><br/>This project will take a holistic approach, spanning from low-level architecture design to high-level OS management, to optimize the reliability, performance, and manageability of in-memory storage. The technical approach will involve understanding the implication and impact of the write endurance issue when cutting-edge NVM is adopted into storage systems. The improved understanding will motivate and aid the design of cost-effective methods to improve the life-time of in-memory storage and to achieve efficient and reliable consistence maintenance."
"1763423","SHF: Medium: Fairness in Software Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","09/15/2018","05/03/2018","Yuriy Brun","MA","University of Massachusetts Amherst","Continuing grant","Sol J. Greenspan","08/31/2022","$594,224.00","Alexandra Meliou","brun@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7798","7924, 7944","$0.00","Software impacts society in many ways and increasingly automates decision-making. For example, software transcribes videos, translates documents, selects what news articles are promoted, and determines who gets a loan or gets hired. It is possible for software to exhibit bias in its operation, whether or not it is intended by the customers or developers of the software. For example, software might be more accurate at transcribing male voices than female ones. Or software may inject societal stereotypes into automated translations, and risk-assessment computations may exhibit racial bias. As more societal functions operate in cyberspace, the importance of software fairness increases. In these settings, data-driven software has the ability to shape human behavior: it affects the products we view and purchase, the news articles we read, the social interactions we engage in, and, ultimately, the opinions we form. Biases in data and software risk forming, propagating, and perpetuating biases in society. This project develops theory, techniques and tools to enable software designers and engineers to describe fairness requirements, test the software for fairness properties, and debug fairness defects. The outcomes of this project will help increase the society's trust in software decisions and in the data the software uses, in turn, increasing potential impact and benefits the software can bring to society.<br/><br/>The project addresses scientific questions behind efficiently and effectively measuring potential bias and helping stakeholders make informed decisions about software. It is not the project's aim to devise policies or eliminate bias in software. Instead, the aim is to provide software testing tools and measures that can be validated for formally specified software fairness properties. To measure bias, the project develops a novel approach for measuring causal relationships between program inputs and outputs. Software testing enables conducting causal experiments consisting of running the software with nearly identical inputs that vary only in a key input characteristic under test. Variations in an input characteristic that affect execution behavior provide evidence of a causal relationship. The project identifies when causal relationships are appropriate for measuring potential bias, develops efficient testing methods for measuring these relationships, and creates tools and techniques to help engineers identify and modify the causes of these relationships.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763654","SHF:Medium:Collaborative Research:Fine-Grain Multithreading through Hardware/Software Co-Design","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/18/2018","Guang Gao","DE","University of Delaware","Continuing grant","Yuanyuan Yang","06/30/2021","$345,408.00","","ggao@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","7798","7924, 7941, 9150","$0.00","The supercomputing landscape has fundamentally changed in the past fifteen years. Chips have evolved from single-thread, single-core to multi-threaded, many-core chips. Even mainstream high-performance chips offer close to 100 hardware threads.  At the same time, accelerators, featuring hundreds or even thousands of hardware threads, have allowed scientists to obtain major performance speedups for certain classes of scientific kernels, thanks to their inherent massively parallel nature. From the software side, programming languages can provide a way to create various types of parallelism, from traditional data-parallel constructs to fine-grain, data-driven ones: directives have been added to leverage instruction-level parallelism (ILP), thus allowing the programmer to identify when the code is vectorizable; accelerator-friendly directives allow code to execute on GPUs or the Intel Xeon Phi; finally, new keywords enable the programmer to express task-dependent parallelism. In order to evaluate the hardware-software trade-offs, the investigators plan to design and develop an abstract machine model for scalable parallel and distributed computing, designing and implementing hardware-assisted mechanisms to realize it. Through a broad dissemination of the research findings and tools to the community via conferences and publications, seminars, and a dedicated website, this research has the potential to foster new directions in holistic and comprehensive solutions important to humanity. In addition, the investigators have recently co-founded a Special Technical Community (Parallel Models & Systems) of the IEEE Computer Society with the specific purpose of fostering research and education in the domain across US and the world.<br/><br/>This project seeks to develop an asynchronous fine-grain event-driven program execution model, Codelet Abstract Machine model (CAM), for thread management in parallel and distributed systems. The research tasks include three major extensions to a dataflow codelet model, implementing CAM by a hardware/software co-design approach and evaluating it using a set of benchmarks and applications. The proposed FPGA-based prototype is built in combination with general-purpose multicore chips and compiler and runtime system currently under development are designed be part of the system to allow high-level programmers to exploit the resulting system targeted to applications ranging from traditional HPC, parallel graph processing, as well as big data frameworks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1629376","XPS: EXPL: Write Locality Theory and Optimization for Hybrid Memory","CCF","SOFTWARE & HARDWARE FOUNDATION, Exploiting Parallel&Scalabilty","09/01/2016","04/06/2018","Chen Ding","NY","University of Rochester","Standard Grant","Anindya Banerjee","08/31/2019","$333,999.00","","cding@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7798, 8283","7798, 7943, 9251","$0.00","A limit on extreme scale computing is the capacity, speed, durability and most importantly, energy efficiency of memory. Persistent memory is a disruptive technology that drastically reduces memory cost and static power but introduces the problems of slow writes and limited write endurance. An effective solution is caching. However, existing caches have been designed for fast reads: they do not minimize the number of write backs from cache to memory.  The intellectual merits of this project are sufficient theories and efficient techniques for optimal write caching in future systems that use persistent memory.  Caching, in both software and hardware, is increasingly prevalent. Hardware caches serve most of the memory accesses on commodity machines as well as super computers. Software caches, e.g. Memcached used by Wikipedia and Facebook, serve most of the data requests from online users.  The project's broader significance and importance are the broad benefits of cache optimization in scientific and consumer computing.<br/><br/>This project develops the formal concept of write locality and the theory and techniques to characterize locality and optimize the cache.  It has three parts.  The first is write locality modeling, which includes a set of metrics that can derive the amount of modified data in cache and the write backs from cache to memory in cache of all sizes and degrees of sharing.  The second is deductive optimization, which mathematically derives the cache performance of different task and data organizations and selects the best solution among a combinatorial number of choices. The last is system modeling and application optimization, which use write locality to model software and hardware cache and to optimize a program."
"1717754","SHF: Small: Accelerating Graph Processing with Vertically Integrated Programming Model, Runtime and Architecture","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2017","07/07/2017","Xuehai Qian","CA","University of Southern California","Standard Grant","Yuanyuan Yang","06/30/2020","$450,000.00","","xuehai.qian@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","7923, 7941","$0.00","Recently, graph processing received intensive interests due to the increasing need to understand relationships. For example, in cyber security, the graph analytics are needed to identify probes on the network. In social media, the graph analytics are employed to figure out the relationships and influences between people. In infrastructure monitoring (e.g. smart building), the graph analytics are crucial in spotting failures based on system dependencies before they become critical and cause cascading failures. On the other hand, in-memory graph processing is becoming more appealing due to recent technology advances (e.g. NDP with 3D integration) that improved the scalability of memory system with lower cost. Therefore, this research program timely considers graph processing(which has broad applications) with the emerging trends in the memory system.<br/><br/>This project will investigate a vertically integrated approach involving programming model, runtime system and architecture to holistically accelerate in-memory graph processing. It contains three research innovations and cross-stack integration: (1) Reducing data movements with novel programming model. It will study a new graph processing programming model,?Two-phase Vertex Program?, designed for PIM that supports a novel ""source-cut"" data partition. (2) Batched regular inter-cube communication and intra-cube locality enhancement. It will examine how to re-organize the computation to make the inter-cube communications happen in a controlled manner. This allows batched communication and the overlapping of computation and communication. To this end, it will study how to partition the cores in the same cube into two groups (Process and Apply unit) to improve intra-cube memory access locality. (3) Co-designed locality-aware scheduler and prefetcher. This project will develop a novel architectural interface so that the software and architecture could interact. On one side, it provides scheduler the capability to query the locality information of scheduling candidates to make better decisions. On the other side, the scheduler could convey the scheduling decisions to architecture so that even a simple prefetcher can precisely fetch the data related to the active vertices that will be scheduled soon. The proposed research will also contribute to society through engaging high-school and undergraduate students from minority-serving institutions into research, attracting women and under-represented groups into graduate education, expanding the computer engineering curriculum with graph processing architectures and runtime systems, disseminating research infrastructure for education and training, and collaborating with the industry."
"1619123","CHS: Small: Translating Compilers for Visual Computing in Dynamic Languages","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2016","12/12/2017","Connelly Barnes","VA","University of Virginia Main Campus","Standard Grant","Anindya Banerjee","08/31/2019","$450,000.00","Westley Weimer, Baishakhi Ray","csb7z@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7798","7923, 7943","$0.00","This collaborative project is developing technologies to enable students, scientists, and other non-expert developers to use computer languages that facilitate rapid prototyping, and yet still automatically convert such programs to have high performance. In this research, the PI and co-PIs focus on programs that operate over visual data, such as programs in computer graphics, computer vision, and visualization. Visual data is important because visual datasets are rapidly growing in size, due to the use of cell-phone cameras, photo and video sharing online, and in scientific and medical imaging. The intellectual merits are that specialized program optimizations are being developed specifically for visual computing and for languages that enable rapid prototyping, alongside techniques that allow the computer to automatically search through different candidate optimizations and choose the fastest one. The project's broader significance and importance are that it will make the writing of computer programs that operate over visual datasets more accessible to novice programmers, make visual computing more accessible to a broader audience, permit faster research and development over visual programs, and make such programs themselves be more efficient.<br/><br/>More specifically, this research program is producing translating compilers that are specialized to handle programs that compute over visual data. The group led by the PI is researching new compilers that translate code from dynamic languages into highly efficient code in a target language. Dynamic languages are defined as those with a very dynamic run-time model, for example, MATLAB, Python, and Javascript. The target language is a language such as C that permits implementation of highly efficient programs. This research framework incorporates ideas from compilers, graphics, computer vision, visual perception, and formal and natural languages. The research will make a number of key intellectual contributions. First, new domain-specific translations and optimizations for visual computing will be formalized into manual rules that can be applied to any input program. Second, the team will research a novel approach of automatically learning translations, instead of using manually-coded rules. This can take the form of learning translation ""suggestions"" from humans, who can interactively suggest better output code. Third, a new search process based on offline auto-tuning will be used to select the translations that result in the fastest program. The success of the project will be verified against a comprehensive test suite of programs from computer vision and graphics."
"1526459","SHF: Small:  Control-Flow and Data-Flow Analysis of Android Software: Foundations and Applications","CCF","SOFTWARE & HARDWARE FOUNDATION, Secure &Trustworthy Cyberspace","09/01/2015","06/24/2015","Atanas Rountev","OH","Ohio State University","Standard Grant","Sol J. Greenspan","08/31/2019","$470,208.00","","rountev@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7798, 8060","7434, 7923, 7944","$0.00","In recent years the growth in the number of computing devices has been driven primarily by smartphones and tablets. For such devices, Android is the dominant platform. The correctness, security, and performance of Android devices is of paramount importance for many millions of users. However, the scientific foundations for software analysis, verification, and transformation in this area are still very inadequate. The proposed work will significantly advance the state of the art in software analysis for Android. The results will be made available to other researchers, which will help design new analyses to improve software quality. Integration of research and education will develop the expertise of new developers of mobile software. Recruitment of underrepresented students will contribute to increased diversity in computing. Through commercial and academic software tools, new software analyses could enter real-world use in the development toolkits of Android programmers, resulting in better software quality and faster time to market.<br/><br/>Android applications are framework-based and event-driven. The complex semantics of the framework event/callback model presents a major challenge to static analysis. The project will develop a precise semantic description that captures essential abstraction of the run-time execution model, including modeling of components, their interactions through calls and callbacks, and their handling of external events. Based on this semantics, the project will design a general program representation of application control flow and data flow, and will develop algorithms for constructing and traversing it. The resulting representation and algorithms can serve as basis for a wide variety of static analyses. Three exemplar client analyses will be designed and evaluated: (1) detection of resource leaks, (2) detection of energy-related defects, and (3) taint analysis. These analyses target important quality problems and can help improve the performance and security of Android software. A suite of public algorithm implementations and experimental subjects will be made publicly available, to enable development and evaluation of existing and new static analyses for Android."
"1755705","CRII: CIF: New Structure-Exploiting and Memory-Efficient Methods for Large-Scale Optimization and Data Analysis","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2018","01/29/2018","Paul Grigas","CA","University of California-Berkeley","Standard Grant","Phillip Regalia","06/30/2020","$175,000.00","","pgrigas@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7797","7935, 8228","$0.00","Large-scale optimization methods have been paramount to the successes of recent applications of machine learning and data analysis in a wide variety of domains. At the same time, certain structural properties of statistical models, such as sparsity or low-rank structure, have proven to be crucial for obtaining meaningful and accurate results in high dimensions. In addition to being highly scalable to large datasets, some optimization algorithms have the desirable property that they directly promote the aforementioned valuable structural properties of models. This project involves developing, analyzing, and implementing novel optimization algorithms that have such beneficial structure-exploiting and also memory-efficiency properties. This project directly involves the mentoring of graduate students, as well as integration of research results into an undergraduate level machine learning course and a graduate level course in optimization and statistical learning.<br/><br/>The foundation for this project is the Frank-Wolfe Method, a particular structure-exploiting first-order gradient optimization algorithm, and the related methodology of in-face directions. In-face directions automatically promote well-structured near-optimal solutions and have encouraging memory-efficiency properties. This research will investigate conditions whereby methods with in-face directions, as applied to convex relaxations of matrix completion and more general atomic norm regularization problems, are guaranteed to have a low memory footprint. Furthermore, this project will extend the reach of methods that incorporate in-face directions to new problem classes, including non-smooth objective functions, non-convex objective functions, and stochastic gradient estimates. The proposed optimization framework and in-face methodology applies very generally, and has potential for broader impact in several areas, including recommender systems, bioinformatics, customer segmentation, sentiment analysis, and medical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617319","CCF: Small: Smart Continually-aware High-Fidelity Sensor Interfaces","CCF","COMM & INFORMATION FOUNDATIONS, SOFTWARE & HARDWARE FOUNDATION","07/15/2016","07/06/2016","Michael Flynn","MI","University of Michigan Ann Arbor","Standard Grant","Sankar Basu","06/30/2019","$499,528.00","Zhengya Zhang","mpflynn@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7797, 7798","7923, 7935, 7945","$0.00","This research addresses the challenges of continually-aware high-fidelity sensing. The key challenges include the energy consumption of the sensor interface and the amount of data that needs to be communicated by the sensor by considering two sensing scenarios in which a sensor must be constantly operate with high fidelity. In the first case, the sensor continuously processes a signal and delivers an output, e.g., a hearing aid operating continuously, and performing sophisticated high-fidelity audio signal processing and yet running from a small battery for a long period of time. In the second scenario, the device detects and acts on a particular sensory stimulus, e.g., the detection and recognition of a speaker (person) or words or sentences. The broader impacts of this research will be enhanced with substantial efforts in outreach and education. These include outreach to K-12 students, research experience for undergraduates from underrepresented minorities, and updated graduate and undergraduate curriculum.<br/><br/>More specifically, this research investigates a new type of sensor interface circuit that efficiently implements both of these high fidelity sensing scenarios. It does this by embedding sophisticated processing and intelligence within the sensor interface architecture. This approach is different from the conventional approach which relies on separate signal processing devices or alternatively sends data to the cloud for processing. The new approach is implemented with little extra cost and energy consumption, beyond that of the original interface circuit. The expected benefit is that the overall energy consumption is greatly reduced and the amount of data generated by the sensor is to be reduced by orders of magnitude."
"1617673","CIF: Small: Harnessing Network Compression Gains: Fundamental Limits and Practical Implementations","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/10/2016","Aaron Wagner","NY","Cornell University","Standard Grant","Phillip Regalia","05/31/2019","$500,000.00","","wagner@ece.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","7923, 7935","$0.00","Past increases in information consumption have been fueled mainly by increasing the bit-carrying capacity of our telecommunications infrastructure, including capital improvements such as laying fiber as well as technological advances that enable this infrastructure to carry more bits. This approach of simply moving more bits now appears to be facing diminishing returns, however. In contrast, there has been less attention paid to how data compression can reduce the number of bits that must be communicated. Thus future increases in per-capita information consumption could be driven by improvements in compression. This project will also contribute to improved STEM education through the development of adoptable course materials for a dedicated graduate-level course on information theory and data compression.<br/><br/>This research examines how to harness gains in compression, especially network gains. The work will focus on lossy compression for two simple networks that are not well understood: (1) the problem in which several encoders observe separate sources and send compressed versions to a centralized decoder, who seeks to reproduce the sources, and (2) the problem in which a single encoder broadcasts a compressed version of a source to multiple decoders, who reproduce the source with the benefit of side information. For both problems the PIs seek to determine the information-theoretic rate-distortion tradeoffs for a practically-important class of instances. For the first problem, the PIs will also measure the network gains that can be obtained in practice by constructing a system to compress synchrophasor data obtained from the power grid.<br/>"
"1335443","XPS:CLCCA:Collaborative Research:Harnessing Highly Threaded Hardware for Server Workloads","CCF","RES EXP FOR TEACHERS(RET)-SITE, SOFTWARE & HARDWARE FOUNDATION, Exploiting Parallel&Scalabilty","09/15/2013","07/17/2014","Alvin Lebeck","NC","Duke University","Standard Grant","Anindya Banerjee","08/31/2019","$365,999.00","","alvy@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1359, 7798, 8283","1359, 7943, 9251","$0.00","Data centers provide the computational and storage infrastructure required to meet today's ever increasing demand for Internet-based services. Web servers deliver a vast range of information on demand, ranging from static content such as files, images, video and audio streaming services, to dynamic content created via scripting languages (e.g,. PHP) or stand alone C/C++ applications (e.g., search results).  Server performance, scaling and energy efficiency (throughput/Watt) are crucial factors in reducing total cost of ownership (TCO) in today's server-based industries.  Unfortunately, current system designs based on commodity multicore processors may not be the most power/energy efficient for all server workloads.  <br/><br/>Today's massively parallel accelerators (e.g., GPUs) provide exceptional performance per Watt for certain workloads versus conventional many core CPUs. Unfortunately, these devices have not found wide-spread general purpose use outside the high-performance computing domain. This project expands the use of massively parallel accelerators to server and operating system-intensive workloads by innovating across the application, runtime, operating system, and architecture layers.<br/><br/>This research builds on the observation that server workload execution patterns are not completely unique across multiple requests. The goal of this project is to develop computer systems (software and hardware) that exploit similarity across requests to improve server performance and power/energy efficiency by launching data parallel executions for request cohorts.  The three primary aspects of this research are 1) mapping traditional thread/task parallel workloads onto data parallel hardware, 2) developing a new accelerator-centric operating system architecture, and 3) developing new architectural mechanisms to support this new class of accelerator workloads and operating system software."
"1629431","XPS: FULL: Breaking the Scalability Wall of Shared Memory through Fast On-Chip Wireless Communication","CCF","Exploiting Parallel&Scalabilty","07/01/2016","06/28/2016","Josep Torrellas","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yuanyuan Yang","06/30/2020","$879,180.00","David Padua, Sasa Misailovic","torrellas@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8283","","$0.00","As transistor sizes continue to scale, we are about to witness stunning levels of chip integration, with 1,000 cores on a single die. At these processor counts, it has been an accepted tenet among many researchers that shared memory does not scale. The reason is the high hardware overhead of supporting fine-grain synchronization and communication between so many cores. This research seeks to disprove this.  It shows that fine-grain data sharing is scalable with the use of fast on-chip wireless communication. The Principal Investigators (PIs) augment each core with a transceiver that enables on-chip broadcast in 5-7 nanoseconds, and design a multicore architecture that supports it. Then, the PIs implement synchronization and communication primitives and libraries that support fine-grain data sharing with an unprecendented low overhead. With these primitives, the PIs redesign popular runtimes such as OpenMP or Cilk, and rethink algorithms and applications. This effort contributes to multidisciplinary research and education on scalable parallel computing at the University of Illinois. The PIs will enhance the courses  that they teach in parallel computer architecture, parallel compilation techniques, and parallel programming. They are also working with the department to broaden the course offerings with multidisciplinary undergraduate courses in the general area of parallel computing --- as part of Illinois' CS+X major structure, where X can be a large number of other disciplines. The work will also have an impact on industry, since it addresses a real, very timely technical problem: extraordinary on-chip integration coupled with unscalable fine-grain data sharing. The ability to work closely with IBM Research and Microsoft Research will be crucial.<br/><br/>This is a cross-disciplinary effort that cuts across three areas: architecture, programming systems, and algorithms and applications. The architecture work focuses on supporting on-chip wireless communication by extending cache coherence transactions, trading-off wireless power for error rate, and supporting multiple wireless channels. The programming systems work focuses on redesigning the MPI communication primitives, making shared memory scalable for OpenMP and Cilk, and designing a best-effort API for application resiliency. The algorithms and applications work focuses on studying and developing algorithms and applications that can take advantage of the architecture. The PIs study problems in the areas of graphs, numerics, dynamic programming, recognition-mining-synthesis, and MapReduce."
"1745748","SHF: SMALL: Collaborative Research: Improving Reliability of In-Memory Storage","CCF","SOFTWARE & HARDWARE FOUNDATION","06/26/2017","07/14/2017","Jianhui Yue","MI","Michigan Technological University","Standard Grant","Yuanyuan Yang","07/31/2019","$176,876.00","","jyue@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","7798","7923, 7941","$0.00","Emerging nonvolatile memory (NVM) technologies, such as PCM, STT-RAM, and memristors, provide not only byte-addressability, low-latency reads and writes comparable to DRAM, but also persistent writes and potentially large storage capacity like an SSD. These advantages make NVM likely to be next-generation fast persistent storage for massive data, referred to as in-memory storage. Yet, NVM-based storage has two challenges: (1) Memory cells have limited write endurance (i.e., the total number of program/erase cycles per cell); (2) NVM has to remain in a consistent state in the event of a system crash or power loss. The goal of this project is to develop an efficient in-memory storage framework that addresses these two challenges. This project involves undergraduate and graduate students. All software artifacts and tools will be made available to the wider research community. The work has broader industrial and economic impact since it will help improve the reliability of data storage systems for data centers and HPC applications. <br/><br/>This project will take a holistic approach, spanning from low-level architecture design to high-level OS management, to optimize the reliability, performance, and manageability of in-memory storage. The technical approach will involve understanding the implication and impact of the write endurance issue when cutting-edge NVM is adopted into storage systems. The improved understanding will motivate and aid the design of cost-effective methods to improve the life-time of in-memory storage and to achieve efficient and reliable consistence maintenance."
"1421443","SHF: Small: Specializing Compilers For High Performance Computing Through Coordinated Data and Algorithm Optimizations","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2014","08/01/2016","Qing Yi","CO","University of Colorado at Colorado Springs","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$493,631.00","","qyi@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","7798","7923, 7942, 7943, 9251","$0.00","This research brings about a new methodology for developing compilers, where the data structure and algorithm implementations of software applications are independently normalized and categorized into commonly occurring patterns, compiler optimizations are made customizable components that can be flexibly composed, and all optimizations are closely coordinated and collectively specialized to attain a highest level of performance. The pattern-based specialization specifically targets a number of domains, e.g., dense/sparse matrix codes, stencil computations, and graph/machine learning algorithms, which are critical to scientific computing.  A uniform annotation interface is provided for developers to concisely document the higher-level semantics of abstractions provided by varying domain-specific and parallel programming libraries, thereby allowing the development of specially customized library-aware compilers that can automatically coordinate the uses of library abstractions to maximize the overall performance of large scale multiprocessor applications. Automated optimization tuning support is provided to support the performance portability of applications on modern heterogeneous computing platforms.<br/><br/>The deliverables of this research include a collection of specialized compiler optimizers, distributed open source online, with associated auto-tuning toolkits to target them for varying modern multi-core and GPU platforms, and with a graphical user interface for users to interactively invoke these optimizers.  These optimizers, together with their interactive configuration interfaces, are expected to fundamentally change how high performance computing applications are developed, while providing computational specialists a toolset to automatically generate optimized library kernels without manually composing assembly codes."
"1618366","CIF: Small: Coding for DNA-Based Storage Systems","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/27/2016","Olgica Milenkovic","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","06/30/2020","$500,000.00","Huimin Zhao","milenkov@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7935","$0.00","With the exceptional growth of data generated from social and economic networks and from wide-scale scientific experiments in physics, biology and astronomy, the problem of efficient and durable data storage has become of paramount importance. To address this challenge, various new solutions for ultra-high density storage media have been considered, including macromolecular (DNA) recording systems. DNA-based systems are inherently nonvolatile, they retain information under standard environmental conditions for thousands of years, and they offer unprecedented information densities and fast data replication/copying rates. This project is concerned with developing new error control coding solutions for DNA-based storage systems and various DNA synthesis (writing) and DNA sequencing (reading) models and technologies. The project includes plans for curriculum development as well as outreach to industry partners and interdisciplinary collaborators.<br/><br/>Error control coding is a critical aspect of all storage systems. The research in this project introduces and develops error control coding techniques for DNA storage including DNA Profile Codes, Asymmetric Lee Distance Codes, and Weakly Mutually Uncorrelated Codes. All of these techniques are expected to improve the accuracy of DNA storage systems by adapting the stored sequences to the media, avoiding common sequencing errors, and simplifying the process of DNA synthesis. These coding methods are expected to allow for random access and reduce the implementation cost of the technology. In turn, this will enable miniaturization of the write system, minimize sequencing error rates, and significantly speed up the process of DNA assembly."
"1617175","SHF: Small: Relaxing Soundness","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2016","07/11/2016","James Riely","IL","DePaul University","Standard Grant","Nina Amla","06/30/2020","$499,577.00","Radhakrishnan Jagadeesan","jriely@cs.depaul.edu","1 East Jackson Boulevard","Chicago","IL","606042287","3123627595","CSE","7798","7923, 7943","$0.00","Replication is a key ingredient in achieving scalability and high availability of computer systems.  This is evident for globally distributed databases, but it is also increasingly true for individual computers, smartphones and even watches, with multiple layers of cache coordinating communication between multiple processor cores.  Where there is replication, consistency is necessarily at conflict with performance and availability. Strong notions of consistency allow simpler application development and debugging, at the cost of performance.  This project explores weaker notions of consistency, investigating the extent to which performance gains can be achieved while maintaining a relatively simple programming model.  <br/><br/>This project investigates abstraction, and under what circumstances can a client programmer safely reason about a data structure using its sequential specification. The project develops a common foundational framework that unites the areas of distributed data structures, relaxed memory models and concurrent data structures. The project explores foundations for languages and tools for shared-memory multi-core programming.  By improving the programming model, the project encourages better use of the resources available in current and future hardware."
"1442672","CyberSEES: Type 1: Meghdoot: A Multi-Cloud Infrastructure for Enhancing Sustainability via Effective Monitoring of Inland Waters and Coastal Wetlands","CCF","CyberSEES","01/15/2015","11/05/2018","Lakshmish Ramaswamy","GA","University of Georgia Research Foundation Inc","Standard Grant","Rahul Shah","12/31/2019","$399,912.00","Suchendra Bhandarkar, Deepak Mishra","laks@cs.uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","CSE","8211","8207","$0.00","The overall goal of the project is the design and prototype implementation of a multi-cloud computing framework that seamlessly integrates community observations, remote sensing measurements, and advanced multimedia analytics for effective environmental monitoring. This infrastructure, called Meghdoot, will be employed for early detection of salt marsh stress in coastal areas and cyanobacterial harmful algal blooms in inland waters. This framework will harness multiple clouds including community clouds, sensor clouds, and computational clouds to develop efficient and timely event detection strategies. In addition, efficacious mechanisms for motivating and enabling community members to contribute high-quality content for enhanced environmental monitoring will be investigated. The first research thrust of the proposed project is to invent mechanisms to effectively leverage community clouds for monitoring lakes and salt marshes. In this community-as-sensors paradigm, data produced by individuals of the community (i.e., blogs, images, tweets) in online social media platforms (Facebook, Twitter, Flickr, etc.) will be used to generate trustworthy, actionable information. This, in turn, will act as the initial trigger for activating the traditional sensing infrastructure. The data from sensor clouds comprised of high resolution cameras and hyperspectral radiometers will be processed through the computational cloud using specialized techniques such as segmentation, feature extraction, registration, indexing, and spectral models for producing estimates of the cyanobacteria concentration and marsh biophysical characteristics from the study sites.  The final research thrust of this project will focus on the optimization of the deployment and operation of the sensor cloud based on the information from the community cloud and the results from the computational cloud.<br/><br/>This project addresses two environmental issues important to coastal states in southeastern United States, namely, harmful algal blooms and marsh browning events. Over the last decade the frequency and magnitude of these environmentally detrimental events have gone up primarily because of drought as well as sea level rise. Therefore, accurate, cost-effective, and targeted monitoring of these events is indispensable to sustainable management of the environment.  The proposed cyber-infrastructure-based warning system will enable early detection and timely implementation of preemptive measures to reduce the frequency and severity of future events while ensuring environmental conservation and sustainability.  The project plans to engage students, community leaders, resource managers, and the general public via training, workshops, and social media in various aspects of the research starting from crowd sourcing to environmental sensor deployments, data acquisition, processing, and interpretation. The success of the proposed project will pave the way for a state-wide automated early detection, warning and rapid response system that can be adopted by state-level environmental agencies to alert restoration officials and lay citizens of impending risks associated with these events."
"1453378","CAREER: Synthesizing Highly Efficient Hardware Accelerators for Irregular Programs: A Synergistic Approach","CCF","SOFTWARE & HARDWARE FOUNDATION","03/01/2015","03/22/2018","Zhiru Zhang","NY","Cornell University","Continuing grant","Sankar Basu","02/29/2020","$359,471.00","","zhiruz@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7798","1045, 7945","$0.00","This CAREER research project aims to significantly improve the design productivity and quality of heterogeneous computer architectures, which extensively integrate specialized hardware accelerators to continue to provide the computing improvements essential to all aspects of our society. Achieving this goal requires the development of a new class of truly integrated design automation methodologies and tools to enable productive modeling, exploration, and generation of hardware accelerators from high-level programs, especially for the irregular programs that are commonplace in emerging application domains such as computer vision, machine learning, physical simulation, and social network analytics. The project also has a broad yet thematically focused plan for educational outreach, which aims to cultivate the next generation of engineers and scientists who can bridge the chasm between the software and hardware design paradigms. The PI will lead hands-on design sessions for underrepresented minority high school students and organize engineering seminars with engaging demonstrations for first-year undergraduates to increase their interest and participation in computer engineering. In addition, the PI will actively integrate the research outcomes into undergraduate and graduate curriculum development, and leverage industrial collaborations to effectively disseminate the research results on heterogeneous computing to a broader audience.<br/><br/><br/>Diminished benefits of technology scaling have led to a growing interest in heterogeneous accelerator-rich system architectures to improve performance under tight power and energy efficiency constraints. Irregular programs are gaining prominence in many important application domains; but these programs are much more difficult to parallelize on conventional data-parallel accelerators such as GPUs, as they typically exhibit less-structured data access patterns and difficult-to-predict dynamic parallelism. This project aims to develop a synergistic design automation framework where a set of novel programming abstractions, architectural templates, synthesis optimization algorithms, and hardware prototypes all play concerted roles to overcome the many challenges raised by the irregular programs. Specifically, the key idea is to automatically generate softly synthesized accelerators that are capable of decoupling data access from computation for tolerating memory latency and performing run-time optimizations for exploiting the irregular parallelism."
"1763699","SHF: Medium: Quantifying and Designing Around Architectural Risk","CCF","SOFTWARE & HARDWARE FOUNDATION, ","06/01/2018","02/22/2019","Timothy Sherwood","CA","University of California-Santa Barbara","Continuing grant","Yuanyuan Yang","05/31/2021","$858,105.00","Zheng Zhang","sherwood@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7798, S138","7798, 7924, 7941","$0.00","Computer applications and technologies are changing at an ever-increasing rate.  This change comes with uncertainty and that uncertainty makes it difficult to make good decisions about how to build systems that are maximally useful in the future. While developing new computer systems has always involved risk, the new magnitude of these uncertainties may now lead to either overly conservative design practices at one end, or designs that have 'fragile' performance at the other end. While risk assessment and management are expected in both business and investment, these aspects are typically treated as independent to questions of performance and efficiency in computer design when in fact they are not.  As hardware and software characteristics become uncertain (i.e. samples from a distribution), the resulting performance distributions quickly grow beyond our ability to reason about them with intuition alone. Through the collaboration of computer system designers and experts in the impacts of technology uncertainty, this project is developing new and fundamental techniques for both quantifying risk and optimizing designs in risk-aware ways. This project is working to transform the way in which architectures and systems, from micro to data-center scale, are designed and analyzed. The investigators are creating new technologies, but also making those technologies available and accessible through open repositories, involving undergraduates at all levels in his research, and integrating basic concepts from these statistical design methods into outreach through ""I love STEM"" and other efforts. This interdisciplinary project is targeted at advancing the frontiers of computer architecture, electronic design automation, and uncertainty quantification. The developed methodologies can also find application far outside the original area of inquiry e.g., impact risk analysis and management across many other engineering systems including renewable energy, robotic systems, and autonomous driving.<br/><br/>The work is demonstrating, for the first time, that it is possible to define, model, quantify, and mitigate computer architectural risk. By bridging ideas of risk and risk-management from economics and fast stochastic algorithms from uncertainty quantification, a new framework for high-level risk-aware computer architecture analysis is being created.  Efficient techniques, using fewer than 50 data points, can effectively estimate architectural uncertainty to enable the rigorous quantification and management of risk during computer architecture design. This framework is embodied in a symbolic / statistical analysis system that eases the exploration of these surprisingly complex design spaces.  A declarative language abstracts away the complexity of new probability-constrained optimization and intelligent sampling methods, while risk-aware micro and macro architectural improvements demonstrate the value of these methods in practice. The methods transform the way one can extract useful models of uncertainty from a limited selections of data points available to each manufacturer, propagate those uncertainties correctly through complex compositions of systems and the interactions of resources, efficiently quantify the impact of those propagated uncertainties on the end figures of merit for the system, encapsulate those methods in a language and solver framework, and develop exemplars of how such uncertainty can be more actively mitigated and controlled.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815882","SHF: Small: Deep Neural Network Inference on Energy-Harvesting Devices","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/23/2018","Nathan Beckmann","PA","Carnegie-Mellon University","Standard Grant","Yuanyuan Yang","09/30/2021","$450,000.00","Brandon Lucia","beckmann@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878","2878, 7923, 7941","$0.00","Intermittently powered, energy-harvesting computers are sophisticated computing, sensing, and communicating systems that do not need a battery or tethered power source.  These energy-harvesting devices will form the foundation of the next generation of internet-of-things (IoT) applications, ranging from wearable and implantable medical devices, to environmental and atmospheric monitoring, to tiny ChipSat-scale satellites in deep space.  Realizing the value of these applications requires intelligent devices that can frequently make decisions locally and autonomously (i.e., without help from other nearby computers). For example, a device may need to decide whether to turn on a battery-draining camera to detect a person of interest, to decide which sensors embedded in concrete to enable to collect the most useful data about an aging bridge, and to decide when and how much data to send from these sensors back to the cloud.  In recent years, statistical inference and machine learning using deep neural networks has proven the most successful method for such decision-making. Machine learning is a crucially important feature for future IoT devices, but today's resource-constrained energy-harvesting systems do not support the high-intensity computations required by deep neural network inference. This project builds the software computer systems and hardware computer architectures required by future, intermittent IoT devices to enable autonomous, intelligent decision-making using machine learning.  This project produces software systems with novel algorithms that enable today's energy-harvesting IoT devices to efficiently make intelligent decisions.  This project will then design novel parallel computer architectures that are designed specifically for efficient operation of machine learning computations with intermittent input power.  These architectures further increase the efficiency of intermittent decision-making by 10s or 100s of times, enabling a new class of intelligent IoT applications that are not possible using today's architectures. The sum of these software and hardware components addresses the existential question of deep machine learning on intermittent systems, demonstrating its viability and realizing its benefits to academia, industry, and in applications important to society, such as defense, healthcare, and civil infrastructure.  This project contributes towards a diverse future workforce, through integration with course curricula, mentoring of students from under-represented minority groups, and technical high school outreach programs.<br/><br/>The key challenge overcome by this project is to make deep neural network inference viable on a resource-constrained, intermittent device. This task requires architecture and software support to tolerate frequent, intermittent power interruptions and to operate with hundreds of microwatts of power instead of the tens or hundreds of milliwatts required by today's machine learning accelerators.  This project develops approximate, intermittent partial re-execution techniques to efficiently tolerate interruptions without the need to unnecessarily checkpoint and restore software state.  The project develops the first intermittence-safe data-parallel architecture, integrating non-volatile memory with an array of simple compute elements. The project includes an immediate path to software and hardware prototypes and lays the groundwork for a future silicon hardware implementation of the architecture.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718267","SHF:Small: Enriching Session Types for Practical Concurrent Programming","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2017","08/29/2017","Frank Pfenning","PA","Carnegie-Mellon University","Standard Grant","Anindya Banerjee","06/30/2020","$450,000.00","Stephanie Balzer","fp@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7943","$0.00","Concurrent programming is becoming increasingly prevalent because of the distributed nature of many applications and the prospect of performance gains.  However, concurrent programming is also notoriously error-prone because of the presence of data races and deadlocks.  A promising approach to concurrent programing is message-passing concurrency, due to its higher-level of abstraction.  Message-passing concurrency has been adopted by various practical programming languages, such as Erlang, Go, and Rust.  Servo, for example, is an experimental browser engine developed in Rust that exploits message-passing concurrency for tasks, such as DOM traversal, layout painting, and JavaScript execution.  This research project studies the application of session types to practical message-passing concurrency, while addressing the pitfalls of data races and deadlocks.  Session types allow the expression and compile-time checking of the protocols of message exchange.  The project's intellectual merits are to lift the expressiveness of session types to accommodate today's concurrent communication patterns, while remaining truthful to their logical foundation.  The project's broader significance and importance are its provision of both a foundational and practical view on concurrent programming, the development of curricular material at the sophomore-level, and a session type extension for Rust.<br/><br/>The logical foundation of this research project is the recently discovered Curry-Howard isomorphism between intuitionistic linear logic and session types, which relates linear propositions to session types, sequent calculus proofs to concurrent processes, and cut reduction to message-passing communication.  Existing work building on this foundation provides strong guarantees, but also narrows the applicability of session types.  The aim of this research is two-fold: (i) to increase the applicability of session types, while keeping their logical foundation intact, and (ii) to demonstrate practicality of the resulting techniques to real-world software development.  For (ii), the project explores the application of the techniques resulting from (i) to the Servo code base.  For (i), the project first explores the introduction of shared channels to support programs that demand sharing by the nature of circumstances or for performance considerations.  Key concerns in this exploration are the prevention of data races along shared channels to guarantee session fidelity and the assurance of a form of global progress.  In its simplest form, global progress will lack deadlock freedom, a property holding in the purely linear setting.  In a second phase, a logical interpretation of deadlock prevention is derived.  In a third phase, the project explores the enrichment of session types with dependent typing for the expression and verification of properties that are not primarily protocol-related.  In all cases, proofs of soundness as well as a prototype of a session-typed concurrent programming language accommodating the developed techniques are given."
"1553042","CAREER: Trading Communication and Storage for Computation to Enhance Energy Efficiency","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2016","02/12/2019","Ulya Karpuzcu","MN","University of Minnesota-Twin Cities","Continuing grant","Yuanyuan Yang","01/31/2021","$346,345.00","","ukarpuzc@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","1045, 7941","$0.00","Looking forward, a fundamental challenge in computer systems is the Power Wall, where, due to imbalances in technology scaling, not all the compute engines that find place on chip can be powered on at the same time. The Power Wall is transforming all levels of the system stack, including the parallel architecture landscape. Communication power, as induced by data movement and the orchestration thereof, along with storage power, dominates computation power. As the degree of concurrency increases, communication power becomes even more prominent due to more frequent data transfers. At the same time, emerging memory technologies are not mature enough to keep up with the corresponding capacity, bandwidth, and performance requirements within a tight power budget. An amnesic processor, can minimize, if not eliminate, the power and performance overhead associated with data storage, retrieval, and communication, thus, operate more energy efficiently. The limited storage of the amnesic machine represents the short-term memory, which lacks long-term memory by construction.<br/><br/>When compared to its classic counterparts, an amnesic machine can facilitate more compute engines to occupy the area once devoted to (long-term) memory, which can further be harnessed for recomputation. Recomputation can transparently convert workloads to become more compute intensive such that they can make a better use of classic architectures optimized for performance, as opposed to energy efficiency. This project intends to unlock the energy efficiency premise of amnesic machines. To this end, the PI proposes to explore compiler and microarchitecture support to decide when to replace remote loads with re-computation. The research has three major thrusts: (i) at compiler time, integer linear programming based optimization will be performed to generate a probabilistic energy-minimal re-computation schedule; (ii) runtime controller will be designed to decide when to re-compute based on the information the compiler provides; and (iii) memory hierarchy will also be explored to maximize energy efficiency. Interaction with emerging technologies and reliability issues will also be considered. The integrated education and outreach plan includes workshop organization, undergraduate curriculum, and Eureka! program for K-12 girls ."
"1822933","SPX: Collaborative Research: Distributed Database Management with Logical Leases and Hardware Transactional Memory","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/30/2018","Andrew Pavlo","PA","Carnegie-Mellon University","Standard Grant","Marilyn McClure","09/30/2021","$500,000.00","","pavlo@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","042Y","026Z","$0.00","Database systems are the foundation of critical applications that maintain large amounts of data. Since single-processor performance plateaued a decade ago, increasing the number of processors or servers has become the only viable way of improving performance in distributed database management systems. Scalability is a daunting challenge in these systems due to the complex coordination among the large number of parallel tasks---a problem that this project seeks to solve. Most existing database systems determine the order among parallel tasks using conventional physical time. These systems require managing distributed locks, which leads to blocking and computation overhead. Other systems use logical time, which can be thought of as position in an order, to eliminate locking, but require centralized generation of the ordering, which is a serious scalability bottleneck as core count increases. This project breaks the abstraction of physical time and replaces it with a new definition of time that incorporates both logical and physical aspects. ""Physiological"" time, termed physiological time for ease of pronunciation, uses logical timestamps to order events and then breaks ties using physical time. This enables novel dependency-avoiding approaches to improving system performance and scalability. <br/><br/>This project applies physiological time to three components in a distributed database system. (1) At hardware level, a new hardware transaction memory (HTM) mechanism will be built, which allows more effective data movement in multi-core processors' caches. (2) A new distributed concurrency control protocol will be designed and implemented to coordinate large numbers of parallel tasks in a distributed database. (3) An efficient parallel indexing data structure will be proposed for both multi-core and distributed databases. All three parts of the project will be prototyped and deployed in hardware/software testbeds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749665","EAGER: Coded Caching for Heterogeneous and Energy Harvesting Systems","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","08/31/2017","Aylin Yener","PA","Pennsylvania State Univ University Park","Standard Grant","Phillip Regalia","08/31/2019","$225,000.00","","yener@ee.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7797","7916, 7935","$0.00","Demand for content with high data rate requirements has reached unprecedented levels. Services like video on-demand, once thought impossible to deliver through resource constrained wireless channels, will become the norm rather than the exception in the near future. Next generation designs thus need to utilize new resources to satisfy user demands. Coded caching is a promising paradigm in this direction in that it offers local data storage as a means to alleviate network congestion. This project aims to make significant progress in bridging the gap between theory and practice of coded caching by addressing the heterogeneity in the data storage capacities of the nodes. This project also puts forward a new content delivery network architecture envisioned to include energy harvesting nodes whose heterogeneous energy availability needs to be explicitly taken into account in designing caching strategies.<br/><br/>The project involves optimizing caching placement and delivery strategies for networks with heterogeneous cache sizes. Building on these strategies, the project will also pursue the research direction where energy sustainability is improved by capitalizing caching in energy harvesting communication networks. The research will utilize information theory, communication theory and optimization as tools and is envisioned to be of fundamental nature. The broader impact of the project includes informing the design of next generation content delivery networks, as well as contributing to the vision of new network architectures with energy harvesting nodes introduced to content delivery networks. The educational broader impact includes dissemination through invited lectures, tutorials, and publications, as well as integration of research results into the graduate curriculum."
"1850045","CRII: SHF Software and Hardware Architecture Co-Design for Deep Learning on Mobile Device","CCF","SOFTWARE & HARDWARE FOUNDATION","02/15/2019","02/11/2019","Cong Wang","VA","Old Dominion University Research Foundation","Standard Grant","Almadena Chtchelkanova","01/31/2021","$175,000.00","","c1wang@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","CSE","7798","7798, 7942, 8228","$0.00","Smartphones have become an indispensable part of our lives, acting as the primary tool for many essential functions. A smartphone carries a rich set of data with all kinds of personal information. Powered by machine learning, mobile applications are utilizing these data for better quality of service. Current practice requires users to offload computation tasks to the cloud with incumbent challenges on privacy, performance and user experience fronts. Boosted by the dramatic increase in mobile processing power, this project seeks to bring machine intelligence to mobile devices. The results are expected to inspire both theoretical and system research in the areas of software foundations for embedded machine intelligence. Lessons learned through this project will be fundamentally important in the designs of the next generation mobile operating system and hardware architecture. The results will be disseminated through publications and talks. <br/> <br/>This project seeks to develop a high-performance, privacy-preserving and energy-efficient mobile-based platform, with an application of behavioral authentication. The research will study the optimal representation of sensing data and develop a compact and powerful neural network architecture. All computation including both inference and training will be performed on the mobile device. A protocol to enable feature transfer between the mobile and the cloud will be developed to reduce overfitting and speed up model convergence, along with a new training algorithm to exploit cache locality and mitigate the memory bottleneck. The optimal combinations of batch size and learning rate will be explored within memory constraints and accuracy requirements to minimize training time. The problem of when and how training should be scheduled on a mobile device will be investigated by considering low-level operation with high-level user interaction to achieve a good balance between performance and resource consumption. All these modules will be integrated and implemented in Android and evaluated on various smartphone models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740126","E2CDA: Type II: Non-Volatile In-Memory Processing Unit: Memory, In-Memory Logic and Deep Neural Network","CCF","Energy Efficient Computing: fr","09/15/2017","07/05/2018","Deliang Fan","FL","University of Central Florida","Continuing grant","Sankar Basu","08/31/2020","$122,980.00","","dfan@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","015Y","7945","$0.00","The objective of this project is to explore leveraging emerging nanoscale spin-orbit torque magnetic random access memory (SOT-MRAM) to develop a non-volatile in-memory processing unit that could simultaneously work as non-volatile memory and a co-processor for next-generation energy efficient and high performance computing system. Such energy efficient in-memory computing system integrates logic and memory units by exploring innovations from emerging spintronic device technology to non-Von Neumann architecture, which is targeting to tackle power wall and memory wall bottlenecks in traditional computing system. It will be crucial for industry and academia to identify next-generation energy efficient and high performance computing platform design. The project also has education and outreach components including new curriculum in post-CMOS devices and circuits for undergraduate/graduate students, engineering outreach to diverse population and other underrepresented groups at the University of central Florida. The project will also directly involve minority and female graduate/ undergraduate students.<br/><br/>The proposed research requires synergistic exploration spanning from device technology to architecture innovation. Specifically, it consists of three research thrusts: (i) exploring novel SOT-MRAM memory array that could implement in-memory logic (AND/OR/XOR) without add-on logic circuits; (ii) investigating non-volatile in-memory processing unit (MPU) architecture that could simultaneously work as nonvolatile memory and co-processor to pre-process raw data within memory to accelerate data/computing intensive applications without sacrificing memory capacity; (iii) exploring MPU to implement in-memory convolution to greatly reduce data communication and accelerate state-of-the-art deep learning convolutional neural networks."
"1822976","SPX: Collaborative Research: Automated Synthesis of Extreme-Scale Computing Systems Using Non-Volatile Memory","CCF","SPX: Scalable Parallelism in t","10/01/2018","07/05/2018","Sumit Jha","FL","University of Central Florida","Standard Grant","Anindya Banerjee","09/30/2022","$500,000.00","","jha@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","042Y","026Z","$0.00","The project investigates the design of a scalable computing infrastructure that uses nanoscale non-volatile memory (NVM) devices for both storage and computation. The project's novelties are (i) the use of multiple parallel flows of current through naturally occurring sneak paths in NVM crossbars for computation; (ii) the replacement of slow organic expert-driven discovery of flow-based computing designs by automated synthesis techniques for accelerated discovery of novel NVM crossbar designs; and (iii) a pervasive focus on fault-tolerance throughout the design of exact, approximate and stochastic flow-based computing designs. The project's impacts are (i) the design of an end-to-end framework that maps compute-intensive kernels written in a high-level programming language onto nanoscale NVM crossbar designs and (ii) the creation of a new scalable capability to perform exact and approximate in-memory digital computations on fault-prone nanoscale NVM crossbars. The team of computer scientists and nanoscience researchers is creating flow-based computing designs for four benchmark problems: the Feynman grand prize problem, computer vision, basic linear algebra, and simulation of dynamical systems. The automatically synthesized NVM crossbar designs are being evaluated using high-performance simulations and experimental benchmarking in a modern nanotechnology laboratory. <br/><br/>Computing using multiple parallel flows of current through data stored in nanoscale crossbars is often fast and more energy-efficient, but the design of such crossbars is highly unintuitive for human designers. The project explores a combination of formal methods for checking satisfiability of Boolean formulae, and artificial intelligence techniques such as best-first search, to automatically synthesize NVM crossbar designs from specifications written in a high-level programming language. The team of computer scientists and nanoscience researchers is pursuing a transformative agenda for extreme-scale computing by leveraging memory devices in NVM crossbars as structurally-constrained fault-prone distributed nano-stores of data, and exploiting the natural parallel flow of current through NVM crossbars for computing over data stored in the distributed nano-stores. The NVM crossbar designs generated from OpenCV, LAPACK, and ODEINT programs are evaluated using the Xyce circuit simulation software and subsequently fabricated for experimental benchmarking. By combining storage and computation on the same device, the project circumvents the von Neumann barrier between the processor and the memory and creates scalable solutions for extreme-scale computing on fault-prone NVM crossbars without introducing substantial changes to the programming model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812495","SHF: Small: Collaborative Research: Integrated Framework for System-Level Approximate Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/11/2018","Ahmed Louri","DC","George Washington University","Standard Grant","Yuanyuan Yang","06/30/2021","$225,000.00","","louri@email.gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","7798","7923, 7941","$0.00","Nanocomputing is encountering fundamental challenges with respect to performance and power consumption; it requires different computational paradigms that exploit specific features in the targeted set of applications as well as an integrated framework for assessing the interactions between hardware and the processing algorithms (software). Approximate (inexact) computing has been advocated as a novel approach for nanocomputing design. Approximate computing generates results that are good enough rather than always fully accurate and correct outputs. Recent advances at circuit level have shown that there is an urgent need to investigate and enable at system-level the flexible utilization, improvement and close monitoring of approximate resources; this allows the efficient and integrated interaction of algorithms and hardware to meet the multiple and often conflicting figures of merit of high performance, lower power consumption and reduced inaccuracy.  The goal of this project is to develop approximate computing systems that are capable of adjusting performance by exploiting relationships between hardware and software (referred to as intra-level) in different applications such as cognitive processing, DSP, big data and scientific processing for which data can be adaptively utilized and manipulated. <br/><br/>This project is an organized effort that combines recent advances in technology with architectural enhancements into an integrated framework for approximate computing that will tackle the critical challenges of emerging computer designs in a comprehensive manner. This framework consists of new functional and computational primitives of hardware resources and related algorithms to allow an evaluation at system-level to meet the desired metrics for approximate computing. Intra-layer relationships such as number representation (such as floating point and logarithm) and accuracy by employing dynamic approximation schemes and data remediation for both communication and computing are also analyzed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814654","SHF: Small: The Whole Program Critical Path Approach to Parallelism","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","05/23/2018","David August","NJ","Princeton University","Standard Grant","Anindya Banerjee","09/30/2021","$500,000.00","","august@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7798","7923, 7943","$0.00","Multicore processors are everywhere, from the smartphones in our pockets to compute nodes in data centers.  The key to speed and energy efficiency in devices with multicore processors is in finding enough parallelism in computer programs to keep as many of the multiple cores active as possible.  Historically, both computers and people have not been very good at finding enough parallelism for multicore processors.  This leaves many cores idle, making multicore devices slow and inefficient.  This project addresses this problem by helping computers and people extract previously unconsidered parallelism, parallelism that the investigator and others have shown to exist but is hidden because it spans large portions of the whole program.  The project's novelties are new methods and tools to help computers and people extract this previously unconsidered parallelism in programs for multicore processors. The project's impacts are increased performance and energy efficiency for all multicore devices, from smartphones to data centers.<br/><br/>Prior work by the investigator and others demonstrated the applicability and importance of considering the loop critical path in extracting scalable parallelism from loops.  Despite the successes of the loop critical path approach, it misses many opportunities, and its gains seem to have plateaued. The problem is that optimal loop-local decisions often dismiss opportunities revealed by considering the whole program critical path. To realize these opportunities, this project goes beyond loop-level optimization by applying the lessons learned in earlier critical-path-based approaches to entire programs.  This project's aim is a dramatically higher degree of parallelism realized by exploiting concurrency across loop invocations and through non-loop sections of programs.  This project's tasks include analyzing information from dynamic data dependences to expose the whole program critical path, making this information readily accessible to both compilers and programmers, and guiding new compiler transformations to create even more scalable and efficient parallel versions of programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822920","SPX: Collaborative Research: Distributed Database Management with Logical Leases and Hardware Transactional Memory","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/30/2018","Srini Devadas","MA","Massachusetts Institute of Technology","Standard Grant","M. Mimi McClure","09/30/2021","$450,000.00","","devadas@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","042Y","026Z","$0.00","Database systems are the foundation of critical applications that maintain large amounts of data. Since single-processor performance plateaued a decade ago, increasing the number of processors or servers has become the only viable way of improving performance in distributed database management systems. Scalability is a daunting challenge in these systems due to the complex coordination among the large number of parallel tasks---a problem that this project seeks to solve. Most existing database systems determine the order among parallel tasks using conventional physical time. These systems require managing distributed locks, which leads to blocking and computation overhead. Other systems use logical time, which can be thought of as position in an order, to eliminate locking, but require centralized generation of the ordering, which is a serious scalability bottleneck as core count increases. This project breaks the abstraction of physical time and replaces it with a new definition of time that incorporates both logical and physical aspects. ""Physiological"" time, termed physiological time for ease of pronunciation, uses logical timestamps to order events and then breaks ties using physical time. This enables novel dependency-avoiding approaches to improving system performance and scalability. <br/><br/>This project applies physiological time to three components in a distributed database system. (1) At hardware level, a new hardware transaction memory (HTM) mechanism will be built, which allows more effective data movement in multi-core processors' caches. (2) A new distributed concurrency control protocol will be designed and implemented to coordinate large numbers of parallel tasks in a distributed database. (3) An efficient parallel indexing data structure will be proposed for both multi-core and distributed databases. All three parts of the project will be prototyped and deployed in hardware/software testbeds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1566483","CRII: SHF: System-Level Detection, Modeling, and Mitigation of DRAM Failures to Enable Efficient Scaling of DRAM Memory","CCF","CRII CISE Research Initiation","04/01/2016","03/31/2016","Samira Khan","VA","University of Virginia Main Campus","Standard Grant","Yuanyuan Yang","03/31/2019","$174,803.00","","smk9u@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","026Y","7798, 7941, 8228","$0.00","Future computing systems will be dominated by enormous amount of data processing. These systems will have to compute over exponentially growing user focused data from ubiquitous network and internet of things (e.g., sensors, self-driving cars, mobile devices, social media). At the same time, the forward progress of scientific innovations will greatly depend on the fast and efficient computation on high volume datasets generated from scientific experiments (analyzing gravitational waves, colliding particles in the particle accelerators, etc.). However, the current computing systems are bottlenecked by memory, but high capacity, scalable memory is essential for fast and efficient data processing in the future. Unfortunately, DRAM, the predominant underlying technology for memory is facing major scaling challenge. As DRAM scales down to smaller technology nodes, cells become more vulnerable, resulting in DRAM failures. Enabling a higher capacity memory system without sacrificing reliability is a major research challenge.<br/><br/>This research focuses on developing fundamental breakthrough that can enable scalable memory system for the future systems. This proposal provides research plan and ideas to solve the DRAM scaling challenge in a completely new approach by separating the responsibility of providing reliable DRAM operation from designing memory cells with smaller feature size. The central vision of this proposal is to develop system-level detection and mitigation techniques for DRAM failures such that cells can be manufactured to be smaller without providing any reliability guarantee.  It is expected that ideas developed in this research will bridge the gap between circuits and systems and will enable a holistic approach to solve the DRAM scaling challenge. The cross-cutting nature of the work will influence circuit-level testing, computer architecture, and OS and systems design and can potentially enable collaboration between different communities (testing and systems/architecture). The ideas developed in this research will not only impact innovation in computing, but will also help numerous scientific fields to take a leap towards new innovations. The results of this research will be integrated to existing and new courses to impact student training and education, designed focusing on attracting the minority groups towards hardware and systems design to enhance diversity in the field."
"1526406","AF:  Small:  Approximation Algorithms for Geometric Network Optimization","CCF","ALGORITHMIC FOUNDATIONS","07/01/2015","06/17/2015","Joseph S. Mitchell","NY","SUNY at Stony Brook","Standard Grant","Rahul Shah","06/30/2019","$450,983.00","Esther Arkin","joseph.mitchell@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7796","7923, 7929, 9102","$0.00","Networks are the backbone of our modern world, from the internet to cellular communication networks, to transportation networks of roads and rails, to the power grid, to social networks, neural networks, computer circuitry, and more.  Optimization problems (such as finding the most efficient way to place sensors or transmitters/relays to achieve coverage and connectivity, or computing a shortest route to visit a set of locations, or determining a set of routes for a fleet of robots to search a domain) arise naturally in logistics applications, including vehicle routing, robotics, advanced transportation systems, and communication.  Many networks are geometric, involving infrastructure deployed in physical spaces or involving geographic coordinates and connectivity based on proximity.  Even abstract networks often have special structure that essentially make them ""geometric"" when viewed appropriately, and this can have an impact on the efficiency of methods used to study them.  This project investigates how to exploit special geometric structure of networks in order to obtain efficient algorithms for solving various optimization problems --- studying them through the lens of computational geometry and approximation algorithms.  A particular challenge addressed by this project is the fact that data is often plagued with errors and sources of uncertainty that must be addressed within the model and the solutions.  <br/><br/>The discovery of ""polynomial-time approximation schemes"" for a wide variety of problems related to the classic ""traveling salesperson problem"" has shown that provable approximation algorithms with substantially better theoretical guarantees are often possible in geometric settings.  The project will advance the state of the art in approximation algorithms for several variants of the vehicle routing problem in geometric domains.  Examples include visibility coverage optimization for static sensors as well as mobile agents (robotic ""watchmen"").  Of special interest are problems involving uncertain geometric data, which may arise from a stochastic process (e.g., weather events) or from imprecise knowledge of deterministic data.  While many of the solution techniques are considered to be purely theoretical, there is some hope that simplifications of earlier techniques will give rise to practical methods and that a deeper understanding of what makes some geometric problems easier to solve than their most general abstract counterparts.  The problems will be attacked on two fronts, through the use of formal algorithmic analysis, with proofs of the tightest possible provable bounds (upper and lower) on worst-case or average-case performance metrics (time, space, and approximation ratio), and through the development of solution techniques designed to be simple, fast, and practical, with new methods and heuristics compared experimentally.<br/><br/>The research has broader impact in transportation engineering, energy optimization, air traffic management, sensor networks, robotics, manufacturing processes and logistics, virtual environments, automated inspection, homeland security, and geographic information systems.  Tools of optimization, network analysis, approximation algorithms, and computational geometry will be developed and applied to attack these problems.  The project will advance the research frontier, while training students at all levels, and from varied disciplines, in the pursuit of research and problem solving.  The project incorporates a tightly-integrated educational mission, through courses, seminars, and training of both graduate and undergraduate students."
"1823015","SPX: Collaborative Research: Automated Synthesis of Extreme-Scale Computing Systems Using Non-Volatile Memory","CCF","SPX: Scalable Parallelism in t","10/01/2018","07/05/2018","Nathaniel Cady","NY","SUNY Polytechnic Institute","Standard Grant","Anindya Banerjee","09/30/2022","$500,000.00","","ncady@sunypoly.edu","257 Fuller Rd.","Albany","NY","122033603","5184378689","CSE","042Y","026Z","$0.00","The project investigates the design of a scalable computing infrastructure that uses nanoscale non-volatile memory (NVM) devices for both storage and computation. The project's novelties are (i) the use of multiple parallel flows of current through naturally occurring sneak paths in NVM crossbars for computation; (ii) the replacement of slow organic expert-driven discovery of flow-based computing designs by automated synthesis techniques for accelerated discovery of novel NVM crossbar designs; and (iii) a pervasive focus on fault-tolerance throughout the design of exact, approximate and stochastic flow-based computing designs. The project's impacts are (i) the design of an end-to-end framework that maps compute-intensive kernels written in a high-level programming language onto nanoscale NVM crossbar designs and (ii) the creation of a new scalable capability to perform exact and approximate in-memory digital computations on fault-prone nanoscale NVM crossbars. The team of computer scientists and nanoscience researchers is creating flow-based computing designs for four benchmark problems: the Feynman grand prize problem, computer vision, basic linear algebra, and simulation of dynamical systems. The automatically synthesized NVM crossbar designs are being evaluated using high-performance simulations and experimental benchmarking in a modern nanotechnology laboratory. <br/><br/><br/>Computing using multiple parallel flows of current through data stored in nanoscale crossbars is often fast and more energy-efficient, but the design of such crossbars is highly unintuitive for human designers. The project explores a combination of formal methods for checking satisfiability of Boolean formulae, and artificial intelligence techniques such as best-first search, to automatically synthesize NVM crossbar designs from specifications written in a high-level programming language. The team of computer scientists and nanoscience researchers is pursuing a transformative agenda for extreme-scale computing by leveraging memory devices in NVM crossbars as structurally-constrained fault-prone distributed nano-stores of data, and exploiting the natural parallel flow of current through NVM crossbars for computing over data stored in the distributed nano-stores. The NVM crossbar designs generated from OpenCV, LAPACK, and ODEINT programs are evaluated using the Xyce circuit simulation software and subsequently fabricated for experimental benchmarking. By combining storage and computation on the same device, the project circumvents the von Neumann barrier between the processor and the memory and creates scalable solutions for extreme-scale computing on fault-prone NVM crossbars without introducing substantial changes to the programming model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1725734","SPX: Secure, Highly-Parallel Training of Deep Neural Networks in the Cloud Using General-Purpose  Shared-Memory Platforms","CCF","SPX: Scalable Parallelism in t","09/01/2017","08/30/2017","Josep Torrellas","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yuanyuan Yang","08/31/2020","$500,000.00","Christopher Fletcher","torrellas@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","042Y","026Z","$0.00","Society is beginning to witness an explosion in the use of Deep Neural Networks (DNNs), withmajor impacts on many facets of human life, including health, finances, family life, andentertainment. To train DNNs, practitioners have preferred to use GPUs and, recently,specialized hardware accelerators. Despite constituting the bulk of a data center?s computeresources, general-purpose shared-memory multiprocessors have been regarded as unattractiveplatforms. In this project, the Principal Investigators (PIs) think that these platformshave high potential. Consequently, this project will develop new techniques to dramaticallyimprove shared-memory multiprocessor performance in training DNNs. Already, shared-memoryservers are compelling for several reasons: they can support a high-degree of parallelism,are general-purpose and easy to program, and provide flexible, fine-grain inter-corecommunication. However, efficiently using shared-memory servers to train DNNs imposes<br/>significant challenges. First, fine-grain synchronization is still expensive, and latenciesare non-trivial. In addition, when DNN training moves to an environment with multiple userssharing the same physical shared-memory platform in the cloud, privacy and integrity becomemajor concerns.<br/><br/>To overcome these challenges, this project will synergistically address architecture andsecurity issues. On the architecture side, it will augment a highly-parallel shared-memoryserver with support for synchronization, data movement, data sharing, and DNN sparsitystructuring. On the security side, it will investigate how shared-memory servers createnovel privacy and integrity threats (for example, leaking the DNN?s sparse structure andforcing incorrect model generation), and how to defend against those threats. The project?sbroader impact is to help enable ?neural network training for everyone,? by making aubiquitous and easy-to-program platform a viable and safe target for running theseimportant, emerging workloads."
"1337399","XPS: SDA: Elasticizing the Linux Operating System for the Cloud","CCF","Computer Systems Research (CSR, Exploiting Parallel&Scalabilty","09/01/2013","07/26/2018","Richard Han","CO","University of Colorado at Boulder","Standard Grant","M. Mimi McClure","08/31/2019","$757,992.00","Eric Keller","richard.han@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7354, 8283","9251","$0.00","One of the major recent advances in computing is the development of large scale data centers, wherein hundreds of thousands of computers may be housed in each data center. In cloud computing, individual applications can each lease computing space to execute on one or more of a data center?s computers.  Cloud applications often need to dynamically adjust the amount of resources that they lease, elastically scaling up or down the amount of processing, memory, storage and/or network bandwidth that they need. Today's cloud-based systems burden application developers by requiring elasticity to be explicitly encoded into their software.  This project seeks instead to investigate an approach that eases the task of elasticizing cloud-based applications by automatically incorporating elasticity at the operating system (OS) level to support dynamic scaling of applications. This project plans to develop an open source software tool called ElasticOS that incorporates elasticity into the Linux OS, with the hope that such a practical tool could lead to significant broader impacts for society, namely transforming the way that major cloud providers deploy applications within their cloud infrastructure, and benefiting application developers by easing the complexity of elastic programming in the cloud.<br/><br/>The intellectual merit and research advances expected from this project concern the development of novel techniques and tools for supporting elasticity of memory, networking, storage, and processing in cloud-based modern operating systems. In particular, the project will explore the feasibility and performance of a new concept to achieve elastic memory by stretching of processes/threads across cloud machines using the idea of elastic page tables. Further research challenges expected to be addressed by the proposal include the following: identifying and building the major components of an elastic OS architecture; devising a way to unify the network address space across multiple nodes so that network I/O can be treated as elastic; discovering a practical adaptive online algorithm for page clustering and placement that exploits application locality and parallelism; extending network elasticity to on-chip networking; discovering methods to accommodate multi-threading in elasticity; and developing a timely and accurate protocol for discovering available elastic cloud resources. The project intends to test four different types of standard applications on top of ElasticOS in order to better understand how to tune the elasticity: a large in-memory database application; a compute-intensive application; a network-intensive Web server application; and a ubiquitous computing application. The PIs are highly qualified to pursue the proposed research, and have well-known expertise in operating systems, networking, mobile cloud computing, computer architecture, wireless sensor networks, and distributed systems.   Additional important broader impacts for society resulting from this project are expected to include enhancing the curriculum of advanced graduate systems courses and enabling undergraduate students, underrepresented minorities and women to participate in the project through programs such as REU and the Colorado Diversity Initiative."
"1814969","SHF: Small: A Scalable Architecture for Ubiquitous Parallelism","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","07/20/2018","Daniel Sanchez Martin","MA","Massachusetts Institute of Technology","Standard Grant","Yuanyuan Yang","09/30/2021","$450,000.00","","sanchez@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7798","7923, 7941","$0.00","With cost-performance gains predicted by Moore's Law slowing down, future computer systems will need to harness increasing amounts of parallelism to improve performance. Achieving this goal requires new techniques to make massive parallelism practical, as current multicore systems fall short of this goal: they squander most of the parallelism available in applications and are exceedingly hard to program. To address these challenges, this project is investigating a novel parallel architecture that efficiently scales to thousands of cores and is almost as easy to program as sequential systems. It achieves these benefits by exploiting ordered parallelism, which is general and abundant but is hard to mine in current systems. The technologies being investigated will make future parallel systems more versatile, scalable, and easier to program. These techniques will especially benefit hard-to-parallelize irregular applications that are key in emerging domains, such as graph analytics, machine learning, and in-memory databases. The prototyping efforts will bring the benefits of ordered parallelism to existing systems. Finally, the infrastructure developed as part of this project will be released publicly, enabling others to build on the results of this work.<br/><br/>Towards the goal of efficiently parallelizing the vast majority of applications while retaining the programming simplicity of sequential systems, this project is investigating and developing the following techniques: (1) distributed data-centric execution, which scales fine-grained ordered parallelism and speculative execution to rack-scale systems with tens of thousands of cores; (2) an expressive execution model that supports seamless combinations of speculative and non-speculative tasks, improving efficiency and parallelism; (3) adaptive speculation and resource management techniques that avoid performance pathologies, reduce wasted work, and make more efficient use of this novel architecture; and (4) an FPGA-based prototype of this architecture that leverages these techniques to exploit ordered parallelism and accelerate important applications. In this architecture, programs consist of tiny tasks with order constraints. The system executes tasks speculatively and out of order, and efficiently speculates thousands of tasks ahead to uncover ordered parallelism. Tasks are distributed to run close to their data, reducing data movement and allowing the system to scale across multiple chips and boards. An early 256-core design demonstrates near-linear scalability on programs that are often deemed sequential, outperforming state-of-the-art algorithms by one to two orders of magnitude.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1563880","SHF: Medium: Collaborative Research:  Run-Time Support for Scalable Concurrent Programming","CCF","Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","09/01/2016","04/27/2016","Nir Shavit","MA","Massachusetts Institute of Technology","Standard Grant","Anindya Banerjee","08/31/2019","$540,000.00","","shanir@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7354, 7798","7924, 7943","$0.00","Highly-concurrent data structures lie at the heart of modern multicore software. This project will provide systematic run-time system support for several techniques that that have proved essential for constructing highly-concurrent data structures. The intellectual merits are to provide a new basis for thinking about how to scale future generations of both hardware and software, and in particular to develop novel uses of operating system kernel functionality, as well as transactional hardware and software techniques. The project's broader significance and importance is the benefit to society provided by higher performing, less expensive, and more reliable software.<br/><br/>The specific techniques addressed are unsynchronized traversals, in which a thread navigates through a linked data structure without writing to memory, and atomic sequences of memory operations, where race conditions are eliminated by making a sequence of individual memory operations appear to take place instantaneously. Although these techniques have been successfully deployed in many ad-hoc instances, they have never been packaged as general-purpose mechanisms because they can have complex and dangerous interactions with standard memory management schemes. This project will exploit recent developments in hardware architectures and operating system structures to develop automated, systematic run-time support, making these techniques accessible to non-specialists."
"1561807","SHF: Medium: Collaborative Research: Run-Time Support for Scalable Concurrent Programming","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2016","04/27/2016","Maurice Herlihy","RI","Brown University","Standard Grant","Anindya Banerjee","08/31/2019","$540,000.00","","herlihy@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7798","7924, 7943, 9150","$0.00","Highly-concurrent data structures lie at the heart of modern multicore software. This project will provide systematic run-time system support for several techniques that that have proved essential for constructing highly-concurrent data structures. The intellectual merits are to provide a new basis for thinking about how to scale future generations of both hardware and software, and in particular to develop novel uses of operating system kernel functionality, as well as transactional hardware and software techniques. The project's broader significance and importance is the benefit to society provided by higher performing, less expensive, and more reliable software.<br/><br/>The specific techniques addressed are unsynchronized traversals, in which a thread navigates through a linked data structure without writing to memory, and atomic sequences of memory operations, where race conditions are eliminated by making a sequence of individual memory operations appear to take place instantaneously. Although these techniques have been successfully deployed in many ad-hoc instances, they have never been packaged as general-purpose mechanisms because they can have complex and dangerous interactions with standard memory management schemes. This project will exploit recent developments in hardware architectures and operating system structures to develop automated, systematic run-time support, making these techniques accessible to non-specialists."
"1539551","CyberSEES: Type 1: Collaborative Research:  High-Performance Image Classification and Search Supporting Large-Scale Seafloor Biodiversity and Habitat Surveys","CCF","CyberSEES","09/01/2015","08/26/2015","Robert Sinkovits","CA","University of California-San Diego","Standard Grant","Rahul Shah","08/31/2019","$350,936.00","David Nadeau","rssinkovits@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8211","8207, 8231","$0.00","Seafloor ecosystems are complex environments populated by a great diversity of organisms. Unfortunately, these ecosystems are increasingly threatened by direct and indirect human activities, including changes in land-use practices, coastal runoff, energy and mineral extraction, and fishing pressure. Developing effective sustainability policies to deal with these ecosystem threats requires that we first understand seafloor communities as they are today, and then track how they change over time as human activities shift and sustainability policies are modified. Recent advances in high-resolution underwater imaging offer new ways to do this. Survey ships can zigzag back and forth above a threatened region, towing a submerged camera system that repeatedly snaps pictures of the seafloor. This produces an enormous and valuable image set that captures the current state of a seafloor ecosystem. Surveys like this have been done for many threatened regions, and more are in progress. Substantial challenges remain to process these image sets. A useful characterization of a seafloor habitat requires knowing which specific types of corals, sponges, starfish, and so forth are present, how many there are, and how they are distributed throughout a region. But with each survey image set containing hundreds of thousands or millions of images, manual processing is impractical. Instead of an army of experts examining these images, computer software can scan each image and automatically recognize the color and texture of different seafloor species. Experimental classification software like this exists today in research laboratories, but the software is slow. To be useful for huge image sets, this software must be revised and optimized to run on the latest high-performance supercomputers. This is the focus of the project, which will yield new optimized classification software that can quickly sweep through enormous image sets to classify and count the species present and provide essential information about the health and biodiversity of threatened seafloor ecosystems, or any other ecosystem with a suitable image set. Then, when surveys are repeated for the same region every few years, this processing can reveal important trends that document the health of a region and the impact of new sustainability policies that aim to mitigate continuing threats to these communities.<br/><br/>This project leverages prior work prototyping seafloor image classification algorithms. These algorithms divide survey images into small tiles, then characterize each tile with a high-dimensionality feature vector that includes metrics on the colors and textures present in the tile, along with water temperature, salinity, and depth data collected by the survey apparatus at the moment the image was captured. Colors in the feature vector are chosen based upon a quantized hue histogram of the tile, while textures are characterized by luminance Discrete-Cosine-Transform (DCT) coefficients. A tile's feature vector is then compared against stored feature vectors for known species within a large classification library. A probability-based selection using a set of nearest-neighbor matches from the library yields a best guess for the species depicted in the image tile. This process is repeated tile after tile, image after image throughout an image survey. Classification performance is strongly a function of the classification library size and the dimensionality of feature vectors used for image tiles and library entries. This project's approach to improve classification performance uses a customized k-d-tree search data structure for the classification library, along with domain knowledge to guide and tune the classification process. The project begins with new methods to cull the tree, prior to classification, by using broad survey characteristics, such as the geographic region covered, water temperature and salinity, the sea bottom type from acoustic data, and so forth. Additional techniques optimize the construction and matching of feature vectors by using survey and library metrics to cull and weigh vector components (such as contextual color gamut and texture detail reduction, principal component analysis to combine and weigh features), reduce the nearest-neighbor set size by using k-d tree metrics on library diversity, restructure the k-d tree to improve common case search and cache performance, and parallelize the search for efficient classification across multiple threads, cores, processors, and nodes in a large compute cluster. Together these new methods are expected to substantially increase classification performance and enable efficient processing for the latest large survey image sets."
"1439011","XPS:FULL:SDA: Reflex Tree - A New Computer and Communication Architecture for Future Smart Cities","CCF","CRCNS, Exploiting Parallel&Scalabilty","10/01/2014","08/08/2014","Tao Wei","RI","University of Rhode Island","Standard Grant","M. Mimi McClure","09/30/2019","$850,000.00","Haibo He, Qing Yang","wei@ele.uri.edu","RESEARCH OFFICE","KINGSTON","RI","028811967","4018742635","CSE","7327, 8283","8089, 8091, 9150","$0.00","This project studies a new computing and communication architecture, reflex-tree, with massive parallel sensing, data processing, and control functions designed to meet the challenges imposed by future smart cities. The central feature of this novel reflex-tree architecture is inspired by a fundamental element of the human nervous system -- reflex arcs, or neuro-muscular reactions and instinctive motions in response to urgent situations that do not require the direct intervention of the brain. The scientific foundation and engineering framework built by this project will pave the way for enhanced monitoring and management of critical smart city infrastructure, from gas/oil pipelines, water management, communication networks, and power grids, to public transportation and healthcare. The interdisciplinary and collaborative nature of the project will inspire broader participation in related areas of research.<br/><br/>Within the human body, a neural reflex arc is able to cause an individual to immediately react to a source of discomfort without the need for direct control from the brain. The reflex-tree architecture mimics such human neural circuits, using massive numbers of intermediate computing nodes, edge devices, and sensors to gather, process, and, most importantly, to react to data concerning critical infrastructure elements. Key innovations of the proposed reflex-tree architecture include: 1) A novel, 4-level, large scale, and application-specific hierarchical computing and communication structure capable of carrying out sensor-based decision-making processes. The required computation and nodal computing power increases at each successive stage in the hierarchy, with the level-1 cloud performing the most complex tasks. 2) A densely distributed fiber-optic sensing network and parallel machine learning algorithms will be developed targeting smart city applications. 3) Novel, complementary machine intelligence algorithms will be developed, providing accurate control decisions via multi-layer adaptive learning, spatial-temporal association, and complex system behavior analysis. 4) New parallel algorithms and software run-time environments will be proposed and developed that are specifically tailored to the novel reflex-tree system architecture.<br/><br/>To demonstrate the feasibility and performance of the reflex-tree architecture, a proof-of-concept prototype will be constructed utilizing a miniaturized, laboratory-scale municipal gas pipeline system. The prototype will incorporate a complete 4-level reflex-tree--a distributed fiber-optic sensing network deployed alongside pipelines, edge devices performing data classification using parallel SVM, intermediate nodes performing massively-parallel spatial and temporal machine learning, and the cloud as the root node running sophisticated parallel behavioral analysis and decision making tasks. The resulting system is a cross layer, high performance, and massively parallel computing platform, providing a foundational sensing and computer architecture for future smart cities."
"1422311","AF: Small: Collaborative Research: Reconfiguration Algorithms","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","08/22/2018","Diane Souvaine","MA","Tufts University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$251,788.00","Csaba Toth, Matias Korman, Greg Aloupis","dls@cs.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7796","7923, 7929, 9251","$0.00","Computational geometry uses graphs, polygons, and arrangements to model physical objects and phenomena. Some of the objects are inherently flexible (e.g., proteins and robotic arms), others are stationary but require adjustments on a regular basis (e.g., communication and transportation networks). The focus of this project is on reconfigurations. The goal is to describe, understand, and control the combinatorial, geometric and topological changes in geometric configurations.<br/> <br/>This research project will develop new algorithms and data structures for modifying geometric configurations in three areas: (1) Optimization problems for in the configuration space of geometric objects, including graph augmentation, variations of the classical TSP tour problem, and network design for multiple criteria. (2) Reconfiguration through discrete moves, where current challenges include designing efficient data structures to support shortest path computation in the configuration space, approximating the diameter and radius of configuration spaces, and deciding whether reconfiguration is possible. (3) Modeling continuous motion, which includes motion planning algorithms and corresponding dynamic data structures for bar-and-joint frameworks, hinged polygons, and disk arrangements, motivated by applications in protein folding. A unified approach to discrete and continuous reconfiguration problems allows breaking down complex systems into elementary operations, which in turn leads to more efficient computational tools.<br/> <br/>The collaboration between faculty members and students from two universities ensures a high quality of training and opens new opportunities for all participating students."
"1527874","CIF: Small: Graph Signal Sampling: Theory and Applications","CCF","COMM & INFORMATION FOUNDATIONS","07/15/2015","07/08/2015","Antonio Ortega","CA","University of Southern California","Standard Grant","Phillip Regalia","06/30/2019","$499,014.00","","ortega@sipi.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","7923, 7936","$0.00","Modern society is increasingly reliant on large scale, distributed, interconnected, and  complex systems, such as the Internet, smart grids, intelligent buildings or highways. Furthermore, much of the information now being generated is also interconnected in complex ways (e.g., the Web).  While these systems and datasets can be monitored by recording relevant data, the volumes of such data make it difficult to address critical tasks, such as anomaly detection, in a timely manner. These datasets often exhibit a natural graph structure, with graph nodes representing measurements or information (e.g., the temperature of a sensor or data from a web page), and graph edges representing the  relationships between nodes (e.g., distance between sensors or links between webpages). This project develops novel methods for sampling of very large scale graph datasets, with the goal of making it possible to measure only a small fraction of carefully selected nodes, while preserving the ability to analyze the whole system. <br/><br/>Sampling theory is a major element of signal processing theory and applications, but has only recently been considered for graph signals. While recent progress has been made under the assumption that the graph is fully known, these techniques are prohibitively expensive for practical datasets of interest. This project addresses fundamental questions for the challenging problem of sampling when only partial graph information is available (e.g., decisions based on smaller subsets of connected nodes). For example, given local graph connectivity information and assumptions about the graph signals of interest, such as their frequency localization, the goal is to identify the best set of vertices to sample locally in order to obtain a reliable estimate of the corresponding global graph signals."
"1849660","Student Travel Support for the 4th International Workshop on Software Analytics (SWAN 2018)","CCF","SPECIAL PROJECTS - CCF","10/01/2018","09/07/2018","Sonia Haiduc","FL","Florida State University","Standard Grant","Almadena Y. Chtchelkanova","09/30/2019","$10,000.00","","shaiduc@cs.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","2878","7556, 7924, 7944, 9102","$0.00","This travel award supports student participation at the 4th International Workshop on Software Analytics (SWAN 2018) happening in Florida, USA on November 5, 2018. The funds seek to encourage US-based students to attend, with priority given to underrepresented groups, US citizens/residents, and those otherwise unable to attend without financial support. The area of software analytics as the basis for empirical research in software engineering leads to a scientific understanding of phenomena surrounding software and its development. The area is emerging and will be boosted by increasing student participation in this start-up workshop series. The workshop is a forum for presentation of work in this field, and as such, it will help expand research and strengthen US participation in the field.<br/><br/>The technical scope of the workshop covers data-driven approaches for data exploration and analysis; predictive analytics for software; web analytics, development analytics; business intelligence tools; large-scale data mining, analysis and analytics for software projects; software analytics for various stakeholders (e.g., managers vs. developers); empirical studies on how software analytics are used in practice and their  effectiveness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1651817","EAGER: Semi-automated Type-directed Programming","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2016","08/12/2016","Peter-Michael Osera","IA","Grinnell College","Standard Grant","Anindya Banerjee","08/31/2019","$159,991.00","","osera@cs.grinnell.edu","1121 Park Street","Grinnell","IA","501121690","6412694983","CSE","7798","7916, 7943, 8206","$0.00","Type-directed programming is a powerful programming paradigm found in strongly-typed functional languages where the types of a program are used to guide its development. Users of such languages frequently comment that their programs ""write themselves"" once they declare the appropriate types. In reality, the actual development process is far from automatic; developers still must apply manual reasoning principles to derive their program even though many of their choices are forced by the language's type system. This project aims to mechanize the type-directed programming process by leveraging techniques from program synthesis and type theory. The intellectual merits of this project are twofold: (1) the expansion of the theoretical foundations of program synthesis with types and (2) the application of these foundations towards program assistance tools that aid in type-directed programming. Beyond merely providing a tool that enhances the productivity of current functional programmers, the project's broader significance and importance is the crystallization of the benefits of type-directed programming in a form that allow non-functional programmers to understand, appreciate, and directly benefit from this programming paradigm.<br/><br/>The project extends prior work in the foundations of program synthesis with types, addressing issues of expressiveness and scalability encountered when adopting these foundations into synthesis tools. Notably, the project unifies type-based and verification-based approaches to program synthesis, allowing rich support for both algebraic and primitive data types as well as providing a common framework for understanding both styles of synthesis.  In addition, the project investigates semi-automated, rather than fully-automated, program synthesis where the user interacts with the synthesis tool throughout the synthesis process. The basis of this approach lies in adopting the refinement tree, a data structure that captures the potential shapes of programs that a synthesizer can produce, into a useful data structure for visualizing and interacting with this tool. By pursuing semi-automated synthesis, these tools scale up to real-world programming environments by using the developer as an oracle whenever the tool would otherwise take too long or get stuck searching for a solution."
"1526118","SHF: Small: Collaborative Research:Text Retrieval in Software Engineering 2.0","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2015","07/20/2015","Andrian Marcus","TX","University of Texas at Dallas","Standard Grant","Sol J. Greenspan","08/31/2019","$200,000.00","","amarcus@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7798","7923, 7944","$0.00","Software systems contain large amounts of textual information captured in various software artifacts, such as, requirements documents, source code, user manuals, etc.  The productivity of software developers and the quality of the software they produce directly depends on their ability to retrieve and understand the textual information present in software.  Since humans cannot process and comprehend so much text, researchers proposed the use of text retrieval techniques to help software developers with many of their daily tasks.  In order to be useful, these techniques need to be properly configured, which requires calibrating many parameters.  As most software developers are not experts in text retrieval, they need help in determining the best text retrieval configuration in a given software engineering context.  The configuration problem is one of the main obstacles in the adoption of such techniques in the software industry, because many approaches proposed by researchers do not generalize well.  The outcomes of this project will transform the way software developers address many of their daily tasks, allowing them to easily adopt the use of text retrieval during software development.  The results of this research will also be used in software engineering courses to support students in their projects.  The new practices that the students will acquire will help them become better software engineers.  The proposed research also brings together work from different computing research communities: software engineering and information retrieval and it will bring new knowledge in both fields.  Existing approaches using text retrieval in software engineering will become more practical, rather than just promising, facilitating migration from the lab into industry and academia.<br/><br/>The outcome of this research will be: (1) a novel approach (called TRinSE2.0), which will achieve automatic, runtime query-based text retrieval configuration; and (2) improvements to important software engineering tasks, in practical settings, focusing on feature and bug location, impact analysis, traceability link recovery, and bug triage.  TRinSE2.0 will be evaluated on open source data, in the classroom, and in industrial settings.  The proposed work will transform the way text retrieval configuration is done in software engineering applications.  New, software-specific measures, as well as proven linguistic-based measures will be used to capture query properties in the context of software engineering tasks and data sets.  Machine learning algorithms will find the best configuration for a given query.  When writing a query to retrieve information from a software project, developers will get the best results, saving them time and effort, improving their productivity and the quality of their work.  The text retrieval configuration problem will no longer be heuristic-based, but it will become data-driven."
"1849463","EAGER: Tensor500: A Streaming Analytics High Performance Computing Benchmark","CCF","SOFTWARE & HARDWARE FOUNDATION, ","09/01/2018","08/20/2018","Timothy Andersen","ID","Boise State University","Standard Grant","Almadena Y. Chtchelkanova","08/31/2019","$99,873.00","Richard Murphy","tim@cs.boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","7798, R254","7916, 7942, 9150","$0.00","Large-scale data analytics is essential to the National Strategic Computing Initiative (NSCI) and NSF's scientific discovery mission. Data-intensive supercomputing applications are increasingly important for handling high-performance computing workloads. However, current benchmarks and performance metrics do not provide useful information on suitability of computing systems for data-intensive applications.  This project will develop a new benchmark that can be used to assess the performance of new system implementations aimed at this important class of applications.<br/> <br/>The research effort will produce a new Stream500 benchmark, complementary to Graph500, that replicates streaming analytics workload. The current benchmark capabilities will be expanded by enabling the exiting web-based infrastructure to accept submissions for multiple benchmarks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717033","CIF:Small:Network Tomography and Resource Allocation","CCF","COMM & INFORMATION FOUNDATIONS","08/15/2017","08/07/2017","Brian Mark","VA","George Mason University","Standard Grant","Phillip Regalia","07/31/2020","$499,998.00","Yariv Ephraim","bmark@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7797","7923, 7935, 7937","$0.00","Part 1.<br/>Communication networks, including the Internet and cellular networks, provide an infrastructure for information and data exchange that is critical to modern society.  As networks have increased in size and complexity to meet the growing demands of users and new multimedia applications, they have also become more difficult to manage.  Consequently, evaluating the performance of a network and allocating resources to improve performance are extremely challenging tasks.  This project will develop an approach to evaluate the performance of a network through endpoint traffic measurements and by sending probe packets through the network.  This approach is referred to as network tomography, since the concept is similar to medical tomography, whereby an image of an organ is reconstructed through virtual sectioning using some form of penetrating wave, e.g., X-rays or ultrasound.  <br/><br/>Current methods for network tomography do not scale well with the size of the network and cannot provide estimates of network performance in real-time.  The project will investigate new methods that will be capable of evaluating the performance of large networks in real-time through endpoint measurements and statistical analysis.  This information will in turn be used to allocate resources within the network in order to enhance network performance.  The ultimate goal of the project is to develop a framework, based on new methodologies for network tomography, to alleviate congestion and service degradation in communication networks such as the Internet, data storage networks, future cellular networks, as well as transportation networks.  The project will advance the field of networking via the development of real-time and scalable methods for network tomography and resource allocation, which will impact society through significant improvements in the performance, efficiency, and reliability of communication and transportation networks.  The project will also involve the development of an educational software tool to simulate and graphically depict the operation of a network, with knobs to allow the user to control the allocation of network resources and visualize the effect of such allocations on network performance.  Students, including some from underrepresented groups, will gain valuable experience from implementing algorithms, running computer simulations, and conducting empirical validation with real data from the Internet and cellular networks.<br/><br/><br/>Part 2:<br/>Performance evaluation and resource allocation of communication networks are extremely challenging tasks due to the tremendous scale and complexity of modern networks.  Traditional approaches to network performance evaluation include queueing theory and computer simulation.  In this project, a framework for network tomography of both traffic rate and delay on network links from endpoint traffic measurements, will be developed and investigated, with application to network resource allocation.  Gallager's model for minimum delay routing in a network will be used to formulate the joint problem of traffic rate and delay estimation, and derive information that can be applied directly to resource allocation.  A new approach to traffic rate tomography, which has the potential to improve upon the accuracy, computational efficiency, and generality of earlier methodologies will be explored.  A recent approach to delay network tomography based on parameter estimation of a partially observable bivariate Markov chain model will be further developed in the context of the proposed framework.  A major focus of the project will be on developing efficient online algorithms for network tomography and resource allocation that can be applied to improve network performance in near real-time.<br/><br/>The proposed investigations are grounded in the theory and estimation of multivariate Markov processes, and explore the interplay between the existing theories of queueing networks and statistical inference. The research will involve the development of new models for network tomography, recursive estimation algorithms, and empirical validation.  The proposed research will be applicable to the performance evaluation of a wide range of networks, including computer networks, cellular networks, and transportation networks.  The research will contribute to new mechanisms to improve the quality-of-service and quality-of experience for users of the Internet and next generation wireless infrastructure.  The proposed approach to network tomography will also help optimize the planning of transportation systems."
"1526848","CIF:  Small: Geometric, Variational Algorithms for Radiometric-Based Shape Reconstruction","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2015","07/27/2015","Anthony Yezzi","GA","Georgia Tech Research Corporation","Standard Grant","Phillip Regalia","07/31/2019","$500,000.00","","ayezzi@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797","7923, 7936","$0.00","The investigator will develop a new class of variational geometric inverse algorithms for reconstructing dense 3D shape of objects from measurements of a scene representing an arbitrary combination of vantage points and/or resolution by employing a common mathematical framework based on generative radiometric models. Reconstruction of shape from raw sensor data is a necessary step for graphical 3D rendering and analysis in scenarios where CAD models are unavailable or impossible. Two examples to be initially explored include unfocussed camera images from different viewpoints and/or focal lengths using thin-lens modelling, and radar signals reflected from nearby objects  with different antenna locations and/or wavelengths.<br/><br/>The framework will be general enough to apply to several related sensor modalities beyond those initially investigated, ranging from infrared, acoustics, and SAR. Furthermore, having a unified model affords the freedom to generate flexible data capture and fusion schemes where not one, but multiple sets of measurements are captured under different viewpoint and sensor setting characteristics, and use the entire set of collected data to infer an estimate of reflectance and geometry that is of superior quality relative to what may be obtained in scenarios where ""isolating a single cue"" is not possible. For instance, with cameras, it may be impossible to fix the viewpoint while capturing images of different focus (isolating focus), or to capture perfectly sharp images because of the finite aperture of the lens (isolating viewpoint). Removing this constraint can enable applications to endoscopy, inspection of pipes and crevices, dental impressions, as well as environmental monitoring."
"1815101","CIF: Small: Structured High-dimensional Data Recovery from Phaseless Measurements","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/22/2018","Namrata Vaswani","IA","Iowa State University","Standard Grant","Phillip Regalia","09/30/2021","$499,041.00","Chinmay Hegde","namrata@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7797","7923, 7935, 9102","$0.00","Phase retrieval (PR), or 'signal recovery from phaseless measurements', is a problem that occurs in numerous signal/image acquisition domains, such as Fourier ptychography and sub-diffraction imaging, in which only the magnitude (intensity) of certain linear projections of the signal or image can be measured. While PR is a classical problem, in recent years there has been renewed interest in PR with the goal of developing provably correct and fast algorithms. Much of this work, however, does not assume any structure on the signal, and as a result necessarily requires more measurements than the unknown signal's length. This can be a challenge when moving to very high resolution imaging because it implies a proportionally higher cost of data acquisition (in terms of time, number of sensors, or power consumption). Dynamic imaging of time-varying scenes, e.g., live biological samples, poses an even greater challenge. We address this limitation by exploiting two common classes of structural assumptions - sparsity and low-rank -- to enable fast and low cost high-resolution imaging.  A diverse group of graduate and undergraduate students is involved in the research.<br/><br/>This project develops the first set of provably correct, fast, and low-sample-complexity algorithms for phaseless low rank matrix recovery in two settings. The first involves recovery from phaseless linear projections of each column of the matrix. This finds applications in phaseless dynamic imaging when the (vectorized) image sequence is well approximated by a low rank matrix, e.g., slow changing dynamic scenes. The second setting involves recovery from phaseless linear projections of the entire matrix. This is useful when the image itself can be modeled as being low rank. This project also develops provably fast and statistically efficient sparse PR algorithms and explores extensions to learning generalized linear models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717943","CIF: Small: Distributed Statistical Inference with Compressed Data","CCF","COMM & INFORMATION FOUNDATIONS","07/15/2017","07/07/2017","Lifeng Lai","CA","University of California-Davis","Standard Grant","Phillip Regalia","06/30/2020","$449,996.00","","lflai@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7797","7923, 7936","$0.00","Due to the rapid growth of size and scale of datasets and desire to harnessing parallel processing capabilities of multiple machines, distributed statistical inference and machine learning, in which available data are stored in multiple machines who are allowed to communicate with each other with limited communication budgets, have attracted significant research interests. There are two basic scenarios for the distributed setting: sample partition and feature partition. Although there have been many recent work on the design of inference algorithms for the sample partition scenario, there has been limited work on the feature partition scenario. The focus of this project is to characterize the fundamental limits and develop distributed statistical algorithms for the feature partition scenario from information theoretic perspective.<br/><br/>Compared with the sample partition scenario, the feature partition scenario is significantly more challenging. This research addresses these challenges by focusing on two research thrusts. Thrust 1 focuses on designing interactive encoding schemes for inference. The main idea is that, by interacting with each other, the terminals can coordinate their compression so that the decision maker can obtain more information about the parameter while using the same communication resources, which will lead to a better inference performance. Thrust 2 designs function computing schemes for inference, in which the machines compute a function of observations without recovering them first and then perform inference from this function. The main motivation for this idea is that recovering observations or a compressed version of them is not necessary in the distributed inference setup, as the final goal of the distributed inference is to infer the value of the unknown parameter."
"1641774","EAGER:   Stream500: A New Benchmark and Infrastructure for Streaming Analytics","CCF","SOFTWARE & HARDWARE FOUNDATION, ","08/01/2016","07/28/2016","Timothy Andersen","ID","Boise State University","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$119,544.00","Richard Murphy","tim@cs.boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","7798, P275","7916, 7942, 8237, 9150","$0.00","Large-scale data analytics is essential to the National Strategic Computing Initiative (NSCI) and NSF's scientific discovery mission. Data-intensive supercomputing applications are increasingly important for high performance computing workloads. Current benchmarks and performance metrics do not provide useful information on suitability of computing systems for data-intensive applications. <br/>This research effort will produce a new Stream500 benchmark, complementary to Graph500, that replicates streaming analytics workload. The current benchmark capabilities will be expanded by enabling the exiting web-based infrastructure to accept submissions for multiple benchmarks."
"1618615","CIF: Small: Fundamental Limits of Wireless Communications with Stochastic Information Flows and Queueing Constraints","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/27/2016","Mustafa Gursoy","NY","Syracuse University","Standard Grant","Phillip Regalia","06/30/2019","$442,320.00","","mcgursoy@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7797","7923, 7935","$0.00","Mobile data traffic has experienced unprecedented growth recently and is predicted to grow further over the coming years. This exponential growth in the flow of mobile data and multimedia content has significant implications on wireless networks. For one, such wireless multimedia traffic requires certain quality of service guarantees. Another consequence is heterogeneity in network traffic. Wireless networks now carry heterogeneous stochastic traffic in diverse environments, and successful design of networks and effective quality of service provisioning for mobile multimedia communications critically depend on the appropriate choice of source traffic models. Motivated by these factors, this project addresses the important and timely topic of wireless communication with service guarantees. The outcomes of this research will significantly contribute to the design of next generation wireless networks supporting multimedia traffic with service guarantees, and are poised to have significant impact on the society due to extremely fast growth in mobile video traffic.<br/><br/>The methodology of this project is centered around combining tools from information theory and stochastic network calculus to establish a strong analytical framework by rigorously determining the fundamental limits of wireless communication under queuing constraints. In particular, this project investigates the maximum throughput in single- and multi-user wireless channels in the presence of random data arrivals and statistical queuing constraints. First, an idealistic setup with Gaussian codebooks and reliable communication with no bounds on the code lengths and no errors is addressed. Subsequently, wireless throughput is characterized in the more practical regimes of finite block-length coding and of finite-alphabet inputs with arbitrary distributions."
"1618653","CIF: Small: Hybrid analog-digital schemes for joint source-channel coding of digital sources","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/10/2016","Javier Garcia-Frias","DE","University of Delaware","Standard Grant","Phillip Regalia","05/31/2019","$403,168.00","","jgarcia@eecis.udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","7797","7923, 7935, 9150","$0.00","This project aims at the development of novel schemes for high throughput transmission of digital sources over noisy channels for a wide range of source and channel environments. The research effort will result in significant advancements in communications systems, including the case in which the data presents temporal or spatial correlations. Since multiple data sets, such as sensor networks and medical data, present this type of structures, this project has the potential to impact society as a whole.  <br/><br/>The main goal of this research is to study the application of hybrid analog-digital joint source-channel coding schemes for the transmission of digital sources over noisy channels. The proposed hybrid schemes consist of two sub-blocks concatenated in parallel:  First, a digital-to-analog encoder, which will produce either real numbers or many multiple discrete points from the input bits. Second, a digital encoder, which will produce coded bits from the input bits. The key idea is to take advantage of the analog component of the hybrid scheme to achieve high throughput communications, and robustness against changes in the channel conditions, something that is challenging with standard digital channel codes. On the other hand, the use of pure digital-to-analog encoders would lead to significant error floors, with the consequent performance degradation. However, thanks to the use of the digital encoder sub-block, these error floors can be substantially reduced to achieve an excellent performance. Hybrid analog-digital practical coding systems have not been applied for the encoding of digital sources. The development of these techniques is of great interest in practical applications such as image and video coding, sensor networks and medical applications."
"1841188","EAGER: Closed-loop Silicon-biomolecular Systems with Integrated Synthesis-fluidics-nanopore Interfaces","CCF","ELECT, PHOTONICS, & MAG DEVICE, SOFTWARE & HARDWARE FOUNDATION, COMPUTATIONAL BIOLOGY","10/01/2018","07/27/2018","Luis Ceze","WA","University of Washington","Standard Grant","Mitra Basu","09/30/2020","$199,906.00","Jeffrey Nivala","luisceze@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1517, 7798, 7931","7916, 7945, 7946","$0.00","Given the slowing down of cost-performance gains from Moore's law and the scalability limits of digital storage technology, biomolecules are attractive alternatives for information storage and processing -- DNA data storage in particular is especially interesting due to its density, durability, and path to feasibility. At the same time, electronics will likely continue to be an integral part of computing systems, due to its high performance and the ability to be engineered. Hence, it is natural to consider hybrid biomolecular-electronic systems. Towards this end, this project is focused on building a fully integrated, closed-loop system that includes DNA synthesis, molecular sensors (nanopore), and fluidics for novel applications in biomolecular information processing. If successful, the scientific community will be provided with new and easy-to-use methods to accelerate molecular data storage-processing-computing and synthetic biology experimentation. This will make molecular computing and synthetic biology applications more accessible to the broader community. Integrating in-vivo and in-vitro biomolecular components with silicon systems can lead to innovations in a range of areas from health diagnostics and therapies, new materials, food, to information technology.<br/><br/>The investigators will engineer a toolbox of new molecular parts that can be used to store and transmit information in the form of nanopore-addressable molecular barcoding of synthetic DNA and proteins. A digital fluidics system will employ computer vision techniques for reliable control of droplet movements. The investigators will develop machine learning techniques to analyze raw nanopore sensor data for low-cost and high-throughput identification of molecular outputs. Demonstration of these components within the integrated system to be developed as part of this project will show proof-of-principle applications that advance molecular information processing capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618427","AF: Small: Reconstructing Mixtures of DNA Sequences from High-Throughput Sequencing Data","CCF","ALGORITHMIC FOUNDATIONS, COMM & INFORMATION FOUNDATIONS","09/01/2016","06/03/2016","Haris Vikalo","TX","University of Texas at Austin","Standard Grant","Mitra Basu","08/31/2019","$400,000.00","","hvikalo@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7796, 7797","7923, 7931","$0.00","In a wide range of problems in genomics and personalized medicine, it is of critical importance to accurately reconstruct distinct nucleotide sequences present in a heterogeneous mixture. Examples include viral quasispecies reconstruction, mapping repertoire of immune cells, and haplotyping. While recent advancements in high-throughput DNA sequencing have enabled affordable studies of genetic variations, technological limitations of sequencing platforms as well as potentially non-uniform frequencies of the sequences in a mixture render the analysis of heterogeneous mixtures a challenging and computationally intensive task.<br/><br/>This research aims to develop fast and accurate algorithms for reconstruction and frequency estimation of sequences in diverse mixtures that will assist practitioners in pharmacogenomics and personalized medicine. The project includes a focus on fostering diversity, dissemination of new interdisciplinary research across disciplines, and enrichment of the educational experience of participating engineering students.<br/><br/>Specific goals of the project include: First, the design and analysis of matrix factorization methods for accurate and efficient reconstruction of distinct sequences present in a heterogeneous mixture and for estimation of their frequencies. In the proposed framework, sequence reconstruction is formulated as the problem of factorizing structured, partially observed low-rank matrices and efficiently solved by exploiting salient features of high-throughput sequencing data. Second, the development of a methodology for the analysis of dynamically evolving mixtures of sequences temporally sampled by means of high-throughput sequencing. This research thrust will lead to novel sequence reconstruction methods capable of tracking the evolution of sequences over time and accurate identification of their frequencies. The third and final goal is the development of algorithmic solutions to specific sequence diversity analysis problems that fully exploit structural features of the respective applications and thus enable superior performance."
"1704552","AF: Medium: Collaborative Research: Sequential and Parallel Algorithms for Approximate Sequence Matching with Applications to Computational Biology","CCF","SOFTWARE & HARDWARE FOUNDATION, COMPUTATIONAL BIOLOGY","07/01/2017","06/29/2017","Srinivas Aluru","GA","Georgia Tech Research Corporation","Standard Grant","Mitra Basu","06/30/2020","$525,000.00","","aluru@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798, 7931","7924, 7931, 7942","$0.00","Sequence matching problems are central to the field of genomics, both in analyzing naturally occurring sequences such as genomes and in analyzing data from sequencing instruments. Often, methods that can accommodate a small number of differences within the matching regions suffice in practice. Such methods, described as alignment-free or approximate sequence matching methods, have typically relied on heuristics. This project work is advancing the field by creating a mathematical framework and solving multiple approximate sequence matching problems with provably efficient run-time guarantees. Project work is also supporting the development of practical heuristics inspired and supported by the mathematical framework, development of parallel methods for solving large-scale problems on high performance parallel computers, and studying the impact of these methods on important applications. Project results are made available through open source software for use by practitioners. Results from this research will be incorporated into courses taught by the PIs, and disseminated more broadly through book chapters and tutorials and accompanying slides. The project will support research scientist and Ph.D. students in interdisciplinary training for launching them into productive careers focused on important problems of current relevance. Undergraduate participation is planned through course projects.<br/><br/>Project work builds upon recent progress in alignment-free genome comparison methods, and exploits the controlled error characteristics of data generated by high-throughput sequencers, and the many bioinformatics applications enabled by them. Project objectives include developing a robust algorithmic framework for designing newer alignment-free methods based on approximate substring composition, and developing sequential and parallel algorithms for pairwise approximate sequence matching among large sequence data sets. The goal is to develop algorithms that are asymptotically superior to quadratic alignment-based approaches, and achieve good practical performance either directly or through further development of practical heuristic that rely on the underlying theory. The developed techniques will be further investigated in the context of important applications such as read error correction, genome mapping, and assembly. Though conducted in the context of computational biology, some of the methods are potentially applicable to other areas such as text processing and information retrieval. Broader research community will be impacted through release of software modules and project work in important application areas."
"1719155","SHF: Small: Scalable and Practical Detection of Invariants for Software Inspection","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2017","05/24/2017","William Griswold","CA","University of California-San Diego","Standard Grant","Sol J. Greenspan","06/30/2020","$499,999.00","Massimiliano Menarini","wgg@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7798","7923, 7944","$0.00","Due to the intractability of completely testing software, code review by humans remains an important contributor to software assurance.Current tools for code review provide relatively simple information, such as a listing of the differences between the current source code and the previous version.Consequently, many code reviews miss important implications of the differences, such as inadequate testing or a software bug.This research is investigating richer representations of the difference between two software versions, with the goal of making it easier to spot defects and keep them out of software slated for release.The approach hinges on using runtime analysis for mining software repositories, making it an exemplar of a ""big data"" approach to quantitative software engineering.<br/><br/>Specifically, the research is exploring the promise of a foundational technology called invariant detection, which gathers data from the software's runs and produces a summary of the behavioral properties of its components.In the context of code review, these properties, if they disagree with expectations, reveal inadequate testing or actual software defects.Because the properties are not just syntactic, they can highlight the impact of code changes on the behavior of unmodified code.Because property summaries can be voluminous, they are presented as a difference between the properties of the current software version and the previous one.The research is also addressing challenges to practical application of the approach.Novel techniques are being developed for improving test suite adequacy, efficiently acquiring traces, and calculating invariants.Core to the approach is that the version-to-version changes to software are often incremental and contained, permitting substantial reuse of data from prior runs.In the course of addressing these challenges, the research is developing and extending a scalable, automated code review infrastructure, enabling practical validation of the approach in both laboratory experiments and case studies.Ultimately, the research holds the promise to put new tools in the hands of practicing software developers, helping them find bugs and improve their test suites."
"1527706","SHF: Small: Empirical Autotuning of Parallel Computation for Scalable Hybrid Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2015","07/06/2015","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Almadena Y. Chtchelkanova","06/30/2019","$450,000.00","Jakub Kurzak","dongarra@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","7798","7923, 7942, 9150","$0.00","Today, scientific and engineering computing is synonymous with parallel computing, and applications such as climate modeling, drug design, aircraft design, etc. utilize very large supercomputer installations, with power consumption measured in MegaWatts, and the cost of electricity measured in millions of dollars. At the same time, every parallel application requires some level of tuning to ensure that the software is mapped appropriately to the hardware. Otherwise, suboptimal performance can lead to lost cycles, kilowatt-hours, and, ultimately, dollars. Tuning the application by making repeated runs is also a wasteful option at very large scale. The DARE project addresses this problem by tuning the application through modeling and simulation of its behavior at very large scale, rather than actually running it. Therefore, resources required for tuning are marginal compared to those consumed in production runs. DARE is based on the observation that the same approach that replaces a wind tunnel with a computer simulation of the airfoil can be applied to the software itself. Two aspects of today's high-end computing landscape make the DARE work unique: 1) the prevalence of hardware accelerators, such as Graphics Processing Units and Xeon Phi co-processors, and 2) adoption of task-based, dynamic, work scheduling systems as an alternative to traditional, lock-step parallel programming models. In particular, DARE combines three components into a refinement loop: a hardware analysis component, a kernel modeling component, and a workload simulation component. The role of the hardware analysis component is to extract the basic hardware information, such as processing power and data link speed. The role of the kernel modeling component is to provide performance models of the serial kernels that constitute the building blocks of the parallel program. Finally, the role of the simulation component is to simulate large-scale parallel workloads.<br/><br/>The hardware analysis component gathers the basic knowledge about the system, such as: the number of CPU sockets per shared memory node, the number of CPU cores in each socket, the cache hierarchy, existence of hyper-threading, number of NUMA nodes and proximity of CPUs to NUMA nodes, number of GPU accelerators or Xeon Phi co-processors and capacities of their device memories, and the topology and bandwidth of data links, both within each node (busses), and between nodes (network switches). Part of this knowledge can be gathered by using appropriate query APIs, such as hwloc, netloc, PAPI, and those provided in the CUDA SDK, OpenCL SDK, and Xeon Phi SDK. Synthetic tests can be used for parameters that cannot be established in this manner.<br/>Kernels are essentially the serial building blocks of parallel problems. Although kernels are usually characterized by serial control flow, most of the time they already rely on a high degree of data parallelism. Today's CPUs get most of their performance from SIMD parallelism, and GPUs get their performance from massive SIMT parallelism. The role of the kernel modeling component is two-fold: 1) to tune kernels for maximum performance at a given granularity, 2) to provide the kernel performance model as a function of granularity, which is changing to accommodate parallel execution.<br/>DARE turns to a stochastic time-stepping simulation in order to predict the performance of a dynamic runtime scheduler for two fundamental reasons: 1) Building good performance models on the basis of benchmarking actual parallel runs requires a significant number of runs with significant problem sizes, which is simply too time consuming. And 2), the impact of many tuning parameters is too complex to be modeled by sparsely sampling the tuning space and fitting simple curves / surfaces to the sample points. The answer to the problem is to replace the run with a time stepping simulation, where a given task-based scheduler is used for assigning tasks to cores, but instead of invoking actual kernel tasks, control is passed to a progress tracking simulation system, which relies on kernel performance models to simulate the execution of the tasks and produce a virtual trace of the simulated execution. The performance advantage is twofold: 1) Simulating a single run is much faster than actually making that run, and 2) Many simulations can be run in parallel allowing for fast sweeps through a large parameter search space.<br/>DARE replaces the standard waterfall autotuning process with a process that is incremental and iterative in nature. The power of the DARE approach lies in the mutual refinement loop, where each of the three phases is capable of massively pruning the search space for the other two. As a result, very high quality models can be built for a particular workload, since time is being spent refining the model for the conditions that actually apply, rather than sampling the search space in areas never touched at runtime."
"1815139","AF: Small: Homogeneous and Heterogeneous Network Learning with Applications in Computational Biology","CCF","COMPUTATIONAL BIOLOGY","07/01/2018","06/28/2018","Jing Li","OH","Case Western Reserve University","Standard Grant","Mitra Basu","06/30/2021","$199,964.00","","jingli@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","7931","7923","$0.00","Heterogeneous networks have been widely used in modeling real-world complex systems and have been a powerful tool in studying complex biological problems. Link prediction in heterogeneous networks has been one of the key computational problems in this context. However, research on efficient and effective algorithms for link prediction in heterogeneous networks is still in its infancy, especially for networks that integrate multiple data sources. The overall objective of this project is to develop efficient, robust, effective and integrative algorithms as well as software tools for the link prediction problem in heterogeneous networks, and to apply the algorithms on real biological applications with practical significance. The project will also help in promoting teaching and training of a new generation of computational biologists.<br/><br/>The investigators of the project will solve the link prediction problem in heterogeneous networks by proposing two computational approaches: one is based on a kernel regularized least square framework with multi-view learning for data incompleteness, and the other one is to utilize a weighted nonnegative matrix tri-factorization approach that provides a unified framework for link prediction and data imputation. Both approaches will be applied to the problem of drug combination prediction, which is formulated as a link prediction problem in heterogeneous networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817048","SHF: Small: Algorithms and Software for Scalable Kernel Methods","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/25/2018","George Biros","TX","University of Texas at Austin","Standard Grant","Almadena Y. Chtchelkanova","06/30/2021","$476,172.00","","gbiros@gmail.com","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7923, 7942","$0.00","Scientists and engineers are increasingly interested in using machine learning methods on huge datasets that cannot be processed on a single workstation.  At the same time public and private institutions are making significant investments on high-performance computing (HPC) clusters equipped with thousands of leading edge processors and network connectivity. However, despite the availability of such HPC systems, data analysis tasks are mostly restricted to a single or a few workstations. The reason is that, with few exceptions, existing machine learning software does not scale efficiently on HPC systems. The need to process in-situ large scientific and engineering datasets is not met with current software and significant downsampling is required in order to use existing tools. A serious bottleneck in current artificial intelligence (AI) workflows is the significant cost of training for large scale problems. The slow convergence of existing methods and the large number of calibration hyper-parameters (learning rate, batch size, and other knobs that control the performance of the AI system) make training extremely expensive. Design and analysis of scalable optimization algorithms for faster training, that is the fitting of the machine learning (ML) model parameters to the data, are needed for analytics in real time  and at scale, which is the goal of this project.<br/><br/>The proposed research will introduce novel numerical methods and parallel algorithms for second-order/Newton methods that will be tailored to machine learning (ML) models and will be many orders of magnitude faster than the existing state of-the-art (first-order methods like steepest descent). The researchers plan to design, analyze, and implement robust approximations for covariance matrices, a class of matrices in AI and computational statistics, used in statistical analysis (e.g., sampling, risk assessment, and uncertainty quantification). The investigators plan to design, analyze, and implement scalable fast algorithms in the context of high-performance computing for the so called nearest-neighbor problem, a particular method in ML, data analysis, and information retrieval. The resulting software library will provide a means for end-to-end tools for discovery and innovation and provide new capabilities in the NSF XSEDE infrastructure project. Along with  research activities, an educational and dissemination program is designed to communicate the results of this work to both students and researchers, as well as a more general audience of computational and application scientists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812467","SHF: Small: Collaborative Research: Integrated Framework for System-Level Approximate Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","06/11/2018","Fabrizio Lombardi","MA","Northeastern University","Standard Grant","Yuanyuan Yang","06/30/2021","$262,000.00","","lombardi@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7798","7923, 7941","$0.00","Nanocomputing is encountering fundamental challenges with respect to performance and power consumption; it requires different computational paradigms that exploit specific features in the targeted set of applications as well as an integrated framework for assessing the interactions between hardware and the processing algorithms (software). Approximate (inexact) computing has been advocated as a novel approach for nanocomputing design. Approximate computing generates results that are good enough rather than always fully accurate and correct outputs. Recent advances at circuit level have shown that there is an urgent need to investigate and enable at system-level the flexible utilization, improvement and close monitoring of approximate resources; this allows the efficient and integrated interaction of algorithms and hardware to meet the multiple and often conflicting figures of merit of high performance, lower power consumption and reduced inaccuracy.  The goal of this project is to develop approximate computing systems that are capable of adjusting performance by exploiting relationships between hardware and software (referred to as intra-level) in different applications such as cognitive processing, DSP, big data and scientific processing for which data can be adaptively utilized and manipulated. <br/><br/>This project is an organized effort that combines recent advances in technology with architectural enhancements into an integrated framework for approximate computing that will tackle the critical challenges of emerging computer designs in a comprehensive manner. This framework consists of new functional and computational primitives of hardware resources and related algorithms to allow an evaluation at system-level to meet the desired metrics for approximate computing. Intra-layer relationships such as number representation (such as floating point and logarithm) and accuracy by employing dynamic approximation schemes and data remediation for both communication and computing are also analyzed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1553471","CAREER: Generative Programming and DSLs for Safe Performance Critical Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","04/15/2016","06/06/2018","Tiark Rompf","IN","Purdue University","Continuing grant","Anindya Banerjee","03/31/2021","$303,459.00","","tiark@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","1045, 7943","$0.00","Most performance critical software is developed using very low-level techniques, close to the underlying hardware. But low-level code in unsafe languages attracts security vulnerabilities, developer productivity suffers without the software engineering benefits of higher-level languages, and in the age of heterogeneous hardware and big data workloads, a single hand-optimized codebase may no longer provide good performance across different target platforms. Generative programming is a radical rethinking of the role of high-level languages and low-level languages. Instead of running whole systems in a high-level managed language runtime, the idea is to focus the abstraction power of high-level languages on composing pieces of low-level code, making runtime code generation and domain-specific optimization a fundamental part of the program logic.  This project will conduct a fundamental study of generative design patterns, which will be extracted from existing and emerging program generators and domain-specific languages. The intellectual merits are a deeper understanding of how to develop software in a generative style. The project's broader significance and importance are to establish generative programming as a part of every performance-minded programmer?s toolbox, enabling the use of high-level programming in more situations than currently possible.<br/><br/>Generative programming, and the shift in perspective that goes along with it, has been shown to be extremely effective in areas like databases (query compilation), protocol and data format parsers, hardware circuit generation, signal processing kernels, machine learning, and big data processing on heterogeneous computing devices?traditional strongholds of low-level languages. But while the general idea of program generation is well understood, the technique has remained esoteric?a black art, accessible only to the most skilled and daring of programmers. What is missing is a discipline of practical generative programming, including design patterns, best practices and so on. To achieve these broader goals, the project includes an education program, which, driven by the project?s research, will teach generative programming to a wide audience of students and developers in industry. This education effort will also serve as a large-scale usability study, closing the feedback loop into the research on generative programming techniques."
"1717075","CCF-BSF: AF:Small: Time-Message Tradeoffs in Distributed Algorithms","CCF","ALGORITHMIC FOUNDATIONS","07/01/2017","06/19/2017","Gopal Pandurangan","TX","University of Houston","Standard Grant","Rahul Shah","06/30/2020","$462,598.00","","gopalpandurangan@gmail.com","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","7796","7923, 7926, 7934","$0.00","Real-world  distributed communication networks such as the Internet, peer-to-peer networks, ad hoc wireless and sensor networks as well as data center networks used for large-scale data processing are an integral part of today's digital society. Distributed/decentralized algorithms underlie the efficient operation of these networks, e.g., distributed shortest paths algorithms are used for routing in the Internet, and distributed graph algorithms are used for finding communities in social networks. Hence, designing and analyzing efficient distributed algorithms is an important research task which will lead to faster and more resource-efficient performance in real-world networks. To address the more realistic situations, this project will aim to design distributed algorithms which simultaneously optimize the (1) running time and (2) number of messages. This research will help in the design of efficient and scalable distributed algorithms with provable performance guarantees. It can impact algorithm design in peer-to-peer and ad hoc wireless sensor networks, and distributed processing of large-scale data. The PI plans to develop a new course and a textbook on distributed network algorithms that is closely related to the research proposed above. University of Houston is a designated Hispanic serving public Tier 1 research institution and the PI will make efforts to involve minority and underrepresented students in research.<br/><br/>Two fundamental performance measures of a distributed algorithm that determine its efficiency are the running time and the number of messages used by the algorithm. Research in the last three decades has focused to a large extent on optimizing either one of the two measures separately, typically at the cost of the other. However, in many real-world applications, it is important to design distributed algorithms that simultaneously optimize both the measures. This project will investigate how distributed algorithms can be designed that work well under both measures. Furthermore, it will study the precise relationship between the two measures, in particular, how one can trade off one measure with respect to the other measure. This project will study time-message tradeoffs in distributed algorithms for various fundamental problems, including leader election, minimum spanning tree, shortest paths, and random walks. Specific goals of the project are: (1) Given a bound on one measure, design distributed algorithms that are optimal with respect to the other measure; (2) Obtain lower bounds on the complexity of one measure while fixing the other measure; (3) Obtain  tradeoff relationships that characterize the dependence of one measure on the other. (4) Obtain efficient distributed algorithms that operate on large-scale graphs."
"1453508","CAREER: Towards Practical Deterministic Parallel Languages","CCF","SOFTWARE & HARDWARE FOUNDATION","02/15/2015","09/19/2017","Ryan Newton","IN","Indiana University","Continuing grant","Anindya Banerjee","01/31/2020","$442,681.00","","rrnewton@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7798","1045, 7943","$0.00","Title: CAREER: Towards Practical Deterministic Parallel Languages<br/><br/>Parallel, multicore processors have become ubiquitous, but parallel programming has not. This gap implies that many everyday programs do not fully use the hardware on which they run. The problem persists because traditional parallel programming approaches are high-risk: a parallel program can yield inconsistent answers, or even crash, due to unpredictable interactions between simultaneous tasks.  Certain classes of programs, however, admit strong mathematical guarantees that they will behave the same in spite of parallel execution. That is, they enable deterministic parallel programming. Functional programming, extended with ""LVars"" --shared-state data structures that support commutating operations-- is one such model. While this theoretical model has been proven deterministic, significant questions remain regarding practical aspects such as efficiency and scalability. This research addresses those questions by developing new LVar data structures and scaling them to larger distributed memory machines. The intellectual merits are in the development of novel algorithms that support parallel programming. Further, the LVar model provides a new lens through which to view problems in parallel programming, which can lead to downstream discoveries. The project's broader significance and importance are (1) its potential to lower the cost and risk of parallel programming and (2) its educational goal: to employ deterministic parallel programming in the introductory programming course at both a university level, and in K-12 education. Changing how programming is taught may be necessary for leveraging hardware parallelism to become a normal and unexceptional part of writing software.<br/><br/>Three specific technical challenges are addressed in this research. First, LVars traditionally require more storage space over time, because ""delete"" operations do not commute with others. Semantically, the state-space of each LVar forms a join semi-lattice and all modifications must move the state ""upwards"" monotonically. Nevertheless, this project investigates new ways that LVars can free memory, using a concept of Saturating LVars. Second, this research seeks to formalize the relationship of LVar-based parallel programs to their purely functional counterparts, characterizing asymptotic performance advantages. Finally, this project explores the scalability of LVar-based programming abstractions in a distributed memory setting, where they share similarities with recent distributed programming constructs such as concurrent replicated data structures."
"1836936","FMitF: Collaborative Research: Synergies between Program Synthesis and Neural Learning of Graph Structures","CCF","FMitF: Formal Methods in the F","01/01/2019","09/04/2018","Mayur Naik","PA","University of Pennsylvania","Standard Grant","Nina Amla","12/31/2022","$450,000.00","","mhnaik@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","094Y","062Z, 8206","$0.00","A challenging problem in a diverse and growing body of applications concerns automatically generating computer programs that satisfy desired functional requirements. Two promising and complementary approaches that have recently emerged to address this problem are program synthesis and neural learning. This project aims to synergistically combine the two approaches to improve the productivity of programmers and the quality of software. The project also aims to train graduate students at the intersection of formal methods and machine learning, engage undergraduate students in research through internships, and disseminate results in the form of publicly available course materials and open-source software artifacts.<br/><br/>Program synthesis ensures that the generated program is correct with respect to a logical specification. Moreover, users can easily guide the synthesizer away from an undesired program and towards a desired one, by changing the specification. On the other hand, neural learning can handle user requirements that are impossible to provide via a logical specification -- a fact highlighted by the success of neural networks in domains such as natural language processing, computer vision, and robotics. Moreover, neural networks scale extremely well, by virtue of their ability to learn latent patterns that repeat across different programs. This project builds upon recent progress in program synthesis by developing novel learning-based mechanisms that enable flexible specifications, richer verifiers, and scalable solvers. In the realm of machine learning, it enables deep neural networks to provide correctness guarantees that are typically required when reasoning about rich structured data. In doing so, it develops novel architectures and methodologies for representation learning, reinforcement learning, and learning with limited data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837030","FMitF: OpenRDC: A Framework for Implementing Open, Reliable, Distributed, Network Control","CCF","FMitF: Formal Methods in the F","10/01/2018","09/07/2018","Aarti Gupta","NJ","Princeton University","Standard Grant","Anindya Banerjee","09/30/2022","$1,000,000.00","Jennifer Rexford, David Walker","aartig@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","094Y","062Z, 071Z, 9102","$0.00","Computer networks, whether connecting servers across a data center or users across the globe, are an important part of society's critical infrastructure. However, existing network protocols and services are simply not worthy of the trust society now places in them. Today's networks suffer from poor performance, cyberattacks, configuration errors, software bugs, and more, leading to serious consequences for consumers, businesses, and governments alike. The goal of this project is to enable the design and operation of better networks which requires enabling both innovation (to create better protocols and services) and verification (to ensure these services work correctly). A major part of the functionality of the network depends on the software running in the control plane, which computes routes, collects and analyzes network measurement data, balances load over multiple paths or servers and even hosts in-network applications. This project involves the theory, design, and implementation of OpenRDC, a new platform constructed for programming reliable, distributed network control planes.  <br/><br/>The technical core of OpenRDC centers around computations of Stable Information Trees (SITs) that communicate information (e.g., traffic conditions, failure information, available external routes, end-host job statistics, etc.) across a network, and then perform local actions to change network functionality or record information gathered. These structured computations suffice to express core control plane algorithms and yet can also be converted into logical representations that can be used to verify a variety of important properties of operational networks ranging from reachability to access control to multi-path consistency. The OpenRDC platform will simultaneously: (1) allow researchers to develop new control-plane algorithms, (2) enable automatic verification of network properties, and (3) make use of emerging programmable switch capabilities. The project involves acceleration of the development of new control-plane algorithms, via new abstractions for network programming. The project will also define new compiler technology for translating these abstractions to programmable network hardware. In addition, its open source infrastructure will lay a foundation for academic and industrial engagement and for the training of students. The project will also have impact on formal methods, with new algorithms for the verification of graph-oriented programming languages based on abstraction and modular decomposition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837132","FMitF: Collaborative Research: Formal Methods for Machine Learning System Design","CCF","FMitF: Formal Methods in the F, CYBER-PHYSICAL SYSTEMS (CPS)","10/01/2018","09/10/2018","Sanjit Seshia","CA","University of California-Berkeley","Standard Grant","Nina Amla","09/30/2022","$294,000.00","","sseshia@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","094Y, 7918","062Z, 8206, 8234, 9102","$0.00","Machine learning (ML) algorithms, fueled by massive amounts of data, are increasingly being utilized in several critical domains, including health care, finance, and transportation. Models produced by ML algorithms, for example deep neural networks, are being deployed in these domains where trustworthiness is a big concern. It has become clear that, for such domains, a high degree of assurance is required regarding the safe and correct operation of ML-based systems. This project seeks to provide a systematic framework for the design of ML systems based on formal methods. The project seeks to review and improve almost every aspect of the design flow of ML systems, including data-set design, learning algorithm selection, training of ML models, analysis and verification, and deployment.  The theory and ideas generated during the project will be implemented in a new software toolkit for the design of ML systems in the context of cyber-physical systems.<br/><br/>The project focuses on cyber-physical systems (CPS), which is a rich domain to apply formal methods principles. Moreover, the research ideas from this project can be readily applied to other contexts. A key aspect of this research is the use of a semantic approach to the design and analysis of ML systems, where the semantics of the target application and a formal specification for the full system, comprising the ML component and other components, are cornerstones of the design methodology. The project employs a range of formal methods, including satisfiability solvers, simulation-based verification, model checking, specification analysis, and synthesis to improve all stages of the ML design flow. Formal techniques are also used for the tuning of hyper-parameters and other aspects of the training process, to aid in debugging misclassifications produced by ML models, and to monitor ML systems at run time and ensure that outputs from ML models are used in a manner that ensures safe operation at all times.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836978","FMitF: Collaborative Research: Formal Methods for Machine Learning System Design","CCF","FMitF: Formal Methods in the F, SOFTWARE & HARDWARE FOUNDATION","10/01/2018","09/10/2018","Somesh Jha","WI","University of Wisconsin-Madison","Standard Grant","Nina Amla","09/30/2022","$406,000.00","Xiaojin Zhu","jha@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","094Y, 7798","062Z, 8206","$0.00","Machine learning (ML) algorithms, fueled by massive amounts of data, are increasingly being utilized in several critical domains, including health care, finance, and transportation. Models produced by ML algorithms, for example deep neural networks, are being deployed in these domains where trustworthiness is a big concern. It has become clear that, for such domains, a high degree of assurance is required regarding the safe and correct operation of ML-based systems. This project seeks to provide a systematic framework for the design of ML systems based on formal methods. The project seeks to review and improve almost every aspect of the design flow of ML systems, including data-set design, learning algorithm selection, training of ML models, analysis and verification, and deployment.  The theory and ideas generated during the project will be implemented in a new software toolkit for the design of ML systems in the context of cyber-physical systems.<br/><br/>The project focuses on cyber-physical systems (CPS), which is a rich domain to apply formal methods principles. Moreover, the research ideas from this project can be readily applied to other contexts. A key aspect of this research is the use of a semantic approach to the design and analysis of ML systems, where the semantics of the target application and a formal specification for the full system, comprising the ML component and other components, are cornerstones of the design methodology. The project employs a range of formal methods, including satisfiability solvers, simulation-based verification, model checking, specification analysis, and synthesis to improve all stages of the ML design flow. Formal techniques are also used for the tuning of hyper-parameters and other aspects of the training process, to aid in debugging misclassifications produced by ML models, and to monitor ML systems at run time and ensure that outputs from ML models are used in a manner that ensures safe operation at all times.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836822","FMitF: Collaborative Research: Synergies between Program Synthesis and Neural Learning of Graph Structures","CCF","FMitF: Formal Methods in the F, SPECIAL PROJECTS - CCF","01/01/2019","09/04/2018","Le Song","GA","Georgia Tech Research Corporation","Standard Grant","Nina Amla","12/31/2022","$450,001.00","","lsong@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","094Y, 2878","062Z, 8206","$0.00","A challenging problem in a diverse and growing body of applications concerns automatically generating computer programs that satisfy desired functional requirements. Two promising and complementary approaches that have recently emerged to address this problem are program synthesis and neural learning. This project aims to synergistically combine the two approaches to improve the productivity of programmers and the quality of software. The project also aims to train graduate students at the intersection of formal methods and machine learning, engage undergraduate students in research through internships, and disseminate results in the form of publicly available course materials and open-source software artifacts.<br/><br/>Program synthesis ensures that the generated program is correct with respect to a logical specification. Moreover, users can easily guide the synthesizer away from an undesired program and towards a desired one, by changing the specification. On the other hand, neural learning can handle user requirements that are impossible to provide via a logical specification -- a fact highlighted by the success of neural networks in domains such as natural language processing, computer vision, and robotics. Moreover, neural networks scale extremely well, by virtue of their ability to learn latent patterns that repeat across different programs. This project builds upon recent progress in program synthesis by developing novel learning-based mechanisms that enable flexible specifications, richer verifiers, and scalable solvers. In the realm of machine learning, it enables deep neural networks to provide correctness guarantees that are typically required when reasoning about rich structured data. In doing so, it develops novel architectures and methodologies for representation learning, reinforcement learning, and learning with limited data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717088","SHF:Small:Software-Defined Radio: From High-level Language to Hardware","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2017","08/31/2017","Geoffrey Mainland","PA","Drexel University","Standard Grant","Anindya Banerjee","08/31/2020","$443,935.00","","gbm26@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7798","7923, 7943","$0.00","Software-defined radio (SDR) promises to bring the flexibility of software development to radio protocol design. Unfortunately, SDR platforms that support high-data rate protocols are difficult to program and require low-level, expert knowledge that has little to do with radio protocol design. This project makes high-data rate radio protocol design accessible to a wider community of developers by developing a new high-level language for expressing SDR applications, freeing protocol designers from worrying about the low-level details of a particular SDR platform. The intellectual merits are a new understanding of the language abstractions and runtime facilities needed to translate high-level code to efficient low-level implementations. The project's broader significance and importance are to drastically lower the barrier to entry for high-speed software-defined radio platforms and to produce new tools for the SDR community that facilitate both research and education.<br/><br/>The proposed research builds on Ziria, a language for writing radio physical layer (PHY) implementations. Ziria will be extended in to support new radio protocol applications and to support new hardware platforms, including FPGAs. A key project goal is portability: protocols written in Ziria should be portable across SDR platforms without significant loss of performance. Furthermore, users must be able to make use of all parts of heterogeneous SDR platforms---from FPGAs to CPUs to DSPs---seamlessly and without rewriting code. Finally, this project applies knowledge gained in the SDR domain to new domains, such a video encoding and decoding, where low-level hardware is also commonly targeted."
"1618014","CCF:  Small:  Accelerating Irregular Algorithms using Cache-Coherent FPGA Accelerators","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/27/2016","James Hoe","PA","Carnegie-Mellon University","Standard Grant","Yuanyuan Yang","07/31/2020","$330,000.00","","jhoe@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7941","$0.00","Until recently, FPGA acceleration of computations has largely focused on algorithms that exhibit a high degree of regularity and predictability in their parallelism and memory access. The advent of high-capacity FPGA accelerators connected to the processor and main memory through a high-performance cache-coherent interconnect enables algorithms with irregular parallelism to be considered. These irregular algorithms, including many data analytic and machine learning kernels, operate on very large, memory-resident, pointer-based data structures. This project will study the opportunity to accelerate irregular algorithms for performance and energy efficiency on emerging cache-coherent FPGA accelerators. The outcome of this investigation has potential for practical commercial impact by helping to establish cache-coherent FPGA acceleration as a viable new platform option for accelerating irregular algorithms that are fundamental to datacenter workloads. This project will also provide valuable training to both graduate and undergraduate students, and improve graduate-level coursework.<br/><br/>Instead of the traditional ""off-load"" model of FPGA acceleration, this project seek to develop a new tightly-coupled FPGA-processor collaboration model that takes advantage of the low-latency, fine-grain shared-memory interactions between the processor and FPGA that are now possible. The project studies fine-grain concurrent mappings of irregular algorithms where the processor and FPGA work together---each leveraging its own characteristic advantages, e.g., large cache, high frequency ALUs for the processor and energy-efficient spatial hardware concurrency for the FPGA---to outperform what either can achieve alone. An integral part of the investigation is also to develop new insights toward what should cache-coherent FPGA accelerators ultimately look like, especially with the support for irregular algorithms in mind."
"1553452","CAREER: Social Computation: Fundamental Limits and Efficient Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","02/15/2016","02/07/2018","Sewoong Oh","IL","University of Illinois at Urbana-Champaign","Continuing grant","Phillip Regalia","01/31/2021","$359,550.00","","sewoong@cs.washington.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","1045, 7935","$0.00","Social computing systems bring enormous value to society by harnessing the data generated by members of a community. While each individual alone can offer only limited knowledge, time, and effort, a crowd together can solve challenging societal problems. General approaches to aggregate such data from crowds exists, but are typically suboptimal and inefficient. This project applies information-theoretic techniques to explore the fundamental sample/computation/accuracy trade-offs in social computing. The success of the proposed research will make progress towards a society that efficiently learns from the activities of its members for greater societal good. The proposed research is strongly integrated with an education plan that aims to develop a new graduate course on algorithmic foundations of social computing and innovative adaptive learning platforms that integrates the technology of social computing into the domain of education. <br/><br/>The project will investigate several topics: (1) characterizing the fundamental trade-offs between the available budget and the accuracy of the answers in crowd-sourcing platforms, by applying information-theoretic tools and methods; (2) designing efficient algorithms achieving this fundamental trade-off using techniques from spectral graph theory and coding theory, as well as by applying a family of novel rank-breaking approaches to reduce complexity; and (3) characterizing the three-way fundamental trade-offs between computational complexity, sample size, and accuracy in aggregating preferences from partially observed traces of online and mobile activities."
"1452163","CAREER: Role of geometry in dynamical modeling of human movement: Applications to activity quality assessment across Euclidean, non-Euclidean, and function spaces","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2015","02/27/2018","Pavan Turaga","AZ","Arizona State University","Continuing grant","Phillip Regalia","05/31/2020","$484,167.00","","pavan.turaga@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7797","1045, 7935, 7936, 9251","$0.00","Human movement is a complex multi-dimensional dynamical process arising out of complex non-linear interactions between various muscles and joints as well as interactions with external forces and objects. These physical factors often impose certain analytical constraints on movement data obtained from external sensors, often with associated geometric representations. Examples of such representations abound in the area of human activity analysis, where several contemporary and emerging human shape and movement representations such as silhouettes, stick-figure sequences, orientation signals, and optical- flow have intricate signal spaces, often described in the language of Riemannian geometry.  The research thrusts focus on foundational methods for extracting meaningful dynamical attributes that will dovetail into the application thrust. The application centers on computational modeling of qualities of physical activities from low-fidelity sensing infrastructure, which is an important component for technology-mediated physical rehabilitation and preventive interventions.<br/> <br/>Classical vector-valued signal processing and machine learning has had a large impact in the area of understanding and modeling human activities, but their applicability is significantly limited when it comes to signal-spaces with non-Euclidean geometry. This research project aims to advance a new class of robust, non-parametric dynamical modeling approaches, which will extend from classical vector-valued observation spaces, to finite-dimensional Riemannian manifolds, as well as to infinite-dimensional function-spaces. In order to be general enough to account for various geometric spaces as mentioned here, a framework that is free of restrictive assumptions on parametric forms of dynamics is needed. Further, integrating dynamical analysis with Riemannian geometric theory allows the analysis to extend to several feature spaces including depth maps, shape sequences, stick-figures, and orientation data, without re-defining the fundamental models for activity analysis."
"1652303","CAREER: Efficient Fine-grained Algorithms","CCF","ALGORITHMIC FOUNDATIONS","02/01/2017","02/27/2019","Barna Saha","MA","University of Massachusetts Amherst","Continuing grant","Rahul Shah","01/31/2022","$317,850.00","","barna@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7796","1045, 7926","$0.00","Ever since the inception of computation, the fundamental question has been- what problems can be solved by computers? And which ones can be solved in a reasonable time? This brought in the notion of polynomial time solvable problems which are considered efficient vs NP-hard problems which take prohibitively long time to solve. The field of approximation algorithms, along with the theory of inapproximability, deals with which of the NP-hard problems can be solved efficiently by allowing approximate solutions. However, this crude distinction of algorithmic efficiency--polynomial vs NP-hard, is insufficient when handling today's large scale of data. We need a finer-grained design and analysis of algorithms that pinpoints the exact exponent of polynomial running time, and a better understanding of when a speed-up is not possible. Unfortunately, except for a few problem-specific innovations, the study of such algorithms is deeply lacking in the literature.<br/><br/>This project targets to build a unified theory of fine-grained algorithm design to study fast approximation algorithms, and their fine-grained hardness. Developing systematic techniques that emphasize on the trade-offs between running time, approximation and randomness, and aid in designing low-complexity parallel algorithms will significantly improve the state of the art. Moreover, motivated by core machine learning applications, the project proposes an alternate model of efficiency via classical query complexity. Tools from classical query complexity have previously inspired development of fast algorithms and vice versa; the PI expects similar connections to happen in this project. Elements of this endeavor will be integrated with new courses, many of the algorithms developed herein will be implemented and the close connection of the PI with industry will result in possible adaptation of methodologies.<br/><br/>The project will address a suite of important optimization problems, from long-standing open questions to modern problems with diverse applications. It will introduce fresh tools from additive combinatorics, fourier analysis, rate distortion theory, and circuit complexity for analysis of algorithms, and establishing lower bounds. Specifically, new generic techniques of amnesic dynamic programming, fast matrix-product over semiring, and low-degree polynomial method will be developed to design fine-grained approximation algorithms and their parallel counterparts. Time complexity may not always be the primary measure of efficiency. There are many core machine learning problems where query complexity, that quantifies the amount of labeled data acquired via active querying, is more important. The project will analyze for the first time the query complexity of basic learning problems, and explore its connections to developing fast algorithms."
"1755876","CRII: SHF: Enabling Neuroevolution in Hardware","CCF","SOFTWARE & HARDWARE FOUNDATION","01/15/2018","01/11/2018","Tushar Krishna","GA","Georgia Tech Research Corporation","Standard Grant","Sankar Basu","12/31/2019","$175,000.00","","tushar@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798","7945, 8228","$0.00","Over the past few years, machine learning algorithms, especially neural networks (NN) have seen a surge of popularity owing to their potential in solving a wide variety of complex problems across image classification and speech recognition. Unfortunately, in order to be effective, NNs need to have the appropriate topology (connections between neurons) for the task at hand and have the right weights on the connections. This is known as supervised learning and requires training the NN by running it through terabytes to petabytes of data. This form of machine learning is infeasible for the emerging domain of autonomous systems (robots/drones/cars) which will often operate in environments where the right topology for the task may be unknown or keep changing, and robust training data is not available. Autonomous systems need the ability to mirror human-like learning, where we are continuously learning, and often from experiences rather than being explicitly trained. This is known as reinforcement learning (RL). The goal of this project will be on enabling RL in energy-constrained autonomous devices. If successful, this research will enable mass proliferation of automated robots or drones to assist human society. The learnings will also be used to develop new courses on cross-layer support for machine learning. <br/><br/>The focus of the research will be on neuroevolution (NE), a class of RL algorithms that evolve NN topologies and weights using evolutionary algorithms.  The idea is to run multiple ""parent"" NNs in parallel, have the environment provide a reward (score) to the actions of all NNs, and create a population of new ""child"" NNs that preserve those nodes and connections from the parents that lead to actions producing the maximum reward. Running NE algorithms over multiple iterations has been shown to evolve complex behaviors in NNs. Unfortunately, NEs are computationally very expensive and have required large scale compute clusters running for hours before converging. A characterization of the computation and memory behavior of NE algorithms will be performed, and opportunities to massively parallelize these algorithms across genes (i.e., nodes and connections in the NN) will be explored. The evolutionary learning steps of crossover and mutation will be performed over specialized hardware engines, and a low-power architectural platform running NE algorithms at the edge will be demonstrated. Furthermore, the proposed research will serve as the foundation for further research in fast and energy-efficient RL algorithms to help realize general-purpose artificial intelligence."
"1912051","CAREER: Fast algorithms via a spectral theory for graphs with a prescribed cut structure","CCF","ALGORITHMIC FOUNDATIONS","10/22/2018","12/20/2018","Ioannis Koutis","NJ","New Jersey Institute of Technology","Continuing grant","Balasubramanian Kalyanasundaram","06/30/2019","$46,055.00","","i.koutis@gmail.com","University Heights","Newark","NJ","071021982","9735965275","CSE","7796","1045, 7933, 9150","$0.00","Critical applications involving very large data sets require algorithms that run fast and provide strong performance guarantees. Among the numerous examples are the analysis of medical scans and the acquisition -via imaging- of connectivity in neural systems, an important task in current computational neuroscience. These problems are very often approached by first modeling the data as networks -also called graphs- and then applying graph-specific algorithms to solve them. Among many possibilities, algorithms that rely on certain algebraic representations of graphs have become very appealing due to recent theoretical progress that renders them very time-efficient.  However, efficiency appears to come at the cost of an occasionally inferior quality in the generated solutions. Via the proposed extensions of the theory studying these algebraic representations, the project will design new algorithms with strong guarantees and wide applicability.<br/><br/>Spectral graph theory studies the connections between algebraic and combinatorial properties of graphs. It is well known that these connections can be far from tight. For example, two given graphs may have approximately the same cuts, but significantly different eigenvalues and eigenvectors. As a result, spectral algorithms for cut problems on graphs, albeit very fast, do not provide good approximation guarantees. This project will extend aspects of spectral graph theory to a spectral theory for cut structures, defined as sets of graphs with approximately prescribed cuts. The central question of the new theory is:  What kind of spectral properties can be realized by graphs within a given cut structure?<br/><br/>A goal of the project is to show that any cut structure contains graphs whose eigenvectors provide tight information about its cuts.  The project will also study algorithms for the efficient computation of these special graphs, by essentially modifying the spectrum of an input graph without significantly altering its cuts. Then, the combination of spectral modification and classical spectral algorithms will yield fast algorithms with enhanced approximation guarantees.  The project will draw from connections of spectral graph theory with graph decompositions discovered in the context of oblivious routing algorithms. In turn, it is expected that the project will have an impact on routing problems too. In later stages the project will study the theoretical limits of spectral modification. It will also examine the descriptive quality of the developed theory in the performance of algorithms and other phenomena on interesting classes of graphs, such as social or biological networks.<br/><br/>The project will freely disseminate prototype implementations of the new algorithms and will apply them to computer vision and machine learning problems in industry and academia. Applications will be pursued via selected interdisciplinary collaborations. The balance between theoretical and applied work will serve a broader educational effort at both the undergraduate and graduate level, which will also include the introduction of new courses. A significant part of the research will be carried out at the University of Puerto Rico, and so the project is expected to have a significant impact in the education of underrepresented minorities."
"1850356","CRII: CIF: Practical and Timely Coded Caching for Dynamic and Volatile Networks","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2019","01/30/2019","Shirin Saeedi Bidokhti","PA","University of Pennsylvania","Standard Grant","Phillip Regalia","05/31/2021","$175,000.00","","saeedi@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7797","7797, 7935, 8228, 9102","$0.00","Caching is a promising technology that is revolutionizing traditional data communication networks from host-based to information-centric architectures. In information centric networks, the focal point is content rather than where it can be retrieved; hence, one can replicate and store (or cache) content at various nodes or storage units throughout the network so that it can be accessed faster locally without burdening the server and the global network. This is desirable in Internet of Things (IoT) applications where content is distributed, and delay is of crucial importance. However, there are stringent constraints on device storage capabilities and computational power. As such, the existing caching mechanisms do not suit IoT domains very well. This project combines two main aspects of caching, namely, information theory and networking, to develop practical and timely coded caching schemes for dynamic and volatile networks such as the IoT. This will allow information-centric networks to achieve their full potential as a replacement to conventional host-based data communication networks of today.<br/><br/>The project will develop practical and efficient caching and coding schemes for dynamic and volatile networks such as the IoT. It develops a new framework to model capabilities of users in caching content on the fly. This framework allows one to address practical constraints (such as storage capacities and freshness of information) by bringing concepts such as Age of Information (AoI) and cache replacement techniques from network theory and queuing theory into picture while preserving coding opportunities and optimality of the schemes (in an information theoretic sense).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1748493","Workshop on Future Directions for Algorithms in Biology","CCF","COMPUTATIONAL BIOLOGY","11/01/2017","08/11/2017","Carleton Kingsford","PA","Carnegie-Mellon University","Standard Grant","Mitra Basu","10/31/2019","$98,936.00","","carlk@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7931","7556, 7931","$0.00","Computation has become essential to progress in biology, and an important and difficult research goal is to develop efficient, correct, and useful algorithms to analyze biological data and answer biological questions. This workshop will bring together the community of computational biologists who are interested in algorithmic questions to explore future directions and open problems in applying algorithms to problems in biology. The workshop will host distinguished scientists, students, and junior faculty to discuss emerging research areas, open problems, and grand challenges in algorithmic biology. This will solidify the algorithmic underpinnings of the field of computational biology, jumpstart new collaborations, and help set directions for the field for many years. It will ensure that the most important problems are tackled by the field, and that those who are interested in working on algorithms for those problems are aware of them. The activity will result in publically available video lectures and a published synthesis of the discussion and results of the workshop. It will advance the education of students interested in biology, computational biology, and algorithms by providing a venue for identifying important research questions and providing educational resources such as video lectures. A diverse group of speakers and researchers will participate, broadening the viewpoints represented in the field of algorithmic computational biology. <br/><br/> The workshop will explore algorithmic questions in areas such as genomics, metagenomics, population genetics, systems biology, biological image analysis, phylogenetics, evolution, RNA and protein structure, proteomics, transcription and translation, experiment design, bioinformatics, and biological data management. The focus of the workshop will be on identifying, publicizing, and formalizing algorithmic open questions and better understanding how algorithmic approaches and thinking can aid biological investigations. The computational focus will be on algorithms and techniques with provable properties (e.g. runtime, correctness, approximation ratio), rather than heuristics. The workshop will host invited and community speakers, and be structured around scientific lectures, discussion sessions, and open problem sessions."
"1717320","AF:  Small:  Collaborative Research:  Computational Representations for Design and Fabrication of Developable Surfaces","CCF","Cyber-Human Systems (CHS), ALGORITHMIC FOUNDATIONS","08/01/2017","08/02/2017","Keenan Crane","PA","Carnegie-Mellon University","Standard Grant","Balasubramanian Kalyanasundaram","07/31/2020","$250,000.00","","kmcrane@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367, 7796","7367, 7923, 7933","$0.00","This project develops mathematical representations and computer algorithms for three-dimensional forms that can be assembled from a collection of flat pieces without incurring material stress, known as piecewise developable geometry. These will drive new developments in the emerging industry of advanced 3D manufacturing: Not only can developable geometry can easily be cut from a rich array of thin flat materials like plywood or sheet metal, but it can also provide a novel geometric approach to tool path planning that improves the efficiency and accuracy of shapes fabricated via computer-controlled flank milling.  Such processes offer a competitive advantage for manufacturing in the US by reducing cost, increasing complexity of fabricated forms, and automating tasks previously only achievable by hand (e.g., robotic folding of developable forms).  A fundamental issue addressed by the research is automatic approximation of an arbitrary curved surface by a small number of developable pieces---at present this process must be carried out laboriously by expert engineers and designers, severely limiting the scope and impact of developable manufacturing processes.  More broadly, algorithmic models of developable geometry enrich basic understanding in the area of discrete differential geometry, which seeks to reformulate classical geometric knowledge from a discrete, algorithmic point of view.  This area provides a crucial link between modern geometric theory and industrial/applied applications that need to incorporate data and computation, and students trained in this project will be well-equipped to contribute in 3d manufacturing. <br/><br/>The project builds on foundations from smooth and discrete differential geometry: rather than view discrete meshes as mere numerical approximations, the unifying goal is to develop data structures that directly encode the most salient features of piecewise developable geometry.  Two key observations are that (i) flattenability alone is not a sufficient characterization for discrete developability, often leading to nasty ""crumpling"" behavior and (ii) the curvature of a piecewise developable surface is encoded entirely by the shape of its patch boundaries, a fact often exploited in garment design.  These observations lead to two primary thrusts, namely (i) representations for discrete developability that naturally avoid crumpling by guaranteeing the existence of discrete ""ruling lines,"" and (ii) efficient algorithms for developable surface design based on sparse descriptions of curvature along critical feature curves.  A cross-cutting theme is physical considerations for fabrication, e.g., translation between simple geometric models and material/constitutive properties relevant to the production of physical artifacts."
"1526386","SHF: Small: Techniques and Frameworks for Exploiting Recent SIMD Architectural Advances","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","06/30/2015","Gagan Agrawal","OH","Ohio State University","Standard Grant","Almadena Chtchelkanova","06/30/2019","$449,999.00","","agrawal@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7798","7923, 7942","$0.00","Single Instruction Multiple Data (SIMD) parallelism has been available in common processors for several decades now, and has been widely exploited for dense matrix and other regular problems. Recent architectural trends, such as increasing width of SIMD lanes coupled with instruction sets, provide significantly enhanced functionality.  There is a need to effectively use the new architectural features for dynamic or irregular applications, which have not been easy to perform on parallel on SIMD processors in the past.   <br/><br/>The PI proposes comprehensive research program on mapping various classes of unstructured and/or irregular applications to modern SIMD novel architectural features and developing optimized compiler transformations from programs written in CUDA and OpenCL. This research proposal proposes to address the challenge of developing and executing unstructured and/or irregular applications using novel SIMD processors' instructions such as scatter and gather. The PI plans to design compiler transformation methods from CUDA and OpenCL programs to increase the number of contiguous accesses  and decrease the number of cache lines from which data needs to be accessed. These novel transformation methods are targeting applications such as unstructured grid kernels, sparse matrix computations, and graph and tree traversals.  The PI plans to continue development of an Operator Overloading based library."
"1846046","CAREER: Learning-Based Hardware and Software Techniques for Quality-of-Service-Aware Cloud Microservices","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2019","01/30/2019","Christina Delimitrou","NY","Cornell University","Continuing grant","Yuanyuan Yang","05/31/2024","$99,096.00","","delimitrou@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7798","1045, 7941, 9102","$0.00","Datacenters support a large and ever-increasing fraction of the world's digital computation power, including search engines, social networks, and machine learning analytics. As modern cloud services grow in popularity, their design shifts from supporting complex monolithic applications, to supporting collections of specialized, loosely-coupled microservices. Such microservices impact resource requirements by requiring fast network processing and low-latency memory accesses to achieve their quality-of-service (QoS) constraints. Dependencies among microservices also complicate compute cluster management, and can cause cascading QoS violations, hurting availability and service reliability. Guaranteeing the responsiveness expected from cloud services while using datacenters efficiently requires instead a joint hardware-software approach. This project takes a holistic view towards designing a system stack for interactive cloud microservices running on large-scale datacenters that is QoS-aware, and resource-efficient. By pursuing automated, learning-based techniques, this project highlights the value of leveraging practical machine learning techniques to better navigate the increasing complexity of the cloud, as more datacenter services switch to this new application model.<br/><br/>At the hardware level, this project first quantifies the implications microservices have on server design, and second, explores their potential for hardware acceleration. At the software level, this work is developing a new cluster manager that accounts for the dependencies among microservices in an automated and transparent-to-the-user way, and guarantees end-to-end performance. Finally, to eliminate the cascading effects of QoS violations between microservices, this project includes a data-driven, online performance forecasting system. This system leverages the massive amount of monitoring data collected by cloud systems to anticipate upcoming QoS violations, and act on them before they degrade performance. By innovating in both hardware and software, this work will achieve performance and efficiency gains that neither hardware- nor software-only approaches can provide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718771","CIF:Small:Model-Based Blind Demixing for Signal Processing and Machine Learning","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","08/28/2018","Justin Romberg","GA","Georgia Tech Research Corporation","Standard Grant","Phillip Regalia","08/31/2020","$499,703.00","Kiryung Lee","jrom@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7797","7923, 7936","$0.00","The research centers on novel numerical methods and supporting theory for the multichannel convolutive blind demixing (MCBD) problem, where the responses for a set of inter-related time-invariant systems are estimated by observing only their outputs. The MCBD problem arises in many well-known applications in signal processing and communications; one of our goals for this project is to provide a unified framework for solving these problems that has a firm algorithmic and theoretical foundation. The goals are to provide a fundamental analysis of the information theoretic limits of MCBD, along with scalable algorithms that operate with provable performance guarantee at or near these limits. New applications of the MCBD problem will also be explored in the area of machine learning. In particular, the investigators will study how solutions to the MCBD problem can be used as an efficient method for both for the initialization in training deep convolutional neural networks, and for solving inverse problems associated with generative models.<br/><br/>The work will combine classical statistical approaches and modern optimization-based techniques for constrained inverse problems. Of particular interest is the role that structure plays on making the problem identifiable, and on the stability of the solutions when the observations are corrupted by noise. Scenarios where this structure comes from domain-specific knowledge will be considered, along with scenarios where the model is data-driven. The algorithms developed in the project will be validated on applications in astronomical imaging, neuroimaging, medical imaging, seismic imaging, underwater acoustics, and deep learning. The proposed research has direct relevance to next-generation array processing for massive MIMO communications, device-to-device communication for the Internet-of-Things, and new integrated circuit RF transmitters. The work also may open a new direction in parallel MRI. The research activities will be complemented by new graduate courses focusing on modern mathematical methods for the next generation of data scientists."
"1718700","AF: Small: Algorithms for New Memory Models","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/31/2017","Jeremy Fineman","DC","Georgetown University","Standard Grant","Rahul Shah","08/31/2020","$347,980.00","","jfineman@cs.georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","7796","7923, 7926, 7934","$0.00","Accessing memory and storage has a substantial impact on the performance of computer programs, particularly when operating on large data sets. As such, memory-efficient algorithms (or algorithms designed to optimize for memory accesses) have received significant attention both in theory and practice. With new developments in memory technology and usage, however, the classic models less accurately capture true system performance characteristics. The goal of this project is to develop a theory for new memory models that more closely capture important performance features arising from recent shifts in memory technology and usage. The project advances the state of the art in several directions, including new performance models, new memory-efficient algorithms in these models, new techniques for understanding the performance of algorithms, and new lower bounds to explain the limits of algorithm performance. The algorithms studied are themselves fundamental building blocks in larger systems, and importing any new efficient solutions into existing programming platforms could directly improve the performance of diverse software systems. As part of the project the PI will develop educational materials, and any reference implementations developed as part of the project will be made available to the public.<br/><br/>This project studies algorithms in two classes of new memory models, namely the cache-adaptive model and the asymmetric-memory models.  The cache-adaptive model is a way of modeling the fluctuations in effective memory size that occur when multiple processes compete for space in a shared cache. Efficient cache-adaptive algorithms should have more robust and predictable performance on parallel systems. This project considers the following areas relating to cache-adaptive algorithms: (1) new cache-adaptive algorithms, (2) new cache-adaptive data structures, (3) more general performance theorems, and (4) how sensitive the algorithms and their analyses are to worst-case adversaries. An asymmetric-memory model is a model where writes are significantly more expensive than reads; asymmetric models are relevant, e.g., to phase-change memories and other emerging memory technologies. This project studies (1) new algorithms for asymmetric memory models, including implicit representations of solution outputs that allow for a sublinear number of writes, and (2) lower bounds for algorithms in these models."
"1723379","AitF:  FULL: Collaborative Research:   PEARL: Perceptual Adaptive Representation Learning in the Wild","CCF","Algorithms in the Field","09/01/2016","03/14/2017","Kate Saenko","MA","Trustees of Boston University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$173,754.00","","saenko@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7239","012Z","$0.00","Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or ""domain."" In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a ""dog"" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms ""in the field.""<br/><br/>This project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets."
"1566363","CRII: SHF: Regression Testing for Projects with Distributed Software Histories","CCF","CRII CISE Research Initiation, SOFTWARE & HARDWARE FOUNDATION","05/15/2016","05/15/2017","Milos Gligoric","TX","University of Texas at Austin","Standard Grant","Sol J. Greenspan","04/30/2019","$183,000.00","","gligoric@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","026Y, 7798","7798, 7944, 8228, 9251","$0.00","Developers practice regression testing -- running tests against each<br/>project commit -- to check that project changes do not break any<br/>functionality. While important, regression testing is expensive due to<br/>the number of tests and the number of commits. Regression test<br/>selection (RTS) techniques speed up regression testing by skipping to<br/>run tests that are not affected by recent changes, and regression test<br/>prioritization (RTP) techniques reorder tests to run failing tests<br/>faster. Existing regression techniques analyze only two adjacent<br/>commits, effectively assuming a linear software history. However, the<br/>revolution in version-control systems changed the shape of software<br/>histories. Distributed software histories are complex graphs of<br/>branches and merges, which do not match the simplistic view of the<br/>existing techniques. Although existing regression techniques can be<br/>run if a distributed software history is linearized, these techniques<br/>underutilize the data available in the distributed software history.<br/><br/>To speed up regression testing, the PI proposes to design techniques<br/>that utilize the data available in the entire distributed software<br/>history. This project proposes four tasks to improve RTS and RTP: (1)<br/>non-adjacent commit reuse - discover likely optimal commit to be used<br/>in each analysis rather than always analyzing adjacent commits; (2)<br/>multi-commit analyses - design methods that analyze more than two<br/>commits rather than always analyzing only two commits; (3)<br/>command-aware methods - specialize the methods for various commands<br/>that create each commit rather than be command-unaware; and (4)<br/>unified implementation and evaluation - share the implementation and<br/>results among techniques, and evaluate the techniques on open-source<br/>and industrial projects. The broader impacts of improving RTS and RTP<br/>are to increase developers' productivity and reduce the resource usage<br/>during testing phases."
"1815718","SHF: Small: Architectural Techniques for Energy-Efficient Brain-Machine Implants","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","08/23/2018","Abhishek Bhattacharjee","NJ","Rutgers University New Brunswick","Standard Grant","Yuanyuan Yang","09/30/2021","$465,995.00","","abhib@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7798","7923, 7941, 8089, 9251","$0.00","This project focuses on the development of neural prostheses or brain implants to advance the scientific community's understanding of how the brain works, and to take a step towards devising treatment for neurological disorders. Brain implants are devices that are surgically embedded under the skull (of animals or humans in the context of scientific experiments and treatment of neurological disorders respectively) and placed on brain tissue, where they stimulate and record from hundreds of neurons. These devices are being used today to record neuronal electro-physiological data to unlock mysteries of the brain; to treat symptoms of Parkinson's disease, Tourette's syndrome, and epilepsy, with techniques like deep brain stimulation; and to offer treatment to those afflicted by paralysis or spinal cord damage via motor cortex implants. A key design issue with brain implants is that they are highly energy constrained, because they are embedded under the skull, and techniques like wireless power can heat up the brain tissue surrounding the implant. This project offers architectural techniques to lower the power consumption and energy usage of processing elements integrated on brain implants, whether they are general-purpose processors, customized integrated circuits, or programmable hardware. In tandem with its scientific studies, this project integrates an educational component to train high-school students, undergraduates, and PhD students on neuro-engineering techniques crucial to the society's continued efforts to shed light on how the brain works. <br/><br/>In terms of technical details, this project performs the first study on architectural techniques to improve the energy efficiency of embedded processors on implants by leveraging their existing low-power modes. Low-power modes can be used in the absence of interesting neuronal activity, which corresponds to periods of time when the implant is not performing useful work and the processor can be slowed down. A critical theme of this project is to show that hardware traditionally used to predict program behavior (e.g., branches or cache reuse) can also be co-opted to also predict brain activity, and hence anticipate interesting/non-interesting neuronal spiking. Such predictors can consequently be used to drive the implant processor in and out of low power mode. This project studies how to design hardware brain activity predictors that predict neuronal activity accurately, scalably, and efficiently, and how to integrate such predictors with low power modes on commodity embedded processors. The techniques are drawn from hardware machine-learning approaches for program prediction and consider neuronal spiking data extracted from brain sites on mice, sheep, and monkeys. Successful deployment of these approaches is expected to save as much as 85% of processor energy, effectively quadrupling battery lifetimes on implants being designed for mice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1533933","XPS: FULL: DSD: Collaborative Research: Parallelizing and Accelerating Metagenomic Applications","CCF","SOFTWARE & HARDWARE FOUNDATION, Exploiting Parallel&Scalabilty","09/01/2015","05/15/2017","Yuan Xie","CA","University of California-Santa Barbara","Standard Grant","Yuanyuan Yang","08/31/2019","$572,000.00","","yuanxie@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7798, 8283","7941, 9251","$0.00","The importance of metagenomics arises from the fact that over 99% of<br/>the species yet to be discovered are resistant to cultivation. Unlike<br/>single genome sequencing, assembly of a metagenome is intractable and<br/>is in large part, an unsolved mystery.  Moreover, the advent of high<br/>throughput sequencing is fueling rapid generation of enormous<br/>metagenomic datasets. There is no available sequenced genome for a<br/>majority of the species. There is a need to determine the number of species in<br/>a metagenomic dataset as well as the abundance of each of these<br/>species.  The key steps (Assembly and Clustering) in the metagenomics<br/>analysis algorithms are compute-intensive, while the sheer amount of<br/>data the algorithms operate on is staggering.  The most promising way<br/>to tackle the computational challenges is to build special purpose<br/>hardware, dedicated solely to suitable algorithms.<br/><br/><br/><br/>The main objective of this project is to develop a range of flexible,<br/>affordable, parallel, fast hardware-accelerated bioinformatics<br/>solutions, using GPGPU, FPGA, and ASIC, for metagenomic analytics to provide an alternative to<br/>expensive computer clusters. Specifically,  hardware solutions for<br/>metagenomic clustering and assembly will be developed. Several<br/>acceleration methodologies, including parallel software mapping and<br/>special hardware design, are proposed to explore the parallelism<br/>inside the applications and to improve the data access bandwidth in<br/>the hardware running bioinformatics applications. The ideas proposed<br/>in this work will be evaluated in a multi-pronged manner using a<br/>combination of simulation, emulation and prototyping efforts. Further,<br/>the PIs will use a combination of commercial tools, collaborator<br/>resources and  existing internal tools. The research will be <br/>conducted in collaboration with  industrial partners. Through close<br/>collaboration with several industry partners,  direct transfer of many<br/>ideas to industry is enabled. The outcome of this research will,<br/>therefore, have a direct impact on future bioinformatics application<br/>solutions. This project will involve graduate and undergraduate<br/>students in all aspects of the research. The PIs will actively<br/>integrate the research results from this project into the graduate and<br/>undergraduate curricula, and develop new interdisciplinary courses on<br/>bioinformatics and computer architecture to train the next generation<br/>work-force. Finally, the tools and techniques developed in this<br/>research will be made available through   web-sites for use by other<br/>educators, researchers, and industry practitioners."
"1616297","AF: Small: Entropy Maximization in Approximation, Learning, and Complexity","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","05/17/2016","James Lee","WA","University of Washington","Standard Grant","Tracy J. Kimbrel","08/31/2019","$466,000.00","","jrl@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","7923, 7926, 7927, 9251","$0.00","Entropy plays a distinguished role in the world.  The second law of thermodynamics tell us that, in closed systems, entropy always increases; it is maximized at thermodynamic equilibrium.  Given a collection of data, the ""principle of maximum entropy"" asserts that, among all hypothetical probability distributions that agree with the data, the one of maximum entropy best represents the current state of knowledge.<br/><br/>Moreover, if one considers a convex set of probability distributions, the problem of maximizing a strongly concave function (like the Shannon entropy) over this set is computationally tractable and has a unique optimal solution.  This project is concerned with the structure and computational utility of entropy maximizers in algorithm design, machine learning, complexity theory, and related areas of discrete mathematics.  In particular, the project will study the role of entropy maximization in encouraging simplicity in the optimum solution.  This property stands to reason:  The entropy maximizer should intuitively contain only the information implied by the constraints and nothing more.<br/><br/>The scope of the project includes not only classical entropy functionals like the Shannon entropy and Kullback-Leibler divergence, but also the analogous notions for quantum states (von Neumann entropy).  The study of quantum entropy maximizers has far-reaching applications in semi-definite programming and communication complexity.  Moreover, much of the theory extends to other Bregman divergences, and this is particularly relevant for applications in online algorithms where certain smoothed entropy functionals become relevant.  A portion of the project concerns entropy optimality on path spaces.  This perspective provides a novel view of Markov processes on discrete and continuous spaces.  The PI will employ this viewpoint to study rapid mixing of Markov chains, as well smoothing properties of the noise operator on the discrete hypercube (a topic with remarkable applications in complexity theory and hardness of approximation).<br/><br/>Finally, it should be mentioned that iterative algorithms for finding entropy maximizers can be viewed in the framework of entropy-regularized gradient descent; such algorithms are fundamental in machine learning (boosting) and online convex optimization (multiplicative weights update).  This provides a powerful connection to large bodies of work, and a substantial motivation for the project is to create a bridge of ideas and techniques between the two perspectives.<br/><br/>Broader impact of the project includes training of the next generation of scientists, including at the undergraduate level.  This project presents a number of opportunities for undergraduate researchers to contribute in a meaningful and substantial way, while at the same time receiving valuable mentoring and experience as developing scientists."
"1617315","SHF: Small: Efficient In-Memory Computing Architecture Based on RRAM Crossbar Arrays","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/03/2016","Wei Lu","MI","University of Michigan Ann Arbor","Standard Grant","Yuanyuan Yang","05/31/2019","$400,000.00","","wluee@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","7923, 7941","$0.00","Conventional digital computers, with separate processor and memory, face increasing challenges in today?s ?big data? era as the constant movement of data between the processor and the memory causes significant delay and energy consumption. This problem, termed the ?von Neumann bottleneck?, affects performance for both complex tasks such as image and video processing as well as embedded applications such as distributed sensor networks where high speed and low power are critical. This project aims to develop a new computer architecture based on emerging resistive random access memory (RRAM) crossbar arrays, where the memory and logic functions exist at the same physical locations and computation is achieved in the physical memory by directly reading out stored outputs for a given operation. By leveraging the unique properties of emerging devices with a new computation architecture, this project will profoundly advance the frontier of nanoscale device and computer architecture research, and enable high-speed and low-power computation for applications ranging from servers to Internet of Things (IoTs). This program will have significant impact on the research community and the semiconductor industry, while providing interdisciplinary training of graduate and undergraduate students, and draw broad participation of students of different levels and backgrounds in collaborative research and education.<br/><br/>This approach takes full advantage of the high-storage density, non-volatility, and random-access capabilities of RRAM arrays. RRAM devices operate based on the resistance change when the device is subjected to a programming or reset pulse. Consequently, the resistance not only stores information but also directly regulates information (i.e. current) flow in the circuit, thus implementing both memory and logic functions simultaneously. Previous studies on RRAM-based circuits focus on soft computing tasks where the environment and the tasks are complex but inaccuracies and approximations are tolerated. This project aims to develop a computing system that can perform accurate arithmetic operations efficiently using RRAM crossbar arrays. The system will be optimized for throughput and energy, and experimentally demonstrated using fabricated high-density RRAM arrays, along with the development of a toolset for design automation."
"1618800","CIF: Small: Algebraic Network Information Theory","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","05/07/2017","Bobak Nazer","MA","Trustees of Boston University","Standard Grant","Phillip Regalia","05/31/2019","$511,495.00","","bobak@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7797","7923, 7935, 9251","$0.00","The demand for data services has grown exponentially over the past decade, driven by data-hungry applications such as mobile video, and is forecasted to accelerate through the next decade. To keep up with this demand, new infrastructure is being built at a rapid pace, leading to denser, more complex deployments. Concurrently, mobile processing power has increased dramatically, which opens the door for the inclusion of more sophisticated communication techniques as part of developing standards. Within this context, network information theory acts as a powerful theoretical framework for assessing techniques across complex network topologies and setting benchmarks for practical deployments. Recent efforts have uncovered examples of techniques that sit outside this classical framework, and exploit the algebraic structure inherent to multi-user communication to attain higher performance. This project will develop an algebraic network information theory that unifies classical and modern communication techniques, and can serve as a foundation for future information-processing networks. The project is complemented by several educational and outreach activities, including workshops, summer schools, and tutorials.<br/><br/>This project aims to take a comprehensive view of algebraic, information-theoretic techniques for efficient communication across networks. The overarching goal is to develop an algebraic network information theory starting from the accessible concepts of joint typicality and discrete memoryless channels and sources. The project is organized into three thrusts. The first thrust outlines a unifying problem statement based on joint typicality as well as algebraic packing and covering lemmas. The second thrust pursues a fundamental understanding of the limits of optimal decoding. Finally, the third thrust aims to transport geometric insights from lattice-based codes to obtain new bounds and algorithms for linear codes."
"1526215","CIF: Small: Collaborative Research: Communications with Energy Harvesting Nodes","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","07/15/2015","Xiaodong Wang","NY","Columbia University","Standard Grant","Phillip Regalia","08/31/2019","$270,496.00","","wangx@ee.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7797","7923, 7936","$0.00","Wireless networks composed of nodes that can harvest energy from the environment represent the green future of communications. Utilizing the harvested ambient energy improves the environmental impact of wireless devices in the global scale, while extending the network lifetime indefinitely and making the devices truly mobile. Energy harvesting (EH) brings new dimensions to system design in the form of randomness and intermittency of available energy, as well as additional system issues such as energy storage capacity and processing complexity. Additionally, the deployment of EH wireless networks calls for novel techniques for resource and interference management, cooperation among EH nodes, as well as cross-layer optimization of the network.<br/> <br/>The research focuses on a new set of challenges brought about by wireless networks composed of EH nodes. The research directions are divided into four major thrusts. (1) The first challenge is to understand the fundamental limit of EH communications in the information theoretic setting, accounting for random and intermittent supply of available energy, finite battery capacity with storage and withdrawal efficiencies. The special cases for low-power and high-power regimes are of particular interest. (2) In order to make efficient use of the available resources, joint energy scheduling and spectral and/or spatial resource allocation in EH networks exploiting the inherent structure of the resource allocation problems becomes an important problem. (3) Different EH nodes in the network can share their harvested energy either through the presence of power line between them, or through wireless charging. The research studies previous thrusts with energy and data cooperation. (4) The research investigates efficient distributed cross-layer solutions to maximize the total system utility subject to data and energy queuing stability constraints."
"1763747","SHF: Medium: Training Sparse Neural Networks with Co-Designed Hardware Accelerators: Enabling Model Optimization and Scientific Exploration","CCF","SPECIAL PROJECTS - CCF","07/01/2018","07/02/2018","Keith Chugg","CA","University of Southern California","Continuing grant","Almadena Y. Chtchelkanova","06/30/2021","$758,600.00","Leana Golubchik, Peter Beerel, Panayiotis Georgiou","chugg@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","2878","075Z, 7924, 7942","$0.00","Machine learning systems are critical drivers of new technologies such as near-perfect automatic speech recognition, autonomous vehicles, computer vision, and natural language understanding.  The underlying inference engine for many of these systems is based on neural networks.  Before a neural network can be used for these inference tasks, it must be trained using a data corpus of known input-output pairs.  This training process is very computationally intensive with current systems requiring weeks to months of time on graphic processing units (GPUs) or central processing units in the cloud.  As more data becomes available, this problem of long training time is further exacerbated because larger, more effective network models become desirable.  The theoretical understanding of neural networks is limited, so experimentation and empirical optimization remains the primary tool for understanding deep neural networks and innovating in the field.   However, the ability to conduct larger scale experiments is becoming concentrated with a few large entities with the necessary financial and computational resources.  Even for those with such resources, the painfully long experimental cycle for training neural networks means that large-scale searches and optimizations over the neural network model structure are not performed.  The ultimate goal of this research project is to democratize and distribute the ability to conduct large scale neural network training and model optimizations at high speed, using hardware accelerators.  Reducing the training time from weeks to hours will allow researchers to run many more experiments, gaining knowledge into the fundamental inner workings of deep learning systems.  The hardware accelerators are also much more energy efficient than the existing GPU-based training paradigm, so advances made in this project can significantly reduce the energy consumption required for neural network training tasks.<br/><br/>This project comprises an interdisciplinary research plan that spans theory, hardware architecture and design, software control, and system integration.  A new class of neural networks that have pre-defined sparsity is being explored.  These sparse neural networks are co-designed with a very flexible, high-speed, energy-efficient hardware architecture that maximizes circuit speed for any model size in a given Field Programmable Gate Array (FPGA) chip.  This algorithm-hardware co-design is a key research theme that differentiates this approach from previous research that enforces some sparsity during the training process in a manner incompatible with parallel hardware acceleration. In particular, the proposed architecture operates on each network layer simultaneously, executing the forward- and back-propagation in parallel and pipelined fully across layers.  With high precision arithmetic, a speed-up of about 5X relative to GPUs is expected.  Using log-domain arithmetic, these gains are expected to increase to 100X or larger. Software and algorithms are being developed to manage multiple FPGA boards, simplifying and automating the model search and training process. These algorithms exploit the ability to reconfigure the FPGAs to trade speed for accuracy, a capability lacking in GPUs.  These software tools will also serve as a bridge to popular Python libraries used by the machine learning community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1252995","CAREER: Expanding Developers' Usage of Software Tools by Enabling Social Learning","CCF","SOFTWARE & HARDWARE FOUNDATION, SOFTWARE ENG & FORMAL METHODS","08/01/2013","05/30/2017","Emerson Murphy-Hill","NC","North Carolina State University","Continuing grant","Sol J. Greenspan","07/31/2019","$495,721.00","","emerson@csc.ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798, 7944","1045, 7944, 9251","$0.00","Creating and maintaining software is an increasing challenge for software developers. Software development tools can help meet this challenge, but most developers use only a small subset of the available tools because they are unaware of the existence of relevant tools. To learn about relevant tools, prior work suggests that most effective approach is social learning, where a developer learns about a tool from a peer, yet research also suggests that social tool learning is rare. The aim of this research is to increase the frequency and effectiveness of social tool learning, and in turn, fundamentally advance our understanding of how technology can mediate tool learning in software development and beyond. In reaching this aim, we will help improve the software on which people increasingly rely.<br/><br/>The researchers will create a testbed system that will record a continuous screencast of the developer's work, indexed with time-stamped data on tool usage. By comparing the developer's tool usage data with her peers', the system selects a list of candidate tools that the developer does not use, but that her peers do use. The system then encourages peers to teach and learn from one another by sharing tool-usage clips chosen from each others' screencasts. The approach aims to enable distributed and asynchronous developers to share tool knowledge efficiently, effectively, and frequently. The testbed will allow the researchers to experiment with various types of tool learning to determine how, why, and when different types of tool learning are effective or ineffective."
"1816518","CIF: Small: Collaborative Research: A software toolbox for computing and exploring the fundamental limits of information systems","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/11/2018","James Plank","TN","University of Tennessee Knoxville","Standard Grant","Phillip Regalia","09/30/2021","$180,005.00","","plank@cs.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","7797","7923, 7935, 9150","$0.00","This project aims to build an open-source software toolbox under the GNU-GPL license to facilitate the investigation of information systems (e.g., data storage systems, streaming data structures, and content delivery systems) using information theoretical methods, more precisely, to facilitate the derivation of outer bounds and identifying novel code constructions.  By building modern software tools which are able to take advantage of the advance in computer hardware and software, this effort can help the community more efficiently incorporate computational intelligence into information theoretic research.<br/> <br/>The main theoretical foundation for the approach is the entropy linear programming framework, and techniques based on symmetry and implication relations that can reduce the complexity of such programs. The core components of the toolbox are various software tools to perform information theoretic analysis and exploration of information systems in a computational manner. The completion of such a toolbox can enable researchers to build, with minimal programming efforts, computer software which takes in a description of the information system of interest, processes the relation among its components, formulates an appropriate optimization problem, and invokes a computational optimization solver to produce meaningful results regarding the fundamental limits and code constructions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822191","SHF: Medium: Collaborative Research: Next-Generation Message Passing for Parallel Programming: Resiliency, Time-to-Solution, Performance-Portability, Scalability, and QoS","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2017","05/31/2018","Anthony Skjellum","TN","University of Tennessee Chattanooga","Continuing grant","Almadena Y. Chtchelkanova","05/31/2020","$364,798.00","","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","7798","7924, 7942, 9150, 9251","$0.00","Parallel programming based on MPI is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics.  Emerging supercomputer systems will have more faults and MPI needs to be able to workaround such faults to be appropriate to these emerging situations, rather than causing an entire application to fail.  Collaborative, transformative message passing research for High Performance Computing (HPC) critical to performance-portable parallel programming in new and forthcoming scalable systems (with a strategy of ""best practice-first, standardization-later"") is being reduced to practice. A substantial subset of the Message Passing Interface (MPI-3/4) application programmer interface is being made fault tolerant through extensions with weak collective transactions that synchronize between parallel tasks. <br/><br/>This research studies  the novel model that localizes faults, provides tunable fault-free overhead, allows for multiple kinds of faults, enables hierarchical recovery, and is data-parallel relevant.  Fault modeling of underlying networks is being studied. Application developers control the granularity and fault-free overhead in this effort. Performance and scalability results of the middleware prototype are being demonstrated principally through compact applications that relate to real use cases of practical and academic interest. The impact of this work ranges from users of the largest supercomputers in government labs to practical clusters that have long-running, time-critical applications, and to space-based and other parallel processing in ""hostile"" environments where faults occur more frequently than in past years.  The project is producing usable free software that will be widely shared in the community as well as guidance on how better parallel programs can be written in academia, industry, and government.  The project also provides guidelines for how to update existing or legacy programs to use the new capabilities that are being reduced to practice."
"1812965","CIF:Small: A Computationally-Enabled Rate Region Theory via Symmetry and Hierarchy","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/25/2018","John Walsh","PA","Drexel University","Standard Grant","Phillip Regalia","09/30/2020","$324,998.00","","jwalsh@ece.drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7797","7923, 7935","$0.00","Three timely applied engineering problems - limiting delay in communications for streaming media and remote control, coding information in data centers, and squeezing more capacity out of wireless and wired networks - all have fundamental limits that are dictated by a common family of underlying abstract rate region problems in information theory and coding.  The underlying goal of this project is to advance the method of solution for these information theoretic rate region problems into the modern results-oriented data-driven massive computation age.  Humans no longer tally large balance sheets, lay out massive integrated circuits, factor matrices, or solve linear programs by hand. Instead, we have recognized that computers are far better suited to doing these things both accurately and rapidly.  This project will demonstrate that determining the coding rate regions that dictate the key design tradeoffs in these applied problems is a problem that is also best solved with computational methods.  The research work to be carried out will enable the algorithms and software the PI has developed to reach larger instances of these problems by exploiting advanced notions of symmetry.  Additionally, a hierarchical theory that enables complicated larger instances of these problems to be solved by combining computer generated solutions to simpler, smaller, constituents will be further developed.<br/><br/>The abstract problems under study in this award are to determine the capacity regions of networks under network coding.  Work under the project will proceed in three thrusts.  The first thrust will show with specific worked examples and driver routines in software, how limits in distributed information storage systems, coded protocols for limited delay streaming media and remote control over multipath routed networks, and squeezing extra capacity out of wireless and wired communications networks, all can be formulated as abstracted network coding problems.  The second thrust will utilize novel notions of symmetry of polyhedra to push algorithms and software the PI has developed to determine network coding capacity regions to as large problems as possible.  The third thrust advances a structural theory enabling capacity regions of large networks to be inferred through simple computations combining carefully selected constituent networks.  Throughout the project, care will be taken to illustrate the ideas by showing how to reduce the applied engineering design problems to a form that the algorithms and software that will be developed can solve.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1414172","SHF: Large: Collaborative Research: Exploiting the Naturalness of Software","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2014","04/23/2018","Premkumar Devanbu","CA","University of California-Davis","Continuing grant","Sol Greenspan","06/30/2020","$1,240,108.00","Zhendong Su, Vladimir Filkov","devanbu@cs.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7798","7798, 7925, 7944, 8091, 8624, 9251","$0.00","This inter-disciplinary project has its roots in Natural Language (NL) processing. Languages such as English allow intricate, lovely and complex constructions; yet, everyday, ``natural? speech and writing is simple, prosaic, and repetitive, and thus amenable to statistical modeling. Once large NL corpora became available, computational muscle and algorithmic insight led to rapid advances in the statistical modeling of natural utterances, and revolutionized tasks such as translation, speech recognition, text summarization, etc.  While programming languages, like NL, are flexible and powerful, in theory allowing a great variety of complex programs to be written, we find that ``natural? programs that people actually write are regular, repetitive and predictable. This project will use statistical models to capture and exploit this regularity to create a new generation of software engineering tools to achieve transformative improvements in software quality and productivity. <br/> <br/>The project will exploit language modeling techniques to capture the regularity in natural programs at the lexical, syntactic, and semantic levels. Statistical modeling will also be used to capture alignment regularities in ``bilingual? corpora such as code with comments, or explanatory text (e.g., Stackoverflow) and in systems developed on two platforms such as Java and C#.  These statistical models will help drive novel, data-driven approaches for applications such as code suggestion and completion, and assistive devices for programmers with movement or visual challenges. These models will also be exploited to correct simple errors in programs. Models of bilingual data will used to build code summarization and code retrieval tools, as well as tools for porting across platforms. Finally, this project will create a large, curated corpus of  software, and code analysis products, as well as a corpus of alignments within software bilingual corpora, to help create and nurture a research community in this area."
"1408981","SHF: Medium: Collaborative Research: Automatic Locality Management for Dynamically Scheduled Parallelism","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2014","03/21/2019","Matthew Fluet","NY","Rochester Institute of Tech","Standard Grant","Anindya Banerjee","05/31/2020","$236,744.00","Umut Acar","mtf@cs.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7798","7924, 7943","$0.00","Automatic Locality Management for Dynamically Scheduled Parallelism<br/><br/>Today's multicore and manycore computers provide increasing amounts of computational power in the form of parallel processing coupled with a complex memory organization with many levels of hierarchy and orders of magnitude difference in cost between accessing different levels. When software exhibits spatial and temporal locality, meaning that it reads and writes memory addresses that are close to one another in relatively small time span, it is able to primarily access data in fast caches, rather than in slow main memory, and deliver good sequential and parallel performance. Unfortunately, with software written in high-level managed programming languages it is difficult to ensure or to predict the amount of spatial and temporal locality, due to the lack of low-level programmer control and the complexities of and interactions between the specific hardware platform and the thread scheduler and the memory manager. This project explores techniques for automatic management of locality in high-level managed programming languages executing on parallel computers with sophisticated memory hierarchies. Using the theoretical models, efficient algorithms, and practical implementations being developed in the project, programmers are able to reason about the expected locality of their programs independent of the target hardware, while a runtime system, including thread scheduler and memory manager, maps the program onto specific hardware to achieve the established performance bounds.<br/><br/>In particular, this project addresses the problem of automatically managing locality via the runtime system of a high-level garbage-collected parallel functional programming language. A comprehensive approach that considers scheduling, memory allocation, and memory reclamation together is used, allowing the thread scheduler to influence the memory manager and vice versa. A key insight of this research program is to view the allocated data of a program as a hierarchical collection of task- and scheduler-mapped heaps. This view guides the theoretical cost model that enables a programmer to reason about locality at a high-level, the efficient algorithms that control when to create and to garbage collect a heap with provable bounds, and the practical implementation that delivers automatic locality management in a parallel functional programming language. The intellectual merits are advances in understanding the interaction of thread scheduling and memory management with locality on modern parallel hardware, the development of high-level, machine-independent cost model, and a synthesis of programming languages, algorithmic theory, and system design to address the challenges of automatic locality management. The broader impacts are improvements in software quality and programmer productivity, the creation of a parallel functional programming language usable in both education and research, and the integration of results into courses and outreach activities.<br/>"
"1513201","SHF: Medium: Energy Efficient Computing on GPU-based Heterogeneous Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2015","06/30/2016","Laxmi Bhuyan","CA","University of California-Riverside","Continuing grant","Almadena Chtchelkanova","05/31/2020","$750,000.00","Zizhong Chen","bhuyan@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7924, 7941","$0.00","The current trend in designing future multiprocessors is to integrate hundreds of cores and hardware accelerators (HAs), such as GPUs, on a single platform. However, as the system size scales, the power and energy consumption of these heterogeneous multiprocessors vastly exceed the budget. The primary focus of the project is to develop energy efficient techniques that can be implemented in the GPU with proper coordination with the CPUs. Energy reduction in GPU-based heterogeneous multiprocessors can be achieved by extending current CPU techniques to GPU.  The project develops new runtime techniques for GPU core scaling and Dynamic Voltage and Frequency scaling (DVFS), and combines them with basic changes in algorithms and data structures to improve energy efficiency. <br/><br/>Existing models for performance and energy consumption assume 100% utilization of the processing cores and perfect overlap with memory access during execution. This project develops a more accurate model for energy efficiency taking into account the algorithm, data structure, caching and memory coalescing for different applications. A runtime system is being developed that monitors the GPU core and memory utilizations together with the energy consumption while executing an application. The runtime adjusts the number of cores and/or frequency level dynamically through prediction, but continues to make corrections as the execution proceeds. The runtime is extended to heterogeneous systems consisting of both CPU and GPU.<br/>The current DVFS techniques for scientific computing applications cannot fully eliminate slacks, therefore, are not energy optimal. By leveraging the algorithmic characteristics, a frequency scheduling technique is developed for linear algebra applications to achieve better energy efficiency.<br/>The project optimizes the energy efficiency while partitioning and designing tasks of an application. Without loss of generality, Cholesky factorization is used as an example and an energy efficient scheduler is developed. <br/><br/>The project develops software products that can be readily applied to existing large scale heterogeneous computers executing scientific applications that are suitable for defense, energy and critical infrastructure projects. Also applications like weather forecasting and structural dynamics are developed that have a great impact on society. The research content is integrated to graduate courses to provide training to students for designing and programming heterogeneous systems. The project aims to produce very high quality Ph.D. graduates including female students. The University of California, Riverside is known for its large proportion of Hispanic students, and UCR is a minority-serving institution. The project supports recruiting underrepresented minority and female students."
"1615563","SHF: Small: Automatically Localizing Functional Faults In Deployed Software Applications","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2016","07/08/2016","Mark Grechanik","IL","University of Illinois at Chicago","Standard Grant","Sol Greenspan","06/30/2020","$350,896.00","","drmark@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","7798","7923, 7944","$0.00","Even though most software applications are tested before they are released to customers, these applications still contain production (or field) functional faults that result in field failures, which have costly consequences and are expensive to fix. Due to their limitations, existing automatic debugging approaches do not adequately isolate and identify production faults for field failures.  Prior interviews of test managers and studies of bug repositories revealed that programmers spent close to 50% of their time on average to localize production faults, which is a major factor in software system and software project failures. The educational innovation of this project is in developing an integrated approach to teaching by applying probabilistic graphical models to software engineering problems.<br/> <br/>The goal of this proposal is to create a novel theoretical foundation that allows stakeholders to predict and localize functional faults for field failures automatically with a high degree of precision using symptoms only (e.g., the sign of the output value is incorrect) and without instrumenting deployed applications to collect runtime data, thus avoiding the deployment runtime overhead, and without having any tests with oracles to uncover the fault, without performing contrasting successful and failed runs, and without collecting runtime data from field failures. With this theoretical foundation, researchers can collaborate more closely in planning the future of fault localization by expanding each other's results based on probabilistic graphical models as common abstractions.  Based only on failure symptoms occurring during deployment of a given application, the location of faults in the source code will be determined, as well as navigation paths from likely faults to the code that can fix these faults. The project will create, evaluate and deploy: (1) new theories, algorithms and techniques for automatically obtaining probabilistic graphical models that approximate specific fault models for software applications; (2) a novel way in which model-based differential diagnoses are used to perform abductive reasoning to localize production faults given symptoms for field failures, and (3) a comprehensive experimentation framework for evaluating the effectiveness of the algorithms for localizing production faults. In addition to localizing production functional faults, the implementation can be used as a broad experimental platform for creating and testing hypotheses for various software debugging and testing ideas, e.g., for guiding test selection and prioritization."
"1617071","SHF: Small: Architectural Support for Reliable ReRAM Crossbar Memory","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","05/31/2016","Jun Yang","PA","University of Pittsburgh","Standard Grant","Yuanyuan Yang","06/30/2019","$450,000.00","Youtao Zhang","juy9@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","7798","7923, 7941","$0.00","Driven by prevailing applications that process extreme volume of data, the quest for large memory capacity has become increasingly strong. Conventional DRAM-based memory is facing severe technology scaling limitations such as process variations, which hinders the growth in memory density at reasonable cost. Such challenges inspire the search for alternative memory technologies such as the emerging non-volatile Resistive RAMs (ReRAM). ReRAM exploits resistance of Metal-Oxide-Metal structure to represent stored information. It has shorter read and write latency, and better write endurance when compared with other non-volatile memories. ReRAM exhibits superior scalability and can be architected to build high-density memories using a crossbar structure, or 3D stacking. Such features make ReRAM a competitive technology as a DRAM replacement to achieve significant large memory capacity for modern data intensive applications. Education objectives will be achieved through broad dissemination of results via publications, research seminars, tutorials, software demonstrations, conference participation, and technology transfer initiatives. Continuous student training will be carried through involving graduate, undergraduate students, especially underrepresented students in this research. <br/><br/>There are major difficulties in building a large memory using the crossbar ReRAM architecture. The reliability of the memory is challenged by its large sneak leakage, operation disturbance and endurance. This research aims to tackle those challenges by investigating novel cell-array organizations and management techniques to reduce sneak leakage, minimize disturbance, prolong the lifetime and improve the overall reliability of the new memory structure. This research will ensure that future resistive memories can be developed to become reliable at high density, which helps to fuel the continuation of Moore?s Law in memory advancement."
"1845514","CAREER: Marlin: A Unified Framework for Automatic and Interactive Quantitative Program Analysis","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2019","03/05/2019","Jan Hoffmann","PA","Carnegie-Mellon University","Continuing grant","Anindya Banerjee","06/30/2024","$95,561.00","","jhoffmann@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","1045, 7798, 7943","$0.00","Achieving reliability and security of software systems that we use on a daily basis is one of the most pressing challenges of modern technology. It has been demonstrated that software verification with mathematical methods is an important component in meeting this challenge. However, most extant verification projects and tools focus on demonstrating the functional correctness of software. They do not analyze important quantitative properties of software such as resource usage, side channels, and probabilistic guarantees, which are crucial for reliability and security. The project's novelty is the design and implementation of a general framework for quantitative verification that can be applied to analyze resource usage, probabilistic programs (that incorporate randomness), and side channels. The project's impact is that this framework enables software developers to reduce the energy consumption of data centers, to mitigate serious security vulnerabilities, and to connect statistical safety guarantees to software systems that have machine-learning components. The project also provides a pedagogical opportunity for curriculum development and outreach activities. Quantitative verification and analysis tools implemented in the project are being integrated in Carnegie-Mellon's undergraduate courses on functional programming and data structures and algorithms, to both help students reason about the complexity of their code, and help instructors and teaching assistants automatically grade programming assignments by verifying complexity requirements. As part of the project's outreach activities, the investigator is designing two course modules for high-school students that are rolled out through existing programs at Carnegie-Mellon.<br/><br/>Current research on quantitative analysis and verification is often problem-specific, separated into manual or automatic techniques, and there is little cross-fertilization between different areas. The aim of this project is to develop Marlin, a unified framework for quantitative verification. A distinctive feature of Marlin is the tight integration of interactive and automatic reasoning. This includes converting manually derived quantitative properties into constraints that can be consumed by automatic techniques and supporting more lightweight forms of automation beyond full inference. Marlin is based on a full-featured probabilistic programming language and an expressive quantitative program logic that supports compositional and relational reasoning. Specific innovations of Marlin include easily-understood descriptions of sub-languages for which the automation is guaranteed to succeed, the automatic generation of worst-case inputs, tail-bound analysis with higher moments, and automatic relational reasoning.  Marlin's foundation is shared by three specialized quantitative analysis tools: Resource Aware ML (RaML), a language for static resource analysis; ParML, a new language for side-channel free programming; and Borel, a tool for probabilistic inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757636","REU Site: Research Experience for Undergraduates in High Efficiency Computing","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2018","01/26/2018","Euzeli dos Santos","IN","Indiana University","Standard Grant","Rahul Shah","01/31/2021","$358,076.00","John Lee","eudossan@iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","1139","9250","$0.00","Information Technology (IT) systems range from small embedded systems such as smartphones to large computer facilities such as data centers. Whether small or large, IT systems proportionately use lots of energy. They also waste a lot, as in shorter battery life or higher data-center operating costs, so improving their efficiency fulfills a critical national need. The objective of this project is to propose solutions to increase energy efficiency in IT  systems by: 1) developing a well-trained student workforce, and 2) proposing new techniques and approaches capable of reducing energy consumption and losses. Students will be introduced to the research area of applying power optimization techniques to IT systems. The target populations are underrepresented students and those from institutions that do not offer PhDs. In addition to academic mentoring by Indiana University Purdue University Indianapolis (IUPUI) faculty, this project includes mentors from industry.<br/><br/>Traditionally, IT curricula have been developed with disciplines that focus on parameters, such as performance and runtime optimization. Little or no attention has been devoted to achieving energy efficiency and consumption thresholds, especially from the software and algorithmic perspectives. This project will provide original experiences and singular opportunities for undergraduate students to work on innovative strategies to reduce energy consumption and losses in IT systems. In addition to the potential change in the career paths of the selected students, this project suggests how the electrical and computer engineering curricula can be impacted to incorporate energy efficiency in both hardware and software curricula.<br/><br/>This project will demonstrate novel methodologies in reducing energy consumption and losses within IT systems, which can be accomplished with a multidisciplinary approach. Topics will range from hardware to software solutions with efficiency as the main figure of merit. Traditionally, Multiple-Voltage Multiple-Frequency (MVMF) has been used to reduce energy consumption in computing systems. This project proposes MVMF2, which not only dynamically changes the computer clock frequency, but also dynamically changes power supply switching frequency for further loss reduction. While MVMF2 is considered a hardware optimization managed by software, the second research topic - a higher efficiency power supply based on a new power converter design - is purely a hardware solution. The third and fourth research topics are software solutions to reduce energy consumption in high memory demand applications and processing big matrix multiplication, respectively. The fifth research topic deals with energy consumption reduction techniques in wireless Internet of Things devices by modeling and optimizing power saving modes, such as reduced power operation, cyclical sleep, among others."
"1553645","CAREER: Application-centric, Reliable and Efficient High Performance Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2016","02/12/2019","Dong Li","CA","University of California - Merced","Continuing grant","Almadena Chtchelkanova","01/31/2021","$396,646.00","","dli35@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092598670","CSE","7798","1045, 7942, 9251","$0.00","Mission-critical scientific simulations (e.g., climate simulation and fluid dynamics simulation) and enterprise workloads (e.g., search and encryption) running on large-scale computing systems are jeopardized by the increase of faults and errors in hardware and software. Understanding the vulnerability of these large-scale applications is important to minimize performance and power. Lack of the knowledge of application vulnerability forms a major bottleneck of execution efficiency, and jeopardizes HPC simulation capabilities. Previous works rely on random fault injection or detailed architecture analysis to evaluate application vulnerability. They can be slow and inaccurate. There is a big gap between the needs of reliable and efficient HPC and what the current methodologies can provide. <br/>This research explores a new methodology to understand application vulnerability. It investigates new analytical and statistical models to quantify and characterize application vulnerability based on a novel metric and application semantics (including algorithm semantics and data semantics). The PI integrates modeling techniques into a broader context for vulnerability analysis to improve the modeling accuracy and explore reliable and efficient protection for applications while examine the interplay between reliability, power, and performance.<br/><br/>The outcome from this research will provide support for execution correctness and efficiency of large-scale applications running on future computing systems that demand high data integrity. The proposed research will affect design of reliable applications and algorithms. Built upon the collaboration with industry, the research outcome is expected to be tangible and have direct impact on realistic scientific problems. Furthermore, the tight coupling between research components and education components creates a HPC learning culture to engage students in HPC, addressing HPC workforce shortage in the nation."
"1757773","REU Site: Software Assurance and Security in Emerging Technologies: Research Experience for Undergraduates","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2018","02/07/2018","Hassan Takabi","TX","University of North Texas","Standard Grant","Rahul Shah","01/31/2021","$359,996.00","Renee Bryce","takabi@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","1139","9250","$0.00","This project establishes a Research Experiences for Undergraduates (REU) Site at the University of North Texas on the topic of software assurance and security in emerging technologies. The proposed REU Site will i) expose undergraduate students to hands-on research experiences in security and software development and testing, ii) enhance their research experience, iii) promote their interests in research, and iv) seed their desire to pursue advanced degrees and careers in CS. The project aims to increase the number of students that pursue graduate studies by exposing students to exciting and attainable research goals, giving them the opportunity to build their resumes with research experiences and publications, and providing a supportive community of peers and mentors that encourage them to be successful. By introducing competition and collaboration as drivers for research innovation, the project intends to achieve a unique and cohesive undergraduate research experience that entices students to pursue graduate studies. Students will gain a background to learn state-of-the-art technical knowledge, the ability to think independently, and the confidence to work on challenging problems. As a result, they will be better prepared to enter the graduate school pipeline. The project's broader significance and importance are that it trains three diverse cohorts of ten students recruited from minorities and underrepresented groups including female students. <br/><br/>The project's intellectual merits are that it exposes students to an intense research experience that combines security and software development and testing research with emerging technologies (i.e., web, mobile, and wearable applications) as artifacts used for evaluation of the different research techniques. The intellectual merit of this REU site is reflected in each individual research project, which has interesting and crucial applications that participants can easily identify with. Some projects, designed for highly talented students, seek to advance knowledge and understanding by addressing unsolved problems. Others seek to employ security and testing techniques to solve the real-world problems. The projects will also apply an intellectual approach to encourage students through active learning, free and open exchange of ideas, team spirit, and recognition and rewards for achievements. Specifically, the objectives are to foster i) creative and independent thinking, ii) problem-solving skills and hands-on experience, iii) enthusiasm and confidence, and iv) critical thinking, communication and organization skills. The student projects are designed to include individual and collaborative tasks that foster a strong connection to the group and a better understanding of the larger context of their work. Students work in collaborative teams that explore research questions and innovations in the areas of message integrity, computation on encrypted data for different resource-constrained devices, test suite prioritization, and test suite reduction. The students will investigate cryptographic algorithms and protocols that are suitable for mobile and wearable environments and aim to achieve end-to-end message level integrity and to support computing on encrypted data. They will also conduct empirical studies that hypothesize about the fault detection effectiveness of test suite reduction and test suite prioritization by coverage-based metrics. Empirical studies examine the techniques in the context of emerging technologies (web, mobile, wearable, etc.)."
"1820537","SHF: Small: Collaborative Research: Multi-level Non-volatile FPGA Synthesis to Empower Efficient Self-adaptive System Implementations","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2017","03/12/2018","Jingtong Hu","PA","University of Pittsburgh","Standard Grant","Sankar Basu","07/31/2019","$138,045.00","","jthu@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","7798","7923, 7941, 7945, 9150, 9251","$0.00","Self-adaptivity is a key requirement for many electronic devices to consistently interact with the dynamic, uncertain, and noisy physical environment. While Field Programmable Gate Arrays (FPGAs), being reconfigurable, are a natural platform for implementing such devices, it is becoming more and more difficult for traditional FPGAs to keep up with the ever-increasing scale and complexity of self-adaptive applications due to the limited scalability, high leakage power, and severe process variations of CMOS technologies. A set of prior research projects demonstrated that it is technically feasible to construct FPGAs based on non-volatile memories (NVMs). These NV-FPGAs offer attractive features such as better scalability, superior energy efficiency, near-zero power-on delay, anti-radiation, as well as the ability to store more than one bit per cell. However, NV-FPGAs also display a complex design space involving information density, read and write speeds, data retention time, and device endurance. When used for self-adaptive systems, the distinctive NVM characteristics may influence reconfiguration speed, clock frequency, circuit functionality, memory performance, and/or device lifetime.<br/><br/>This project addresses this technology gap as it prepares NV-FPGAs for more demanding self-adaptive systems. This project aims to fine-tune various procedures on the FPGA synthesis flow based on NVM characteristics, so as to exploit their advantages and mitigate their shortcomings. First, considering the needs of self-adaptive applications, this project fine-tunes various steps on the FPGA synthesis flow. Novel techniques are proposed to optimize task scheduling, data allocation, logic mapping, placement, and routing to improve reconfiguration speed, energy efficiency, reliability, and endurance of NVM FPGAs. Second, this project explores the rich NVM design space and sets different optimization goals for look-up tables, flip-flops, and on-chip memories. The success of this project will lead to a long-lasting, rapid-adaptive, reliable, and energy-efficient platform better suited to the needs of a wide range of applications with self-adaptivity requirement, including healthcare, wellness, industry, and even military applications, all of which are critical for the United States to drive its new strategies of innovation and technology. It will also train a diverse type of engineers to design the future generation of embedded and cyber-physical systems with the cutting-edge technology of non-volatile memories. Algorithms and tools developed in this project will be made publicly available so that they will benefit the entire scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717415","SHF: Small: Collaborative Research: Discerning and Recommending Context-Specific Best Practices in DevOps-Oriented Software Development","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2017","04/20/2018","Bogdan Vasilescu","PA","Carnegie-Mellon University","Standard Grant","Sol Greenspan","06/30/2020","$319,351.00","","vasilescu@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7798, 7923, 7944, 9251","$0.00","This project is a scientific study of modern software development practices, which has become known as DevOps. The DevOps culture seeks to bring changes into software production as quickly as possible without compromising software quality, primarily by automating the processes of building, testing, and deploying software. In practice, DevOps engineers can choose between a multitude of tools, including configuration management, cloud-based continuous integration, and automated deployment. Often individual tools are used without much guidance on how they fit in the big picture, and questions about best practices abound in online forums. However, existing answers are typically generic rules of thumb or dated advice, mostly based on third-party experiences, often non-applicable to the specific context. In fact, current empirical evidence on the effectiveness of DevOps practices is much fragmented and incomplete. State-of-the-art decision-making support, based on hard data and informed advice, can help DevOps engineers discern the best choices and practices for their tasks.<br/><br/>The proposed research is grounded in contingency theory, where the emphasis is on task context when reasoning about the effectiveness of practices. The goal of this project is to learn and convey structured, context-dependent analytics on best practices in DevOps environments, by mining and analyzing data from the collaborative coding platform GitHub. Using established and novel qualitative and quantitative techniques, this research will: (1) identify clusters of software projects that share similar context variables; and (2) within a context of interest, discern the conditions under which DevOps practices such as continuous integration are most (and least) effective. This will result in actionable knowledge and tool support for DevOps teams, to customize efficient project practices to their environment, as well as advance the theory and practice of software engineering, especially as it relates to distributed, fast paced, automation-heavy environments."
"1850029","CRII: CIF: Unifying Scheduling and Optimization Techniques to Speed-up Distributed Stochastic Gradient Descent","CCF","COMM & INFORMATION FOUNDATIONS","03/01/2019","01/31/2019","Gauri Joshi","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","02/28/2021","$175,000.00","","gaurij@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7797, 7935, 8228, 9102","$0.00","Stochastic gradient descent (SGD) is at the core of state-of-the-art supervised learning, which is revolutionizing inference and decision-making in many diverse applications such as self-driving cars, robotics, personalized search and recommendations, and medical diagnosis. Thus, improving the speed of stochastic gradient descent is a timely and important research problem. Due to the massive scale of neural network models and training data sets used today, it has become advantageous to parallelize SGD across multiple computing nodes. Although parallelizing SGD boosts the amount of data processed per iteration, it exposes the algorithm to unpredictable node slowdown and communication delays stemming from variability in the computing infrastructure. The goal of this project is to design provably fast SGD algorithms that easily lend themselves to distributed implementations, and are robust to fluctuations in computation and network delays as well as unpredictable node failures. This project can assist in making machine learning universally accessible, without requiring access to expensive high-performance computing infrastructure. An open-source implementation of the resulting adaptive distributed SGD algorithms will be released. The research outcomes will also be incorporated into two new machine learning classes at Carnegie Mellon University, and into curriculum development and research sampler workshops for K-12 teachers and students.<br/><br/>The speed of single-node SGD is typically measured in terms of the convergence of training error with respect to the number of iterations. In distributed SGD, the runtime per iteration depends on system-level factors such as the computation delays at worker nodes and the gradient aggregation mechanism. Thus, there is a critical need to understand the error convergence with respect to the wall-clock time rather than the number of iterations. This project will improve the true convergence of distributed SGD with respect to wall-clock time by jointly optimizing the runtime-per-iteration and error-versus-iterations. It will consider two popular distributed SGD frameworks, the parameter server model and the communication-efficient SGD model. The research is expected to provide novel runtime and error analyses of distributed SGD in these frameworks and design the first adaptive distributed SGD algorithms that strike the best error-runtime trade-off.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1408940","SHF: Medium: Collaborative Research: Automatic Locality Management for Dynamically Scheduled Parallelism","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2014","06/03/2014","Umut Acar","PA","Carnegie-Mellon University","Standard Grant","Anindya Banerjee","05/31/2019","$962,951.00","Guy Blelloch","umut@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7924, 7943","$0.00","Title: Automatic Locality Management for Dynamically Scheduled Parallelism<br/><br/>Today's multicore and manycore computers provide increasing amounts of computational power in the form of parallel processing coupled with a complex memory organization with many levels of hierarchy and orders of magnitude difference in cost between accessing different levels. When software exhibits spatial and temporal locality, meaning that it reads and writes memory addresses that are close to one another in relatively small time span, it is able to primarily access data in fast caches, rather than in slow main memory, and deliver good sequential and parallel performance. Unfortunately, with software written in high-level managed programming languages it is difficult to ensure or to predict the amount of spatial and temporal locality, due to the lack of low-level programmer control and the complexities of and interactions between the specific hardware platform and the thread scheduler and the memory manager. This project explores techniques for automatic management of locality in high-level managed programming languages executing on parallel computers with sophisticated memory hierarchies. Using the theoretical models, efficient algorithms, and practical implementations being developed in the project, programmers are able to reason about the expected locality of their programs independent of the target hardware, while a runtime system, including thread scheduler and memory manager, maps the program onto specific hardware to achieve the established performance bounds.<br/><br/>In particular, this project addresses the problem of automatically managing locality via the runtime system of a high-level garbage-collected parallel functional programming language. A comprehensive approach that considers scheduling, memory allocation, and memory reclamation together is used, allowing the thread scheduler to influence the memory manager and vice versa. A key insight of this research program is to view the allocated data of a program as a hierarchical collection of task- and scheduler-mapped heaps. This view guides the theoretical cost model that enables a programmer to reason about locality at a high-level, the efficient algorithms that control when to create and to garbage collect a heap with provable bounds, and the practical implementation that delivers automatic locality management in a parallel functional programming language. The intellectual merits are advances in understanding the interaction of thread scheduling and memory management with locality on modern parallel hardware, the development of high-level, machine-independent cost model, and a synthesis of programming languages, algorithmic theory, and system design to address the challenges of automatic locality management. The broader impacts are improvements in software quality and programmer productivity, the creation of a parallel functional programming language usable in both education and research, and the integration of results into courses and outreach activities."
"1617889","CIF: Small: Combining Information Theoretic Security and Stochastic Control to Study Advanced Persistent Threats","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/10/2016","Parvathinathan Venkitasubramaniam","PA","Lehigh University","Standard Grant","Phillip Regalia","05/31/2020","$342,695.00","","parv.v@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","7797","7923, 7935","$0.00","Despite tremendous advances in cryptography and communication security, information attacks -- both passive such as eavesdropping, and active such as unauthorized data injection --, can severely impair the functioning of modern infrastructural systems that combine cyber communication systems and networked physical components. The ability of adversaries to monitor transmitted data or introduce false information for sustained periods of time whilst staying undetected can result in leakage of sensitive information or cause critical damages to underlying systems with consequences ranging from airline collisions, power blackouts to malfunctioning nuclear reactors. In this research, rigorous frameworks are developed to study vulnerabilities of cyber physical systems to such persistent security threats with the goal of designing novel and resilient system controllers. The systematic approach to study adversarial behavior will not only enable effective cyber policing, but also lay a platform for developing technologies to prevent the next generation of cyber terrorism that aims to cripple basic infrastructural systems in energy, healthcare, transportation etc.  The education and outreach components will facilitate an enhanced awareness in society of potential vulnerabilities of the burgeoning Internet of Things and the path towards cyber physical security.<br/> <br/>This research will study two key challenges in securing cyber physical systems: preventing retrieval of physical system information through continually monitored cyber flows, and limiting disruption to system operations through continually hacked cyber flows. Incorporating the strengths of information theoretic security and statistical inference methodologies into a dynamic programming framework which models cyber physical system evolution as a function of external information and internal control, this research will study quantitatively the trade-offs between information security and the system operational performance, and through the process, develop attack detection and mitigation methodologies."
"1750047","CAREER: Advancing On-chip Network Architecture for GPUs","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2018","02/11/2019","Lizhong Chen","OR","Oregon State University","Continuing grant","Yuanyuan Yang","01/31/2023","$166,239.00","","chenliz@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7798","1045, 7941","$0.00","Graphics Processing Units (GPUs) have been playing critical roles in numerous disciplines and sectors, as well as many emerging fields that might not otherwise be possible. Examples include autonomous driving, virtual reality, medical imaging, and parallel computing in HPC systems and data-centers. This success should be largely credited to the massively parallel computing capability of GPU architectures, which can integrate thousands of processing cores on a single chip. To continue meeting growing performance expectations and energy-efficiency, a key challenge over the next decade and beyond is how to support on-chip communications among the vast number of processing cores and provide fast and efficient data transfer to feed concurrent computations.<br/><br/>This project establishes an integrated research and education program to investigate cross-cutting approaches and techniques to advance and improve the effectiveness of on-chip networks (NoCs) in GPU systems, as well as to create academic course materials and outreach activities for the education and broad dissemination of the proposed subjects. The research component has three main thrusts: 1) increasing fundamental understanding of GPU NoCs by investigating, among many other open problems, the criticality, scalability and sensitivity of NoCs to GPU architecture; 2) designing and implementing cost-effective router-based and routerless GPU NoCs, and 3) co-optimizing NoC design with other GPU subsystems such as cache and warp scheduling. The education and outreach components include developing automated tools to increase the effectiveness of simulation-based course projects and engage students from diverse backgrounds, strengthening architecture course offerings, organizing interdisciplinary seminar courses, and leading various outreach activities on K-12 education, women and underrepresented groups."
"1659871","REU Site: Sensor, Signal and Information Processing Devices and Algorithms","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2017","01/17/2017","Andreas Spanias","AZ","Arizona State University","Standard Grant","Rahul Shah","01/31/2020","$299,923.00","Jennifer Blain Christen","spanias@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","1139","9250","$0.00","This three year REU site will recruit and train nine undergraduate students each summer and engage them in research endeavors on the design of sensors including student training in mathematical methods for extracting information from sensor systems.  The investigators along with a team of faculty advisors will supervise a series of multidisciplinary projects in the design of integrated sensor systems.   In addition to the planned projects, the faculty leaders of this program will organize a series of industry collaborative training activities for the students.  This REU features multidisciplinary synergies across different research labs that provide access to unique sensor and algorithm technology. The program also includes crosscutting modules, and workshops in public speaking, policy, standards, ethics, patents, SBIR planning, and outreach.   Annual REU workshops will train students to communicate with stakeholders who can help establish new standards. An evaluation unit will assess REU goals annually using feedback from academia, industry and stakeholders.  The program engages minority colleges to broaden participation and enhance recruitment. <br/><br/>The REU will address STEM problems associated with sensor applications in internet of things, health monitoring and security.  Specific objectives of the REU site are to:  a) introduce students to general research practices by immersing them in government and industry research activities, b) engage students in integrated design of sensor devices and relevant information processing methods, c) motivate students to pursue research careers,   d) provide cross cutting skills in presentation, developing patents, entrepreneurship, and building awareness on social implications, policies and standards.  The REU projects are designed to introduce students to an array of sensor device design technologies that emphasize low power circuits, flexible electronics, MEMS, and embedded systems.  During the same period, projects will train REU students to interpret data from sensors by studying and programming machine learning algorithms, sensor fusion methods, and techniques to interpret big data sets."
"1527535","SHF: Small: Optimizing Consolidation Efficiency of Emerging Virtualized Cloud Applications on Contemporary Server Architecture","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","08/10/2015","Tao Li","FL","University of Florida","Standard Grant","Yuanyuan Yang","07/31/2019","$460,000.00","","taoli@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7798","7923, 7941","$0.00","Optimizing Virtual Machine (VM) consolidation performance has been one of the most critical tasks faced by the cloud provider community. Nevertheless, when scaling virtual platforms to handle the proliferation of cloud applications, challenges arise due to constraints on performance degradation, especially in those environments where VM consolidation density continues to grow. Worse, the consolidated workloads are shifting from conventional single-task computation-oriented applications to large-scale and complex workloads. They generate many diverse interactions and communication patterns and some of these applications have a large irregular memory footprint and high memory consumption, which presents a significant challenge to optimizing the efficiency of virtual machine consolidation. Currently, there emerge some new techniques for contemporary server architecture. Server manufacturers are replacing traditional Uniform Memory Access (UMA) machines with Non-Uniform Memory Access (NUMA) ones due to the latter's higher memory bandwidth and better system scalability. On the other hand, the throughput-oriented graphics processing units (GPUs) are being increasingly deployed to cloud data center servers to meet computation demands. Modern hypervisors begin to virtualize GPU resources and deliver efficient and reliable performance to hosted virtual machines. <br/><br/>To embrace the opportunities and address the associated challenges, this research project will develop techniques to improve the consolidation efficiency of emerging virtualized cloud applications as the advancement of the underlying server architecture continues. The project objectives include: (1) System-wide consolidation performance profiling and optimization for NUMA architecture; (2) Graphic-as-a-Service (GaaS) workload consolidation overhead characterization and minimization; and (3) Collective workload consolidation optimizations in terms of both NUMA and GPU server configurations. This project, which synergistically integrates emerging server architecture features and virtual machine hypervisor resource management, will open the door for a new class of efficient scale-out computing platforms for cloud and big data computing. It will contribute to enabling computing systems to stay on track with its historic scaling and hence benefit numerous real-life applications. This project will also contribute to society through engaging under-represented groups, and research infrastructure dissemination for education and training."
"1350264","CAREER: Smart Monitoring and Inspection with Unmanned Aerial Vehicles","CCF","COMM & INFORMATION FOUNDATIONS","02/01/2014","02/07/2018","Usman Khan","MA","Tufts University","Continuing grant","Phillip Regalia","01/31/2020","$482,692.00","","khan@ece.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7797","1045, 7936, 9251","$0.00","Wireless Sensor Networks (WSNs) are being envisioned to monitor health and vitals of the nation?s critical infrastructure, e.g. bridges, roads, and buildings. Other applications include monitoring remote space stations and off-earth structures, e.g. the inflatable lunar habitats. A significant challenge in making remote monitoring a reality is data collection from the sensors, as their batteries do not allow long-range transmission. This research enables data collection and inspection via Unmanned Aerial Vehicles (UAVs), invoking subsequent investigation in dynamic task allocation, distributed path planning, and collaborative navigation in GPS-denied/indoor settings. In particular, this study develops the theoretical foundations for autonomous monitoring and inspection, where WSNs operate in a complete harmony with a network of UAVs. The WSN provides intelligible information to the UAVs for responsive and diagnostic actions; while the UAVs coordinate (and may also bring outside directives) to adapt the WSN to the environment and application demands.<br/><br/>To undertake this effort, this research casts the predominantly nonlinear UAV control, navigation, and planning problems in a completely linear framework. The linearity of the underlying dynamics builds upon some of the classical work in convex geometry and Euclidean metrics. Subsequently, the distributed control problem is formulated using structured systems theory, by which efficient graph-theoretic controllability and actuation methods are developed. These features play a significant role in establishing rigorous analytical arguments and in implementing a fully functional remote monitoring and inspection prototype. The transformative approach adapts to the application needs and is robust to imperfections in the underlying communication and environment uncertainties. The study further involves several outreach efforts via social coding platforms and workshops for minority, women in engineering, and K-12 students."
"1850182","CRII: AF: Faster Iterative Decisions within First-order Optimization Methods","CCF","ALGORITHMIC FOUNDATIONS","06/01/2019","01/30/2019","Swati Gupta","GA","Georgia Tech Research Corporation","Standard Grant","Balasubramanian Kalyanasundaram","05/31/2021","$175,000.00","","swatig@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","079Z, 7796, 8228, 9102","$0.00","At the heart of most user-centric algorithms today is an optimization engine trying to provide the best decision with the information observed so far in time. Examples include recommending preferred items to new consumers, computing least congested routes in a road network with changing traffic, distributing power radially over electricity grids with changing demand, and so on. All these problems have an inherent ""combinatorial"" structure that constrains the possible decisions one can take. This combinatorial structure becomes even more complex when these decisions are required to incorporate fairness and reduce disparate impact, such as a balanced representation of female/male scientists when a search query for ""scientists"" is made. These combinatorial problems are computationally challenging, necessitating rigorous yet fast algorithms to run these efficiently in real-world scenarios where decisions must be made in real-time. A large class of optimization methods prevalent in learning applications, the so-called first-order optimization methods, work by repeatedly solving perturbations of similar combinatorial subproblems.  This research project will explore whether the knowledge of ""how"" the solution was computed to previous subproblems within first-order optimization methods can be used to speed up computations in subsequent iterations. This project has the potential of speeding up a wide range of applications whenever a constrained decision with combinatorial structure must be made in real-time as mentioned above. The interdisciplinary nature of this work, spanning first-order optimization methods and combinatorial algorithms, will benefit students helping prepare a stronger and a holistic STEM workforce with impact in a large number of applications - at both undergraduate and graduate levels.<br/><br/>This project serves as a cornerstone for proper integration of first-order optimization methods (like Frank-Wolfe (FW), mirror descent (MD) and their variants) and the theory of combinatorial optimization with wide-ranging applications. It brings together the theory of data structures, approximation algorithms, parametric analysis in combinatorial optimization and the computational requirements of iterative first-order optimization methods to achieve amortized speeds ups in overall runtime. MD and FW variants rely only on the function value and gradient information at a single data point in each iteration and this property has rendered these methods to be used in numerous real-time machine learning applications. When making constrained combinatorial decisions as described above, these methods repeatedly compute two main subproblems: (i) linear optimization in each iteration of the FW variants, and (ii) projections or convex minimization in each iteration of the MD variants. With this award, the investigator will explore (a) identification of combinatorial primitives suitable for amortized analysis within first-order methods, (b) use of previously discovered tight cuts for iterative projections within mirror descent variants, (c) decomposition of convex problems, (d) construction of smarter warm start solutions for first-order optimization, (e) weaker but relevant approximate models and algorithms for repeated perturbed subproblems. With recent results closing the gap of possible convergence rates for first-order optimization in various settings, these questions have become of paramount importance to enable the next scale up in runtime of constrained decision-making to real-time user-centric applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1657336","CRII: SHF: Design and Analysis of Processing-Near-Memory Enabled GPU Architecture","CCF","CRII CISE Research Initiation","02/01/2017","01/25/2017","Adwait Jog","VA","College of William and Mary","Standard Grant","Yuanyuan Yang","01/31/2020","$175,000.00","","adwait@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","026Y","7941, 8228","$0.00","Graphics Processing Units (GPUs) are becoming an inevitable part of everycomputing system because of their ability to enable orders of magnitude faster andenergy-efficient execution. However, the necessary and continuous scaling of GPUsin terms of performance and energy efficiency will not be an easy task.Prior works have shown that two biggest impediments towards this scaling are thelimited memory bandwidth and the excessive data movement across different levelsof the memory hierarchy. In order to alleviate these two issues, die-stacking technologyis gaining momentum in the realm of high-performance energy-efficient GPU computing.This technology not only enables very high memory bandwidth for better performancebut also provides support for processing-near-memory (PNM) to reduce data movement,access latencies, and energy consumption. Although these technologies seem promising,the architectural support and execution models for PNM-based GPUs and theirimplications on the entire system design have largely been unexplored. This project takes a fresh look at the design and execution model of a PNM-enabled GPU, which consists of multiple memory stacks and each memory stack incorporatesa 3D-stacked logic layer that can consist of multiple PNM GPU cores and other uncorecomponents. Considering that GPUs are becoming an inevitable part of every computingsystem ranging from warehouse-scale computers to wearable devices, the insights resulting from this research can have a long-term positive impact on the GPU-basedcomputing. The findings of this research will be incorporated to existing and new undergraduate and graduate courses, which will directly help in educating and training students, including women and students from diverse backgrounds and minority groups.<br/><br/>First, a detailed design space exploration will be performed, whichwill involve the study of the impact and interactions of different design choicesrelated to PNM cores (e.g., register file, SIMD width, pipeline components,warp occupancy), uncore components at the logic layer (e.g., caches) and stackedmemory (e.g., number of stacked memories). Second, acomputation distribution framework (CDF) will be developed that will answer: a) when is itpreferable to map computations to PNM cores, b) which PNM coresand computations they should be?, and c) how can we effectively take advantage of both PNM and regular GPU cores? The CDF will leverage different static andruntime strategies to address many of such similar questions to push theenvelopes of energy efficiency and performance even further. The proposed research components will be evaluated via a wide-range of GPGPU applications. If successful, the findings of this research would better equip PNM-enabledGPUs to effectively alleviate the two major bottlenecks: memory bandwidth andenergy."
"1629564","XPS: FULL: Collaborative Research: Parallel and Distributed Circuit Programming for Structured Prediction","CCF","Exploiting Parallel&Scalabilty","08/15/2016","08/11/2016","Jason Eisner","MD","Johns Hopkins University","Standard Grant","Anindya Banerjee","07/31/2019","$415,000.00","","jason@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","8283","","$0.00","This project develops a system for ""circuit programming,"" which allows a programmer to focus on the high-level solution to a problem rather than on the details of how the computation is organized. Circuit programming consists of writing rules that describe how data items depend on one another. The intellectual merits lie in the design of a new programming language for specifying these rules, along with the algorithms whereby the computer automatically finds efficient strategies for managing the necessary computations on available parallel hardware.  The project's broader significance and importance lie in its potential to streamline work in areas such as artificial intelligence and machine learning.  With the growing complexity of systems in these areas and their need to process big data in depth, research and teaching typically get bogged down in programming details, especially for parallel platforms; this project aims to delegate those details to automatic methods.<br/><br/>The research develops a programming system for Dyna, a circuit programming language that enables concise specification of large function graphs that may be cyclic and/or infinite. Dyna employs (1) a pattern-matching notation that augments pure Prolog with evaluation and aggregation and (2) an object-like mechanism for dynamically defining new sub-circuits as modifications of old ones.  This project is building an adaptive system that can mix forward and backward chaining to seek a fixpoint of the circuit and to update this fixpoint as the inputs change.  The system will perform compile-time and runtime analysis of the Dyna program and will map it to Habanero, a system for scheduling parallel computations on multicore processors, with extensions for task priorities, task cancellation, GPU execution, and distributed execution."
"1811109","SHF: Small:Extremely Energy-Efficient Monolithic 3D System Architectures","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","05/18/2018","Niraj Jha","NJ","Princeton University","Standard Grant","Sankar Basu","06/30/2021","$450,000.00","","jha@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7798","7923, 7945","$0.00","With the slowing down of Moore's Law, traditional 2D scaling is not expected to deliver the area, power, and performance benefits the semiconductor industry has been counting on.  Thus, the time has come to go vertical, accommodating processor cores, accelerators, cache, and main memory in the same package. There are two ways of doing this: use through-silicon vias (TSVs) or monolithic 3D integration. TSVs do not have memory-on-logic stacking success stories.  Monolithic 3D integration uses monolithic inter-tier vias that have a much smaller diameter than TSVs, and enable many different design styles: transistor-level monolithic, gate-level monolithic, and block-level monolithic.  While fabrication and test techniques for monolithic 3D integration are maturing, monolithic 3D system architectures have not been investigated in depth.  The work on this project fills this gap. The methodologies and tools developed under this grant will be made available on the web. They will also be made available to the industry.  The material will be included in course materials, and under-represented graduate students will be attracted to this research through Princeton Fellowships.  Results will be disseminated through research articles and seminars.<br/><br/>There are various ""walls"" confronting computer system architects these days.  The Power Wall constrains the portion of the chip that can be powered on.  This is also known as the dark silicon problem.  The Memory Wall prevents efficient access to off-chip memory. Monolithic 3D integration has the potential to significantly alleviate the problems associated with these walls, especially for abundant-data problems, such as machine learning and inference, which are becoming commonplace. This project seeks to improve the energy efficiency of monolithic 3D system architectures by a factor of 500 relative to traditional system architectures.  It will do so by exploiting synergies across the device, logic, memory, accelerator, micro-architecture, chip multiprocessor, and monolithic 3D IC levels of the design hierarchy, providing a common computation platform from high-performance mobile devices to data centers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617732","SHF: Small: DeSCPar: Decoupled Supply-Compute Communication Management for Heterogeneous, Accelerator-Oriented Parallelism","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/28/2016","Margaret Martonosi","NJ","Princeton University","Standard Grant","Yuanyuan Yang","07/31/2019","$300,000.00","","martonosi@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7798","7923, 7941","$0.00","Over the past decade, the deceleration of Moore's Law and Dennard Scaling has required computing to make a dramatic shift towards the use of on-chip parallelism in order to achieve computer systems performance scaling at acceptable power budgets. Beyond that, systems have also dramatically increased their use of heterogeneous processing elements and specialized accelerators. Much of the complexity of this heterogeneous, accelerator-oriented parallelism is exposed without sufficient abstraction to programmers and compiler writers. As a result, achieving high performance often requires that programs include detailed, platform-specific tailoring, particularly regarding the staging of data as it moves between memory and compute elements, and from one compute element to another. This platform-specific tailoring limits the portability of such programs; when a new chip implementation is released, extensive software reworks are often required to reclaim that high performance. Overall, the result is that heterogeneous parallelism is reducing the performance portability of application software. The DeSCPar research attacks this problem and represents important research in improving programmability. Developed tools will be distributed as free software, including a DeSCPar simulator and design tools. In addition, the project includes a broad set of activities around improving the diversity of the computing workforce.<br/><br/>The DeSCPar approach uses Decoupled Supply-Compute Parallelism to achieve portable performance on highly parallel highly-heterogeneous systems. Inspired by Decoupled Access-Execute approaches, DeSCPar likewise decouples value computations from the memory accesses and address computations that ""feed"" them. By using automated slicing techniques to split code into a data supply portion and a computation portion, high-performance memory optimizations can be achieved while retaining high-level application portability. In DeSCPar, value computation operations are targeted to run on a CompD which may be a hardware accelerator, a specifically-optimized CPU, or a general-purpose CPU. Likewise, memory supply code is aimed at a SuppD, which can be specifically optimized for its task. By employing varied combinations of SuppD and CompD units, richly heterogeneous systems can be built, and software can be automatically mapped onto them. This project: (i) proposes and prototypes automated compiler techniques (based on LLVM) for slicing and optimizing DeSCPar code; and (ii) proposes and evaluates hardware design optimizations based on the DeSCPar organizational structure."
"1822738","SPX: Collaborative Research: Moving Towards Secure and Massive Parallel Computing","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/29/2018","Mohammad Hajiaghayi","MD","University of Maryland College Park","Standard Grant","Tracy Kimbrel","09/30/2019","$68,333.00","","hajiagha@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","042Y","026Z","$0.00","Modern computing systems have moved beyond single-core, single-processor devices to more modern multi-core parallel processors operating in networked systems and available in warehouse-scale clouds popularized by industries and the government. This new parallel, interconnected, big-data world requires fundamental research on multiple levels from algorithms to systems and computer architecture. This project seeks to take initial steps in the study of the expansive set of algorithms and systems issues in this important research challenge by building and developing new general frameworks for massive parallel computation, often involving privacy and security, in real-life scenarios. <br/><br/>The investigators? long-term goals include two directions.  As the first thrust of this effort, the investigators aim to design fundamental and efficient algorithms for massive parallel computations in the practical MapReduce framework, in particular by reducing the number of rounds in this framework. As the second thrust of this effort, the investigators aim to augment current parallel environments and architectures with better data structures and abstractions to develop simplified and fast implementations of fundamental algorithms such that everyone can use them in practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822805","SPX: Collaborative Research: Moving Towards Secure and Massive Parallel Computing","CCF","SPX: Scalable Parallelism in t","10/01/2018","12/13/2018","Elaine Shi","NY","Cornell University","Standard Grant","Tracy Kimbrel","09/30/2019","$0.00","","elaine@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","042Y","026Z","$0.00","Modern computing systems have moved beyond single-core, single-processor devices to more modern multi-core parallel processors operating in networked systems and available in warehouse-scale clouds popularized by industries and the government. This new parallel, interconnected, big-data world requires fundamental research on multiple levels from algorithms to systems and computer architecture. This project seeks to take initial steps in the study of the expansive set of algorithms and systems issues in this important research challenge by building and developing new general frameworks for massive parallel computation, often involving privacy and security, in real-life scenarios. <br/><br/>The investigators' long-term goals include two directions.  As the first thrust of this effort, the investigators aim to design fundamental and efficient algorithms for massive parallel computations in the practical MapReduce framework, in particular by reducing the number of rounds in this framework. As the second thrust of this effort, the investigators aim to augment current parallel environments and architectures with better data structures and abstractions to develop simplified and fast implementations of fundamental algorithms such that everyone can use them in practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1253205","CAREER: Information Theory Beyond Capacity","CCF","Networking Technology and Syst, COMM & INFORMATION FOUNDATIONS, COMM & INFORMATION THEORY","02/01/2013","05/05/2017","Yury Polyanskiy","MA","Massachusetts Institute of Technology","Continuing grant","Phillip Regalia","07/31/2019","$625,238.00","","yp@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7363, 7797, 7935","1045, 7363, 7556, 7935","$0.00","Unprecedented technological progress in the last decades makes information theory an ever more exciting and important discipline. The modern world is swarming with information streams pervading the radio, wires, fiber optic cables, and on-chip networks. Yet we are unable to answer the most basic questions such as the impact of delay on the capacity of multiple-antenna wireless channels, or the fundamental principles of protecting computation networks from local process variation in silicon chip fabrication.  As such, the main purpose of this project is to advance the state-of-the-art in the fundamental limits of delay-constrained wireless communication. Computation of the impact of delay constraint in wireless communication will allow assessments of the degree of suboptimality of currently employed systems and industry standards, and likewise shed light on novel and higher-performing wireless systems.<br/><br/>The progress on non-asymptotic information theory is inseparable from understanding of non-Shannon information measures and their data-processing properties on general (non-linear) graphs.  The progress on this topic is expected to provide the theoretical tools required for exploration of complex information processing systems including non-communication ones, such as fault-tolerant chips and noise-resistant circuits.  Advanced converse techniques and graph-based data-processing will open information theory to new fields and is expected to reinvigorate the progress in the converse bounds for multi-terminal (network) problems.<br/><br/>The curriculum will be broadly disseminated through online resources, OpenCourseWare and MITx/edX.  The analysis of real-world communication systems also presents a rich field for undergraduate research opportunities (UROPs). Popularizing finite blocklength results is likewise expected to have industrial impact, especially in areas related to wireless and time-critical communication. The compiled performance charts and delay-constrained analysis will guide the design of next-generation mobile standards and help in fair assessment of intellectual property."
"1535929","AitF: FULL: Collaborative Research: Optimizing Networked Systems with Limited Information","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS","09/01/2015","04/26/2018","Ravi Sundaram","MA","Northeastern University","Standard Grant","Tracy Kimbrel","08/31/2019","$375,924.00","Rajmohan Rajaraman","koods@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7239, 7796","012Z, 7926, 9251","$0.00","Over the past decades, the world's dominant computational infrastructure has gradually transitioned from individual personal computers to massive networked systems of unprecedented scale and complexity. Not only has this led to tremendous technological and engineering challenges, but it has also called into question fundamental assumptions in classical algorithmic theory. A defining distinction is the limited information that algorithms in such decentralized and heterogeneous systems have access to. For example, a content delivery network serves a heterogeneous set of end devices, and has to optimize performance often without knowledge of the device it is optimizing for. Similarly, a data center scheduler must be optimized for future demands that it is oblivious to. In this project, the PIs seek to address novel algorithmic questions in non-clairvoyant models of computation that arise in real world networked systems. The successful completion of this project will lead to new advances in building reliable and resilient information networks. The project will enable collaborations and exchange of ideas between theoreticians and practitioners, and will provide extensive training in real world algorithms to several graduate students, with attention paid to gender diversity and participation of under-represented groups.<br/> <br/>The goal of the project is to design novel algorithms with provable guarantees for networked systems in limited information settings. In particular, the PIs plan to address key algorithmic problems in the three dominant computational infrastructure models in the Internet: (a) data centers: allocating parallelizable jobs requiring multiple resources on processing nodes and clusters; (b) wide-area network of clusters: long-term planning of resource deployment and synergistic operations in the client-server model; and (c) P2P browser clouds: content delivery in web and gaming applications and swarm computing on a fabric of a large number of loosely coupled unreliable browsers. These problem domains are characterized by uncertainty and limited information for several reasons, including uncertainty about future predictions, autonomy of individual components in the networked system, and distributed implementation of the network architecture. The algorithms designed as part of this project will be evaluated on and optimized for real world testbeds."
"1629548","XPS: FULL: Collaborative Research: PARAGRAPH: Parallel, Scalable Graph Analytics","CCF","Exploiting Parallel&Scalabilty","09/01/2016","08/12/2016","Ponnuswamy Sadayappan","OH","Ohio State University","Standard Grant","Aidong Zhang","08/31/2019","$546,875.00","Louis-Noel Pouchet, Srinivasan Parthasarathy","sadayappan.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8283","","$0.00","Many real world problems can be effectively modeled as complex relationship networks or graphs where nodes represent entities of interest and edges mimic the interactions or relationships among them. The number of such problems and the diversity of domains from which they arise is growing. However developing high-performance applications to extract useful information from such datasets is very challenging.  Graphical processing units are very attractive for such applications because they offer higher computational performance and energy efficiency than standard multi-core processors. However, the development of high-performance applications for them is currently much more challenging than  parallel program development for standard  multi-core processors. Effective  application development to use graphical processing units generally requires that developers possess considerable expertise on their architectural characteristics and use specialized programming models and performance optimization techniques. Thus, simultaneously achieving high performance and high user productivity for data analytics applications for such devices is a daunting challenge.<br/><br/>This project proposes a scalable high-level software framework to enable the productive development of high-performance applications for graphical processing units. It features two distinct abstractions to address the performance and productivity challenges in developing graph/data analytics applications: 1) a frontier-centric abstraction that is based on a common iterative characteristic of many of these applications, with a dynamically moving active frontier of vertices (or edges) where computation is centered, and 2) an abstraction based on sparse linear algebra primitives, exploiting the dual relationship between sparse matrices and graphs. A benchmark suite of graph analytics applications will be developed and evaluated using both abstractions, enabling insights into the effectiveness of these alternate high-level abstractions for a range of analytics applications. The benchmark suite and the software framework will be publicly released."
"1536003","AitF: FULL: Collaborative Research: PEARL: Perceptual Adaptive Representation Learning in the Wild","CCF","Algorithms in the Field","09/01/2015","08/14/2015","Trevor Darrell","CA","University of California-Berkeley","Standard Grant","Tracy Kimbrel","08/31/2019","$200,000.00","","trevor@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7239","012Z","$0.00","Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or ""domain."" In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a ""dog"" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms ""in the field.""<br/><br/>This project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets."
"1717268","AF:  Small:  Collaborative Research:  Computational Representations for Design and Fabrication of Developable Surfaces","CCF","Cyber-Human Systems (CHS), ALGORITHMIC FOUNDATIONS","08/01/2017","08/02/2017","Eitan Grinspun","NY","Columbia University","Standard Grant","Balasubramanian Kalyanasundaram","07/31/2020","$249,999.00","","eitan@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7367, 7796","7367, 7923, 7933","$0.00","This project develops mathematical representations and computer algorithms for three-dimensional forms that can be assembled from a collection of flat pieces without incurring material stress, known as piecewise developable geometry. These will drive new developments in the emerging industry of advanced 3D manufacturing: Not only can developable geometry easily be cut from a rich array of thin flat materials like plywood or sheet metal, but it can also provide a novel geometric approach to tool path planning that improves the efficiency and accuracy of shapes fabricated via computer-controlled flank milling.  Such processes offer a competitive advantage for manufacturing in the US by reducing cost, increasing complexity of fabricated forms, and automating tasks previously only achievable by hand (e.g., robotic folding of developable forms).  A fundamental issue addressed by the research is automatic approximation of an arbitrary curved surface by a small number of developable pieces---at present this process must be carried out laboriously by expert engineers and designers, severely limiting the scope and impact of developable manufacturing processes.  More broadly, algorithmic models of developable geometry enrich basic understanding in the area of discrete differential geometry, which seeks to reformulate classical geometric knowledge from a discrete, algorithmic point of view.  This area provides a crucial link between modern geometric theory and industrial/applied applications that need to incorporate data and computation, and students trained in this project will be well-equipped to contribute in 3d manufacturing. <br/><br/>The project builds on foundations from smooth and discrete differential geometry: rather than view discrete meshes as mere numerical approximations, the unifying goal is to develop data structures that directly encode the most salient features of piecewise developable geometry.  Two key observations are that (i) flattenability alone is not a sufficient characterization for discrete developability, often leading to nasty ""crumpling"" behavior and (ii) the curvature of a piecewise developable surface is encoded entirely by the shape of its patch boundaries, a fact often exploited in garment design.  These observations lead to two primary thrusts, namely (i) representations for discrete developability that naturally avoid crumpling by guaranteeing the existence of discrete ""ruling lines,"" and (ii) efficient algorithms for developable surface design based on sparse descriptions of curvature along critical feature curves.  A cross-cutting theme is physical considerations for fabrication, e.g., translation between simple geometric models and material/constitutive properties relevant to the production of physical artifacts."
"1733701","AitF: Collaborative Research: A Framework of Simultaneous Acceleration and Storage Reduction on Deep Neural Networks Using Structured Matrices","CCF","Algorithms in the Field","09/15/2017","09/13/2017","Xue Lin","MA","Northeastern University","Standard Grant","Rahul Shah","08/31/2020","$348,037.00","David Kaeli","xuelin@coe.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7239","9102","$0.00","  Deep neural networks (DNNs) have emerged as a class of powerful techniques for learning solutions in a number of challenging problem domains, including computer vision, natural language processing and bioinformatics.  These solutions have been enabled mainly because we now have computational accelerators able to sift though the myriad of data required to train a neural network.   As the size of DNN models continues to grow, computational and memory resource requirements for training will also grow, limiting deployment of deep learning in many practical applications. <br/><br/>      Leveraging the theory of structured matrices, this project will develop a general framework for efficient DNN training and inference, providing a significant reduction in algorithmic complexity measures in terms of both computation and storage.  <br/>The project, if successful, should fundamentally impact a broad class of deep learning applications.  It will explore accelerating this new structure for deep learning algorithms targeting emerging accelerator architectures, and will evaluate the benefits of these advances across a number of application domains, including big data analytics, cognitive systems, unmanned vehicles and aerial systems, and wearable devices.  The interdisciplinary nature of this project bridges the areas of matrix theory, machine learning, and computer architecture, and will affect education at both Northeastern and CCNY, including the involvement of underrepresented and undergraduate students in the rich array of research tasks.<br/><br/>  The project will: (1) for the first time, develop a general theoretical framework for structured matrix-based DNN models and perform detailed analysis and investigation of error bounds, convergence, fast training algorithms, etc.; (2) develop low-space-cost and high-speed inference and training schemes for the fully connected layers of DNNs; (3) impose a weight tensor with structure and enable low computational and space cost convolutional layers; (4) develop high-performance and energy-efficient implementations of deep learning systems on high-performance parallel platforms, low-power embedded platforms, as well as emerging computing paradigms and devices; (5) perform a comprehensive evaluation of the proposed approaches on different performance metrics in a variety of platforms.The project will deliver tuned implementations targeting a range of computational platforms, including ASICs, FPGAs, GPUs and cloud servers. The hardware optimizations will focus on producing high-speed and low-cost implementations of deep learning systems."
"1733742","AitF: Collaborative Research: Automated Medical Image Segmentation via Object Decomposition","CCF","Algorithms in the Field","08/15/2017","08/07/2017","Xiaodong Wu","IA","University of Iowa","Standard Grant","James Donlon","07/31/2021","$392,256.00","","xiaodong-wu@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7239","7495, 7926","$0.00","Medical image segmentation, the process of dividing a medical image into meaningful objects such as organs, tumors, etc., is a critical tool that allows medical professionals to provide customized medical care to patients.  In the past this highly technical, individualized care has required experts to manually analyze the images, a process that is very expensive in both time and money.  Over the past decade, enormous technological advances have been made in biomedical imaging, leading to a large amount of new and improved medical data which has created a demand for algorithms which can process this data faster and more thoroughly.  Researchers have worked extensively to develop these medical image segmentation algorithms, but current algorithms suffer from the following drawbacks: 1) they do not have the capability of effectively representing diverse shapes of a wide variety of medical objects and/or 2) they require substantial interaction from an expert user.  This research will develop a novel medical image segmentation algorithm that can be applied to various types of medical images and will be able to be executed by any user with basic computer literacy.  Many important objects will be able to be handled with the same algorithm, such as livers, prostates, and vertebrae.  This research allows medical experts to spend less time analyzing a wide variety of medical images and more time directly working with patients. <br/><br/>The algorithm will work for any medical imaging object of interest whose shape can be decomposed into a small number of components with a very simple geometric structure.  For example, livers may be slightly different from person to person, but almost all livers can be represented as a union of two or three ""star-shaped"" components.  A component is defined to be star-shaped if there is a center point in the component such that the line segment connecting the center to every other point in the component is contained within the object.   If the center of a single star-shaped component is known, then the whole component can be very quickly identified by computer algorithms, but as the number of components increases, the simultaneous computation of all the components becomes much more difficult.  This research will develop algorithms which can automatically compute the centers of the star-shaped components for many medical imaging objects such as livers, prostates, and vertebrae, and further will develop algorithms that can simultaneously identify all the components for the objects.  The result will be a single algorithm that will be applied to many scenarios and can be executed by non-technical users."
"1733874","AitF: Collaborative Research: Automated Medical Image Segmentation via Object Decomposition","CCF","Algorithms in the Field","08/15/2017","08/07/2017","Matthew Gibson","TX","University of Texas at San Antonio","Standard Grant","James Donlon","07/31/2021","$366,250.00","","MATTHEW.GIBSON@UTSA.EDU","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","7239","7495, 7926","$0.00","Medical image segmentation, the process of dividing a medical image into meaningful objects such as organs, tumors, etc., is a critical tool that allows medical professionals to provide customized medical care to patients.  In the past this highly technical, individualized care has required experts to manually analyze the images, a process that is very expensive in both time and money.  Over the past decade, enormous technological advances have been made in biomedical imaging, leading to a large amount of new and improved medical data which has created a demand for algorithms which can process this data faster and more thoroughly.  Researchers have worked extensively to develop these medical image segmentation algorithms, but current algorithms suffer from the following drawbacks: 1) they do not have the capability of effectively representing diverse shapes of a wide variety of medical objects and/or 2) they require substantial interaction from an expert user.  This research will develop a novel medical image segmentation algorithm that can be applied to various types of medical images and will be able to be executed by any user with basic computer literacy.  Many important objects will be able to be handled with the same algorithm, such as livers, prostates, and vertebrae.  This research allows medical experts to spend less time analyzing a wide variety of medical images and more time directly working with patients. <br/><br/>The algorithm will work for any medical imaging object of interest whose shape can be decomposed into a small number of components with a very simple geometric structure.  For example, livers may be slightly different from person to person, but almost all livers can be represented as a union of two or three ""star-shaped"" components.  A component is defined to be star-shaped if there is a center point in the component such that the line segment connecting the center to every other point in the component is contained within the object.   If the center of a single star-shaped component is known, then the whole component can be very quickly identified by computer algorithms, but as the number of components increases, the simultaneous computation of all the components becomes much more difficult.  This research will develop algorithms which can automatically compute the centers of the star-shaped components for many medical imaging objects such as livers, prostates, and vertebrae, and further will develop algorithms that can simultaneously identify all the components for the objects.  The result will be a single algorithm that will be applied to many scenarios and can be executed by non-technical users."
"1854742","AitF: Collaborative Research: A Framework of Simultaneous Acceleration and Storage Reduction on Deep Neural Networks Using Structured Matrices","CCF","Algorithms in the Field","08/15/2018","10/25/2018","Bo Yuan","NJ","Rutgers University New Brunswick","Standard Grant","Rahul Shah","08/31/2020","$367,884.00","","bo.yuan@soe.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7239","","$0.00","  Deep neural networks (DNNs) have emerged as a class of powerful techniques for learning solutions in a number of challenging problem domains, including computer vision, natural language processing and bioinformatics.  These solutions have been enabled mainly because we now have computational accelerators able to sift through the myriad of data required to train a neural network.   As the size of DNN models continues to grow, computational and memory resource requirements for training will also grow, limiting deployment of deep learning in many practical applications. <br/><br/>      Leveraging the theory of structured matrices, this project will develop a general framework for efficient DNN training and inference, providing a significant reduction in algorithmic complexity measures in terms of both computation and storage.  <br/>The project, if successful, should fundamentally impact a broad class of deep learning applications.  It will explore accelerating this new structure for deep learning algorithms targeting emerging accelerator architectures, and will evaluate the benefits of these advances across a number of application domains, including big data analytics, cognitive systems, unmanned vehicles and aerial systems, and wearable devices.  The interdisciplinary nature of this project bridges the areas of matrix theory, machine learning, and computer architecture, and will affect education at both Northeastern and CCNY, including the involvement of underrepresented and undergraduate students in the rich array of research tasks.<br/><br/>  The project will: (1) for the first time, develop a general theoretical framework for structured matrix-based DNN models and perform detailed analysis and investigation of error bounds, convergence, fast training algorithms, etc.; (2) develop low-space-cost and high-speed inference and training schemes for the fully connected layers of DNNs; (3) impose a weight tensor with structure and enable low computational and space cost convolutional layers; (4) develop high-performance and energy-efficient implementations of deep learning systems on high-performance parallel platforms, low-power embedded platforms, as well as emerging computing paradigms and devices; (5) perform a comprehensive evaluation of the proposed approaches on different performance metrics in a variety of platforms.The project will deliver tuned implementations targeting a range of computational platforms, including ASICs, FPGAs, GPUs and cloud servers. The hardware optimizations will focus on producing high-speed and low-cost implementations of deep learning systems."
"1640078","E2CDA: Type I: Collaborative Research: Energy Efficient Learning Machines (ENIGMA)","CCF","Energy Efficient Computing: fr","09/01/2016","09/17/2018","Subhasish Mitra","CA","Stanford University","Continuing grant","Sankar Basu","08/31/2019","$678,480.00","H.-S. Philip Wong","subh@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","015Y","7945","$0.00","The project will aim to develop computing hardware and software that improve the energy efficiency of learning machines by many orders of magnitude. In doing so it will enable large societal adoption of such machines, paving the way for new applications in diverse areas such as manufacturing, healthcare, agriculture, and many others. For example, machines that learn the behavioral trends of individual human beings by collecting data from myriads of sensors may be able to design the most appropriate drugs. Similarly, one may envision machines that learn trends in the weather and thereby assist in predicting the most optimized preparations for the next crop cycle. The possibilities are literally endless. However, the canonical learning machines of today need huge amount of energy, significantly hindering their adoption for widespread applications. The goal of this project will be to explore, evaluate and innovate new hardware and software paradigms that could reduce energy dissipation in learning machines by a significant amount. The team of researchers consists of experts in mathematics, neuroscience, electronic devices and materials and computer circuit and system design that will foster a unique platform for both innovative research and interdisciplinary training of graduate students.<br/><br/>We are witnessing a regimental shift in the computing paradigm. For a  vast  number  of  applications, cognitive functions such as classification, recognition, synthesis, decision-making and  learning  are gaining rapid importance in a world that is infused with sensing modalities, often paraphrased under a common term of ""Big Data"", that are in critical need of efficient information-extraction. This is in sharp contrast to the past when the central objective of computing was to perform calculations on numbers and produce results with extreme numerical accuracy. We aim to approach this problem by exploiting cognitive models that have shown efficacy in ""one shot"" learning. In this approach, the information is represented by means of high dimensional (HD) vectors. These vectors follow a set of predetermined mathematical operations that ensure that the resulting vector after such operations is unique. The uniqueness can in turn be used as ""learning"" and the predefined nature of mathematical operations make the learning ""one shot"". When paired with traditional artificial neural network or deep learning algorithms,  such  ""one  shot"" learning could significantly reduce the number of necessary computing operations, leading to orders of magnitude reduction in energy dissipation. We shall explore the entire computer hierarchy, staring from materials and devices, all the way up to system design and optimization to exploit the unique capabilities afforded by the HD computing, with the ultimate objective of realizing energy efficient learning machines (ENIGMA)."
"1640060","E2CDA: Type I: Collaborative Research: Energy Efficient Learning Machines (ENIGMA)","CCF","Energy Efficient Computing: fr","09/01/2016","09/18/2018","Sayeef Salahuddin","CA","University of California-Berkeley","Continuing grant","Sankar Basu","08/31/2019","$1,108,185.00","Bruno Olshausen, Jan Rabaey","sayeef@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","015Y","7945","$0.00","The project will aim to develop computing hardware and software that improve the energy efficiency of learning machines by many orders of magnitude. In doing so it will enable large societal adoption of such machines, paving the way for new applications in diverse areas such as manufacturing, healthcare, agriculture, and many others. For example, machines that learn the behavioral trends of individual human beings by collecting data from myriads of sensors may be able to design the most appropriate drugs. Similarly, one may envision machines that learn trends in the weather and thereby assist in predicting the most optimized preparations for the next crop cycle. The possibilities are literally endless. However, the canonical learning machines of today need huge amount of energy, significantly hindering their adoption for widespread applications. The goal of this project will be to explore, evaluate and innovate new hardware and software paradigms that could reduce energy dissipation in learning machines by a significant amount. The team of researchers consists of experts in mathematics, neuroscience, electronic devices and materials and computer circuit and system design that will foster a unique platform for both innovative research and interdisciplinary training of graduate students.<br/><br/>We are witnessing a regimental shift in the computing paradigm. For a  vast  number  of  applications, cognitive functions such as classification, recognition, synthesis, decision-making and  learning  are gaining rapid importance in a world that is infused with sensing modalities, often paraphrased under a common term of ""Big Data"", that are in critical need of efficient information-extraction. This is in sharp contrast to the past when the central objective of computing was to perform calculations on numbers and produce results with extreme numerical accuracy. We aim to approach this problem by exploiting cognitive models that have shown efficacy in ""one shot"" learning. In this approach, the information is represented by means of high dimensional (HD) vectors. These vectors follow a set of predetermined mathematical operations that ensure that the resulting vector after such operations is unique. The uniqueness can in turn be used as ""learning"" and the predefined nature of mathematical operations make the learning ""one shot"". When paired with traditional artificial neural network or deep learning algorithms,  such  ""one  shot"" learning could significantly reduce the number of necessary computing operations, leading to orders of magnitude reduction in energy dissipation. We shall explore the entire computer hierarchy, staring from materials and devices, all the way up to system design and optimization to exploit the unique capabilities afforded by the HD computing, with the ultimate objective of realizing energy efficient learning machines (ENIGMA)."
"1535972","AitF: FULL: Collaborative Research: Optimizing Networked Systems with Limited Information","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS","09/01/2015","08/20/2015","Bruce Maggs","NC","Duke University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$367,990.00","Debmalya Panigrahi","bmm@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7239, 7796","012Z, 9251","$0.00","Over the past decades, the world's dominant computational infrastructure has gradually transitioned from individual personal computers to massive networked systems of unprecedented scale and complexity. Not only has this led to tremendous technological and engineering challenges, but it has also called into question fundamental assumptions in classical algorithmic theory. A defining distinction is the limited information that algorithms in such decentralized and heterogeneous systems have access to. For example, a content delivery network serves a heterogeneous set of end devices, and has to optimize performance often without knowledge of the device it is optimizing for. Similarly, a data center scheduler must be optimized for future demands that it is oblivious to. In this project, the PIs seek to address novel algorithmic questions in non-clairvoyant models of computation that arise in real world networked systems. The successful completion of this project will lead to new advances in building reliable and resilient information networks. The project will enable collaborations and exchange of ideas between theoreticians and practitioners, and will provide extensive training in real world algorithms to several graduate students, with attention paid to gender diversity and participation of under-represented groups.<br/> <br/>The goal of the project is to design novel algorithms with provable guarantees for networked systems in limited information settings. In particular, the PIs plan to address key algorithmic problems in the three dominant computational infrastructure models in the Internet: (a) data centers: allocating parallelizable jobs requiring multiple resources on processing nodes and clusters; (b) wide-area network of clusters: long-term planning of resource deployment and synergistic operations in the client-server model; and (c) P2P browser clouds: content delivery in web and gaming applications and swarm computing on a fabric of a large number of loosely coupled unreliable browsers. These problem domains are characterized by uncertainty and limited information for several reasons, including uncertainty about future predictions, autonomy of individual components in the networked system, and distributed implementation of the network architecture. The algorithms designed as part of this project will be evaluated on and optimized for real world testbeds."
"1822809","SPX: Collaborative Research: Moving Towards Secure and Massive Parallel Computing","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/29/2018","Clifford Stein","NY","Columbia University","Standard Grant","Tracy J. Kimbrel","09/30/2019","$68,331.00","","cliff@ieor.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","042Y","026Z","$0.00","Modern computing systems have moved beyond single-core, single-processor devices to more modern multi-core parallel processors operating in networked systems and available in warehouse-scale clouds popularized by industries and the government. This new parallel, interconnected, big-data world requires fundamental research on multiple levels from algorithms to systems and computer architecture. This project seeks to take initial steps in the study of the expansive set of algorithms and systems issues in this important research challenge by building and developing new general frameworks for massive parallel computation, often involving privacy and security, in real-life scenarios. <br/><br/>The investigators? long-term goals include two directions.  As the first thrust of this effort, the investigators aim to design fundamental and efficient algorithms for massive parallel computations in the practical MapReduce framework, in particular by reducing the number of rounds in this framework. As the second thrust of this effort, the investigators aim to augment current parallel environments and architectures with better data structures and abstractions to develop simplified and fast implementations of fundamental algorithms such that everyone can use them in practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1535987","AitF: FULL: Collaborative Research: PEARL: Perceptual Adaptive Representation Learning in the Wild","CCF","Algorithms in the Field","09/01/2015","08/14/2015","Mehryar Mohri","NY","New York University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$399,983.00","","mohri@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7239","012Z","$0.00","Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or ""domain."" In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a ""dog"" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms ""in the field.""<br/><br/>This project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets."
"1718994","AF: Small: Topological Approximation Techniques in Computational Geometry","CCF","ALGORITHMIC FOUNDATIONS","09/15/2017","09/05/2017","Sergey Bereg","TX","University of Texas at Dallas","Standard Grant","Rahul Shah","08/31/2020","$302,454.00","","besp@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7796","7923, 7929","$0.00","In mathematics, several theorems in discrete geometry theorems are established by elegant proofs that a solution exists, without saying how to find one.  For example, the memorably named Ham Sandwich theorem says (in three dimensions) that if you have ham, bread, and cheese, you can cut all three in half with one straight cut (even if you left the cheese in the refrigerator.)  These theorems can actually be important for algorithms -- one would like to compute a hyperplane in d-dimensions that splits d-data sets evenly to divide the work.  This project explores special cases of these problems that can produce algorithms that construct exact or approximate solutions, which are important for data analysis. Because the challenging problems can often be stated very simply, in terms of colored points, this research will involve undergraduates, graduate students, and even a summer course for high school students.<br/><br/>This proposal deals with finding more geometric and effective proofs (leading to efficient algorithms) for some of the  fundamental theorems of convex and discrete geometry -- such as colored versions of Caratheodory and Tverberg's theorems, ham sandwich and other partitioning results. The most elegant proofs of such theorems often involve topological methods  -- using some fixed point theorem, Sperner's lemma, Borsuk-Ulam or more general characteristic class arguments.  By giving proofs that do not depend on topology, this project can create faster algorithms for partitioning."
"1540428","BSF:2014389: Networked Markets","CCF","SPECIAL PROJECTS - CCF","09/01/2015","08/06/2015","Yaron Singer","MA","Harvard University","Standard Grant","Nina Amla","08/31/2019","$40,000.00","","yaron@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","2878","2878","$0.00","In recent years, online social networks have emerged as an important medium for communication, and as a result can provide an effective platform for markets.  Due to the scale and complexity of social networking applications, markets in these environments require a  different theory than the theory which guides the design of traditional internet markets.  This project aims to study markets that operate in networked environments.  The goal of this research is to develop both theory and empirical methods that are suitable for networked market design. <br/><br/>This research has three major thrusts.  The first goal is to study incentives in network cascades.  Information cascades spread information in networks and incentivizing individuals to spread large cascades is a central problem for network market design.  The second objective is to study competition in networked markets.  In some cases, cascades are initiated by competing agencies that seek to influence individuals to adopt the technology or idea they promote. More generally, competition in networks spurs complex dynamics that are challenging to analyze.  This project analyzes equilibrium of such interactions, as well as principles of market design in these complex environments.  The third thrust develops data mining techniques to study the economics of social marketing platforms. The main challenge in such markets is that the social networking services do not control the content being published, and users often post advertisements without any indication. This makes analysis of social marketing platforms a challenging data mining problem of detecting covert advertisements. These methods for analysis and design of networked markets could have a significant impact on internet economics."
"1717741","CCF-SHF: Small: CRONUS: High-Level Reasoning of Low-Level Isolation","CCF","SOFTWARE & HARDWARE FOUNDATION","09/15/2017","08/30/2017","Suresh Jagannathan","IN","Purdue University","Standard Grant","Nina Amla","08/31/2020","$450,000.00","","suresh@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","7923, 7942, 8206","$0.00","Many real-world widely-used web services, like those built and maintained by Amazon, Facebook, and others, encapsulate complex program logic within transactions.  Although serializability is the gold standard used to reason about the behavior of concurrently executing transactions, its enforcement cost has led many commercial database systems to provide support for, and encourage the use of, weaker variants, in which a transaction may witness effects from other transactions as it executes, weakening the strong isolation guarantees provided by serializability.  Weak isolation, while improving availability complicates program reasoning, making it challenging to verify database application correctness, or implement useful program transformations, optimizations, and testing/debugging tools.  Safety and security are thus compromised.  Further complicating matters is the interplay between the database and weak consistency, a property of the underlying data store that exploits replication among geo-distributed mirrored sites to improve throughput and availability.  <br/><br/>Not surprisingly, weak isolation and weak consistency interact in subtle and non-trivial ways.  To better understand this interaction, this project develops new foundational principles and new language abstractions and implementation techniques sensitive to the different behaviors possible when strong isolation and consistency guarantees are loosened. The project investigates new infrastructure for verifying and implementing high-level programs on modern replicated data stores that support only weak enforcement of consistency among replicas and isolation among transactions. Results from this effort will increase the robustness of many widely-used Web services and systems, and lower the effort, risk, and cost associated with developing and certifying modern distributed database applications. The investigators will involve graduate and undergraduate students in this research."
"1619370","SHF: Small: New Directions in Groebner Basis based Verification using Logic Synthesis Techniques","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","05/16/2016","Priyank Kalla","UT","University of Utah","Standard Grant","Nina Amla","07/31/2019","$390,999.00","","kalla@ece.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798","7923, 8206, 9150","$0.00","With the spread of Internet and mobile devices, transferring information robustly, safely and securely has become more important<br/>than ever. Computer hardware associated with such operations performs sophisticated arithmetic computations, which requires careful, custom design of such circuits. Custom design raises the potential for bugs in the circuits, compromising their security. Verification of the correctness of such circuits is an imperative. However, their arithmetic nature makes them notoriously hard to verify, and contemporary algorithms lack the wherewithal to address this problem.  <br/><br/>This project investigates the application of computational algebra, Groebner basis techniques, to formally verify circuits for such applications. In particular, the project analyzes the use of the circuit itself as the underlying data-structure to perform Groebner basis computations and verification. By using logic synthesis as a bridge to connect computational algebra algorithms with circuit design, the project addresses the challenge and scalability of hardware verification. The project impacts computer-aided verification technology, secure system design, and it advances knowledge and application in mathematics as well as computer engineering. Validation of hardware for cyber-security also protects the privacy and security of data, which has a direct impact on our society."
"1540512","BSF:2014424:Time-Message Tradeoffs in Distributed Algorithms","CCF","SPECIAL PROJECTS - CCF","09/01/2015","08/06/2015","Gopal Pandurangan","TX","University of Houston","Standard Grant","Nina Amla","08/31/2019","$50,000.00","","gopalpandurangan@gmail.com","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","2878","2878","$0.00","Distributed algorithms underlie the efficient operation of large-scale  communication networks, e.g., distributed shortest paths algorithms are used for routing in the Internet. Two fundamental performance measures of distributed algorithms are the running time and the number of messages used  by the algorithm. Research in the last three decades has focused to a large extent on optimizing  either one of the two measures separately, typically at the cost of the other. This project investigates how distributed algorithms can be designed to work well under both measures simultaneously. This may have significant implications in many emerging applications,  especially in the context of large-scale distributed communication networks and distributed processing of large-scale data.<br/><br/><br/>The project studies time-message tradeoffs in distributed algorithms for various specific fundamental problems, such as leader election, shortest paths,  minimum spanning tree construction, and random walks. These are fundamental primitives in distributed computing where optimizing both time and messages simultaneously has so far been elusive. Specific goals of the project are to: (1) design distributed algorithms that are optimal with respect to the other measure when given a particular value of one measure;  (2) obtain lower bounds on the complexity of one measure while fixing the other measure; (3) obtain  tradeoff relationships that characterizes the dependence of one measure on the other; and (4) obtain efficient distributed algorithms that operate on large-scale graphs. This research will yield efficient and scalable distributed algorithms with provable performance guarantees. This project could potentially impact algorithm design in real-world distributed networks and distributed computations over large-scale data. The project trains students and postdoctoral fellows to tackle research problems in distributed algorithms."
"1855706","SHF: Small: Turning Visual Noise into Hardware Efficiency: Viewer-Aware Energy-Quality Adaptive Mobile Video Storage","CCF","SOFTWARE & HARDWARE FOUNDATION","11/01/2018","10/22/2018","Na Gong","AL","University of South Alabama","Standard Grant","Yuanyuan Yang","09/30/2021","$300,000.00","","nagong@southalabama.edu","307 University Boulevard","Mobile","AL","366880002","2514606333","CSE","7798","7923, 7941, 9102, 9150","$0.00","Mobile devices, such as smart phones, are being increasingly utilized for watching videos, since they can be conveniently used for this purpose anywhere anytime, such as commuting on a subway or train, sitting in a waiting room, or lounging at home. Due to the large data size and intensive computation, video processing requires frequent memory access that consumes a large amount of power, limiting battery life and frustrating mobile users. On one hand, memory designers are focusing on hardware-level power-optimization techniques without considering how hardware performance influences viewers' actual experience. On the other hand, the human visual system is limited in its ability to detect subtle degradations in image quality; for example, under conditions of high ambient illumination, such as outdoors in direct sunlight, the veiling luminance (i.e., glare) on the screen of a mobile device can effectively mask imperfections in the image, so that under these circumstances a video can be rendered in lower than full quality without the viewer being able to detect any difference. This isolation between hardware design and viewer experience significantly increases hardware implementation overhead due to overly pessimistic design margins. This project integrates viewer-awareness and hardware adaptation to achieve power optimization without degrading video quality, as perceived by users. The results of this project will impact both basic research on hardware design and human vision, and provide critical viewer awareness data from human subjects, which can be used to engineer better video rendering for increased battery life on mobile devices. The project will directly involve undergraduate and graduate students, including females and Native Americans, in interdisciplinary research. <br/><br/>Developing a viewer-aware mobile video-memory solution has proven to be a very challenging problem due to (i) complex existing viewer-experience models; (ii) memory modules without runtime adaptation; and (iii) the difficulty of viewer-experience analysis for hardware designers. This project addresses the problem by (i) focusing on the most influential viewing-context factor impacting viewer experience - ambient luminance; (ii) proposing novel methodologies for adaptive hardware design; and (iii) integrating a unique combination of expertise from the investigators, ranging from psychology to Integrated Circuit design and embedded systems. Specifically, this project will (i) experimentally and mathematically connect viewer experience, ambient illuminance, and memory performance; (ii) develop energy-quality adaptive hardware that can adjust memory usage based on ambient luminance so as to reduce power usage without impacting viewer experience; and (iii) design a mobile video system to fully evaluate the effectiveness of the developed methodologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1629657","XPS: FULL: Collaborative Research: PARAGRAPH: Parallel, Scalable Graph Analytics","CCF","Exploiting Parallel&Scalabilty","09/01/2016","08/12/2016","John Owens","CA","University of California-Davis","Standard Grant","Aidong Zhang","08/31/2019","$328,123.00","","jowens@ece.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8283","","$0.00","Many real world problems can be effectively modeled as complex relationship networks or graphs where nodes represent entities of interest and edges mimic the interactions or relationships among them. The number of such problems and the diversity of domains from which they arise is growing. However developing high-performance applications to extract useful information from such datasets is very challenging.  Graphical processing units are very attractive for such applications because they offer higher computational performance and energy efficiency than standard multi-core processors. However, the development of high-performance applications for them is currently much more challenging than  parallel program development for standard  multi-core processors. Effective  application development to use graphical processing units generally requires that developers possess considerable expertise on their architectural characteristics and use specialized programming models and performance optimization techniques. Thus, simultaneously achieving high performance and high user productivity for data analytics applications for such devices is a daunting challenge.<br/><br/>This project proposes a scalable high-level software framework to enable the productive development of high-performance applications for graphical processing units. It features two distinct abstractions to address the performance and productivity challenges in developing graph/data analytics applications: 1) a frontier-centric abstraction that is based on a common iterative characteristic of many of these applications, with a dynamically moving active frontier of vertices (or edges) where computation is centered, and 2) an abstraction based on sparse linear algebra primitives, exploiting the dual relationship between sparse matrices and graphs. A benchmark suite of graph analytics applications will be developed and evaluated using both abstractions, enabling insights into the effectiveness of these alternate high-level abstractions for a range of analytics applications. The benchmark suite and the software framework will be publicly released."
"1717530","CIF: Small: Foundations of Belief Sharing in Human-Machine Systems","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/28/2017","Lav Varshney","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","06/30/2020","$405,455.00","","Varshney@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7936","$0.00","This work aims to develop mathematical laws and foundational principles for belief sharing in systems with human and machine intelligence working together to make robust decisions. Prior work in statistical signal processing and in psychology has only considered technological limitations or human limitations independently, but jointly considering informational limitations of both humans and machines is critical in engineering future sociotechnical systems, especially when people are overwhelmed by too much information. A theory for fundamental limits and optimal designs for such systems is lacking.<br/><br/>Rather than systems with agents sharing either raw data or local decisions, we develop intermediate designs based on sharing beliefs. Belief sharing increases modularity among networked units compared to central analysis of raw data, yet also strengthens coordination compared to decentralized local decisions. We build on our prior bounded rationality models of people and stochastic models of artificial intelligence to determine optimal mixed human-machine architectures. First, we find fundamental information-theoretic limits of belief-sharing under Bayes risk and discrete choice models, new kinds of CEO problems. As a key substep, this involves determining fundamental Ziv-Zakai bounds on Bayesian estimation under non-quadratic criteria. We then use quantization and decision theory to develop optimal architectures that have cognitively- and algorithmically-limited agents, including optimal categorization of beliefs and judgment pooling rules taking human behavior into account. Finally, we consider the language needed to communicate beliefs in complicated network structures to achieve collective intelligence, studying Nash equilibria balancing focal and shared concerns."
"1629397","XPS: FULL: A Cross-Layer Approach Toward Low-Latency Data-Parallel Applications in Rack-Scale Computing","CCF","Exploiting Parallel&Scalabilty","09/01/2016","09/02/2016","Mosharaf Chowdhury","MI","University of Michigan Ann Arbor","Standard Grant","M. Mimi McClure","08/31/2020","$825,000.00","Barzan Mozafari","mosharaf@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8283","","$0.00","Although many modern applications, e.g., exploratory analytics and scientific visualization, come with stringent latency requirements, today's in-memory and scale-out solutions often provide only best-effort services. A root cause of unpredictability lies in the traditional design principle of minimizing I/O operations. With the advent of faster storage and networks in rack-scale computing, however, I/O may no longer be scarce anymore. This project revisits the tradeoffs and design principles of scale-out, low-latency applications in this emerging context. Bounded response times will reduce over-provisioning and foster new applications (e.g., business intelligence, robotics, and intensive care units) that require consistent performance. Project findings will be integrated into undergraduate and graduate curricula, and software artifacts will be open-sourced for the wider community across academia and industry.  <br/><br/>This project aims to leverage the influx of new hardware capabilities to enable applications based on bounded response times as their primary design criteria. Specifically, the project leverages approximation, speculation, and scheduling to mask variabilities in latency-sensitive applications. The key technical challenge in realizing this vision lie in making a set of tradeoffs different from the norm: (i) rather than striving for less I/O, this project trades I/O off for better memory locality and aggressively speculate to reduce response times; (ii) when needed, it resorts to approximation techniques for bounded response times; and finally, (iii) it develops new approximation- and speculation-aware schedulers to increase resource efficiency. The project also investigates theoretical and empirical boundaries of approximate and speculative processing as well as new spatiotemporal scheduling techniques in rack-scale computing."
"1704204","CIF:Medium:Convex Optimization for Blind Inverse Problems","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","07/01/2017","09/13/2018","Gongguo Tang","CO","Colorado School of Mines","Continuing grant","Phillip Regalia","06/30/2021","$554,801.00","Michael Wakin","gtang@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","2878, 7797","7797, 7924, 7936","$0.00","One of the fundamental tasks in processing sensor and imaging data<br/>is to solve an inverse problem, determining the nature of some<br/>fundamental structure that produced that data. Such problems are<br/>often underdetermined, meaning that the number of unknowns exceeds<br/>the number of observations, and these problems are even more<br/>complicated in the blind setting, where the fundamental structure<br/>may undergo some unknown transformation en route to the sensor.<br/>This project considers a number of such blind inverse problems,<br/>including non-stationary deconvolution, where an unknown point<br/>spread function changes over time; multi-band signal<br/>identification, where line spectrum estimation is extended to<br/>signals with multiple narrow frequency bands; super-resolution<br/>radar imaging, where extended and accelerating targets may cause<br/>unwanted spreading in the delay-Doppler space; and simultaneous<br/>blind deconvolution and phase retrieval. Conventionally, all of<br/>these problems have been studied separately. This project<br/>investigates all of the problems jointly under a unifying<br/>optimization and analysis framework.<br/><br/>By modeling unknown transformation operators using subspaces, the<br/>investigators transform each non-convex inverse problem into a<br/>linear inverse problem of recovering a structured signal. This<br/>signal is a parsimonious mixture of lifted atoms generated by the<br/>original atomic set, allowing the investigators to enforce its<br/>simplicity using a new atomic norm. The investigators develop novel<br/>analysis techniques to demonstrate the optimality of this framework<br/>by deriving sampling complexities that achieve the<br/>information-theoretical limits, calculating mean-squared denoising<br/>errors that match the minimax rates, and developing parameter<br/>estimation bounds that approach the Cramer-Rao bounds. The project<br/>builds on the investigators' combined expertise in signal<br/>processing, convex optimization in continuously parameterized<br/>inverse problems, and geometric modeling."
"1815428","CIF: Small: Fundamental limits of interactive communications","CCF","COMM & INFORMATION FOUNDATIONS","01/01/2019","06/22/2018","Natasha Devroye","IL","University of Illinois at Chicago","Standard Grant","Phillip Regalia","12/31/2021","$499,996.00","","devroye@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","7797","7923, 7935, 9102","$0.00","The proposed research aims to understand how interaction affects the fundamental performance of communication systems. ""Interaction"" denotes when current decisions such as what to input to the channel may be altered based on past decisions, for example through feedback or access to channel outputs.  To assess the impact of interaction, this project uses the technology-independent formalism of information theory, which quantifies the fundamental limits of data compression and transmission.  This provides upper bounds and benchmarks for real-world performance of interactive communication systems, which will be more realistic than current bounds which consider idealized notions of feedback. This is expected to refine the design of communication systems in the noisy feedback and two-way settings of relevance in many existing and future communication paradigms.<br/><br/>The role of interaction in one-way channels with noisy feedback and two-way channels will be looked at using three information theoretic metrics: 1) the somewhat crude capacity viewpoint; 2) a more refined error exponents angle; and finally 3) the even more refined second order coding / finite-blocklength context.  One-way channels with noisy feedback highlight the potential utility or futility of adapting current inputs to past noisy channel outputs, extending the understanding from the current state of the art which for the most part considers noiseless / perfect output feedback.  Two-way channels where two users wish to exchange messages over a common channel and do so interactively take this understanding one step further by capturing not only the impact of interaction / feedback, but also the tension  between using a common channel to send data in one direction versus feedback in the other. The outcomes of the proposed research will provide a comprehensive understanding of interaction in two more realistic and relatively unexplored interactive settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1643271","SHF: Medium:Collaborative Research: Architectural and System Support for Building  Versatile Memory Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","01/25/2016","06/30/2018","Zhao Zhang","IL","University of Illinois at Chicago","Continuing grant","Yuanyuan Yang","06/30/2019","$375,000.00","","zzhang@iastate.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","7798","7924, 7941, 9150","$0.00","Computer memory design is moving into a new era with emerging NVM (non-volatile memory) technologies for increasingly data-intensive applications. This project will investigate a novel memory architecture called versatile memory system that hosts heterogeneous memory technologies to provide a powerful in-memory computing engine, addressing the increasing demands for memory performance, energy efficiency, and reliability from data-intensive applications. It is based on a holistic approach, from low-layer hardware design to high-layer OS management, to address the challenges of complexity and efficiency arising from the integration of heterogeneous NVM technologies. It allows the memory system to be self-adaptive to meet varying application demands on performance, energy efficiency, and reliability. The project will study the framework, critical hardware support, and feasible and meaningful functionalities of the versatile memory system, aiming at improving the performance, energy efficiency, reliability, and manageability of computing systems from mobile to server platforms. It is expected that the outcome of the project will have broader impact on the design of modern computer memory systems both in academia and industry."
"1513899","SHF:Medium:Collaborative Research: Architectural and System Support for Building Versatile Memory Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","06/13/2018","Zhichun Zhu","IL","University of Illinois at Chicago","Continuing grant","Yuanyuan Yang","06/30/2019","$225,000.00","","zzhu@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","7798","7924, 7941, 9150","$0.00","Computer memory design is moving into a new era with emerging NVM (non-volatile memory) technologies for increasingly data-intensive applications. This project will investigate a novel memory architecture called versatile memory system that hosts heterogeneous memory technologies to provide a powerful in-memory computing engine, addressing the increasing demands for memory performance, energy efficiency, and reliability from data-intensive applications. It is based on a holistic approach, from low-layer hardware design to high-layer OS management, to address the challenges of complexity and efficiency arising from the integration of heterogeneous NVM technologies. It allows the memory system to be self-adaptive to meet varying application demands on performance, energy efficiency, and reliability. The project will study the framework, critical hardware support, and feasible and meaningful functionalities of the versatile memory system, aiming at improving the performance, energy efficiency, reliability, and manageability of computing systems from mobile to server platforms. It is expected that the outcome of the project will have broader impact on the design of modern computer memory systems both in academia and industry."
"1848966","EAGER: New Algorithms for Feature-Efficient Learning","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","09/11/2018","Lev Reyzin","IL","University of Illinois at Chicago","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2020","$100,000.00","","lreyzin@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","7796","7916, 7926","$0.00","The main goal of this exploratory research project is to invent theoretically sound and practical machine-learning algorithms designed to perform well under various limitations involving access to data features during deployment.  This tackles a major difficulty encountered in many machine-learning applications: in running an algorithm, accessing features of the data can be time consuming or costly.  For example, in medical diagnosis, features of patients may correspond to results of medical tests, which can take significant time to run, carry enormous cost, and even impose heath risks.  Current machine-learning techniques are ill-equipped to tackle such impediments.  This project involves approaches that incorporate feature-efficient optimization into the training phase of machine-learning algorithms and also the creation of new frameworks for reducing both error rates and costs associated with acquiring features. Successful developments in feature-efficient algorithms create an important advance for application areas ranging from medical diagnosis to query-answering on the World Wide Web.  Additional facets of this project include incorporating its research findings into graduate courses and broadening participation in research.<br/><br/>This project investigates new models for jointly optimizing feature costs, prediction time, and classification error rates, to create feature-efficient predictors.  Techniques for this exploratory project include solving original optimization problems, creating novel machine-learning reductions, and analyzing the problem via statistical query oracles.  Another aspect of this work is to tackle a budgeted learning formalization by moving the feature-cost optimization into the training phase of budgeted boosting classifiers and support vector machines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1409106","CIF: Medium: Collaborative Research: Content Delivery over Heterogeneous Networks: Fundamental Limits and Distributed Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2014","06/20/2014","Pramod Viswanath","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","05/31/2019","$600,000.00","Bruce Hajek, Rayadurgam Srikant","pramodv@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7924, 7935","$0.00","There has been a paradigm shift in the design of communication networks due to the content revolution: the focus has shifted from making a connection to delivering content.  Content Distribution Networks carry much of the data of the modern Internet, with video alone occupying nearly 80%, consuming a massive amount of resources.  The energy cost of data centers rivals that of medium sized countries. Traditional content distribution network designs have involved centralized capital-intensive infrastructure. For scalable performance, next generation content distribution networks will have to harness the ""edge-cloud,"" consisting of vast numbers of highly capable end user devices such as smart phones, tablets, and e-readers. In this project we address the issues of models, fundamental limits, and storage and delivery algorithms associated with the distributed nature of next generation content distribution networks.  The main goal of this project is to develop a comprehensive theory for characterizing the fundamental capacity and performance limits of such networks, and to design the associated content storage and distribution algorithms to achieve these limits. <br/><br/>Content distribution networks generate the vast majority of all internet traffic, and we expect our results to guide the next generation of such networks.  In particular, we explore ways to support robust and efficient distributed storage and delivery of content over heterogeneous networks. An integral and key aspect of our proposal is to closely tie the theoretical and conceptual aspects of the project with the practical and real-world engineering issues. This approach is highlighted by building testbeds at each of the universities of the investigators as well as specific to each of the main topics (storage, centralized delivery, and peer-to-peer delivery) of the proposal.<br/><br/>As multimedia content consumption becomes the centerpiece of today's on-line experience, the results of this work are expected to contribute to a seamless end-user experience, while freeing up significant backhaul resources and avoiding the saturation that would otherwise plague today's network infrastructure."
"1518703","SHF: Large: General-Purpose Approximate Computing Across the System Stack","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","07/01/2015","09/17/2018","Luis Ceze","WA","University of Washington","Continuing grant","Yuanyuan Yang","06/30/2020","$2,399,764.00","Mark Oskin, Emina Torlak, Daniel Grossman","luisceze@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","2878, 7798","7798, 7925, 7941","$0.00","Energy efficiency is a fundamental challenge facing the IT industry. Benefits go beyond reduced power demands in data centers and longer battery life in mobile devices. It is a fundamental enabler of future systems as we approach the limits of silicon device scaling. Therefore, providing a novel and holistic approach to energy efficiency in computer systems can have a transformative effect on IT and society. Many important applications---e.g., computer vision, novel user interfaces, signal processing, web search, augmented reality, and big-data analytics---can inherently tolerate some forms of inaccurate computation at various levels. With approximate computing, this fact can be exploited for fundamentally more efficient computing systems. This is a direct analog to Daniel Kahneman's model of how our brains work: they do cheap and quick reasoning (using System 1) in an approximate way, and when required, they do more expensive (and tiring) detailed thinking (using System 2). This research project will develop a analogous model for computer systems, from hardware to programming tools. <br/><br/>Taking advantage of approximate computing requires significant innovation: programming models, tools for testing and debugging, and system support with quality guarantees. This project will develop a comprehensive solution across the system stack, from programming language to hardware. To demonstrate the potentials, prototypes of compelling applications amenable to approximate computing (e.g., computer vision) will be created. The project involves work on systems, programming languages, formal methods, and architecture, matching the inter-disciplinary expertise of the PI team. In addition to research papers, the project scope also includes releasing tools, benchmarks, and general infrastructure to the academic and industrial communities. The PIs have a history of inclusion of minorities and undergraduate students in their research efforts."
"1703575","CIF:Medium:Collaborative Research: Foundations of Coding for Modern Distributed Computing","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","05/01/2017","09/13/2018","Amir Avestimehr","CA","University of Southern California","Continuing grant","Phillip Regalia","04/30/2021","$280,306.00","","avestime@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","2878, 7797","7797, 7924, 7935","$0.00","Coding and information theory provide a very rich body of knowledge from theory to concepts to constructions for creating, leveraging, and removing ?redundancy"" in ways that have revolutionized the digital era. This project brings these concepts and techniques to bear in a new field: large-scale distributed computing. The modern paradigm for large-scale distributed computing systems is driven by ""scaling out"" of computations across clusters consisting of as many as tens or hundreds of thousands of machines. As such, there is an abundance of resource redundancy that can be exploited. This project develops a foundation for ""coded computing"", a new framework that combines coding with distributed computing to overcome several fundamental challenges limiting the performance of today's large-scale distributed computing platforms. The research outcomes of the project will be integrated into education and will be disseminated broadly.<br/><br/>This project takes a principled and foundational approach to providing a unified coding framework to tackle three key challenges in large-scale distributed computing: significant delays due to straggling nodes; large communication loads between computing nodes; and massive input data-sets. In particular, three novel coding concepts are proposed for distributed computing: coding for injecting computation redundancy to mitigate straggler issues; coding to trade local computation with global communication; and coding for statistically principled data sketching. The unified role of codes in both doing fast sketching and in providing robustness to straggler node delays and communication bottlenecks is also studied."
"1716352","SHF: Small: Reclaiming Dark Silicon via 2.5D Integrated Systems with Silicon Photonic Networks","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2017","08/29/2017","Ayse Coskun","MA","Trustees of Boston University","Standard Grant","Sankar Basu","08/31/2020","$450,000.00","Milos Popovic, Ajay Joshi","acoskun@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7798","7923, 7945, 9102","$0.00","Emerging applications in the growing domains of cloud, internet-of-things, and high-performance computing require higher levels of parallelism and much larger data transfers compared to applications of the past. In tandem, power and thermal constraints limit the number of transistors that can be used simultaneously on a chip and this limit has led to the ?Dark Silicon? problem. These difficulties in harnessing the full potential of computer chips exacerbate the challenge of building efficient high-performance systems. This project proposes to use 2.5D integration technology with silicon-photonic networks-on-chip (NOCs) to build heterogeneous computing systems that provide the desired parallelism, heterogeneity, and network bandwidth to handle the demands of the next-generation applications. A major outcome of the project will be a set of optimization methods that will enable efficient and robust design and operation of 2.5D systems with silicon-photonic NOCs. The project seeks to accelerate the design of high-performance, energy-efficient systems that are able to cater to growing bandwidth and performance needs and, in this way, enable a wider spectrum of cognitive data-intensive applications. Planned educational and outreach activities include the design of tutorials and workshops focused on training future engineers on cross-layer design problems, involvement of undergraduate, under-represented minority, and women students in various aspects of the research, and interactions with research centers and industry to accelerate technology transfer.<br/><br/>The research goal of the proposal is to design novel cross-layer design automation methods for 2.5D-integrated heterogeneous systems with silicon-photonic NOCs, and to quantitatively demonstrate the benefits of these 2.5D systems with respect to energy efficiency, robustness, and performance. The proposed work bridges the gap among device, physical design, architecture, and application layers when designing systems with silicon-photonic NOCs to dramatically improve system efficiency and robustness. Specific project thrusts include designing: (1) a modeling stack to quantify cross-layer interactions (from devices to applications) in a 2.5D system and inform optimizers to enable energy-efficient silicon-photonic NOC design and management; (2) design-time methods that orchestrate architecture design, chiplet placement, NOC design/routing, laser placement, and cooling design to maximize system performance under power and temperature constraints; (3) thermally-aware design/runtime optimization techniques that are aware of silicon-photonic device properties and their sensitivity to thermal variations; and (4) cross-layer optimization methods that help navigate a complex space of design choices and runtime knobs."
"1720635","XPS: FULL: DSD: Collaborative Research: Parallelizing and Accelerating Metagenomic Applications","CCF","Exploiting Parallel&Scalabilty","07/01/2016","02/21/2017","Raj Acharya","IN","Indiana University","Standard Grant","Yuanyuan Yang","08/31/2019","$176,853.00","","acharya@cse.psu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8283","","$0.00","The importance of metagenomics arises from the fact that over 99% of<br/>the species yet to be discovered are resistant to cultivation. Unlike<br/>single genome sequencing, assembly of a metagenome is intractable and<br/>is in large part, an unsolved mystery.  Moreover, the advent of high<br/>throughput sequencing is fueling rapid generation of enormous<br/>metagenomic datasets. There is no available sequenced genome for a<br/>majority of the species. There is a need to determine the number of species in<br/>a metagenomic dataset as well as the abundance of each of these<br/>species.  The key steps (Assembly and Clustering) in the metagenomics<br/>analysis algorithms are compute-intensive, while the sheer amount of<br/>data the algorithms operate on is staggering.  The most promising way<br/>to tackle the computational challenges is to build special purpose<br/>hardware, dedicated solely to suitable algorithms.<br/><br/><br/><br/>The main objective of this project is to develop a range of flexible,<br/>affordable, parallel, fast hardware-accelerated bioinformatics<br/>solutions, using GPGPU, FPGA, and ASIC, for metagenomic analytics to provide an alternative to<br/>expensive computer clusters. Specifically,  hardware solutions for<br/>metagenomic clustering and assembly will be developed. Several<br/>acceleration methodologies, including parallel software mapping and<br/>special hardware design, are proposed to explore the parallelism<br/>inside the applications and to improve the data access bandwidth in<br/>the hardware running bioinformatics applications. The ideas proposed<br/>in this work will be evaluated in a multi-pronged manner using a<br/>combination of simulation, emulation and prototyping efforts. Further,<br/>the PIs will use a combination of commercial tools, collaborator<br/>resources and  existing internal tools. The research will be <br/>conducted in collaboration with  industrial partners. Through close<br/>collaboration with several industry partners,  direct transfer of many<br/>ideas to industry is enabled. The outcome of this research will,<br/>therefore, have a direct impact on future bioinformatics application<br/>solutions. This project will involve graduate and undergraduate<br/>students in all aspects of the research. The PIs will actively<br/>integrate the research results from this project into the graduate and<br/>undergraduate curricula, and develop new interdisciplinary courses on<br/>bioinformatics and computer architecture to train the next generation<br/>work-force. Finally, the tools and techniques developed in this<br/>research will be made available through   web-sites for use by other<br/>educators, researchers, and industry practitioners."
"1617617","AF: Small: Incremental and Asynchronous Projective Splitting Methods for Mathematical Programming","CCF","CI REUSE, ALGORITHMIC FOUNDATIONS","09/01/2016","08/01/2016","Jonathan Eckstein","NJ","Rutgers University Newark","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2019","$457,072.00","","jeckstei@rci.rutgers.edu","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","CSE","6892, 7796","7433, 7923, 7933, 7934","$0.00","A key to solving large computational and mathematical problems, such as analyzing large datasets or planning for the operation of an electrical power grid or any other complicated systems with an uncertain future demands and supplies, is decomposing into smaller solvable subproblems or subsystems, then coordinating and integrating their results, and decomposing again into adjusted subproblems.  Properly designed decomposition methods repeat a decomposition - coordination cycle that converge to the solution of the entire original, non-decomposed problem.  The PI is working with Sandia National Laboratories and has particular interest in problems that arise in operating electrical power grids with high penetration of renewable generation sources, like solar and wind, where weather has unplanned affects the supply.   <br/><br/>This project studies a new way to perform decomposition, called ""incremental projective operator splitting"" (IPOS) or ""block-iterative splitting.""  It is related to a popular decomposition method called the alternating direction method of multipliers (ADMM) but is far more flexible.  While essentially all prior decomposition methods follow a rigid cycle of decomposition and coordination steps, with every decomposition step encompassing all the subsystems, the new method has much greater flexibility:  only a subset of subsystems need to be considered between coordination steps, and decomposition and coordination calculations can overlap asynchronously.  This flexibility should allow more efficient use of parallel computers by eliminating rigid synchronization points.  This property is important because most future growth in computer performance is anticipated to result from larger numbers of parallel processing units, and only parallel computers will be able to manipulate the large datasets and decision problems we hope to analyze. <br/><br/>Because the new IPOS methods are so flexible, there are numerous ways in which they could be used on the same class of problems.  The main goal of this project is to develop and experimentally evaluate strategies for applying IPOS on parallel computers.  It will focus on two common problem classes, large-scale data analysis and planning under uncertainty, using real-world input data to the maximum practical extent.  Other research topics include sharpening the mathematical theory of IPOS, and extending this theory to cover a broader range of problems, and development and release of software based on this new theory."
"1718335","SHF: Small: Vertically Integrated Persistence","CCF","SOFTWARE & HARDWARE FOUNDATION","09/15/2017","08/30/2017","Raju Rangaswami","FL","Florida International University","Standard Grant","Almadena Y. Chtchelkanova","08/31/2020","$458,000.00","","raju@cs.fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7798","7923, 7942, 9251","$0.00","A new generation of data storage devices based on persistent memory technology are becoming available. Persistent memory devices address many shortcomings of previous generation technology including performance, power consumption, and device size. They have the potential to empower businesses and consumers with new and better capabilities. When these devices made available, either as part of a packaged digital service or within self-contained devices, they have the potential to vastly benefit the consumers of technology. This project builds the core software capabilities necessary for ensuring that the full potential of this new technology is realized and delivered to the end consumer. In particular, this project creates enabling technologies for operating data center servers, personal desktop and laptop computers, as well as mobile tablet computers and smart-phones more efficiently and at lower cost.<br/> <br/>Existing software support for storage are built around a set of core assumptions and goals that are rendered obsolete by persistent memory based storage devices. In this project, the PI will develop a new set of guiding principles for software that supports such devices from the ground up. In particular, this purpose-built software takes a vertically integrated approach using a minimalistic cross-layer design across components of the software stack. The new stack addresses the cornerstones of systems infrastructure including all levels of software and hardware, including application, compiler, the operating system, and the device access hardware infrastructure including the device itself. Educational activities include involvement of undergraduate students and incorporation of project research into courses taught by the PI. Outreach includes recruitment of under-represented minority students for participation in the project."
"1628384","XPS: FULL: Emerging Nonvolatile Memory for Analog-iterative Numerical Methods","CCF","Computer Systems Research (CSR, Exploiting Parallel&Scalabilty","09/01/2016","08/13/2017","Jing (Jane) Li","WI","University of Wisconsin-Madison","Standard Grant","M. Mimi McClure","08/31/2020","$833,000.00","Stephen Wright, Mikko Lipasti","jli587@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7354, 8283","9251","$0.00","A new type of computer memory - crosspoint resistive memory - has<br/>emerged as a likely candidate to replace current memory technology in<br/>future computing systems. This memory allows for potential computer<br/>designs with high memory capacity and with memory incorporated<br/>directly into processing units.  Novel thinking about computational<br/>methods is required to exploit the potential of these novel<br/>systems. This project will explore new, fundamental methods in the<br/>field of numerical optimization that are suited to implementation on<br/>computer systems that incorporate crosspoint resistive memory. The<br/>field of optimization is chosen as a testbed because of its importance<br/>to a wide range of scientific disciplines.<br/><br/>Crosspoint memory has unprecedented advantages in capacity and access<br/>latency. Substantial innovation is required to fully exploit the<br/>potential benefits of integrating memory into processing units;<br/>current algorithms are unsuitable, because they are constrained by the<br/>von Neumann bottleneck. The PIs will design the Gigascale Analog Iterative<br/>Network Solver (GAINS), a system architecture to enable efficient<br/>in-situ data processing. GAINS alters the application-, architecture-,<br/>and logic/circuit-level abstractions that enable designers and<br/>developers at each layer to work independently. (i) It promotes<br/>matrices to a first-class data type. (ii) It integrates computataion<br/>and memory, avoiding pitfalls of conventional memory<br/>hierarchies. (iii) It exploits multi-valued representations in storage<br/>and computation.  (iv) It replaces binary logic circuits with<br/>multi-valued analog circuits, reducing area overhead and power<br/>consumption. The PIs will investigate the effects of this changed paradigm<br/>on the design of algorithms in numerical optimization and machine<br/>learning."
"1812746","AF: Small: Collaborative Research: Certification for Semi-Algebraic Sets with Applications","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","08/17/2018","Jonathan Hauenstein","IN","University of Notre Dame","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$250,000.00","","hauenstein@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7796","7923, 7933","$0.00","Traditionally, computers can very quickly calculate with numbers that have a limited number of digits to get answers that are accurate to a certain precision, or they can much more slowly manipulate formulas and symbols to get exact answers.  Recently, a new class of algorithms, called numerical path following algorithms, have been successfully applied to approximate solutions for problems in algebraic geometry, combinatorics, and optimization that were once thought to be purely symbolic in nature. The results of such numerical computations are typically not certified, as they are generated using heuristic methods that relax non-continuous properties of the input into continuous ones. The aim of this research project is to give certification techniques for these non-continuous problems and demonstrate that certificates can be computed with not too much extra work given numerical data. An essential part and motivation for this research is a variety of application areas in other fields such as efficiently handling singularities in reliable geometric computation, certification of optima for semidefinite programs, proving existence of multistability in chemical reaction networks, and exceptional motion in mechanism design. By investigating the practical limits of certifiable methods, this project aims to help specialists decide when they can apply certification methods for their purposes. Moreover, by developing new methods that reduce the gap between certified and non-certified versions, researchers will have the guarantee of certified methods in more of their computations. Integration of education and research is essential to the success of this proposal with this project supporting<br/>the inclusion of graduate and undergraduate students in the research team.<br/><br/>The focus of this research is to certify and enhance the handling of polynomial equations and inequalities with exact coefficients which have degenerate solutions known only approximately. The difficulty is that, in many cases, the roots of the exact system behave discontinuously under perturbations of the coefficients. Hence, in these non-continuous cases, traditional numerical certification methods, such as interval arithmetic or alpha-theory, cannot work alone. The study of these degenerate cases is the main topic of this project with the fundamental idea to combine numerical certification techniques with symbolic computations. This project will use insights gained from numerical data to drastically improve the complexity of the computation of exact, symbolic objects, and in turn, use insights from symbolic computation to turn an ill-posed problem into a well-posed one. The hybrid symbolic-numeric approach, using early termination upon success, aims to reduce the complexity in comparison with purely symbolic methods. New techniques for regularizing/deflating singular roots will simplify computations related to singularities and improve applications including the visualization of singular curves lying on a real surface. Additionally, this project will improve the complexity of certification routines by exploiting symmetry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817212","AF: Small: A New Approach to Analysis and Design of Algorithms for Stochastic Control and Optimization","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","08/23/2018","Rahul Jain","CA","University of Southern California","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$399,999.00","","rahul.jain@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7796","7923, 7926, 7933","$0.00","Randomized algorithms for stochastic optimization and control underpin many developing technologies such as Artificial Intelligence (AI), Autonomous Robotics, and Big Data Analytics. Their development is hampered by a lack of suitable mathematical tools. In many cases, current mathematical techniques such as those based on Stochastic Lyapunov theory are rather difficult to use, thus necessitating invention of customized techniques for algorithm design for each problem and its analysis. This project will develop a new class of mathematical techniques, called probabilistic contraction analysis, that are easier to use, and more broadly applicable. The project's aim is not just analysis of existing algorithms, but development of analysis tools with an eye on design. The project outcomes can accelerate development of new algorithms for stochastic control and optimization problems that arise in many important application fields such as AI, Autonomy, Big Data Analytics, etc. The project will train under-represented and/or female PhD students and postdocs, as well as high school students and teachers.<br/><br/>Given a randomized algorithm for stochastic optimization and control, this project views each iteration as applying a random operator, and develops new ""probabilistic contraction"" analysis techniques, created by the investigator, that use stochastic dominance arguments to show convergence to probabilistic fixed points. Specifically, the investigator will develop empirically-inspired algorithms for optimal control of continuous state and action space Markov decision processes, and unconstrained and constrained stochastic optimization problems. The techniques to be developed may be useful for a broader class of stochastic iterative algorithms, and lead to development of a probabilistic fixed point theory of random operators on Banach spaces.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813340","AF: Small: Collaborative Research: Certification for Semi-Algebraic Sets with Applications","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","08/17/2018","Agnes Szanto","NC","North Carolina State University","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$249,994.00","","aszanto@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7796","7923, 7933, 9102","$0.00","Traditionally, computers can very quickly calculate with numbers that have a limited number of digits to get answers that are accurate to a certain precision, or they can much more slowly manipulate formulas and symbols to get exact answers.  Recently, a new class of algorithms, called numerical path following algorithms, have been successfully applied to approximate solutions for problems in algebraic geometry, combinatorics, and optimization that were once thought to be purely symbolic in nature. The results of such numerical computations are typically not certified, as they are generated using heuristic methods that relax non-continuous properties of the input into continuous ones. The aim of this research project is to give certification techniques for these non-continuous problems and demonstrate that certificates can be computed with not too much extra work given numerical data. An essential part and motivation for this research is a variety of application areas in other fields such as efficiently handling singularities in reliable geometric computation, certification of optima for semidefinite programs, proving existence of multistability in chemical reaction networks, and exceptional motion in mechanism design. By investigating the practical limits of certifiable methods, this project aims to help specialists decide when they can apply certification methods for their purposes. Moreover, by developing new methods that reduce the gap between certified and non-certified versions, researchers will have the guarantee of certified methods in more of their computations. Integration of education and research is essential to the success of this proposal with this project supporting<br/>the inclusion of graduate and undergraduate students in the research team.<br/><br/>The focus of this research is to certify and enhance the handling of polynomial equations and inequalities with exact coefficients which have degenerate solutions known only approximately. The difficulty is that, in many cases, the roots of the exact system behave discontinuously under perturbations of the coefficients. Hence, in these non-continuous cases, traditional numerical certification methods, such as interval arithmetic or alpha-theory, cannot work alone. The study of these degenerate cases is the main topic of this project with the fundamental idea to combine numerical certification techniques with symbolic computations. This project will use insights gained from numerical data to drastically improve the complexity of the computation of exact, symbolic objects, and in turn, use insights from symbolic computation to turn an ill-posed problem into a well-posed one. The hybrid symbolic-numeric approach, using early termination upon success, aims to reduce the complexity in comparison with purely symbolic methods. New techniques for regularizing/deflating singular roots will simplify computations related to singularities and improve applications including the visualization of singular curves lying on a real surface. Additionally, this project will improve the complexity of certification routines by exploiting symmetry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816352","SHF: Small: Dynamic Analysis on Code Fragments","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","08/23/2018","Wei Le","IA","Iowa State University","Standard Grant","Sol J. Greenspan","09/30/2021","$485,993.00","","weile@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7798","7923, 7944, 9102","$0.00","In many software engineering environments, software developers would like to understand, test, debug and verify a relatively small fragment of code instead of the entire program. However, currently available program analysis and testing tools only work on programs that are whole, in the sense that they can be compiled and executed as a whole program. When the goal is to target only a small code fragment, it is excessively expensive and time-consuming to test and analyze the targeted code in the context of entire programs, which requires configuring a whole system, setting up the execution environment, and finding suites of test inputs that supply all the needed data values to execute the whole program.  Software development processes would be much more efficient and effective if a program property for the code fragment of interest could be checked as a standalone unit of code. This research project addresses the technical challenges in converting code fragments into testable units. If software testing and analysis could be applied successfully to code fragments, rather than whole programs, the overall process of developing correct code would be streamlined. This project focuses on dynamic program analysis, which is the analysis of computer software that is performed by executing programs in a run-time environment (as opposed to static analysis, which is analysis of the source code). The project will generate algorithms, tools, and data that can advance the state of the art of program analysis tools and software engineering practice. The results will be disseminated through conferences, classrooms, open source projects, industrial collaborations and STEM volunteer opportunities.<br/><br/>For dynamic program analysis to be effective, the target program must be executed with sufficient test inputs to produce interesting (e.g., incorrect or anomalous) behaviors. Dynamic analysis is made difficult by frequently huge numbers of program paths, and the large number of inputs that must be tested. Often, the tests are ineffective because they produce an overwhelming number of false positives and often fail on parts of the program that are not even relevant to the purpose of understanding, debugging or verifying the targeted code.  The project will develop an approach to take code fragments, selected or constructed from the original program, and generate compilable and executable units, using new syntactic patching techniques to enable dynamic analysis on code fragments. It will also develop a techniques to search for and select meaningful code fragments on which to operate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823398","FoMR: Collaborative Research: Dependent ILP: Dynamic Hoisting and Eager Scheduling of Dependent Instructions","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/22/2018","Soner Onder","MI","Michigan Technological University","Standard Grant","Yuanyuan Yang","09/30/2021","$214,868.00","","soner@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","2878","021Z, 2878, 7798, 7941","$0.00","Instruction-level parallelism (ILP) in computing allows different machine-level instructions within an application to execute in parallel within a micro-processor. Exploitation of ILP has provided significant performance benefits in computing, but there has been little improvement in ILP in recent years. This project proposes a new approach called ""eager execution"" that could significantly increase ILP. The success of many applications depends on how efficiently they can be executed. The proposed eager execution technique will benefit applications that span those running on mobile devices to large data applications running on the ever-growing number of data centers. Enabling better systems at all scales will further enable the ubiquitous computing that continues to pervade lives.<br/><br/>The project's approach includes the following advantages: (1) immediately-dependent consumer instructions can be more quickly delivered to functional units for execution; (2) the execution of instructions whose source register values have not changed since its last execution can be detected and redundant computation can be avoided; (3) the dependency between a producer/consumer pair of instructions can sometimes be collapsed so they can be simultaneously dispatched for execution; (4) consumer instructions from multiple paths may be speculatively executed and their results can be naturally retained in the paradigm to avoid re-execution after a branch misprediction; and (5) critical instructions can be eagerly executed to improve performance, which include loads to prefetch cache lines and pre-computation of branch results to avoid branch misprediction delays.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823403","FoMR: Secure, Light-Weight Speculative Engines for Coordinated and Cohesive Speculation in Future Memory Systems","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/22/2018","Paul Gratz","TX","Texas A&M Engineering Experiment Station","Standard Grant","Yuanyuan Yang","09/30/2021","$225,000.00","","pgratz@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","2878","021Z, 2878, 7798, 7941","$0.00","The scaling of computer systems through the next decade poses a grand challenge due to the slowing down of the decades-long trend of packing more processors into a given area at the same cost.  In this new era, the benefits of improved manufacturing processes will largely end.  Thus, processor performance must be won through processor design advancement.  The goal of this research is to develop secure techniques to improve processors performance by accurately speculating on future memory references and data values. <br/><br/>At a high level the research leverages light-weight speculation engines, speculating on program path and data values without the overheads of a full core, predicting reference patterns and speculating on read/write sets to alleviate memory latency effects on instruction-level parallelism (ILP) extraction.  This project addresses a critical need for new architectural techniques which securely improve both power efficiency and ILP.  Furthermore, this project addresses the need for computer engineers at all levels with an understanding of efficient, secure processor design, as the computer industry struggles to attract talent with the skills to meet these challenges.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823417","FoMR: Collaborative Research: Dependent ILP: Dynamic Hoisting and Eager Scheduling of Dependent Instructions","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/22/2018","David Whalley","FL","Florida State University","Standard Grant","Yuanyuan Yang","09/30/2021","$215,000.00","","whalley@cs.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","2878","021Z, 2878, 7798, 7941","$0.00","Instruction-level parallelism (ILP) in computing allows different machine-level instructions within an application to execute in parallel within a micro-processor. Exploitation of ILP has provided significant performance benefits in computing, but there has been little improvement in ILP in recent years. This project proposes a new approach called ""eager execution"" that could significantly increase ILP. The success of many applications depends on how efficiently they can be executed. The proposed eager execution technique will benefit applications that span those running on mobile devices to large data applications running on the ever-growing number of data centers. Enabling better systems at all scales will further enable the ubiquitous computing that continues to pervade lives.<br/><br/>The project's approach includes the following advantages: (1) immediately-dependent consumer instructions can be more quickly delivered to functional units for execution; (2) the execution of instructions whose source register values have not changed since its last execution can be detected and redundant computation can be avoided; (3) the dependency between a producer/consumer pair of instructions can sometimes be collapsed so they can be simultaneously dispatched for execution; (4) consumer instructions from multiple paths may be speculatively executed and their results can be naturally retained in the paradigm to avoid re-execution after a branch misprediction; and (5) critical instructions can be eagerly executed to improve performance, which include loads to prefetch cache lines and pre-computation of branch results to avoid branch misprediction delays.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1527464","SHF: Small: Collaborative Research: Multi-level Non-volatile FPGA Synthesis to Empower Efficient Self-adaptive System Implementations","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","05/15/2017","Chengmo Yang","DE","University of Delaware","Standard Grant","Yuanyuan Yang","07/31/2019","$266,000.00","","chengmo@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","7798","7923, 7941, 9150, 9251","$0.00","Self-adaptivity is a key requirement for many electronic devices to consistently interact with the dynamic, uncertain, and noisy physical environment. While Field Programmable Gate Arrays (FPGAs), being reconfigurable, are a natural platform for implementing such devices, it is becoming more and more difficult for traditional FPGAs to keep up with the ever-increasing scale and complexity of self-adaptive applications due to the limited scalability, high leakage power, and severe process variations of CMOS technologies. A set of prior research projects demonstrated that it is technically feasible to construct FPGAs based on non-volatile memories (NVMs). These NV-FPGAs offer attractive features such as better scalability, superior energy efficiency, near-zero power-on delay, anti-radiation, as well as the ability to store more than one bit per cell. However, NV-FPGAs also display a complex design space involving information density, read and write speeds, data retention time, and device endurance. When used for self-adaptive systems, the distinctive NVM characteristics may influence reconfiguration speed, clock frequency, circuit functionality, memory performance, and/or device lifetime.<br/><br/>This project addresses this technology gap as it prepares NV-FPGAs for more demanding self-adaptive systems. This project aims to fine-tune various procedures on the FPGA synthesis flow based on NVM characteristics, so as to exploit their advantages and mitigate their shortcomings. First, considering the needs of self-adaptive applications, this project fine-tunes various steps on the FPGA synthesis flow. Novel techniques are proposed to optimize task scheduling, data allocation, logic mapping, placement, and routing to improve reconfiguration speed, energy efficiency, reliability, and endurance of NVM FPGAs. Second, this project explores the rich NVM design space and sets different optimization goals for look-up tables, flip-flops, and on-chip memories. The success of this project will lead to a long-lasting, rapid-adaptive, reliable, and energy-efficient platform better suited to the needs of a wide range of applications with self-adaptivity requirement, including healthcare, wellness, industry, and even military applications, all of which are critical for the United States to drive its new strategies of innovation and technology. It will also train a diverse type of engineers to design the future generation of embedded and cyber-physical systems with the cutting-edge technology of non-volatile memories. Algorithms and tools developed in this project will be made publicly available so that they will benefit the entire scientific community."
"1703678","CIF:Medium:Collaborative Research: Foundations of Coding for Modern Distributed Computing","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","05/01/2017","07/17/2018","Kannan Ramchandran","CA","University of California-Berkeley","Continuing grant","Phillip Regalia","04/30/2021","$556,123.00","Michael Mahoney","kannanr@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","2878, 7797","7797, 7924, 7935","$0.00","Coding and information theory provide a very rich body of knowledge from theory to concepts to constructions for creating, leveraging, and removing ?redundancy"" in ways that have revolutionized the digital era. This project brings these concepts and techniques to bear in a new field: large-scale distributed computing. The modern paradigm for large-scale distributed computing systems is driven by ""scaling out"" of computations across clusters consisting of as many as tens or hundreds of thousands of machines. As such, there is an abundance of resource redundancy that can be exploited. This project develops a foundation for ""coded computing"", a new framework that combines coding with distributed computing to overcome several fundamental challenges limiting the performance of today's large-scale distributed computing platforms. The research outcomes of the project will be integrated into education and will be disseminated broadly.<br/><br/>This project takes a principled and foundational approach to providing a unified coding framework to tackle three key challenges in large-scale distributed computing: significant delays due to straggling nodes; large communication loads between computing nodes; and massive input data-sets. In particular, three novel coding concepts are proposed for distributed computing: coding for injecting computation redundancy to mitigate straggler issues; coding to trade local computation with global communication; and coding for statistically principled data sketching. The unified role of codes in both doing fast sketching and in providing robustness to straggler node delays and communication bottlenecks is also studied."
"1527486","CIF:  Small: Collaborative Research: Communications with Energy Harvesting Nodes","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","07/24/2017","Vaneet Aggarwal","IN","Purdue University","Standard Grant","Phillip Regalia","08/31/2019","$237,304.00","","vaneet@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7923, 7936, 9251","$0.00","Wireless networks composed of nodes that can harvest energy from the environment represent the green future of communications. Utilizing the harvested ambient energy improves the environmental impact of wireless devices in the global scale, while extending the network lifetime indefinitely and making the devices truly mobile. Energy harvesting (EH) brings new dimensions to system design in the form of randomness and intermittency of available energy, as well as additional system issues such as energy storage capacity and processing complexity. Additionally, the deployment of EH wireless networks calls for novel techniques for resource and interference management, cooperation among EH nodes, as well as cross-layer optimization of the network.<br/> <br/>The research focuses on a new set of challenges brought about by wireless networks composed of EH nodes. The research directions are divided into four major thrusts. (1) The first challenge is to understand the fundamental limit of EH communications in the information theoretic setting, accounting for random and intermittent supply of available energy, finite battery capacity with storage and withdrawal efficiencies. The special cases for low-power and high-power regimes are of particular interest. (2) In order to make efficient use of the available resources, joint energy scheduling and spectral and/or spatial resource allocation in EH networks exploiting the inherent structure of the resource allocation problems becomes an important problem. (3) Different EH nodes in the network can share their harvested energy either through the presence of power line between them, or through wireless charging. The research studies previous thrusts with energy and data cooperation. (4) The research investigatesefficient distributed cross-layer solutions to maximize the total system utility subject to data and energy queuing stability constraints."
"1815971","CIF: Small: Learning on Graphs","CCF","SPECIAL PROJECTS - CCF","06/15/2018","06/11/2018","Francois Meyer","CO","University of Colorado at Boulder","Standard Grant","Phillip Regalia","05/31/2021","$426,527.00","","fmeyer@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","2878","075Z, 7923, 7935, 8089, 8091","$0.00","Not unlike road networks, significant disruptions in the pattern of connections between brain regions have profound effect on brain function. International projects, such as the Human Connectome Project and the Autism Brain Imaging Data Exchange initiative, provide open access to massive datasets that can be used to learn the association between brain connectivity and psychiatric or neurological diseases. This award responds to the current lack of analytical and computational methods to quantify changes in the organization of brain functional networks. This project proposes to design novel machine learning algorithms that will lead the way toward precision medicine in psychiatry and neurology. The award will train several graduate students to work on big-data challenges in precision medicine. The source code that will implement the algorithms will be made publicly available in the form of open source toolboxes.<br/><br/>The availability of large datasets composed of graphs creates an unprecedented need to invent tools in statistical learning to study ""graph-valued random variables"". The first goal of this project is to develop theory and algorithms to estimate the mean and variance of a set of graphs. This basic problem is at the core of several statistical inference problems about population of graph ensembles. The second aim is to develop statistical methods that can detect significant structural changes (e.g., alteration of the topology and connectivity, etc.) in a time series of graphs. The third aim is concerned with the question of learning functions defined on a graph ensemble. The proposed approach relies on the ability to equip a graph ensemble with a metric, effectively turning the learning problem into the question of extending functions defined on a metric space.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813090","CIF: Small: Collaborative Research: Optimal Provision of Backhaul and Radio Access Networks: A Cross-Network Approach","CCF","COMM & INFORMATION FOUNDATIONS","09/28/2017","05/24/2018","Mingyi Hong","MN","University of Minnesota-Twin Cities","Standard Grant","Phillip Regalia","08/31/2019","$89,714.00","","mhong@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7797","7923, 7935, 9150","$0.00","The explosive growth in the number of smart consumer devices leads to projections that within 10 years' time, wireless cellular networks need to offer 1000x throughput increase over the current 4G technology. By that time the network should be able to deliver fiber-like user experience boasting 10 Gb/s individual transmission rate for data intensive cloud-based applications. To move such a huge amount of data from the network to the users' handheld devices in real time, revolutionary network infrastructure and advanced network provision are required. Two key enablers of the envisioned future mobile networks are the ultra-dense deployment of base stations and centralized cloud-based processing. This project addresses the challenging problem of managing such densely deployed, cloud-based radio access networks.<br/><br/>The proposed research includes the introduction of a unified cross-network framework to manage a cloud-based radio access network. Important aspects of resource management in different sub-networks, including the backhaul and the cloud networks, will be considered. The project focuses on providing theoretical insights as well as designing practical algorithms for the optimal provisioning of a cloud-based radio access network. Fundamental computational issues of key cross-network resource management tasks will be investigated, revealing their intrinsic complexity. Moreover, practical schemes capable of optimally utilizing resources across networks will be developed, using cross-network optimization formulations. These algorithms will be optimized to fully exploit the computational resources offered by the cloud centers, addressing key issues such as parallel/distributed implementation, asynchronous computation, and load balancing. The goal is to determine how computational resources should be deployed through the proposed cross-network framework to dramatically improve the performance of a cloud-based radio access network.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814803","CIF: Small: Massive MIMO for Massive Machine-Type Communication","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","05/22/2018","Xiaodong Wang","NY","Columbia University","Standard Grant","Phillip Regalia","09/30/2021","$499,936.00","Alexei Ashikhmin","wangx@ee.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7797","7923, 7935","$0.00","The objective of this project is to investigate massive multiple-input multiple-output (MIMO) coding and signal processing techniques for massive machine-type communications, which is an important scenario in the emerging 5G wireless systems. Key features of the massive machine-type communication system include the massive device connectivity and the sporadic device activity pattern. The massive MIMO technique is well suited for accommodating a large number of devices and maintaining a constant level of energy efficiency that is independent of the number of devices. In addition to the potential technical impacts, the investigators plan to incorporate the  research into graduate and undergraduate curricula and to also develop summer projects to facilitate K-12 outreach.<br/><br/>This project considers the uplink of a massive machine-type communication system consisting of a base station equipped with a large number of antennas, as well as a large number of potential device nodes, out of which a fraction are active at any given time. On the transmitter side, to address the shortage of orthogonal pilot sequences when the number of devices is large, multi-base codes will be designed that contain a large number of orthogonal subsets of vectors. To address the high-complexity of device activity detection when the number of nodes is large, pilot sequences based on various codes and Grassmannian modulation constellations will be designed that can be decoded efficiently with complexity independent of the number of nodes.  On the receiver side, joint device activity detection and channel estimation will be developed by exploiting the sparsity in both device activity and the channel structures. Moreover, when the base station employs a smaller number of radio frequency chains compared with the antenna size, matrix or tensor completion techniques will be developed for channel estimation or data decoding in the presence of missing data, by exploiting the low-rank property of the received signals.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718540","SHF: Small: High-Level Programming Models for GPUs","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2017","05/31/2017","John Reppy","IL","University of Chicago","Standard Grant","Anindya Banerjee","06/30/2020","$390,388.00","","jhr@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7798","7923, 7943","$0.00","Modern Graphics-Processor Units (GPUs) are capable of performance that, just a few years ago, would have been classified as supercomputer-level. With the trend of integrating GPU cores into heterogeneous multicore processors, GPUs are becoming an important source of future performance growth in mainstream processors.  Unfortunately, GPUs are notoriously hard to program, especially for irregular parallel computations.  This project aims to address the challenges of programming GPUs by supporting higher-level programming models with advanced compilation techniques.  The intellectual merits of the proposed work are that it advances the state of the art in compilation techniques and programming models for GPUs and other accelerator architectures.  The broader impact of the project is to widen the applicability of GPUs to a wider range of computational problems and, in turn, to help make GPUs useful to a broader community of users by supporting higher-level programming models for GPUs that are easier to program.<br/><br/>The project focuses on the use of Nested Data Parallelism (NDP) and the supporting global flattening transformation, which supports irregular parallelism by compiling it down to flat data parallelism.  While NDP provides a high-level elegant programming model for many kinds of irregular parallel computations, a straightforward implementation is not competitive with hand-written GPU code. The goal of this project it to develop and evaluate a collection of techniques for compiling NDP code to GPU with the objective of making NDP competitive with hand-written CUDA and OpenCL code.  The work will be carried out in the context of a compiler for Blelloch's NESL language, which is a small first-order functional language that embodies the core concepts of NDP. NESL provides a small, but expressive, context for the proposed research.  The work is evaluated by benchmarking against handwritten CUDA and OpenCL solutions for various irregular parallel algorithms."
"1619245","SHF: Small:  Hardware-Software Co-Designed Coherence: A Complete Coherence Solution for Performance-, Energy-, and Complexity-Efficiency","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/17/2016","Sarita Adve","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yuanyuan Yang","05/31/2019","$450,000.00","","sadve@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7923, 7941","$0.00","As the benefits from transistor scaling slow down, future performance increases in computing systems will increasingly rely on architectural advances. Today processors use parallelism and increasing amounts of specialization to provide this performance growth. An efficient memory hierarchy is key to achieving the full potential of both of these techniques. The coherence protocol and memory consistency model are at the heart of the complexity-, performance-, and energy-efficiency of the memory hierarchy. Unfortunately, across a variety of systems, coherence protocols and consistency models continue to struggle to obtain an appropriate balance between complexity, performance, and energy consumption. Recently, there has been work on hybrid hardware-software co-designed protocols, exemplified by the DeNovo protocol, which takes a different approach, combining the best of pure hardware and pure software protocols. The key insight is that if software is disciplined, then it is possible to design more efficient hardware. Multiple versions of the DeNovo system have successively relaxed the software restrictions. Introducing such a new technology in classrooms to both graduate and undergraduate students will better prepare them for future memory trend and challenges. Disseminating the results of this research via publications, seminars, tutorials etc. will bring new technology awareness to the community and create more synergy among academia and industry.<br/><br/>Prior work has established the potential for DeNovo as a general-purpose system with significant advantages over the state-of-the-art. However, this work of necessity has been limited to simple workloads. Consideration of DeNovo as a viable system for widespread industrial adoption requires demonstrating an integrated system that can run complex workloads (e.g., operating systems) and legacy binaries. This project addresses the remaining research issues to achieve this goal. Although this work is driven by considerations for hybrid hardware-software coherence protocols, the intellectual contributions extend beyond those protocols as well; e.g., integrated support for efficient, coherent data accesses using a variety of disciplines ranging from completely unstructured to highly structured, statically analyzable accesses; a systematic exploration of relaxed atomics, a widely accepted difficulty in current memory consistency models; and understanding concurrent data structures and system code in a coherence neutral way."
"1715153","SHF: Small: Automated Detection and Repair of Errors in Event-Driven Applications","CCF","CI REUSE, SOFTWARE & HARDWARE FOUNDATION","07/15/2017","08/29/2017","Frank Tip","MA","Northeastern University","Standard Grant","Anindya Banerjee","07/31/2020","$443,979.00","","f.tip@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","6892, 7798","7433, 7923, 7943, 7944, 8004","$0.00","Event-driven software plays an exceedingly important role in modern society, in a variety of domains that include browser-based software for the web, server-side applications, and apps running on mobile devices. In such applications, control flow is driven by events that are triggered by users or by external systems. Writing event-driven software is notoriously error-prone, and when programmers make mistakes, various types of errors manifest themselves that are notoriously difficult to debug. The quality of event-driven web-based software remains a significant challenge, and programming errors undermine confidence in companies and institutions that offer goods or services on-line, and may put them at risk legally and financially. The intellectual merits of this project are the development of practical algorithms and tools for detecting and automatically repairing errors in event-driven software. The project's broader significance and importance follows from improvements in the quality of web-based software that are enabled by practical tools for finding and fixing errors.<br/> <br/>The technical focus of this project is on the development of well-founded solutions for accommodating event-driven control flow in static and dynamic program analysis algorithms, implementing them in practical tools, and evaluating them on real-world software. Specific research topics under study include the development of: precise interprocedural data flow analysis algorithms that are capable of preventing data flow along infeasible control-flow paths, static analysis algorithms for JavaScript code that makes use of promises, a mechanism for asynchronous computation that is now widely adopted in the JavaScript community, and techniques for automatically repairing event race errors in web applications, by restricting event handler scheduling in the browser according to a specified repair policy. Together, these techniques will facilitate the development of more reliable event-driven software, by enabling programmers to detect and repair errors more quickly.<br/><br/>The project is releasing the developed tools as open source and is building a user community around the tools by ensuring that interested researchers are able to contribute to the codebase. This aspect is of special interest to the software cluster in NSF's Office of Advanced Cyberinfrastructure, which provides co-funding for this award."
"1719074","SHF:Small: Accelerating Graph Analytics Through Coordinated Storage, Memory and Computing Advances","CCF","SOFTWARE & HARDWARE FOUNDATION","09/15/2017","09/13/2017","Murali Annavaram","CA","University of Southern California","Standard Grant","Yuanyuan Yang","08/31/2020","$400,000.00","Antonio Ortega","annavara@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","7923, 7941","$0.00","Graphs represent the relationship between different entities and are the representation of choice in diverse domains, such as web page ranking, social networks, drug interactions, and communicable disease spreading. Due to the sheer size of graphs in these important domains, billions of vertices with tens of billions of edges, graph processing is a data intensive task. The size of the graphs is expected to far exceed the size of the main memory available in many computer systems. As such graph analytics will be hobbled by their inability to quickly access graph vertices and edges from computer storage. Current storage systems are mostly block based and hence treat graph data as a collection of bytes organized into pages. The advent of affordable solid state drives (SSDs) allows one to envision a future where SSDs can be made semantically aware of the underlying graph storage. Rather than treating storage as a collection of blocks, semantic awareness enables SSDs to consider graph structure while deciding on how vertices and edges are laid out, and how to access the graph elements efficiently. <br/><br/>This research advances the vision of semantic graph storage by proposing to make the SSD controller treat graph vertices and edges as first class objects.  In particular,  this research  will design and implement a set of application programming interfaces (APIs)  that allow application developers and algorithmic designers to specify graph layout and query storage systems using graph-oriented access requests, such as finding all the neighbors of a given vertex. A new runtime layer for SSDs will also be developed to exploit the semantic awareness to improve SSD endurance, garbage collection and caching. The benefits of semantic graph storage will be demonstrated by rethinking the implementation of  graph signal processing algorithms  to achieve an order magnitude improvement in performance. Such dramatic performance improvements in turn will enable a variety of compelling societal benefits such as accelerated drug discovery.  This research also provides opportunities for a new generation of students to study, implement and optimize graph analytics on experimental SSD platforms and to study the tradeoffs between clean abstractions and the performance impact of abstractions."
"1422009","SHF: CSR: Small: Toward Smart HPC through Active Learning and Intelligent Scheduling","CCF","Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","08/15/2014","08/13/2014","Zhiling Lan","IL","Illinois Institute of Technology","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$498,800.00","","lan@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7354, 7798","7354, 7923, 7942","$0.00","As high performance computing (HPC) continues to grow in scale, energy and resilience become first-class concerns, in addition to the pursuit of performance. These concerns demand significant changes in many aspects of the system stack including resource management and job scheduling. In order to harness the great potential of extreme scale systems, this project aims to incorporate intelligence into resource management and job scheduling. More specifically, it will develop a framework named SPEaR (Scheduling for Performance, Energy, and Resilience efficiency) for dynamically optimizing the three-dimensional performance, energy, and resilience scheduling. The research focuses on two thrusts: one is active learning to automatically extract valuable performance, energy, and resilience patterns and tradeoffs out of application and system data, and the other is intelligent scheduling to improve and control performance, resilience, and energy efficiency in resource management and scheduling. An event-driven scheduling simulator is being developed for comprehensively evaluating scheduling policies and their aggregate effects. The simulator, along with system logs, will be made available to the broad community under an open source license. <br/>This project creates critical technologies to promote system productivity and makes important advances essential toward smart HPC. Additionally, the learning techniques developed in this project are useful to other big data problems of national interests. The education plan enhances the undergraduate and graduate curricula and broadens the participation from underrepresented groups."
"1460325","CAREER: Analysis and Automation of Systematic Software Modifications","CCF","SOFTWARE & HARDWARE FOUNDATION, CLB-Career","09/01/2014","07/05/2016","Miryung Kim","CA","University of California-Los Angeles","Continuing grant","Sol J. Greenspan","08/31/2019","$437,285.00","","miryung@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7798, 9103","1045, 7944, CL10","$0.00","Software systems evolve. Developers must add features, fix bugs, and rewrite software systems to provide better functionality and higher performance. Existing systems also need to migrate to new hardware, computing environments, programming models, and libraries. There exist redundancies, inefficiencies, and error-proneness in the way that we evolve software systems today. In particular, recent empirical studies indicate that developers often apply similar but not identical changes to similar contexts. Making such systematic, repetitive program modifications is a tedious, manual, error-prone process.<br/><br/>This project will investigate the extent and nature of repetitive program modifications and will design, build, and evaluate a novel approach, called SYDIT, which improves developer productivity in applying systematic changes.  In this approach, developers no longer apply similar changes manually. Instead, developers provide the old and new version of selected code as an example change, and SYDIT will generalize a reusable, abstract, context-aware program transformation from it.<br/>(1) SYDIT will compute program differences between the old and new version of selected code and create a reusable edit script by identifying relevant data and control flow context and by abstracting the edits' content and position.<br/>(2) SYDIT will then automatically identify related candidate change locations and produce concrete, customized edits to each candidate.<br/>Incorporation with testing and change impact analysis will help developers validate suggested changes. SYDIT's new differential delta analysis will help developers understand the effect of ported changes in each target context.<br/>(3) Using a large corpus of project histories, this project will investigate the frequency and types of repetitive changes. The resulting data set will be used to evaluate SYDIT's accuracy and capability and to assess a productivity gain that can be achieved by SYDIT.<br/>The impact of this research will be substantially improved developer productivity in evolving large software systems. By helping developers apply changes to similar contexts exhaustively and inspect the effect of suggested changes, SYDIT will reduce errors of omission and relieve developers from tedious, error-prone hand editing. The empirical studies will expand our understanding of repetitive program changes during software evolution."
"1422569","AF: Small: Randomized Algorithms and Stochastic Models","CCF","ALGORITHMIC FOUNDATIONS","06/01/2014","06/03/2014","Aravind Srinivasan","MD","University of Maryland College Park","Standard Grant","Tracy J. Kimbrel","05/31/2019","$450,000.00","","srin@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7796","7923, 7926","$0.00","The fundamental role played by randomness in computation is one of the key discoveries of research in algorithms and complexity over the last four decades. This manifests itself in at least two ways: through randomized algorithms, and through stochastic models for uncertain data and/or processes. This fundamental role has been accentuated in our Big Data age through tools such as sampling which are vital for streaming and sub-linear algorithms, as well as through the stochasticity that is often inherent in machine learning. In this project, the PI aims to develop or improve some basic techniques in randomized algorithms and stochastic optimization, as well as to apply them to classical and modern problems in combinatorial optimization.<br/><br/>The broader impact of this project will be in several directions including the following. Graduate students will be involved closely in this research, and undergraduate research teams will be exposed to related ideas in algorithms, probabilistic methods, and optimization. High-school students will be taught the foundations of this research, especially the Lovasz Local Lemma and its facets, both through individual mentoring and through group-based teaching. Appropriate aspects of this work will be integrated into the PI's classes. Finally, this work will extend to key applications in areas including vaccination problems and the operation of alternative-energy plants.<br/><br/>This project aims to develop tools that will be general and useful in their own right, as well as techniques that will lead to improvements for targeted, fundamental applications. These applications include well-known problems in combinatorial optimization including asymmetric traveling salesperson, k-median, and stochastic matching, as well as basic issues in application areas such as public health and social networks (vaccination) and alternative energy (operations planning for wind and solar energy plants under stochastic uncertainty). The proposed techniques encompass ""going beyond"" the powerful Lovasz Local Lemma, new iterated applications of Local Lemma-like techniques and their generalizations to problems such as asymmetric traveling salesperson, the nexus of dependent rounding, submodularity, and (matrix) concentration bounds, as well as new types of differential-equation techniques in discrete optimization. As a particular example, work of the PI and his collaborators has shown that the Moser-Tardos algorithm has facets that can help us get well beyond what is guaranteed by the Lovasz Local Lemma; this project aims to go significantly further in this broad direction, with the goal that generalizations of a powerful tool such as the Local Lemma will have new applications in discrete optimization."
"1562306","SHF: Medium: Collaborative Research: Next-Generation Message Passing for Parallel Programming: Resiliency, Time-to-Solution, Performance-Portability, Scalability, and QoS","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2016","06/06/2018","Purushotham Bangalore","AL","University of Alabama at Birmingham","Continuing grant","Almadena Y. Chtchelkanova","05/31/2020","$310,279.00","","puri@uab.edu","AB 1170","Birmingham","AL","352940001","2059345266","CSE","7798","7924, 7942, 9150, 9251","$0.00","Parallel programming based on MPI is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics.  Emerging supercomputer systems will have more faults and MPI needs to be able to workaround such faults to be appropriate to these emerging situations, rather than causing an entire application to fail.  Collaborative, transformative message passing research for High Performance Computing (HPC) critical to performance-portable parallel programming in new and forthcoming scalable systems (with a strategy of ""best practice-first, standardization-later"") is being reduced to practice. A substantial subset of the Message Passing Interface (MPI-3/4) application programmer interface is being made fault tolerant through extensions with weak collective transactions that synchronize between parallel tasks. <br/><br/>This research studies  the novel model that localizes faults, provides tunable fault-free overhead, allows for multiple kinds of faults, enables hierarchical recovery, and is data-parallel relevant.  Fault modeling of underlying networks is being studied. Application developers control the granularity and fault-free overhead in this effort. Performance and scalability results of the middleware prototype are being demonstrated principally through compact applications that relate to real use cases of practical and academic interest. The impact of this work ranges from users of the largest supercomputers in government labs to practical clusters that have long-running, time-critical applications, and to space-based and other parallel processing in ""hostile"" environments where faults occur more frequently than in past years.  The project is producing usable free software that will be widely shared in the community as well as guidance on how better parallel programs can be written in academia, industry, and government.  The project also provides guidelines for how to update existing or legacy programs to use the new capabilities that are being reduced to practice."
"1717370","SHF: Small: Collaborative Research:Discerning and Recommending Context-Specific Best Practices in DevOps-Oriented Software Development","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2017","04/26/2018","Vladimir Filkov","CA","University of California-Davis","Standard Grant","Sol J. Greenspan","06/30/2020","$198,000.00","","filkov@cs.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7798","7798, 7923, 7944, 9251","$0.00","This project is a scientific study of modern software development practices, which has become known as DevOps. The DevOps culture seeks to bring changes into software production as quickly as possible without compromising software quality, primarily by automating the processes of building, testing, and deploying software. In practice, DevOps engineers can choose between a multitude of tools, including configuration management, cloud-based continuous integration, and automated deployment. Often individual tools are used without much guidance on how they fit in the big picture, and questions about best practices abound in online forums. However, existing answers are typically generic rules of thumb or dated advice, mostly based on third-party experiences, often non-applicable to the specific context. In fact, current empirical evidence on the effectiveness of DevOps practices is much fragmented and incomplete. State-of-the-art decision-making support, based on hard data and informed advice, can help DevOps engineers discern the best choices and practices for their tasks.<br/><br/>The proposed research is grounded in contingency theory, where the emphasis is on task context when reasoning about the effectiveness of practices. The goal of this project is to learn and convey structured, context-dependent analytics on best practices in DevOps environments, by mining and analyzing data from the collaborative coding platform GitHub. Using established and novel qualitative and quantitative techniques, this research will: (1) identify clusters of software projects that share similar context variables; and (2) within a context of interest, discern the conditions under which DevOps practices such as continuous integration are most (and least) effective. This will result in actionable knowledge and tool support for DevOps teams, to customize efficient project practices to their environment, as well as advance the theory and practice of software engineering, especially as it relates to distributed, fast paced, automation-heavy environments."
"1723215","SHF: Large: Collaborative Research: Exploiting the Naturalness of Software","CCF","INFORMATION TECHNOLOGY RESEARC, SOFTWARE & HARDWARE FOUNDATION","07/01/2016","01/09/2017","Tien Nguyen","TX","University of Texas at Dallas","Continuing grant","Sol J. Greenspan","06/30/2019","$260,709.00","","nguyen.n.tien@gmail.com","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1640, 7798","7925, 7944","$0.00","This inter-disciplinary project has its roots in Natural Language (NL) processing. Languages such as English allow intricate, lovely and complex constructions; yet, everyday, ``natural? speech and writing is simple, prosaic, and repetitive, and thus amenable to statistical modeling. Once large NL corpora became available, computational muscle and algorithmic insight led to rapid advances in the statistical modeling of natural utterances, and revolutionized tasks such as translation, speech recognition, text summarization, etc.  While programming languages, like NL, are flexible and powerful, in theory allowing a great variety of complex programs to be written, we find that ``natural? programs that people actually write are regular, repetitive and predictable. This project will use statistical models to capture and exploit this regularity to create a new generation of software engineering tools to achieve transformative improvements in software quality and productivity. <br/> <br/>The project will exploit language modeling techniques to capture the regularity in natural programs at the lexical, syntactic, and semantic levels. Statistical modeling will also be used to capture alignment regularities in ``bilingual? corpora such as code with comments, or explanatory text (e.g., Stackoverflow) and in systems developed on two platforms such as Java and C#.  These statistical models will help drive novel, data-driven approaches for applications such as code suggestion and completion, and assistive devices for programmers with movement or visual challenges. These models will also be exploited to correct simple errors in programs. Models of bilingual data will used to build code summarization and code retrieval tools, as well as tools for porting across platforms. Finally, this project will create a large, curated corpus of  software, and code analysis products, as well as a corpus of alignments within software bilingual corpora, to help create and nurture a research community in this area."
"1619415","SHF: Small: Methodology, Tools, and Circuits for Bundled-Data Resilient Asynchronous Design","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","05/12/2017","Peter Beerel","CA","University of Southern California","Standard Grant","Sankar Basu","06/30/2019","$400,000.00","Melvin Breuer","pabeerel@pollux.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","7923, 7945","$0.00","The impact of this research will span both academia and industry, yielding: (1) fundamental theory to design and analyze asynchronous circuits that gracefully adapt to process variability and makes near-threshold (i.e., low-power) computing practical; (2) a CAD flow that makes asynchronous circuits far more attractive for the ultra-low-power market. The research is coupled with a comprehensive plan to foster innovation in engineering, including plans for commercializing this research. The plans also include student training and future workforce development for the electronic chip industry.<br/><br/><br/>The unrelenting demand for longer battery life and energy-efficient designs is increasing the desire for integrated chip voltage supplies to go lower and lower, approaching near threshold levels. This aggravates process variability which forces increasing margins in traditional synchronous clock periods, yielding far-from-optimal solutions.  The focus of this proposal is a recently proposed new resilient asynchronous design style, called the ""Blade"", that is robust to meta-stability, robust to hold times, and architecturally agnostic, not relying on architectural replay to correct for errors. This proposal will create a comprehensive framework to support the efficient design of Blade circuits with emphasis on test, synthesis, and physical design."
"1744082","SHF: Small: Cross-Platform Solutions for Pruning and Accelerating Neural Network Models","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2017","05/25/2017","Hai Li","NC","Duke University","Standard Grant","Sankar Basu","06/30/2019","$424,432.00","","hai.li@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7798","7923, 7945, 8089, 8091","$0.00","Deep neural networks (DNNs) have achieved remarkable success in many applications because of their powerful capability for data processing. The objective of this project is to investigate a software-hardware co-design methodology for DNN acceleration that can be applied to both traditional von Neumann and emerging neuromorphic architectures. The project fits into the general area of ""brain-inspired"" energy efficient computing paradigms that has been of much recent interest. The investigators are also active in various outreach and educational activities that include curricular development, engagement of minority/underrepresented students in research. Undergraduate and graduate students involved in this research will also be trained for the next-generation computer engineering and semiconductor industry workforce.<br/><br/>From a more technical standpoint, a novel neural network sparsification process is to be explored to preserve the state-of-the-art accuracy, while establishing hardware-friendly models of neural network computations.  The result is expected to lead to a holistic methodology composed of neural network model sparsification, hardware acceleration, and an integrated software/hardware co-design.  The project also benefits big data research and industry at large by inspiring an interactive design philosophy between the design of learning algorithms and the corresponding computational platforms for system performance and scalability enhancement."
"1818643","XPS: FULL: Collaborative Research: Parallel and Distributed Circuit Programming for Structured Prediction","CCF","Exploiting Parallel&Scalabilty","10/01/2017","06/18/2018","Vivek Sarkar","GA","Georgia Tech Research Corporation","Standard Grant","Anindya Banerjee","07/31/2019","$88,265.00","","vsarkar@rice.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8283","","$0.00","This project develops a system for ""circuit programming,"" which allows a programmer to focus on the high-level solution to a problem rather than on the details of how the computation is organized. Circuit programming consists of writing rules that describe how data items depend on one another. The intellectual merits lie in the design of a new programming language for specifying these rules, along with the algorithms whereby the computer automatically finds efficient strategies for managing the necessary computations on available parallel hardware.  The project's broader significance and importance lie in its potential to streamline work in areas such as artificial intelligence and machine learning.  With the growing complexity of systems in these areas and their need to process big data in depth, research and teaching typically get bogged down in programming details, especially for parallel platforms; this project aims to delegate those details to automatic methods.<br/><br/>The research develops a programming system for Dyna, a circuit programming language that enables concise specification of large function graphs that may be cyclic and/or infinite. Dyna employs (1) a pattern-matching notation that augments pure Prolog with evaluation and aggregation and (2) an object-like mechanism for dynamically defining new sub-circuits as modifications of old ones.  This project is building an adaptive system that can mix forward and backward chaining to seek a fixpoint of the circuit and to update this fixpoint as the inputs change.  The system will perform compile-time and runtime analysis of the Dyna program and will map it to Habanero, a system for scheduling parallel computations on multicore processors, with extensions for task priorities, task cancellation, GPU execution, and distributed execution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816546","CIF: Small: Collaborative Research: A Software Toolbox for Computing and Exploring the Fundamental Limits of Information Systems","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/11/2018","Chao Tian","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","09/30/2021","$319,215.00","","chao.tian@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7797","7923, 7935","$0.00","This project aims to build an open-source software toolbox under the GNU-GPL license to facilitate the investigation of information systems (e.g., data storage systems, streaming data structures, and content delivery systems) using information theoretical methods, more precisely, to facilitate the derivation of outer bounds and identifying novel code constructions.  By building modern software tools which are able to take advantage of the advance in computer hardware and software, this effort can help the community more efficiently incorporate computational intelligence into information theoretic research.<br/> <br/>The main theoretical foundation for the approach is the entropy linear programming framework, and techniques based on symmetry and implication relations that can reduce the complexity of such programs. The core components of the toolbox are various software tools to perform information theoretic analysis and exploration of information systems in a computational manner. The completion of such a toolbox can enable researchers to build, with minimal programming efforts, computer software which takes in a description of the information system of interest, processes the relation among its components, formulates an appropriate optimization problem, and invokes a computational optimization solver to produce meaningful results regarding the fundamental limits and code constructions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1524852","SHF: Small: Transformations for Synergistic Analysis of Large Evolving Graphs","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","06/30/2015","Rajiv Gupta","CA","University of California-Riverside","Standard Grant","Almadena Y. Chtchelkanova","06/30/2019","$400,000.00","","gupta@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7923, 7942","$0.00","The importance of graph processing has grown with the popularity of graph analytics. An important feature of real-world graphs is that they are constantly evolving (e.g., social networks, networks modeling spreading of a disease etc.). Graph analytics over an evolving graph entails repeating analysis over snapshots of a graph taken at different points in time to observe how features of interest change over time. For large real-world graphs with tens of billions of edges, evolving graph analysis is both highly compute- and memory-intensive. By developing transformations that reorganize the computation and data, techniques for rapid evolving graph analytics on modern computing platforms are being considered. Many students are being trained and educated in this important field.<br/><br/>Graph analysis can greatly benefit from cores and storage available on modern parallel machines. However, effectively exploiting the resources remains an enormous challenge due to irregular nature of parallelism and lack of data locality in graph computations. This work is leveraging two key characteristics, overlapping working sets and computed value stability, to develop techniques for speeding up graph analytics. The techniques being considered include: optimization of reading and writing of large graphs on disk, optimizing inter-node communication on a cluster, and optimizing computation over multiple versions of an evolving graph. These optimizations are being used to greatly enhance the performance of multiple popular graph processing systems. Public dissemination of these software enhancements are also planned."
"1513944","SHF:Medium:Collaborative Research: Architectural and System Support for Building Versatile Memory Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2015","05/23/2018","Xiaodong Zhang","OH","Ohio State University","Continuing grant","Yuanyuan Yang","06/30/2019","$300,000.00","","zhang@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7798","7924, 7941, 9150","$0.00","Computer memory design is moving into a new era with emerging NVM (non-volatile memory) technologies for increasingly data-intensive applications. This project will investigate a novel memory architecture called versatile memory system that hosts heterogeneous memory technologies to provide a powerful in-memory computing engine, addressing the increasing demands for memory performance, energy efficiency, and reliability from data-intensive applications. It is based on a holistic approach, from low-layer hardware design to high-layer OS management, to address the challenges of complexity and efficiency arising from the integration of heterogeneous NVM technologies. It allows the memory system to be self-adaptive to meet varying application demands on performance, energy efficiency, and reliability. The project will study the framework, critical hardware support, and feasible and meaningful functionalities of the versatile memory system, aiming at improving the performance, energy efficiency, reliability, and manageability of computing systems from mobile to server platforms. It is expected that the outcome of the project will have broader impact on the design of modern computer memory systems both in academia and industry."
"1834707","Travel Grant for the 2018 Empirical Software Engineering International Week","CCF","INFORMATION TECHNOLOGY RESEARC, SOFTWARE & HARDWARE FOUNDATION","06/01/2018","05/17/2018","Jeffrey Carver","AL","University of Alabama Tuscaloosa","Standard Grant","Sol J. Greenspan","05/31/2019","$15,000.00","","carver@cs.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870005","2053485152","CSE","1640, 7798","7944, 9150","$0.00","The grant supports travel for graduate students to attend the 2018 Empirical Software Engineering International Week, an international event, which this year will take place during October 2018 in Finland. It consists of several co-located events:  Annual meeting of the International Software Engineering Research Network (ISERN); International Doctoral Symposium on Empirical Software Engineering (IDoESE); International Advanced School on Empirical Software Engineering (IASESE); the International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE).<br/><br/>This activity advances the field of Software Engineering through the exchange of information and collaborations.  The field is growing rapidly due to the emergence of big data code repositories, which enables empirical research.  The goal is to enable the research that will improve software engineering productivity and quality, while encouraging students to pursue a research career. The international dimension of the conference helps develop a globally-aware workforce.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821957","Verification, Validation, and Test of ML Systems (V-TML) Workshop","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","05/15/2018","05/09/2018","David Yeh","NC","Semiconductor Research Corporation","Standard Grant","Sankar Basu","04/30/2019","$49,960.00","","david.yeh@src.org","4819 Emperor Blvd.","Durham","NC","277035420","9199419400","CSE","2878, 7798","073Z, 7556, 7798, 7945, 7947","$0.00","Electronic systems are creating a renewed innovation cycle in many industries, including automobiles and manufacturing. These systems, which are at the heart of innovation in areas such as autonomous systems, often incorporate Machine Learning (ML) components to make decisions. Since safety, reliability, and predictability are paramount in these systems, this award funds a workshop to explore safe, reliable, and predictable applications of machine learning methods and systems, the performance of which often depends on factors that are not within the full control of the user or the system designer. These latter factors could range from uncertainties in training data, to lack of robust algorithms, to probabilistic aspects of inference mechanism etc. The organizing committee, as well as the participants are composed of individuals from academia, industry and the government, include diverse participation from minority, women and underrepresented groups.<br/><br/>Examples of technical aspects to be discussed at the workshop include, but are not limited to: (1) survey of relevant machine learning techniques, (2) data analytics currently used by electronic design automation, and problems in which similar paradigms are used, and (3) robustness, test, validation and verification in the context of ML. The outcomes of the workshop will be reported publicly and shared widely with the research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1423108","SHF: Small: Efficient CPU-GPU Communication for Heterogeneous Architectures","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2014","06/18/2014","Laxmi Bhuyan","CA","University of California-Riverside","Standard Grant","Sankar Basu","06/30/2019","$498,976.00","","bhuyan@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7923, 7941","$0.00","Future chip multiprocessors (CMPs) will have silicon space and technology to incorporate hundreds of cores. The trend is to integrate tens of cores and hardware accelerators (HAs), such as GPUs, on a single platform. The proposed heterogeneous architecture will enable future chips to operate within their power budgets while providing the high-throughput per Watt required for large scientific applications. Many of the top-500 supercomputers integrate thousands of CPUs with GPU accelerators to achieve the desired throughput for scientific applications. Considerable effort, however, is needed to design efficient communication mechanisms between heterogeneous components in such a system. Currently, HAs are not fully integrated with the system architecture; offloading computation from the CPU to the HAs adds large communication overhead. This research project explores comprehensive solutions to this problem through many different techniques. The project has significant broader impact in terms of research publications, graduate student supervision, and minority education because UCR is a minority serving institution.<br/><br/>This project will develop new CPU-GPU communication techniques through static programming and run-time optimization. It will develop a divisible load theory (DLT) technique to overlap communication with computation, and optimize the time and size of data transfer between the CPU and GPU. The research will also develop run-time techniques that can monitor the efficiency of execution and dynamically change the transfer parameters by considering the execution behaviors of different applications. Architectural changes are to be incorporated in the GPU to initiate data transfers based on task execution inside the GPU. Design of the shared virtual memory (SVM) architecture is to be developed, where the accelerator and system memories share a single virtual address space; and CPUs and HAs in the system will communicate through the SVM. The hardware controllers, memory management unit (MMU), GPU cache memory architectures, cache coherence protocols, and other interfaces between the GPU and CPU cores will also be designed. The project proposes suitable hybrid cache coherence protocols and efficient interconnection networks for scalable system design. Finally, run-time system and software interfaces will be developed that can execute multiple multithreaded applications on a heterogeneous multicore architecture."
"1821990","Promoting Student Participation in IEEE Conference on Biomedical and Health Informatics","CCF","COMPUTATIONAL BIOLOGY","04/01/2018","03/22/2018","Yufei Huang","TX","University of Texas at San Antonio","Standard Grant","Mitra Basu","03/31/2019","$12,000.00","","yhuang@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","7931","7556, 7931","$0.00","This award provides travel support for students to attend and present their work at the IEEE International Conference on Biomedical and Health Informatics, 2018 (BHI18). BHI18 is the premier flagship conference sponsored by IEEE Engineering in Medicine and Biology Society (EMBS) in the area of mHealth and health analytics. BHI18 will be held from March. 4-8, 2018 in Las Vegas and jointly organized with IEEE conference on Body Sensor Networks. The goal of the conference is to provide a unique platform to showcase novel sensors, systems, signal processing, analytics and data management services. BHI18 will offer the latest findings of researchers on efficient and innovative signal acquisition, transmission, processing, monitoring, storage, retrieval, analysis, visualization and interpretation of multi-modal signals including physiological, biomedical, biological, social, behavioral, environmental, and geographical data. It will also provide a forum for bioinformatics and computational biology researchers in especially EMBS and IEEE Signal Processing Society to interact and exchange research ideas among them and others in Biomedical and Health Informatics community and discuss future challenges in cutting edge areas including Precision Medicine. As integrated bioinformatics and medical informatics approaches become one of the focus areas in Precision Medicine, BHI18 is uniquely positioned to spark novel ideas and promote new collaborations between researchers in these communities.  <br/><br/>An important mission of the Workshop is to promote participation of graduate students, especial woman and minority students, and foster student education on computational biology for precision medicine. This award will be used to provide travel grants to qualified graduate students, whose papers are deemed of high quality, to support their participation in the conference. The Workshop organization includes the student award committee, which will be responsible for evaluation and selection of student awardees. Particularly, 20-30% of the awards are allocated to women and minority students to participate and submit papers to the Workshop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1756013","CRII: SHF: Optimizing Deep Learning Training through Modeling and Scheduling Support","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","01/23/2018","Feng Yan","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Almadena Y. Chtchelkanova","05/31/2020","$174,990.00","","fyan@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","7798","7942, 8228, 9150","$0.00","Deep learning models trained on large amounts of data using lots of computing resources have recently achieved state-of-the-art training performance on important yet challenging artificial intelligence tasks. The success of deep learning has attracted significant research interest from hardware and software communities to improve training speed and efficiency. Despite the great efforts and rapid progress made, one important bridge to connect software and hardware support with deep learning domain knowledge is still missing: efficient configuration exploration and runtime scheduling. Both the quality of deep learning models and the training time are very sensitive to many adjustable parameters that are set before and during the training process, including the hyperparameter configurations (such as learning rate, momentum, number and size of hidden layers) and system configurations (such as thread parallelism, model parallelism, and data parallelism). Efficient exploration of hyperparameter configurations and judicious selection of system configurations is of great importance to find high-quality models with affordable time and cost. This is however a challenging problem due to a huge search space, expensive training runtime, sparsity of good configurations, and scarcity of time and resources.<br/><br/>The objective of this research work is to systematically study the unique properties of deep learning systems and workloads, and establish new modeling and scheduling methodologies for improving deep learning training. The PI aims to improve the efficiency of discovering high performing models through a dynamic scheduling methodology driven by a novel hyperparameter configuration classification approach. The PI aims at developing an accuracy- and efficiency-aware hybrid scheduling methodology that makes judicious scheduling decisions based on a global view of both the time dimension (accuracy potential) and spatial dimension (efficiency potential) information. This research work integrates techniques in workload characterization, performance modeling, resource management, and scheduling to dramatically speedup the training process while significantly reducing the cost in time and resources. More broadly, this project will gain foundational knowledge about the interaction between software-hardware support and deep learning domain knowledge. This knowledge can help design next generation deep learning systems and frameworks, making deep learning training handy for researchers and practitioners with limited system and machine learning domain expertise. This research will help enhance curriculum and provide research topics for both undergraduate and graduate students, especially students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718820","AF: Small: Algorithms for Solving Real-Life Instances of Optimization and Clustering Problems","CCF","ALGORITHMIC FOUNDATIONS","08/15/2017","08/09/2017","Yury Makarychev","IL","Toyota Technological Institute at Chicago","Standard Grant","Rahul Shah","07/31/2020","$449,986.00","","yury@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7926","$0.00","The project aims to develop efficient algorithms for computational problems that arise in business, engineering, and science. The project will have an impact on theoretical computer science (TCS) by improving our understanding of the nature of real-life instances and designing better algorithms -- with provable performance guarantees -- for them. Additionally, the results will be relevant to researchers in other areas of computer science, including machine learning and optimization. In particular, this project will provide researchers with new practical algorithms. Through its wide-reaching results, the project will strengthen the connection between TCS and other areas of computer science. Further, the project will be of interest to researchers in mathematics, mathematical physics, and statistics, in part because one of the key models considered in this proposal has been introduced and studied in these fields.<br/><br/>The PI will collaborate on this project with graduate and undergraduate students at the Toyota Technological Institute at Chicago (TTIC) and the University of Chicago. Additionally, he will invite PhD students from other universities to work on the project during summer months. The PI will incorporate the topic of this proposal in his graduate courses. Specifically, he will include introductory material on the topic in his graduate Algorithms course and more advanced material in his course on metric geometry in computer science.<br/><br/>Many problems that arise in business, engineering, and science are very hard in the worst case: for them, there are no ""universal"" efficient algorithms -- i.e., algorithms that solve all possible instances in reasonable (polynomial) time. However, real-life instances are usually considerably simpler than the most difficult ones. The goal of this project is to identify what makes real-life instances computationally tractable and to design efficient algorithms for solving them. These algorithms will solve many real-life instances of computational problems by exploiting their structural properties (at the same time, the algorithms may fail to solve the most difficult instances, which, however, almost never appear in practice).<br/><br/>In order to design efficient algorithms -- with provable performance guarantees -- for real-life instances of computational problems, one has to define a formal model for real-life instances. This proposal will explore existing generative and descriptive models for real-life instances of clustering and optimization problems, including semi-random stochastic block models and models based on different stability assumptions. The project will also develop new, more advanced models. The PI will design new algorithms for these models and analyze existing heuristics."
"1231216","A Center for Brains, Minds and Machines: the Science and the Technology of Intelligence","CCF","SCI & TECH  CTRS (INTEG PTRS), INFORMATION TECHNOLOGY RESEARC, STCs -2013 Class","09/01/2013","09/15/2018","Tomaso Poggio","MA","Massachusetts Institute of Technology","Cooperative Agreement","Phillip Regalia","08/31/2023","$30,359,625.00","Ellen Hildreth, Haym Hirsh, Lakshminarayana Mahadevan, Matthew Wilson, Gabriel Kreiman","tp@ai.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1297, 1640, 7202","8089, 8091, 9171, 9218, 9251","$0.00","Today's AI technologies, such as Watson, Siri and MobilEye, are impressive yet still confined to a single domain or task. Imagine how truly intelligent systems --- systems that actually understand their world --- could change our world.  The work of scientists and engineers could be amplified to help solve the world's most pressing technical problems. Education, healthcare and manufacturing could be transformed.  Mental health could be understood on a deeper level, leading in turn to more effective treatments of brain disorders.  These accomplishments will take decades.  The proposed Center for Brains, Minds, and Machines (CBMM) will enable the kind of research needed to ultimately achieve such ambitious goals. The vision of the Center is of a world where intelligence, and how it emerges from brain activity, is truly understood. A successful research plan for realizing this vision requires four main areas of inquiry and integrated work across all four guided by a unifying theoretical foundation. First, understanding intelligence requires discovering how it develops from the interplay of learning and innate structure. Second, understanding the physical machinery of intelligence requires analyzing brains across multiple levels of analysis, from neural circuits to large-scale brain architecture. Third, intelligence goes beyond the narrow expertise of chess or Jeopardy-playing computers, bridging several domains including vision, planning, action, social interactions, and language. Finally, intelligence emerges from the interactions among individuals ? it is the product of social interactions. Therefore, the research of the Center engages four major research thrusts (Reverse Engineering the Infant Mind, Neuronal Circuits Underlying Intelligence, Integrating Intelligence, and Social Intelligence) with interlocking teams and working groups, and a common theoretical, mathematical, and computational platform (Enabling Theory).<br/><br/>The intellectual merit of the Center is its focus on elucidating the mechanisms and architecture of intelligence in the most intelligent system known: the human brain.  Success in this project will ultimately enable us to understand ourselves better, to produce smarter machines, and perhaps even to make ourselves smarter. The Center's potential legacy of a deep understanding of intelligence, and the ability to engineer it, is tantalizing and timeless. It includes the creation of a community of researchers by programs such as an intensive summer school, technical workshops and online courses that will train the next generation of scientists and engineers in an emerging new field -- the Science and Engineering of Intelligence. This new field will catalyze continuing progress in and cross-fertilization between computer science, math and statistics, robotics, neuroscience, and cognitive science. Sitting between science and engineering, it will attract growing interest from the best students at all levels.  The broader impact of the Center program could be to revolutionize K-12, and also 0-K, and 12-life with a deeper understanding of the process of learning.  The ability to build more human-like intelligence in machines will transform our productivity, enabling robots to care for the aged, drive our cars, and help with small-business manufacturing. The Center team is composed of over 23 investigators, many having already made significant accomplishments in multiple research areas relevant to the science and the technology of intelligence. The Center team has a mix of junior and senior researchers, bringing expertise in Computer Science, Neuroscience, Cognitive Science and Mathematics. The institutional partners include nine institutions (MIT, Harvard, Cornell, Rockefeller, UCLA, Stanford, The Allen Institute, Wellesley, Howard, Hunter and the University of Puerto Rico), three of which have significant underrepresented student populations. The academic institutions are complemented by the Center's industrial partners (Microsoft, IBM, Google, DeepMind, Orcam, MobilEye, Willow Garage, RethinkRobotics, Boston Dynamics) and by world-renowned researchers at international institutions (Max Planck Institute, The Weizmann Institute, Italian Institute of Technology, The Hebrew University)."
"1551592","Inter-agency Workshop for Computational Science & Engineering Software Sustainability and Productivity Challenges (CSESSP Challenges)","CCF","SOFTWARE & HARDWARE FOUNDATION, Software Institutes, ","09/15/2015","06/19/2018","Gabrielle Allen","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sol J. Greenspan","05/31/2019","$88,739.00","","gdallen@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798, 8004, o532","7433, 7944, 8004, 8237","$0.00","This grant funds participation of academics for an interagency workshop on Computational Science & Engineering Software Sustainability and Productivity Challenges (CSESSP Challenges), which will take place on Oct 15-16.  See  https://www.nitrd.gov/csessp/.   Government agencies that fund and develop software for Computational Science & Engineering are concerned that planning for software pays attention mostly to the initial development of software and not for the operation, maintenance and ongoing sustainability.  Computational Science & Engineering software often demand decades of useful life, thus these factors are of utmost importance.  The productivity and sustainability issues impede the advancement of computation science and engineering, which is important to US competitiveness.  The workshop will involve a diverse set of participants from different sectors and regions and will pay attention to Inclusion of under-represented groups."
"1763970","AF: Medium: Research in Algorithms and Complexity: Total Functions, Games, and the Brain","CCF","ALGORITHMIC FOUNDATIONS","05/01/2018","04/04/2018","Christos Papadimitriou","NY","Columbia University","Continuing grant","Rahul Shah","04/30/2022","$186,373.00","Mihalis Yannakakis","cp3007@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7924, 7926, 7927, 7932","$0.00","The ubiquitous information environment around us, which has brought to the world unprecedented connectivity and availability of information, as well as newfound opportunities for individual expression, education, work, production and commerce, entertainment, and interpersonal communication, is the result of decades of research in all fields of computer science. Furthermore, our best hope for confronting the many problems this new environment has brought to humanity (privacy and fairness, to mention only two) also lies in new computer science research. Research in theoretical computer science in particular over the past half century has been instrumental in bringing the benefits of Moore's law to bear through fundamental clever algorithms and has made leaps in understanding the capabilities and limitations of computers and their software. In fact, it has articulated one of the most important problems in mathematics and all of science today: is P equal to NP? i.e, is exponential exhaustive search for a solution always avoidable?  The two investigators on this award have over the past four decades contributed much to this edifice of mathematical research in computer science, often in close collaboration. In this project, these investigators will work together in order to attack a new generation of problems: complexity questions in the fringe of the P vs. NP problem, a new genre of algorithms possessing a novel kind of robustness, research at the interface between computer science and economics related to income inequality and market efficiency, as well as research aiming at a better understanding of evolution, and of brain functions as basic as memory and as advanced as language.  The project will train PhD and Masters students and possibly undergraduates as well on these research topics. The findings of this research will be disseminated to students and researchers, both in computer science and in other disciplines, as well as to the general public, through journal and conference publications, undergraduate and graduate courses, seminars, colloquia, as well as public talks and general interest articles.<br/><br/>The project will work on improving our understanding of the complexity of total functions in the class TFNP and its subclasses, in view of recent research progress in that area. On complexity side, the project will: (1) investigate the complexity of an as yet unexplored, from this point of view, Tarski-like fixed-point theorem widely used in economics (2) revisit the approximability of the traveling salesperson problem and (3) explore a new kind of algorithmic notion of robustness based on dense nets of algorithms.  In algorithmic game theory, the project will: (1) explore a new variant of the price of anarchy inspired by wealth inequality, as well as the complexity of market equilibria in markets with production and economies of scale (2) research a new game theoretic solution concept based on the topology of dynamical systems (3) pursue the proof of an intriguing new complexity-theoretic conjecture about the inaccessibility of Nash equilibria.  The work will also explore certain promising directions at the interface of game theory and learning theory.  In the life sciences, the project will explore from the algorithmic point of view the problem of the true nature of mutations, and will extend recent research aiming at the computational understanding of how long-term memory, as well as syntax and language, are achieved in the human brain.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1423657","CIF: Small: Foundations of Wireless Interfering Cellular Networks With and Without Cooperation","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2014","07/20/2018","Mahesh Varanasi","CO","University of Colorado at Boulder","Standard Grant","Phillip Regalia","07/31/2019","$500,000.00","","Mahesh.Varanasi@Colorado.EDU","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7797","7923, 7935","$0.00","This project will investigate interfering cellular networks, which are networks formed by operating multiple cells of a traditional cellular network over the same frequency. In particular, it will develop efficient communication schemes for such networks that (a) deal with interfering data flows between multiple source-destination pairs (b) include cooperative strategies between nodes to exploit resources such as a wired back bone of constrained capacity and connectivity between access points and (c) robustly cope with channel uncertainty. Throughout, approximated capacity characterizations or those that are amenable to computation will be sought, with an emphasis on methods that scale well to a larger number of users.<br/><br/>The goal is to develop improved alternatives to communication methods employed in current cellular networks. The importance of such improved methods in the design of future generations of these networks is evident from the e-commerce that such networks enable, the information they make easily accessible and the increased efficiency of organizations they make possible today."
"1629392","XPS: FULL: Integrating Programming Model, Runtime, Algorithmic, and Architectural Support To Use Inexact and Heterogeneous Hardware for Scientific Computations","CCF","Exploiting Parallel&Scalabilty","08/01/2016","07/08/2016","Gagan Agrawal","OH","Ohio State University","Standard Grant","Anindya Banerjee","07/31/2019","$875,000.00","Radu Teodorescu, Ching-Shan Chou","agrawal@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8283","","$0.00","As the High Performance Computing (HPC) field moves towards even more powerful (Exascale) systems, it faces two key challenges: resilience and power efficiency. Increasing computing power while not significantly increasing the power budget potentially involves new architectural designs, such as the ones with low power and/or low margins. Though future systems are expected to experience an increase in the number of faults (because of increasing number of cores and decreasing feature sizes), energy efficient designs will likely suffer even more errors due to tighter margins and other design compromises. Of particular concern are the errors that escape hardware detection: such errors are said to cause Silent Data Corruption (SDC).  Maintaining correctness of a numerical simulation in the presence of SDCs is a very challenging problem.  The intellectual merit of this project is in combining ideas from numerical methods, programming model design and architecture design to address the correctness of numerical simulations. The broader significance and importance of the project includes impact on scientific and high performance computing, system software for parallel computing, and architectural designs and research. This project will also make several contributions towards education, human resource development, and increasing diversity, with   activities  like  teaching parallel computing (and programming) to diverse audience,  mentoring of  doctoral students, including those from underrepresented groups, and an interdisciplinary training program in Mathematical Biology for undergraduates.  <br/><br/>Technically, the project addresses the challenge of developing and executing scientific applications with energy efficient low-power/margin architectures that experience occasional faults, while maintaining programmer productivity and accuracy of results. This project develops a synergistic research program combining advances in HPC programming models, runtime systems, Near Threshold Voltage (NTV) architectures, and numerical methods (algorithms). Specifically, the project involves close collaboration between researchers from three areas: (a) parallel programming models, applications, and runtime systems, (b) architecture, and (c) finite difference and finite volume numerical models."
"1813598","SHF: SMALL: Streamlining Fork-Based Software Development","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","10/01/2018","03/14/2019","Christian Kastner","PA","Carnegie-Mellon University","Standard Grant","Sol Greenspan","09/30/2021","$498,019.00","","kaestner@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878, 7798","7923, 7944, 9251","$0.00","Collaboration is essential for software development at scale, in both industrial and open-source projects. Inadequate models of collaboration can stifle innovation and severely hurt open-source sustainability. This research investigates how developers collaborate in distributed settings with forks. Forks are public copies of repositories (source code, books, musical composition, lectures, and so forth) in which developers can make changes, including extensions and bug fixes. Developers may, but do not have to, contribute these changes back to the original project. Specifically, this project is concerned with inefficiencies in developer activities, such as lost contributions, redundant development, and fragmented communities. Such inefficiencies are not only wasteful with regard to the scarce developer time, but also often demotivating for new developers. This research will identify which practices are efficient in which contexts and will provide guidance for developers and team leads. This way, this research will support sustainability of software projects, allowing them to draw on more contributions and motivating more developers to contribute.<br/><br/>Combining theory building, empirical data-driven research, and tool building, the research will discover and evaluate existing interventions and develop new ones that steer collaborative development with forks toward better practices, such as better coordination among otherwise independent developers. Using empirical mixed-methods research, from exploratory studies building hypotheses to statistical evaluations on logs of development activities at scale, the research will identify and validate measures of inefficiencies and interventions developers use in practice to improve collaborative development, identifying what is effective in  which contexts. In addition, this research will design, develop, evaluate, and deploy new interventions in terms of light-weight transparency and awareness mechanisms that foster collaboration and integration and reduce redundancies, thus reducing inefficiencies within forked projects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617999","CIF: Small: Collaborative Research: Geometrical and Statistical Modeling of Space-Time symmetries for Human Action Analysis and Retraining","CCF","COMM & INFORMATION FOUNDATIONS","07/15/2016","07/08/2016","Pavan Turaga","AZ","Arizona State University","Standard Grant","Phillip Regalia","06/30/2019","$280,000.00","","pavan.turaga@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7797","7923, 7936","$0.00","This interdisciplinary research aims to advance current understanding and utilization of space-time symmetries in analyzing human movements, using fundamental tools from engineering, geometry, and statistics. Broader applications of this research include home or workplace-based self-reflection of daily activities, promotion of higher efficiency of human movements, and long-term management and/or prevention of movement disorders. While the need for comprehensively and statistically analyzing human kinematics is well chronicled, the current measures are often limited to simplistic quantities such as speeds and acceleration profiles of individual limbs. This project will focus on both spatial and temporal symmetries of limb movements, full body shapes, and complete dynamical actions, for assessment of movements ranging from daily activities to physiotherapeutic exercises.<br/> <br/>Symmetry has been used in the past, in clinical biomechanics, but in a limited way. This project will develop a comprehensive theory, built on fundamental tools from differential geometry and statistical analysis of geometric objects, to represent, quantify, analyze, and classify motions according to their level of symmetry. The specific forms of symmetry will include spatial reflection, temporal reflection, and space-time glide symmetries. This formulation will incorporate data from various sensing modalities and features, including point trajectories and stick figures from motion capture systems, to shape silhouettes and dynamic textures obtained from video sensors. The project outcomes also include the development of a real-time media-system for movement re-training and reflection of common actions, such as sitting to standing (STS). The proposal brings together a strong and inter-disciplinary team of researchers with expertise in computer vision and action recognition (Turaga), differential geometry and statistics (Srivastava), and somatics and kinesiology (Coleman)."
"1560037","REU Site: Software Testing and Analytics","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/15/2016","10/18/2018","Junhua Ding","NC","East Carolina University","Standard Grant","Rahul Shah","02/29/2020","$359,838.00","M.H. Tabrizi","junhua.ding@unt.edu","Office Research Administration","Greenville","NC","278584353","2523289530","CSE","1139","9250","$0.00","This project will establish a three-year REU site in software testing and analytics at East Carolina University (ECU). It will offer a ten-week research program for ten undergraduate students during summer semesters. The faculty-student interaction as well as interaction among students will take different forms such as meetings, seminars, tutorials, workshop, and field trips. The REU project will allow a diverse pool of undergraduate students to experience cutting-edge research experience that will help them to become self reliant in STEM research. Students will gain valuable research skills that will prepare them for their future fields of study, and their exposure to the research will help them to compete for high technology fields in an innovative job market. The research experience will also motivate them to continue onto graduate studies. The REU project also will provide students an opportunity to collaborate with their faculty mentors and student peers across the nation after the summer program. <br/><br/>The sample research projects cover open research topics in software testing and analytics. Software Testing and Analysis of Scientific Software is to investigate the technique for adequately testing complex scientific software systems. The experimental data generated from the testing will be analyzed with machine learning tools for improving the test efficiency and effectiveness. We expect students will master basic principles of software testing and become skillful in creating test strategies and using tools for testing scientific software. Fault Detection Effectiveness and MC/DC Coverage of Combinatorial Test Cases will investigate the integration of combinatorial testing and MC/DC (modified condition/decision coverage) testing. Studies such as how logical expressions can be effectively tested, sensitivity analysis of different partitions of the input domain and factors that may affect combinatorial-based test generation, and a cross comparison between tests generated using different combinatorial testing algorithms will be conducted. Students will receive rigorous training in software testing and software testing research in this project. Software Analytics for Mobile Domain Specific Language (DSL) Construction will analyze program analysis results for the improvement of the development of DSL, and Guided Test Generation for Web Applications will use program analysis results to derive tests for testing web applications. The two projects will offer students the opportunity to learn the principles, applications and experimental study of program analysis."
"1750667","CAREER: Addressing Scalability Challenges in Designing Next-generation GPU-Based Heterogeneous Architectures","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2018","02/12/2019","Adwait Jog","VA","College of William and Mary","Continuing grant","Yuanyuan Yang","01/31/2023","$190,950.00","","adwait@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7798","1045, 7941","$0.00","Graphics processing units (GPUs) are becoming default accelerators in many domains such as high-performance computing (HPC), deep learning, and virtual/augmented reality. Their close integration with high-performance multi-core CPU architectures is also allowing very efficient heterogeneous computing. Going forward, it is imperative that such GPU-based systems scale both in terms of performance and energy efficiency to meet the exascale (and beyond) computing demands of the future. However, sustained scaling of these systems is challenging primarily because a) fabricating a single large die provides very low yield, making it prohibitively expensive, b) memory hierarchy remains a critical performance and energy efficiency bottleneck, and c) programmability and application scalability is hindered by inefficiencies in the shared virtual memory and multi-application support.<br/><br/>This project seeks to address these scalability challenges by rethinking the design of future large-scale GPU-based systems. In particular, this research project revolves around three major components: a) design space exploration of cores (including their organization) and the entire memory hierarchy, b) development of data movement optimization techniques by identifying and then exploiting cache locality via novel synergistic caching and scheduling techniques, and c) improving resource utilization of large-scale system resources by enhancing shared virtual memory and multi-application execution support. All three research components will be evaluated on a newly-developed comprehensive evaluation infrastructure. The findings of this research will be incorporated into new and existing undergraduate and graduate courses. It is expected that the insights resulting from this research would have a long-term positive impact on GPU-based computing, thereby making our daily lives more productive."
"1816812","SHF:Small:Scalable and Precise Program Analyses via Linear Conjunctive Language Reachability","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","10/10/2018","Qirun Zhang","CA","University of California-Davis","Standard Grant","Anindya Banerjee","03/31/2019","$500,000.00","Qirun Zhang","qrzhang@gatech.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7798","7923, 7943","$0.00","Static program analysis provides foundational and practical techniques to help build reliable and secure software. Context-free language (CFL) reachability has been widely adopted for specifying program analysis problems. However, little foundational progress exists on advancing the CFL-reachability framework itself. To support precise and scalable program analyses, an expressive, accessible class of formal language reachability is needed to bridge fundamental formal language research and practical analysis-based tool development. The project's novelties are twofold: a new powerful formalism for specifying program analyses, and algorithms and techniques for realizing a practical framework based on this formalism. The project's impacts are deepened knowledge on and improved capabilities for building precise and scalable static analyses, as well as practical analyses for improving software reliability and security.<br/><br/>This project will explore linear conjunctive language (LCL) reachability as a new static analysis formalism.  In contrast to CFLs, LCLs are closed under all set-theoretic operations and can also be efficiently recognized in quadratic time.  A significant number of advanced program analyses need to match properties described by multiple CFLs simultaneously. LCLs can precisely express many such properties, while CFLs cannot because they are not closed under intersection. Thus, LCL reachability offers a novel perspective in specifying and realizing program analyses. The investigators' initial work on LCL-reachability has shown considerable promise, leading to both more precise and orders of magnitude more scalable alias and taint analysis, two widely-used analyses.  This project aims to fully exploit LCL-reachability's potentials by developing a unified solution for specifying program analysis problems in LCL and implementing novel data structures that support efficient LCL-reachability algorithms. It focuses on (1) theoretical development of the LCL-reachability formulation, (2) efficient algorithms for computing LCL-reachability, and (3) generalizing to practical program analyses. If successful, this project will significantly advance the state-of-the-art in software analysis and verification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1651881","CAREER: Application-specific Power Management","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2017","02/11/2019","John Sartori","MN","University of Minnesota-Twin Cities","Continuing grant","Almadena Chtchelkanova","01/31/2022","$295,252.00","","jsartori@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","1045, 7942","$0.00","A large number of existing and emerging computing applications, including the internet of things, sensor networks, wearable electronics, and biomedical devices, have ultra-low-power requirements. In the low-power embedded systems used by these applications, energy efficiency is the primary factor that determines critical system characteristics such as size, weight, cost, reliability, and lifetime. Existing power management techniques for these systems trade off performance to reduce power. However, given the stringent power and energy constraints of emerging and existing low-power systems, solutions that sacrifice performance to reduce power may be unacceptable. Thus, this research focuses on novel opportunities to reduce power without reducing performance, thereby providing free power and energy savings for emerging low-power systems, and in turn, reducing their size, weight, and cost, and increasing their reliability and lifetime. This research also provides a non-intrusive way to significantly improve the energy efficiency of existing systems without sacrificing any performance or re-designing system hardware or software. Considering the sheer number of low-power processors being produced, their importance for future technologies, and the stringent power and energy constraints of these systems, saving power in such systems can have a significant impact. In addition to the technological impacts of this research, the project will contribute several other benefits, including new project-based research opportunities for undergraduate students and students from underrepresented groups, community outreach to K-12 students and other members of the university and community at large through public project showcases, and global outreach in the form of an educational initiative in Kenya on improving best practices in farming with IoT technology. Data characterizing the impacts of project activities on student learning outcomes will be used to improve the integration of research and educational activities at the PI?s institution.<br/><br/>The research contributions of this project stem from the development of novel techniques for hardware-software co-analysis that can identify the maximal set of hardware resources that an application can use in a processor, irrespective of application inputs. The results of such co-analysis can be used to eliminate any power expended by resources that an application can never use. This novel co-analysis approach can be leveraged to enable a suite of automated application-specific power management techniques, including application-specific timing analysis, which identifies the longest paths that an application can exercise in a processor and determines an application-specific lower-than-nominal minimum operating voltage that is guaranteed to be safe for the application; application-specific power domain formation and power gating, which can provide opportunities to power gate larger areas of logic for longer periods of time than state-of-the-art power gating techniques; application-specific peak power and energy management, which guarantees application-specific bounds on a design?s peak power and energy requirements and enables application-specific sizing for energy storage and harvesting components; and application-specific thermal management, which can identify and avoid hotspots, prevent thermal emergencies, and perform temperature-aware scheduling for a given application."
"1561216","EAGER: Fine-Grained Software Power Prediction and Its Application on Power Management of Heterogeneous Multicore Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2016","01/29/2016","Weisong Shi","MI","Wayne State University","Standard Grant","Almadena Chtchelkanova","07/31/2019","$299,928.00","","weisong@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","7798","7916, 7942","$0.00","Power dissipation and energy consumption are major cost considerations across modern computer systems including data center design, enterprise level server design, battery life management on a smart phone, and circuit layout on a microprocessor. There are numerous approaches that address the hardware aspect of the problem. However, software-behavior-driven power management is not well addressed and understood.<br/><br/>SPONGE, a fine-grained function level software power prediction mechanism, is proposed to dynamically apply proper power saving mechanisms to improve power efficiency and schedule suitable workloads to heterogeneous cores. The PI is proposing a new metric, Power Efficiency of Function (PEF), to evaluate the power efficiency of software at fine granularity. Based on PEF, a software-behavior-driven power management scheme is introduced to improve system power efficiency at runtime. The power management scheme applies power saving mechanisms including thread mapping and voltage and frequency scaling, driven by program characteristics based on the software power prediction of the target platform. PEF enables a quantitative measurement of the ratio of useful work done to resources utilized. By involving software in the loop of power management, it is possible to provide a fine-grained power management scheme with a seamless connection between software and operating systems, especially for power-aware scheduling on heterogeneous multicore architectures."
"1822989","SPX: Collaborative Research: Enabling Efficient Computer Architectural and System Support for Next-Generation Network Function Virtualization","CCF","SPX: Scalable Parallelism in t","08/01/2018","06/19/2018","Tao Li","FL","University of Florida","Standard Grant","Matt Mutka","07/31/2022","$520,000.00","","taoli@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","042Y","026Z","$0.00","Network Function Virtualization (NFV) has been widely adopted by telecommunication and internet service providers for greater performance, flexibility, and adaptability, and is treated as the most promising technology for the upcoming fifth generation (5G) wireless networks. However, ensuring that consolidated next-generation NFV workloads can efficiently run on current, commercially available servers and systems while maintaining optimal server/network utilization remains a challenge. The main reason is that existing solutions only serve as layer-specific optimizations. Due to the loose-coupled optimizations across the system and architectural layers, these solutions lack the holistic and synergistic view to guarantee the performance, resilience, and elasticity posed by the features of 5G NFV. This project aims to optimize the efficiency of consolidation of 5G NFV on commercially available server architectures and systems. The contributions of this project are: (1) rethinking the mechanisms employed in various layers of current NFV deployment and optimization, and (2) re-architecting the abstractions between the layers and applications. The impacts of this project will open the door for a new class of efficient scalable computing platforms for next-generation NFV in the 5G era. This project will also contribute to society through engaging under-represented groups, research infrastructure/tools/benchmarks dissemination for education and training, and technology transfer to industries.<br/><br/>This project proposes to develop: system-wide profiling tools and an automatic, architectural statistics-aware NFV workloads orchestration and benchmarking framework; new techniques that allow NFV applications to leverage virtualized graphic processing units (GPU), and that improve the scheduling of data movement between GPU and smart network interface cards (NICs); new abstractions that allow NFV applications and building blocks to leverage emerging offloading techniques (e.g. smart NIC and GPU remote direct memory access) and a novel architecture to improve the consolidation efficiency, parallelism, and scalability; and novel algorithms and abstractions for operating systems and accelerators to improve the thread, cache and memory management and cross-layer parallelism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1702980","SHF: Medium: Collaborative Research: Machine Learning Enabled Network-on-Chip Architectures Optimized for Energy, Performance and Reliability","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2017","07/25/2017","Ahmed Louri","DC","George Washington University","Continuing grant","Yuanyuan Yang","05/31/2020","$449,955.00","","louri@email.gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","7798","7924, 7941","$0.00","Network-on-Chip (NoC) architectures have emerged as the prevailing on-chip communication fabric for multicores and Chip Multiprocessors (CMPs). However, as NoC architectures are scaled, they face serious challenges. A key challenge in addressing optimized NoC architecture design today is the plethora of performance enhancing, energy efficient and fault tolerant techniques available to NoC designers and the large design space that must be navigated to simultaneously reduce power, improve reliability, increase performance and maintain QoS. <br/><br/>This research proposes a new cross-layer, cross-cutting methodology spanning circuits, architectures, machine learning algorithms, and applications, aimed at designing energy-efficient, reliable and scalable NoCs. This research will result in (1) novel cross-layer design techniques that take a holistic approach of simultaneously reducing power consumption, while still achieving reliability and performance goals for NoCs, (2) a fundamental understanding of the use of hardware-amenable ML for NoC design optimization, (3) software and hardware techniques for monitoring and collecting critical data and key design parameters during network execution to optimize NoC design, and (4) modeling and simulation tools that will improve the architecture community?s design methodologies for evaluating scalable NoCs. The proposed research bridges a very important gap between hardware architects who design power management and fault tolerant techniques at the circuit and architecture level and machine learning scientists who develop predictive and optimization techniques. Due to its cross-cutting nature, the proposed research has the potential to significantly transform the design of next-generation CMPs and System-on-Chips (SoCs) where complex decisions have to be made that affect the power, performance and reliability. The research will also play a major role in education by integrating discovery with teaching and training. The PIs are committed and will continue to expand on outreach activities as part of the proposed project by making the necessary efforts to attract and train minority students in this field."
"1565273","SHF: Medium: Collaborative Research: Scaling On-chip Networks to 1000-core Systems using Heterogeneous Emerging Interconnect Technologies","CCF","SOFTWARE & HARDWARE FOUNDATION","08/10/2015","06/12/2017","Ahmed Louri","DC","George Washington University","Continuing grant","Yuanyuan Yang","07/31/2019","$480,000.00","","louri@email.gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","7798","7924, 7941","$0.00","Power dissipation has become a fundamental barrier to scaling computing performance across all platforms from handheld, embedded systems, to laptops, to servers to data centers. Technology scaling down to the sub-nanometer regime has aided the growth in transistors per chip that has made multi-core architectures a power-efficient approach to harnessing parallelism and improving performance. The computing capabilities of these multi-core architectures can be unleashed only if the underlying Network-on-Chip (NoC) connecting the cores can provide the required bandwidth within the power budget of the chip. However, the design of power-efficient, low-latency and high-bandwidth NoCs using traditional metallic interconnects that can scale to 1000 cores and beyond, is proving to be a significant challenge of enormous proportions. Research has shown that emerging technologies such as photonics and wireless have the potential to alleviate the critical bandwidth, power, and latency challenges of future NoCs. However, hybrid NoC designs taking advantages of both photonics and wireless technologies have not been explored.<br/> <br/>This research proposes to lay the groundwork for completely re-thinking the NoC design and proposes to explore heterogeneity of emerging interconnect technology for designing performance scalable, and power-efficient NoCs. The overall objective is to combine multiple technologies to achieve our challenging goals of (1) scalability to 1000 cores, (2) power efficiency of at least a 50% power reduction as compared to the state-of-the-art metallic interconnects, and (3) high bandwidth and low latency across a wide variety of applications. First, at the architecture level, optics will be deployed for short-range (< 100 cores) to improve local communication and wireless for long-range communication in order to scale the number of cores to 1000 by providing sufficient global bandwidth. Second, at the circuit level, hybrid transceiver architectures will be explored to integrate novel ultra-low power wireless circuits based on SiGe/BiCMOS technology with optical waveguides and ring-resonators to provide the large bandwidth desired for kilo-core designs. Furthermore, wireless communication requirements will be addressed by designing mm-wave/THz frequency broadband and directional antennas based on advanced 3D printing technology. This proposal describes a transformative and viable approach combining technology, architecture, algorithms and applications research for designing scalable and energy-efficient NoCs. The cross-cutting nature of this research will foster new research directions in several areas, spanning technology/energy-aware NoC design, novel computer architectures, and cutting-edge modeling and simulations tools for emerging technologies."
"1628926","XPS: FULL: Hardware Software Abstractions: Addressing Specification and Verification Gaps in Accelerator-Oriented Parallelism","CCF","Exploiting Parallel&Scalabilty","09/01/2016","06/28/2016","Sharad Malik","NJ","Princeton University","Standard Grant","Anindya Banerjee","08/31/2020","$875,000.00","Margaret Martonosi","sharad@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8283","","$0.00","Given slowdowns in semiconductor technology scaling, it has become increasingly challenging to maintain processor performance scaling at acceptable power constraints. In response, microprocessors increasingly use complex architectures with heterogeneous parallelism and specialized compute units known as accelerators.  Accelerators provide high compute performance at reduced power/energy by avoiding the overhead of instruction-programmability. The key challenge, however, is that unlike traditional microprocessor CPUs, accelerators have no durable, portable instruction set architecture (ISA), and instead are programmed via drivers or library APIs. These increase the effort of porting accelerator-oriented programs to other platforms with similar functionality but different implementations. The increased effort has serious consequences for software cost. Furthermore, the fact that accelerators have no formal, durable ISA causes increased verification complexity at a time when it is already the limiting factor in the design of future computing platforms. The intellectual merits of this work are that the research is developing Instruction-Level Abstractions (ILAs) that extend the ISA concept to accelerators in order to address these programming and verification challenges. ILAs offer a formal and high-level summary of the visible state updates that an accelerator will perform on each invocation.  The project?s broader significance and importance are the work?s ability to impact industry designs of future accelerator-based computing platforms and thereby help sustain the US computing industry.<br/><br/>There are two components to an ILA: specifying the state updates, and specifying the Memory Consistency Model, i.e., the permitted ordering of state updates relative to other parallel compute elements. The research develops ILA methodologies that are (i) uniform across accelerators, (ii) symmetric with the ISA of instruction-programmable processors and (iii) unified across both computation (state change) and memory (data/storage state update) abstractions. To show the value of ILAs, the research develops: (i) ILA specification mechanisms for a rich set of accelerators, (ii) synthesis techniques and tools for generating these ILAs automatically, (iii) verification techniques and tools that check these abstractions against implementations and (iv) further tools enabled by ILAs including full-system architectural simulation. Through these efforts, this work addresses fundamental software portability and verification gaps in the design and deployment of accelerator-oriented systems."
"1725456","SPX: Collaborative Research: Ula! - An Integrated Deep Neural Network (DNN) Acceleration Framework with Enhanced Unsupervised Learning Capability","CCF","SPX: Scalable Parallelism in t","09/01/2017","07/22/2017","Yiran Chen","NC","Duke University","Standard Grant","Yuanyuan Yang","08/31/2021","$520,000.00","Hai Li","yiran.chen@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","042Y","026Z","$0.00","In light of very recent revolutions of unsupervised learning algorithms (e.g., generative adversarial networks and dual-learning) and the emergence of their applications, three PIs/co-PI from Duke and UCSB form a team to design Ula! - an integrated DNN acceleration framework with enhanced unsupervised learning capability. The project revolutionizes the DNN research by introducing an integrated unsupervised learning computation framework with three vertically-integrated components from the aspects of software (algorithm), hardware (computing), and application (realization). The project echoes the call from the BRAIN Initiative (2013) and the Nanotechnology-Inspired Grand Challenge for Future Computing (2015) from the White House. The research outcomes will benefit both Computational Intelligence (CI) and Computer Architecture (CA) industries at large by introducing a synergy between computing paradigm and artificial intelligence (AI). The corresponding education components enhance existing curricula and pedagogy by introducing interdisciplinary modules on the software/hardware co-design for AI with creative teaching practices, and give special attentions to women and underrepresented minority groups.<br/><br/>The project performs three tasks: (1) At the software level, a generalized hierarchical decision-making (GHDM) system is designed to efficiently execute the state-of-the-art unsupervised learning and reinforcement learning processes with substantially reduced computation cost; (2) At the hardware level, a novel DNN computing paradigm is designed with enhanced unsupervised learning supports, based on the novelties in near data computing, GPU architecture, and FGPA + heterogeneous platforms; (3) At the application level, the usage of Ula! is exploited in scenarios that can greatly benefit from unsupervised learning and reinforcement learning. The developed techniques are also demonstrated and evaluated on three representative computing platforms: GPU, FPGA, and emerging nanoscale computing systems, respectively."
"1718551","CIF:Small: Covariance Arbitrage: Unlocking the Full Value of Correlation Diversity in Multiuser Wireless Networks","CCF","COMM & INFORMATION FOUNDATIONS","12/15/2017","06/26/2017","Aria Nosratinia","TX","University of Texas at Dallas","Standard Grant","Phillip Regalia","11/30/2020","$399,318.00","","aria@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7797","7923, 7935, 7936","$0.00","In many emerging wireless communication technologies, the acquiring and transport of information about the state of the wireless channels has evolved into a first-order issue in the design and operation of the system. A prime example is observed in massive MIMO, especially the frequency-division duplex variety. This requires careful handling of channel resources that are used for training versus data transmission. The project is dedicated to a careful analysis of phenomena involving channel spatial correlations and synthesis of novel solutions that benefit from them, leading to more efficient channel training and transmission especially in scenarios where said efficiency can have a critical impact on wireless system performance. The research is complemented by educational and outreach activities, including training of graduate and undergraduate students.<br/><br/>In massive MIMO, scattering properties of physical channels produce rank-deficient covariance matrices, opening the door to a number of interesting proposals for recapturing efficiency of channel training notably to use medium- to long-term covariance eigenspaces for pre-beamforming. The variation between covariance properties of wireless nodes also occurs in scenarios other than massive MIMO. The proposed activity is dedicated to the building of a theory of correlation diversity in wireless networks animated by the idea that, much like a trader's arbitrage, it is possible to profit from differences of covariance eigenspaces, either via overlap or via separation of eigenspaces. The proposed activity aims to open a wider domain of applicability than present covariance-based techniques whose best gains are under certain antenna configurations, number of receivers, and covariance rank behavior. The proposed activity also ties in with close counterparts that are found in the time- and frequency-domain, providing an opening to a comprehensive theory of covariance and coherence disparity."
"1563688","SHF: Medium: Collaborative Research:Materials authentication using nuclear quadrupole resonance spectroscopy","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","07/15/2016","09/16/2018","Soumyajit Mandal","OH","Case Western Reserve University","Continuing grant","Sankar Basu","06/30/2020","$650,000.00","Hope Barkoukis","sxm833@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","2878, 7798","7798, 7924, 7945","$0.00","Counterfeit and substandard pharmaceuticals, dietary supplements, and food items containing extremely harmful contaminants have emerged as a major problem worldwide for the health industry. High-value items such as packaged medicines, which are often sold online through untrusted supply chains, are particularly prone to fraud. This project aims to develop and test novel low-cost materials authentication techniques that would enable end users to easily and reliably verify the chemical composition of medicines and dietary supplements. The widespread adoption of the proposed authentication technology could have an impact on public health by significantly enhancing the security of the supply chain for pharmaceutical and food products. <br/><br/>The proposed authentication approach is based on comparing the Nuclear Quadrupole Resonance (NQR) spectra generated by the material under test with reference spectra stored in a secure database. The atoms of about half of all the elements in the periodic table contain so-called quadrupolar nuclei that generate NQR signals. This project will focus on the spectra of nitrogen, which is found in a large majority of pharmaceutical products. The NQR spectra are highly sensitive to chemical composition and physical properties, and thus act as unique ""chemical fingerprints"" that are difficult to emulate or falsify. Low power portable electronics and an integrated system resulting in instrumentation for noninvasive, nondestructive and quantitative testing will be developed for combining sensing, software, and data collection/analysis."
"1822985","SPX: Collaborative Research: Enabling Efficient Computer Architectural and System Support for Next-Generation Network Function Virtualization","CCF","SPX: Scalable Parallelism in t","08/01/2018","06/19/2018","Yang Hu","TX","University of Texas at Dallas","Standard Grant","Matt Mutka","07/31/2022","$460,039.00","","Yang.Hu4@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","042Y","026Z","$0.00","Network Function Virtualization (NFV) has been widely adopted by telecommunication and internet service providers for greater performance, flexibility, and adaptability, and is treated as the most promising technology for the upcoming fifth generation (5G) wireless networks. However, ensuring that consolidated next-generation NFV workloads can efficiently run on current, commercially available servers and systems while maintaining optimal server/network utilization remains a challenge. The main reason is that existing solutions only serve as layer-specific optimizations. Due to the loose-coupled optimizations across the system and architectural layers, these solutions lack the holistic and synergistic view to guarantee the performance, resilience, and elasticity posed by the features of 5G NFV. This project aims to optimize the efficiency of consolidation of 5G NFV on commercially available server architectures and systems. The contributions of this project are: (1) rethinking the mechanisms employed in various layers of current NFV deployment and optimization, and (2) re-architecting the abstractions between the layers and applications. The impacts of this project will open the door for a new class of efficient scalable computing platforms for next-generation NFV in the 5G era. This project will also contribute to society through engaging under-represented groups, research infrastructure/tools/benchmarks dissemination for education and training, and technology transfer to industries.<br/><br/>This project proposes to develop: system-wide profiling tools and an automatic, architectural statistics-aware NFV workloads orchestration and benchmarking framework; new techniques that allow NFV applications to leverage virtualized graphic processing units (GPU), and that improve the scheduling of data movement between GPU and smart network interface cards (NICs); new abstractions that allow NFV applications and building blocks to leverage emerging offloading techniques (e.g. smart NIC and GPU remote direct memory access) and a novel architecture to improve the consolidation efficiency, parallelism, and scalability; and novel algorithms and abstractions for operating systems and accelerators to improve the thread, cache and memory management and cross-layer parallelism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1406810","SHF: Medium: Collaborative Research: Advanced Architectures for Hand-held 3D Ultrasound","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2014","05/20/2014","Chaitali Chakrabarti","AZ","Arizona State University","Standard Grant","Yuanyuan Yang","05/31/2019","$400,000.00","","chaitali@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7798","7798, 7924, 7941","$0.00","Much as every medical professional listens beneath the skin with a stethoscope today, we foresee a time when hand-held medical imaging will become as ubiquitous; ?peering under the skin? using a hand-held imaging device.  Hand-held imaging is not only a matter of convenience; moving the imaging device to the patient (rather than a critical patient to an imaging suite) has been shown to improve clinical outcomes.  Moreover, the portability of hand-held systems can make advanced imaging available to traditionally underserved populations in the rural and developing world.  Today, hand-held imaging is possible with compact, battery-operated ultrasound devices.  However, existing hand-held ultrasound systems produce a low-resolution two-dimensional view within the patient, and fall far short of the image quality possible in state-of-the-art non-portable ultrasound systems.  These larger systems can produce real-time 3D ultrasound images, drastically improving system ease of use, and have already been demonstrated to improve diagnostic efficiency.  Moreover, direct 3D image acquisition enables new diagnostic capabilities that are difficult or impossible to accomplish with 2D, such as measuring volumetric blood flow.  However, forming 3D ultrasound images requires over 5000 times more computing horsepower than comparable 2D imaging.  Because it is in close contact with human skin, an ultrasound scan head must operate within a tight power budget (similar to that of a cell phone) to maintain safe temperatures.  Developing a 3D ultrasound imaging system within this tight power budget requires innovation both in signal processing and computer architecture techniques.<br/><br/>This project will develop a new hardware architecture for 3D hand-held ultrasound that leverages co-design of hardware and beamforming algorithms, three-dimensional die stacking, massive parallelism, and streaming data flow, to enable high-resolution 3D ultrasound imaging in a hand-held device.  The project focuses on three medical ultrasound applications areas: (i) image quality enhancements for general imaging applications, such as abdominal imaging; (ii) advanced 3D motion tracking; and (iii) high-frame-rate 3D flow tracking for cardiac applications.  The proposed research program focuses on hardware acceleration for specific, novel applications of diagnostic ultrasound targeting heart disease and Chronic Obstructive Pulmonary Disease, respectively the 1st and 3rd leading causes of death in the United States.  Project innovations will be demonstrated and evaluated using an FPGA prototype to reconstruct images of physical phantoms captured with existing ultrasound probes."
"1718560","CIF:  Small:  Understanding Microbial Signaling Through a Communication Theoretic Lens","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","07/28/2017","Urbashi Mitra","CA","University of Southern California","Standard Grant","Phillip Regalia","03/31/2019","$180,000.00","","ubli@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","7923, 7935, 9102","$0.00","Bacteria are single-celled organisms wherein the intracellular components such as proteins, DNA and metabolites do not exist in individual components, but are contained by the cell membrane. They constitute some of the earliest forms of life (over three billion years old), and have an aggregate biomass that is larger than that of all animals and plants combined,  numbering over five nonillion (5 x 10^{30}).  Despite their simplicity, the operations and interactions of bacteria are not fully understood, yet microbial communities play a significant role in bioremediation, plant growth promotion, human and animal digestion, disease and drive elemental cycles, the carbon-cycle and the cleaning of water. Thus, it is of significant interest to further understand microbial populations. An interdisciplinary approach toward the understanding of cooperative and anti-cooperative behavior in microbial populations with a focus on signaling methods such as molecular diffusion and electron transfer will be undertaken. The research objective is to investigate biological group behaviors that exploit a variety of coupling mechanisms in order to form structures. In particular, microbial communities that are able to form structures at different scales will be examined. Problems relevant to two key applications motivate the research: microbial fuel cell optimization and infection initiation and suppression and the relationship to quorum sensing. In quorum sensing, when the concentration of a compound produced by the bacteria exceeds a threshold, the bacteria express new genes leading to new community behavior such as luminescence or infection.<br/><br/>The hope is that the fruits of the research will result in novel bio-inspired systems that naturally rely on principles of multi-modal sensing and different modalities of control and environmental conditions. More ambitiously, design methods by which structures in microbial communities can be induced are sought. Ongoing collaborations with biophysicists that have been instrumental in model design and validation via experimental data will be leveraged. Given the expertise of key collaborators, two strains of bacteria will be the focus:  Pseudomonas aeruginosa and Shewanella oneidensis MR-1. In particular: (1) can one optimize the operation of microbial fuel cells exploiting electron transfer in bacterial populations and (2) can one design strategies to understand quorum sensing and potentially prevent infection without the use of antibiotics? Key questions to address include assessing what is the minimal amount information needed in order for distributed entities to form desired structures and geometries as well as engage in desired behaviors. Bacteria have limited capabilities and the distinctions between communication, respiration, and ingestion can often be minimal. These activities can provide either explicit or implicit avenues of communication and thus can exact a control.   Specific problems include: molecular modulation design, bacterial cable formation via multi-terminal communication methods, models for quorum sensing including game theoretic approaches and multicellular interaction."
"1512611","SHF: Medium: Fiat: Correct-by-Construction and Mostly Automated Derivation of Programs with an Interactive Theorem Prover","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2015","07/05/2016","Adam Chlipala","MA","Massachusetts Institute of Technology","Standard Grant","Anindya Banerjee","08/31/2019","$816,016.00","","adamc@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7798","7924, 7943, 9251","$0.00","Title: SHF: Medium: Fiat: Correct-by-Construction and Mostly Automated Derivation of Programs with an Interactive Theorem Prover<br/><br/>To scale to ambitious software-development tasks, programming languages must provide features for abstraction and modularity. Large advances in programming productivity have often come via new features of that kind. This project investigates new program-structuring ideas based fundamentally on machine-checked mathematical proofs. More specifically, through the design of a prototype system Fiat within the Coq proof assistant, the project studies how to derive efficient programs automatically from logical specifications. Programmers may package new notations and associated styles of automation as libraries, and a single program may mix notations, automatically benefiting from the combination of all of their associated automation for deriving efficient programs. In this way, Fiat makes it possible to split a program into parts for functionality and performance, with strong guarantees that bugs in the performance parts can never violate the requirements in the functionality parts. The intellectual merits are widely applicable new ideas in modular program structuring, with strong formal guarantees of correctness. The project's broader significance and importance are based on the potential to improve programmer productivity dramatically, for software projects in a wide variety of contexts; and the project also studies how the idea of mostly automated refinement from specifications can be integrated into introductory programming and discrete-math classes, to drive home the value of logical notation in programming.<br/><br/>The primary case-study domain in the project is practical Internet servers, such as for domain-name lookup or delivery of electronic mail. The goal is to develop Fiat versions of these key services, deriving efficient executable code automatically. Past work on deriving data layers from specifications in the style of SQL is being extended, in addition to exploration of other domains for specification and automated derivation, such as synthesis of parsers from grammars, to use for the protocols that servers speak, the configuration files that they read, etc. Beyond studying how such new libraries may be constructed and composed, the project also investigates how to push the synthesis process to lower abstraction levels than in our prototype implementation, which generates functional programs. The improved Fiat system will derive assembly programs, enabling choice of more effective optimizations thanks to more direct control of machine resources, integrating with the Bedrock Coq library for verified multilanguage programming."
"1714593","SHF: Small: Efficient Verification of Nonlinear Arithmetic","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2017","04/25/2018","Paul Beame","WA","University of Washington","Standard Grant","Nina Amla","08/31/2020","$458,000.00","","beame@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7798","7798, 7923, 8206, 9251","$0.00","In recent decades, formal methods have produced many successes in proving  properties of hardware and software artifacts. However, the lack of general efficient methods to verify designs that involve nonlinear arithmetic --   arithmetic that involves integer multiplication -- has been a major impediment to the effectiveness of verification methods in a broad array of applications. Among many other applications, computations that depend on nonlinear arithmetic are at the heart of CPU designs, cryptography, and structures for efficient storage of data for easy retrieval. This project is focused on achieving efficient verification of hardware and software designs that include nonlinear arithmetic.  <br/><br/>This project explores methods for extending the most widely effective class of current verification methods, which at their heart use Boolean satisfiability (SAT) solvers, to handle designs that include integer multiplication. The investigators have shown that certain properties of integer multiplication circuits that have been conjectured to be representative of the difficulty of verifying nonlinear arithmetic can be efficiently verified in systems of inference that capture the capabilities of state-of-the-art SAT solvers.  This project leverages and extends these theoretical insights to develop practical methods that will succeed in a broad range of verification tasks for software and hardware artifacts that use nonlinear arithmetic. The research results will be widely disseminated in open source tools for people to use for verification leading to software and hardware with a higher level of trustworthiness, including in safety-critical applications that people rely on every day."
"1527867","AF: Small: Distributed Algorithmic Foundations of Dynamic Networks","CCF","ALGORITHMIC FOUNDATIONS","09/01/2015","07/20/2015","Gopal Pandurangan","TX","University of Houston","Standard Grant","Tracy J. Kimbrel","08/31/2019","$400,000.00","","gopalpandurangan@gmail.com","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","7796","7923, 7934","$0.00","The overarching goal of this project is to significantly advance the state of the art in the algorithmic foundations of distributed computing for dynamic networks.  In a dynamic network, the topology of the network---both nodes (representing processors/endhosts) and communication links---changes continuously over time.  Modern networking technologies such as peer-to-peer networks, overlay networks, and ad hoc wireless and mobile networks, are inherently very dynamic; furthermore, they are resource-constrained, unreliable, and vulnerable to attacks.  Distributed/decentralized algorithms are critical to the  efficient operation of large-scale communication networks, e.g., distributed shortest paths algorithms are used for routing in the Internet.  Till recently, much of the distributed algorithmic theory developed over the last three decades has focused mainly on static networks; as such its results do not apply to  dynamic networks.  This necessitates the development of a solid theoretical foundation for robust, secure, and scalable distributed computing for dynamic networks.  Such a foundation is critical  to realize the full  potential of these large-scale   networks that  have a  wide variety of applications including communication,  data storage and retrieval, environment monitoring, electronic commerce, resource distribution and sharing, and search.<br/><br/>The project will develop a rigorous theoretical foundation for  distributed computing in highly dynamic networks.  In particular, it will develop and analyze  distributed algorithms  that scale well to very large-sized networks, are highly robust to dynamic changes and large-scale failures, and are secure against malicious participants in the network.  The project will result in an algorithmic toolkit which will provide the building blocks for performing distributed computation in dynamic networks, besides providing algorithms with performance guarantees and theoretical benchmarks for practitioners.  The project has  the potential to impact the design and engineering of topologically-aware and self-regulating networks, i.e.,  networks that can measure, monitor, and regulate themselves in a decentralized fashion. The PI plans to develop a new course and a textbook on distributed network algorithms that is closely related to the research undertaken. This research will actively involve postdoctoral researchers, graduate students,  and undergraduate students.<br/><br/>The project has two key research goals. First, it will design and analyze scalable and robust distributed algorithms for fundamental distributed computing problems including agreement, leader election, storage and search, and routing. These problems are basic building blocks in distributed computing and are widely used. Motivated by fault-tolerance and security considerations, the project will study the above problems in an adversarial dynamic setting, that can also include the presence of Byzantine (malicious) nodes which may try to foil the distributed algorithm. The project will also study lower bounds on the performance of distributed algorithms including the amount of dynamism that can be tolerated. Second, it will develop fully-distributed algorithms for computing key global metrics of a network and to maintain dynamic networks with desirable properties. This addresses an important issue that is complementary and also critical to the first goal, i.e., how to measure basic parameters of a dynamic network such as its size, connectivity properties, conductance, average degree and other node-related statistics.  A related goal is to construct and maintain dynamic networks with good topological properties such as low diameter, high connectivity, and high conductance. In both the above research goals, the key challenge is to design scalable distributed algorithms that are robust and fault-tolerant even under a high amount of dynamism and the presence of a large amount of Byzantine nodes. The project will build on and significantly extend the distributed algorithmic framework for dynamic networks that was recently developed by the PI and his collaborators."
"1814756","SHF: Small: Ferroelectric Transistor based Coupled Oscillators for Non-Boolean Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","01/01/2017","11/28/2017","Sumeet Gupta","IN","Purdue University","Standard Grant","Sankar Basu","06/30/2020","$450,000.00","","guptask@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","7923, 7945","$0.00","Since conventional computing architectures based on Boolean processing of inputs may be sub-optimal for tasks involving recognition and sensory processing, exploration of new non-Boolean computing technologies has the potential to assume an important role. This project aims at addressing this issue by investigating new post-CMOS devices and circuits, and extensively analyzing their implications at the application level. The research outcomes are likely to have a direct impact on critical applications such as computer aided diagnosis, speech/face recognition, data classification and resource allocation, and benefit several areas such as healthcare, defense, and security. Moreover, power savings achieved with the proposed techniques may translate to longer battery life for mobile systems facilitating a richer user experience and ultra-low power processing. The project will enhance graduate and undergraduate education by integrating the research outputs in the curriculum. The participation of under-represented groups will also be encouraged.<br/><br/>The proposed research utilizes the emerging technology of ferroelectric transistor to design novel low power and compact oscillators by translating the inherent non-linearity of the ferroelectrics to sustained oscillations. A multitude of such oscillators, whose dynamics can be controlled, are coupled to realize a non-Boolean computing fabric. The extent of synchronization amongst the oscillators provides information about the degree of match/mismatch amongst different signals, enabling efficient decision-making for applications such as pattern matching, motion sensing and others. The proposed approach involves extensive co-design of devices and circuits to harness the unique features offered by the ferroelectric transistors for enhancing the energy- and area-efficiencies of the coupled oscillators.  The research effort spans different levels of design abstraction including (i) the exploration of novel ferroelectric based devices amenable for the proposed approach, (ii) new oscillator designs, and (iii) a comprehensive application-level analysis. Extensive benchmarking of ferroelectric based oscillators will be performed against previously explored oscillators to comprehensively quantify their benefits and trade-offs. In addition, detailed investigation establishing the relationships between the device-circuit characteristics and the inherent properties of the ferroelectric materials will be carried out to uncover the fundamental attributes of the proposed oscillators and their rich oscillation dynamics."
"1614562","AF: Small: Extending algorithms for topological notions of similarity","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","01/25/2018","Erin Chambers","MO","Saint Louis University","Standard Grant","Rahul Shah","08/31/2019","$301,821.00","","erin.chambers@slu.edu","221 N. Grand Blvd.","St Louis","MO","631032006","3149773925","CSE","7796","7923, 7929, 9102, 9150, 9251","$0.00","How would you measure the closeness of two signatures or two clay<br/>figurines?  This project explores the mathematics and computation of<br/>measuring similarity between curves or 3-dimensional objects --<br/>whether to compare GPS tracking data to road maps, or medical scans of internal organs to<br/>reference models of healthy organs. While some well understood comparison methods simply<br/>look at how close the objects are, this project aims to design more<br/>sophisticated measures that take into account the underlying structure<br/>(or topology) of the curves and surfaces.  The PI plans to collaborate<br/>with applications areas that use these measurement algorithms to<br/>develop measures that best fit the areas, in addition to continuing<br/>her work in a larger network of shape analysis and computational<br/>topology communities, including organizing workshops focused at<br/>mentoring junior women in these areas.<br/><br/><br/>This project generalizes notions of similarity from curves to graphs, meshes, or<br/>3-manifolds.  The primary problems and techniques come from the<br/>emerging field of computational topology, which combines algorithms<br/>and computational geometry with mathematical foundations and tools<br/>from the area of topology.  In particular, the PI proposes to examine<br/>fundamental topological notions of similarity between curves and<br/>surfaces in some ambient space, based on computing optimal homotopies, homologies, or isotopies. Each<br/>possibility offers a different notion of what it means for two things<br/>to be equivalent in the ambient space, and each can be optimized based<br/>on the notions of area, ""height,"" or ""width."" While several initial<br/>algorithmic results on these measures have been published, there are<br/>many open questions that remain. In addition, recent mathematical<br/>developments indicate many potentially tractable and feasible areas<br/>that are yet to be explored from the algorithmic perspective. Some of<br/>these measures are likely to be hard to compute, which is of interest<br/>to the theoretical community, and approximation or fixed parameter<br/>tractable algorithms may prove practical in applications areas. The<br/>project will also include collaborations in applications areas for<br/>these measures, in order to better evaluate their utility."
"1617892","SHF: Small: S3: Statistical and Structural Analysis for Spreadsheets","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2016","08/26/2016","Emery Berger","MA","University of Massachusetts Amherst","Standard Grant","Sol J. Greenspan","08/31/2019","$347,400.00","","emery@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7798","7923, 7944","$0.00","Spreadsheets are the most commonly used programming environment in the world; there are more than 750 million users of Microsoft Excel alone. Spreadsheets are widely used in government, scientific, and financial settings; over 95% of US firms use them for financial reporting and 85% use them for budgeting and forecasting. Unfortunately, errors are endemic to spreadsheets; a recent study found an error rate of over 95%. Spreadsheet errors have had catastrophic consequences, leading to losses of billions of dollars. This project uses automatic analysis techniques designed specifically for spreadsheets to (a) automatically detect and help prevent errors in spreadsheets, dramatically increasing the reliability of their calculations, (b) reduce the risks of serious mistakes, and (c) potentially save the economy millions if not billions of dollars.<br/><br/>This project develops statistical and structural analyses for spreadsheets (S3). Spreadsheets have unique features that make them different from standard programming languages, and thus demand new program analyses that exploit their characteristics. S3 employs statistical analyses over the spatial and deep structure of spreadsheet formulas to identify spreadsheet cells that are highly  anomalous and thus likely to be wrong. S3 reduces the problem of finding data and formula errors to that of finding anomalous structures via a novel vector representation that combines spatial and structural information (including patterns of dependencies). Applying statistical analyses across these vectors can then identify formulas that are highly unusual in any dimension, and thus likely to be wrong. S3 operates both at an individual spreadsheet level and also incorporates learned models of spreadsheet usage from large bodies of existing spreadsheets to condition the analysis and further reduce false positive rates."
"1618868","SHF: Small: Collaborative Research: Helping Mobile App Developers Make Implementation Decisions Based on App Store Analytics","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","09/14/2016","Meiyappan Nagappan","NY","Rochester Institute of Tech","Standard Grant","Sol J. Greenspan","06/30/2019","$0.00","","mei@se.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7798","7923, 7944","$0.00","The growth of mobile apps in recent years has been aided by Frameworks, Services, and Third Party Libraries (FSTPL), which provide support for user interfaces, advertising, analytics, and other critical app functionality. These FSTPLs enhance developer productivity, improve security, make key functionality easily accessible, and modularize complex and error-prone components. Our prior work shows that there is a tradeoff between the benefits of using FSTPLs and the impact they have on end users; however, developers lack clear guidance on how to manage these tradeoffs. Best practices that can be found online are generally anecdotal and sometimes contradictory. For developers who wish to improve their apps while maintaining the use of FSTPLs, there is no way to quantify or estimate the magnitude or impact of their FSTPL related design and implementation decisions. This motivates us to investigate techniques that can help developers more accurately evaluate FSTPL tradeoffs.  The results of this investigation will advance the state of the art in software engineering methodology and education, benefiting society by leading to the development of apps with higher reliability and usability.  For methodology, this will result in techniques to help developers improve the quality of their apps.  For education it will enhance training of future software developers in using analytical techniques to drive software design and implementation decisions.<br/><br/>In this proposal the PIs will investigate techniques to help app developers evaluate their usage of FSTPLs and their impact on end users? experience. The proposed work will include the design of techniques and methodologies for quantifying the way developers use FSTPLs in their apps and correlating their usage with user ratings and reviews that will be mined from the app stores. Within this project, the PIs will focus on two thrusts. The first will be to design program analysis based techniques that can measure and quantify the usage patterns of FSTPLs in mobile apps. The second will be to perform empirical investigations by applying the program analysis based techniques on apps from app stores and using statistical analysis to understand relationships between the gathered data and various user feedback based metrics, such as the ratings of apps, to learn best and worst practices for using FSTPLs. The techniques and methodologies produced by these two thrusts will allow developers to analyze their apps and determine if their usage of FSTPLs could negatively or positively impact the user experience. The approach will provide objective and quantifiable guidance to developers to make refactoring choices, redesign components, and other such decisions about their apps. Therefore developers will be able to understand how their design and implementation choices affect the users? perception of their apps. More broadly, the proposed work will define a methodology for guiding developers in making design decisions and a way to tie these decisions to ratings based outcomes."
"1758736","CIF: Small: Taming Convergence and Delay in Stochastic Network Optimization with Hessian Information","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2017","09/20/2017","Jia Liu","IA","Iowa State University","Standard Grant","Phillip Regalia","06/30/2019","$317,896.00","","jialiu@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7797","7797, 7923, 7935","$0.00","With the rapid integration of massive amounts of data and new network devices, today's network infrastructures are being stretched to their limits. As a result, recent years have witnessed a critical need for developing fast-converging distributed stochastic network control and optimization algorithms to increase throughput and reduce delay. This research program addresses the challenge of distributed control and optimization for next generation complex network systems, where the rapidly changing network states (e.g., network topologies, channel states, queueing states, etc.) necessitate fast-convergence and low-delay in distributed optimization algorithms. Based on the investigator's recent research on network control and optimization that leverages second-order Hessian information (SOHI), this research will develop a series of new distributed algorithmic techniques that offer orders of magnitudes improvements in both convergence speed and queueing delay compared to the traditional approaches, while attaining the same provable network-utility optimality. <br/><br/>Specifically, the investigator?s research tasks in this project are organized around three inter-related research thrusts that exploit different degrees of SOHI: i) Heavy-ball-based joint congestion control and multi-path routing (partial SOHI); ii) Primal-dual interior-point second-order congestion control and multi-path routing (full SOHI); and iii) SOHI-based distributed control and optimization algorithm designs. This research project takes an integrated and holistic approach that draws techniques from areas of mathematical modeling, optimization theory, control theory, queueing theory, and stochastic analysis. The research project will not only advance the knowledge in the algorithmic design for next generation complex networks, but will also serve a critical need in the general networking research community by exploring new frontiers in SOHI-based network control and optimization."
"1423140","CIF: Small: Communication Theory Foundations and System Optimization of Femto-Caching and D2D Networks for Wireless Video","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2014","07/28/2014","Andreas Molisch","CA","University of Southern California","Standard Grant","Phillip Regalia","07/31/2019","$274,361.00","Giuseppe Caire","molisch@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","7923, 7935","$0.00","Video streaming is fast becoming the dominant source of data traffic in wireless networks. In order to avoid overburdening the cellular infrastructure, this project investigates new systems that exploit caching to provide high throughput with minimal infrastructure cost. This is pursued by exploiting the asynchronous content reuse of on-demand video, i.e., that most people watch the same videos, though at different times. Two approaches to such caching at the wireless edge ar considered, either deploying helper stations (femto-base stations with a cache, but no backhaul), or caching on user devices together with highly spectrally efficient device-to-device (D2D) communications. The investigations will consider the following main topics: (i) develop policies for optimizing video quality in wireless streaming, (ii) investigate scheduling for single-hop transmission between neighboring devices, (iii) investigate coding for D2D transmission for combining D2D with coded multicast schemes. To complement these theoretical investigations, realistic operating conditions (channels, user distributions, etc.) will be analyzed through simulations.<br/><br/>The result of this work will allow more spectrally efficient transmission of wireless video. This will increase overall video consumption, which is beneficial to the movie/entertainment industry, a key US industrial sector. Furthermore, the suggested systems can also free up many wireless resources that can be more efficiently used for other purposes."
"1448333","EAGER: Exploratory Research on the Micron Automata Processor","CCF","INFO INTEGRATION & INFORMATICS, SOFTWARE & HARDWARE FOUNDATION","08/15/2014","05/31/2018","Srinivas Aluru","GA","Georgia Tech Research Corporation","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$300,000.00","","aluru@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364, 7798","7916, 7942, 7946","$0.00","The Micron Automata Processor is a reconfigurable non-Von Neumann co-processor which can be used to simultaneously execute multiple Non-deterministic Finite Automata over one or more input data streams. The technology supports an algorithmic model and a programming abstraction that is radically different from any processor or co-processor that is in existence today. The goal of this exploratory project is to conduct early-concept research on the Automata Processor so that the research community has access to techniques and tools for utilizing the technology. <br/>The project is developing algorithmic building blocks, programming techniques, and libraries for enabling researchers and developers to take advantage of this new architectural paradigm, and designing automata-based algorithms to solve path problems in graphs as a step towards exploring general purpose use of this architecture. The project will lead to fundamental research on how to solve problems using a new computational model. Research results will be shared through publications and release of automata libraries, to enable rapid adoption of this new technology by the computing community."
"1618509","SHF: Small: Enabling Efficient Context Switching and Effective Latency Hiding in GPUs","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/27/2016","Huiyang Zhou","NC","North Carolina State University","Standard Grant","Yuanyuan Yang","07/31/2019","$330,000.00","","hzhou@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7923, 7941","$0.00","Graphics processing units (GPUs), initially designed for computer graphics, are becoming widely used for general purpose computing. This project addresses two important challenges in GPU computing. First, it investigates schemes to enable GPUs to be preempted efficiently, which is critical for GPUs to satisfy the quality of service (QOS) requirement in the cloud environment. Second, the project looks into approaches to significantly improve the latency hiding capability of GPUs. This interdisciplinary research has two practical uses, efficient preemption empowering GPUs as truly shared resource and effective latency hiding improving both the GPU performance and energy efficiency. Graduate student advising and industry collaboration are two key aspects of the project.<br/><br/>The design philosophy of GPUs is to exploit very high degrees of data-level parallelism (DLP), expressed as thread-level parallelism (TLP), to hide long instruction latency. As a side effect, GPUs feature high amounts of on-chip resources to store the contexts or the architectural states of the large numbers of concurrent threads. The large contexts result in long latency for context switching, which makes it difficult for GPUs to be truly shared in cloud servers. This research project leverages the nature of the single-instruction multiple-thread (SIMT) execution model to drastically reduce and compress the GPU context size. Software and hardware approaches are integrated to enable instruction-level preemption for GPUs to meet the QOS requirements. Fast context switching is also used to switch out stalled threads and switch in new ones such that the otherwise idle computing resources can be utilized to provide much higher latency-hiding capability. It essentially achieves higher TLP on GPUs without enlarging their critical on-chip resources."
"1525629","CIF: Small: Collaborative Research: Communicating While Computing: Mobile Fog Computing Over Wireless Heterogeneous Networks","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","08/11/2015","Osvaldo Simeone","NJ","New Jersey Institute of Technology","Standard Grant","Phillip Regalia","08/31/2019","$249,848.00","","osvaldo.simeone@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","7797","7923, 7935","$0.00","An increasing number of applications, including surveillance, medical monitoring, automatic translations and gaming, rely on the capability of mobile wireless devices to carry out computation-intensive tasks in a timely manner. This requirement conflicts with the expectation that mobile devices should run on a battery without needing frequent recharging. A promising solution to this challenge is mobile cloud computing, that is, the offloading of computation-intensive tasks to a cloud service provider by means of wireless transmission. However, the energy and latency entailed by wireless transmission may offset the potential gains of mobile cloud computing. This project proposes to tackle the outlined problem via the development of effective, low-complexity, scalable and flexible offloading strategies that operate over a mobile fog computing architecture, in which small-cell base stations are endowed with computing capabilities to offer proximate wireless access and computing. The insights gained from the successful completion of this project will be beneficial for a gamut of other exciting problem domains that require large-scale optimization, including big data mining, signal processing, machine learning, and smart grid. The research agenda is complemented by a multidisciplinary educational plan that targets both undergraduate and graduate students via hands-on learning and experimentation activities. Industrial collaboration is also envisaged through internship and co-op opportunities. <br/><br/>The inter-layer optimization of the computation and communication resources in a mobile fog computing network yields unstructured nonconvex mixed-integer problems, which are unexplored and challenging, and whose formulation depends on whether the mobile applications are splittable, i.e., divisible into subtasks that can be individually offloaded, or not. Since the problems at hand do not lend themselves to the application of existing iterative optimization techniques, such as Difference-of-Convex programming, a class of scalable and flexible solution methods with controllable convergence, complexity and overhead is introduced based on a novel successive convex approximation framework. In the case of splittable applications, the analytical and algorithmic framework is augmented by the application of message passing strategies that leverage the call graph representation of the mobile applications."
"1817200","CIF: Small: Modeling and Analysis of Microbial Signaling","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","05/30/2018","Urbashi Mitra","CA","University of Southern California","Standard Grant","Phillip Regalia","09/30/2021","$499,407.00","","ubli@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","7923, 7935, 9102","$0.00","Bacteria are single-celled organisms wherein the intracellular components such as proteins, DNA and metabolites do not exist in individual components, but are contained by the cell membrane. They constitute some of the earliest forms of life (over three billion years old), and have an aggregate biomass that is larger than that of all animals and plants combined, numbering over 5 x 10^{30}.  Despite their simplicity, the operations and interactions of bacteria are not fully understood, yet microbial communities play a significant role in bioremediation, plant growth promotion, human and animal digestion, disease and drive elemental cycles, the carbon-cycle and the cleaning of water. Thus, it is of significant interest to further understand microbial populations. An interdisciplinary approach toward the understanding of cooperative and anti-cooperative behavior in microbial populations with a focus on signaling methods such as molecular diffusion and electron transfer will be undertaken. The research objective is to investigate biological group behaviors that exploit a variety of coupling mechanisms in order to form structures. In particular, microbial communities that are able to form structures at different scales will be examined. Problems relevant to two key applications motivate the research: microbial fuel cell optimization and infection initiation and suppression and the relationship to quorum sensing. In quorum sensing, when the concentration of a compound produced by the bacteria exceeds a threshold, the bacteria express new genes leading to new community behavior such as luminescing or infection.<br/> <br/>The fruits of the research should result in novel bio-inspired systems that naturally rely on principles of multi-modal sensing and different modalities of control and environmental conditions. More ambitiously, design methods by which structures in microbial communities can be induced are sought. Ongoing collaborations with biophysicists that have been instrumental in model design and validation via experimental data will be leveraged. Given the expertise of key collaborators, two strains of bacteria will be the focus:  Pseudomonas aeruginosa and Shewanella oneidensis MR-1. In particular: (1) can one optimize the operation of microbial fuel cells exploiting electron transfer in bacterial populations and (2) can one design strategies to understand quorum sensing and potentially prevent infection without the use of antibiotics? Key questions to address include assessing what is the minimal amount information needed in order for distributed entities to form desired structures and geometries as well as engage in desired behaviors. Bacteria have limited capabilities and the distinctions between communication, respiration, and ingestion can often be minimal. These activities can provide either explicit or implicit avenues of communication and thus can exact a control.   Specific problems include: molecular modulation design without the need for timing information, bacterial cable formation via multi-terminal information theoretic methods, models for quorum sensing including decision theoretic approaches, game theory and multicellular interaction.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1526666","AF: Small: Algorithmic Foundations of Hybrid Stochastic Modeling and Simulation Methods with Applications to Cell Cycle Models","CCF","ALGORITHMIC FOUNDATIONS","07/15/2015","06/05/2017","Yang Cao","VA","Virginia Polytechnic Institute and State University","Continuing grant","Mitra Basu","06/30/2019","$399,999.00","John Tyson","ycao@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7796","7923, 7931","$0.00","Complex systems emerging from many biochemical applications often exhibit multiscale features: the systems incorporate a variety of physical processes or subsystems across a broad range of scales. A typical multiscale system may require scales with macroscopic, mesoscopic, and microscopic kinetics, deterministic and stochastic dynamics, continuous and discrete state space, fast-scale and slow-scale reactions, and species of large and small populations. These complex features present great challenges for the modeling and simulation of biochemical systems. The goal of this project is to face these challenges by developing rigorous mathematical theories and innovative numerical algorithms for hybrid modeling methods. Mathematical foundations for error analysis of hybrid methods and algorithms for partitioning a biochemical system into subsystems will be developed and applied to chemical systems in different scales. The proposed work will advance the frontier of computational methods for the simulation of complex biochemical systems and enable automatic regime switching among multiple scales in complex biochemical systems.  The numerical analysis will also contribute to general understanding of errors in discrete stochastic simulation methods. The project will support education and training of graduate and undergraduate students. The simulation methods developed in this project will be incorporated in the modeling and simulation packages JigCell and CoPaSi, all are open for public access. The curriculum material and source code modules will also be available on the university website.<br/><br/>This project is motivated by realistic modeling and simulation of a complex biological control system: the cycle of growth and division in yeast cells. The project focuses on three specific aims. The primary aim is to develop mathematical foundations for error analysis of hybrid methods. Errors will be analyzed in two ways: one approach is based on approximation of chemical master equations for linear systems, and the other is based on a Poisson-process formulation of stochastic simulation trajectories. Errors caused by different partitioning strategies and hybrid solvers will be studied corresponding to different scale regions and interactions among them. Error analysis will also help to determine parameters in partitioning strategies. The second aim is to develop algorithms for hybrid stochastic simulation of biochemical systems, mechanisms to automatically partition reactions and state variables into different scale regions, and software to efficiently simulate multiscale systems. Reactions in a biochemical system are partitioned into four regions according to reactant population and reaction propensity scales. The partitioning strategy is based on analysis of these four regions and actual scale differences. Implementation details of hybrid methods will be carefully studied to achieve high efficiency. The third aim of this project is to develop a realistic stochastic model of the budding yeast cell cycle, which will include protein interactions as well as gene and mRNA dynamics and which will be judged with respect to the phenotypes of wild-type budding yeast cells and ~120 mutant strains. Simulation results of the developed model will be compared with wet-lab experimental data. The ambitious goal is to have a detailed cell cycle model that reflects dynamics at gene and mRNA levels, accounts accurately for known probabilistic features of cell proliferation in yeast cells, and accurately predicts the aberrant behaviors of mutant strains. Algorithms, theories, and software of hybrid methods developed in this project will be applied and tested in the modeling and simulation of this complex cell cycle model."
"1527796","SHF: Small: An Asynchronous Network-on-Chip Methodology for Cost-Effective and Fault-Tolerant Heterogeneous SoC Architectures","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/21/2015","Steven Nowick","NY","Columbia University","Standard Grant","Sankar Basu","07/31/2019","$420,000.00","","nowick@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7798","7923, 7944","$0.00","One of the grand challenges of the next decade is to provide cost-effective and systematic approaches to build large-scale computing systems to meet the country's future technological needs.  Such systems will involve 100s or 1000s of processors, or more, on a single computer chip, which perform massively parallel computation tasks, and can be housed in either large data centers or even in desktop or hand-held consumer products.  Such parallel computing is the key to supporting the continued growth of social networks (texting, Facebook) and rapid Web queries, as well as solving massive scientific computation problems, such as for weather, oceanography, air traffic control, military, and security.  However, there are currently severe cost overheads in designing such systems:  huge energy usage, thermal overheating, poor delivered performance, and complexity in assembling such complex and varied components into a single chip.  Many of these bottlenecks have been exacerbated by the traditional use of a fixed-rate clock, which centrally controls all components on a chip.  The goal of this research is to explore and significantly advance one highly promising alternative solution:  to build ?plug-and-play? easy-to-assemble computer systems without any global clock, but instead through asynchronous (i.e. clock-less) assembly.  Such a solution promises much lower energy consumption, and ease of assembly, of such systems. The project will also train students in asynchronous circuit design and the general area of information and computing technologies.<br/><br/>The particular focus of this research is to develop structured digital interconnection networks for a chip, called ""networks-on-chip"", which form the ""backbone"" of recent commercial parallel computer systems.  While initial solutions with asynchronous design have been promising, they have lacked fundamental features needed to make them viable for industry.  In this proposal, several key features are developed for asynchronous networks-on-chip:  error detection and correction, performance enhancement, and automated design flow.   The approach will also be applied to a realistic cell phone application, to demonstrate the ease-of-design and cost benefits.  Taken together, this new approach promises a significant advance forward in providing the capability to assemble complex parallel computer systems needed in the future."
"1422516","SHF: Small: Pattern-Aware Design","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2014","07/10/2014","Resit Sendag","RI","University of Rhode Island","Standard Grant","Sankar Basu","06/30/2019","$450,000.00","","sendag@ele.uri.edu","RESEARCH OFFICE","KINGSTON","RI","028811967","4018742635","CSE","7798","7923, 7941, 9150","$0.00","Current processors employ aggressive prediction mechanisms to improve  <br/>performance and reduce power. To design effective optimizations, it is  <br/>increasingly important to understand and quantify a program's dynamic  <br/>behavior. However, today, most of these optimizations rely on  <br/>heuristics and are fairly ad-hoc. After some patterns are observed, a  <br/>hardware decision is made and the design space of the hardware  <br/>optimization is explored through simulation to determine the best  <br/>performing configuration. However, because the design is targeted for  <br/>observed and/or anticipated patterns, some dynamic behavior is not  <br/>captured and remains undetected.<br/><br/>This project develops a framework for quantitatively analyzing a  <br/>program's behavior in terms of regularities and patterns to provide  <br/>insights into the design of next-generation hardware prediction  <br/>mechanisms. Inspired by DNA discovery tools, the research adopts  <br/>algorithms used in string processing, compression and bioinformatics,  <br/>in order to summarize applications' dynamic data reference and branch  <br/>outcome behaviors. The findings from the methods and tools developed  <br/>in this project will have a big impact on the design and efficiency of future  <br/>hardware components. The PI will collaborate with industry partners to further  <br/>increase the broader impact. The research will engage graduate and  <br/>undergraduate students and promote the participation of  <br/>underrepresented groups."
"1617390","SHF: SMALL: Collaborative Research: Cloud Mentoring: Guiding Cloud Users for Cost Performance through Testing and Recommendation","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/21/2016","Mary Lou Soffa","VA","University of Virginia Main Campus","Standard Grant","Sol J. Greenspan","07/31/2019","$311,883.00","","soffa@cs.virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7798","7923, 7944, 9251","$0.00","Cloud computing is growing rapidly, with businesses, institutions and individuals moving their workloads to clouds. Cloud users benefit from low cost ownership, a pay-as-you-go pricing model where they only pay for the procured resource usage, and the ability to dynamically scale the resource usage up and down. However, the applications running in clouds usually experience unpredictable performance, which makes it extremely challenging for the users to choose resource configurations that meet their cost and performance requirements. This problem is further complicated as users do not have physical control over cloud computers and are forced to make their decisions based on convoluted cloud performance reports.<br/><br/>This research addresses the need to support users in achieving their cost-performance requirements as they port their applications to various cloud services. In particular, the research is embodied in an envisioned testing and recommendation system that determines proper resource management policies that meet performance and cost requirements. By taking a software-testing-based approach, the research provides solutions using only user-accessible information to satisfy user requirements, addresses the limits of static analysis techniques that rely on performance predictability.<br/><br/>Such a testing and recommendation system enables non-experts to port their applications to various clouds in a cost effective way. The novel framework, testing and recommendation approaches, data sets, and experimental infrastructure developed within the project will be released open source to advance knowledge and understanding within software engineering and cloud computing. The PIs will continue to involve students of underrepresented groups and continue their involvement in mentoring workshops for students."
"1749570","CAREER: Verifying Distributed System Implementations","CCF","SOFTWARE & HARDWARE FOUNDATION","04/01/2018","12/12/2017","Zachary Tatlock","WA","University of Washington","Continuing grant","Nina Amla","03/31/2023","$108,748.00","","ztatlock@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7798","1045, 8206","$0.00","Billions of people depend on distributed systems every day for health care, banking, transportation, and more.  Despite costly testing efforts, these complex services still fail in practice, leading to data loss and major service outages that threaten everyone's convenience, finances, and safety. This project is developing the tools and techniques necessary to verify (mathematically prove) safety and reliability for distributed systems implementations under any combination of network and machine misbehaviors. The intellectual merits are to develop compositional verification techniques where the programmer can independently prove correctness for applications and reliability for fault-tolerance components. The broader significance and importance are to provide rigorous reliability guarantees for the core computational infrastructure society depends on and to train a new generation of engineers who will create high-performance, verified distributed systems implementations.<br/><br/>This project aims to make verification tractable by developing verified system transformers which automatically wrap simple systems with fault tolerance mechanisms guaranteed to preserve equivalence. This approach separates concerns of application correctness from fault tolerance which eases proof effort and enables greater code reuse. Industrial practitioners are already using an early prototype verified system transformer as a guide in exploring designs and alternate implementations. An outcome of this project is an extensive library of such transformers covering a broad range of critical distributed system features including reconfiguration, ring maintenance, and software update.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1526877","SHF:  Small:  Layout Assessment and Optimization for Disruptive Patterning and Device Technologies","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2015","05/12/2017","Puneet Gupta","CA","University of California-Los Angeles","Standard Grant","Sankar Basu","05/31/2019","$370,000.00","","puneet@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7798","7923, 7945, 9251","$0.00","Continued scaling of semiconductor technology is essential to preserve the promise of reduced cost, more functional integration and improved performance - all within the same or smaller energy footprint for electronic systems. With integrated circuit fabrication already working at the edge of its physical limits, the semiconductor industry is likely to see several radical changes in the manufacturing and device technologies in the next few years and decades.  The broader impact of this proposal lies partly in the artifacts (software, exploration outcome data, etc.) which would enable the academia and industry communities to assess and optimize advanced semiconductor technologies. In addition to enhancing graduate curriculum and engaging undergraduate students in research, the PI will co-organize the Los Angeles Computing Circle: a summer program to expose high school students to non-traditional aspects of computing. <br/><br/>On the manufacturing front, the two radical changes that may be seen are (1) use of Extreme Ultraviolet (EUV) light in lithography which has over 10X smaller wavelength than what is used currently with associated improvement in resolution (i.e., potential to shrink transistor and wire dimensions even further); and (2) adoption of Directed Self Assembly (DSA): a scheme where desired patterns self-assemble onto the wafer with some guidance. The other disruption in electronics technology is likely to come from development of vertically oriented transistors (i.e., the current through them flow in z direction instead of x or y) which hold the promise of reduced sleep power, improved performance, smaller area and amenability to integrate heterogeneous materials. The focus of the project is to enable these disruptive technologies that will require substantial changes in design infrastructure and manufacturing-side software tools to be viable. Algorithms and software frameworks for technology development, design layout generation and manufacturability assessment will be developed."
"1751230","CAREER: Spiking Neural Circuits and Networks with Temporally Dynamic Learning","CCF","EPSCoR Co-Funding","07/01/2018","06/08/2018","Kurtis Cantley","ID","Boise State University","Continuing grant","Sankar Basu","06/30/2023","$256,896.00","","kurtiscantley@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","9150","1045, 7945, 9150","$0.00","The human brain is adept at processing vast amounts of real-world information, learning new concepts, and adapting to changing environments. Emulating the plasticity and cognitive abilities of the brain using electronics will enable new paradigms in computing not currently accessible with any digital system. Achieving this goal requires new, efficient approaches that mimic an array of complex mechanisms observed in biology. Chief among these are the rules governing changes in the strength of connections between neurons. Synaptic connection strengths are believed to be ultimately responsible for memory, reasoning, perception, and other higher-order functions. A combination of established and emerging semiconductor device and circuit technologies will be used in this project to investigate networks with biologically-realistic learning. Integrated education and outreach activities will enhance understanding and awareness of biological and electronic neural networks for middle and high school students, regionally and nationally. <br/><br/>This CAREER award supports the development of electronic spiking neural networks (SNNs) that capture the dynamic synaptic learning modalities found in the brain. Circuit blocks with short-term memory will be used in conjunction with devices that change electrical resistance over longer periods to achieve synaptic learning that depends on spiking frequency as well as individual spike times. A primary objective is the design, fabrication, and testing of a neuro-synaptic architecture with learning rates tunable over a range of milliseconds to hours. The physical system will then be used to study cognitive tasks that combine classical conditioning and spatio-temporal pattern recognition. Sensitivity of the network to device variations and control parameters will be examined in terms of pattern recognition accuracy, power consumption, and stability. Benchmarks will also be applied to quantify system performance relative to other techniques. Ideas and concepts generated by this research will ultimately advance the capabilities of intelligent machines employed for a wide variety of tasks such as sensory processing, inference and outcome prediction, anomaly detection, pattern recognition and classification, and big data analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1302179","AF: Medium: Algorithmic Foundations for Phylogenetic Networks","CCF","Genetic Mechanisms, INFORMATION TECHNOLOGY RESEARC, CROSS-EF ACTIVITIES, ALGORITHMIC FOUNDATIONS, COMPUTATIONAL BIOLOGY","04/15/2013","04/06/2016","Luay Nakhleh","TX","William Marsh Rice University","Continuing grant","Mitra Basu","03/31/2019","$1,025,725.00","","nakhleh@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","1112, 1640, 7275, 7796, 7931","1112, 7465, 7796, 7924, 7931, 8750, 9177, 9179, 9251","$0.00","Phylogenies, or evolutionary histories, play a central role in biology as a framework within which to understand all of Life's diversity.  In Charles Darwin's Origin of Species, the depiction of an evolutionary history of species took the shape of a tree. Ever since, trees have been the most commonly used structure to model evolutionary histories. However, while trees capture how, for example, one species splits into two that subsequently diversify and how genetic material is transmitted from ancestor to descendants, they fail to capture other evolutionary events. For example, some plant species arise due to hybridization between pairs of other species. In the microbial world, bacteria transmit genetic material horizontally by various means. In these cases, a tree gives an incomplete picture of the evolutionary history at best, and a very misleading one in the worst case. Indeed, a more appropriate model of evolutionary histories in these cases is a phylogenetic network, which extends the tree model by incorporating evolutionary events such as hybridization and horizontal gene transfer. Despite an increased research activity in the area of phylogenetic networks in recent years, their reconstruction and evaluation remain largely ad hoc processes and limited in their applicability to specific datasets. <br/><br/>To enable the development of methodologies for systematic reconstructing and evaluating phylogenetic networks, this project is aimed at developing (1) algorithms for evaluating the quality of phylogenetic networks using genomic data, and (2) algorithms for searching the phylogenetic network space to enable automatic inference of evolutionary histories. <br/><br/>The outcome of the proposed work will help significantly extend the applicability of phylogeny to groups of organisms for which trees are inappropriate, as well as to understand the phylogenetic network model, thus allowing for a more systematic development of methodologies for its accurate reconstruction. Further, the results will enable a more accurate reconstruction of the Tree of Life, help unravel microbial genome diversification, facilitate the reconstruction of hybridization scenarios in plants and other groups of organisms, and help understand the mechanisms by which microbes develop resistance to antibiotics. <br/>   <br/>The project provides outstanding opportunities for training graduate and undergraduate students in an interdisciplinary research area."
"1828184","CIF: Student Travel Support for the 2018 IEEE International Symposium on Information Theory","CCF","INFORMATION TECHNOLOGY RESEARC, COMM & INFORMATION FOUNDATIONS","04/01/2018","03/07/2018","Mahesh Varanasi","CO","University of Colorado at Boulder","Standard Grant","Phillip Regalia","03/31/2019","$20,000.00","","Mahesh.Varanasi@Colorado.EDU","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1640, 7797","7556, 7935","$0.00","The IEEE International Symposium on Information Theory (ISIT) is the premier international conference in information theory, the scientific discipline that studies the mathematical foundations of reliable communications, whether by telephony, television, wireless or optical systems, and the fundamental limits of data storage, such as CDs, DVDs, flash drives, or other semiconductor media. The symposium series has run since 1954, originally every two years, and yearly since 2000. This project funds the participation of US-based students in this event. Participation at this conference will enhance the research experiences for students and provide increased opportunities for new collaborations.<br/><br/>Students form an integral part of the Information Theory community, and the IEEE ISIT Conference continues to promote student participation through the ISIT Jack Kiel Wolf Best Student Paper Award, the creation of the ISIT tutorial series, and the Information Theory Society Student Committee. Students comprise about one-third of the participants at ISIT, and the travel funds offered through this award will support the attendance at the 2018 ISIT by students enrolled in a graduate or undergraduate program in the United States and whose papers have been accepted for presentation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617365","CIF: SMALL: MASSIVE MIMO SYSTEMS: Novel Channel Modeling and Estimation Methods","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2016","07/28/2016","Bhaskar Rao","CA","University of California-San Diego","Standard Grant","Phillip Regalia","07/31/2019","$300,000.00","","brao@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7923","$0.00","The demand for wireless services and higher wireless throughput continues to grow exponentially. To meet this growth, massive multiple-input multiple-output (MIMO) has been identified as an enabling technology in next generation wireless systems. A challenge in realizing the vision is the estimation of the wireless channel between the transmitter and receiver as the number of transmitting antennas becomes large. The channel modeling and estimation challenge is addressed in this research for a variety of deployment scenarios; frequency division duplex (FDD) systems, time division duplex (TDD) systems, and distributed massive MIMO systems. In addition to having a significant impact on the theoretical foundations and algorithms relevant to next generation wireless systems, this research will involve several graduate students who will be trained in the latest wireless technology and also result in novel tools that have fundamental and wider import.<br/><br/>The channel modeling and estimation research includes the development of line-of-sight channel estimation via advanced sparse signal recovery algorithms like sparse Bayesian learning with the goal of reducing training overhead. The non-line-of-sight environment is considered from a novel dictionary learning perspective to enable low dimensional representations of the channel. These representations along with compressive channel learning will lead to the development of techniques that significantly reduce the feedback overhead for FDD systems. For TDD systems, the research involves the development of data-aided channel estimation techniques to improve channel estimates well beyond what is possible with pilot-only training. In addition, the research includes an in-depth study of the tradeoffs of distributed massive MIMO array design to develop insights necessary for selecting the optimal array configuration."
"1618310","SHF: SMALL: Collaborative Research: Cloud Mentoring: Guiding Cloud Users for Cost Performance through Testing and Recommendation","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/21/2016","Lori Pollock","DE","University of Delaware","Standard Grant","Sol Greenspan","07/31/2019","$206,357.00","","pollock@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","7798","7923, 7944, 9150, 9251","$0.00","Cloud computing is growing rapidly, with businesses, institutions and individuals moving their workloads to clouds. Cloud users benefit from low cost ownership, a pay-as-you-go pricing model where they only pay for the procured resource usage, and the ability to dynamically scale the resource usage up and down. However, the applications running in clouds usually experience unpredictable performance, which makes it extremely challenging for the users to choose resource configurations that meet their cost and performance requirements. This problem is further complicated as users do not have physical control over cloud computers and are forced to make their decisions based on convoluted cloud performance reports.<br/><br/>This research addresses the need to support users in achieving their cost-performance requirements as they port their applications to various cloud services. In particular, the research is embodied in an envisioned testing and recommendation system that determines proper resource management policies that meet performance and cost requirements. By taking a software-testing-based approach, the research provides solutions using only user-accessible information to satisfy user requirements, addresses the limits of static analysis techniques that rely on performance predictability.<br/><br/>Such a testing and recommendation system enables non-experts to port their applications to various clouds in a cost effective way. The novel framework, testing and recommendation approaches, data sets, and experimental infrastructure developed within the project will be released open source to advance knowledge and understanding within software engineering and cloud computing. The PIs will continue to involve students of underrepresented groups and continue their involvement in mentoring workshops for students."
"1813253","SHF: Small: Collaborative Research: Automatically Enhancing Quality of Social Communication Channels to Support Software Developers and Improve Tool Reliability","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","10/01/2018","08/31/2018","Lori Pollock","DE","University of Delaware","Standard Grant","Sol Greenspan","09/30/2021","$265,927.00","","pollock@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","2878, 7798","7923, 7944, 9102, 9150, 9251","$0.00","Social communication channels (e.g., Stack Overflow, Slack, and GitHub) play an increasingly important role in software developer productivity, as developers use these channels to collaborate and coordinate activities. Researchers have shown additional value in mining information from these sources to develop and improve recommendation systems, software integrated development environments, and other tools. Users of these systems complain about the varying and often poor quality of the information shared on these channels despite the intrinsic mechanisms for maintaining quality (e.g., voting, accepted answers). This project aims to address the need for automation in curating for increased quality of the information shared in developer communication channels. The envisioned system uses multiple software developers' social communication channels as parallel sources of evidence to establish quality of information in each channel.<br/><br/>This project will contribute to the state of the art by tackling three major challenges to bring automated curation of developer communication into practical use. The first aim is to develop analyses for automatically determining and improving the quality of developers' interactive communications on social media sites with regard to quality concerns including poor software properties, obsolescence, contradictions, unreliable sources, and duplication. The second aim is to explore different feedback mechanisms to indicate quality findings back to the communities. Third, the project will develop techniques to customize quality measurement and feedback to a particular developer's context. The resulting tools, data sets, and experimental infrastructure developed as part of the project will be released, enabling other researchers and practitioners to build on the project's results and ultimately advance the quality of the modern software development ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755762","CRII: SHF: Building Visibility into the Cognitive Processes of Software Engineers via Biosensors","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2018","01/24/2018","Christopher Parnin","NC","North Carolina State University","Standard Grant","Sol Greenspan","01/31/2020","$159,662.00","","cjparnin@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7944, 8228, 9251","$0.00","The goal of this research project is to study cognitive effort associated with tasks that computer programmers perform. This is important because the limits of cognitive load present a barrier to improving a programmer's productivity and quality. Using newly available biometric technologies such as  fMRI, EMG and eye-tracking, it is possible to conduct studies of cognitive processes in programmers. Such studies make it possible to provide scientific answers to questions such as which programming language constructs are easier to use, what kinds of training produce better programmers, or how different programming environments and situational variables are more conducive to writing the best code. The main thrust of this project is to overcome the significant challenges that currently prevent researchers in the software engineering community from widely adopting these methods. This project performs some of the groundbreaking work that is needed to provide robust measurements of cognitive processes in programmers using the biometric technologies. The resulting tailored methodologies and measurement devices will support a community of researchers working in this space. <br/><br/>The project will develop techniques to use fMRI to measure difficulty in understanding certain programming constructs. This will be done by measuring cognitive effort, as measured by neural efficiency, and level of concentration via neural deactivation. Similarly, measurements of cognitive processes activated during other code comprehension tasks will be obtained.  The objective is to obtain a validated set of measures that predict the cognitive load associated with understanding code, which is necessary to create the scientific basis for the studies. The work will lead to combining different measurements, such as eye-tracking and EMG, to establish more sensitive measurement tools. The project will address technical concerns such as maintaining clean signal data from sensors and learning how to calibrate devices to maintain validity of results, as well as practical concerns such as how to perform measurements on human subjects without interfering with the measurements.  Ultimately, such studies should lead to a better understanding what constitutes best practices in software development and training of software engineers."
"1629403","XPS: FULL: Collaborative Research: Maximizing the Performance Potential and Reliability of Flash-based Solid State Devices for Future Storage Systems","CCF","Exploiting Parallel&Scalabilty","07/01/2016","06/28/2016","Xiaodong Zhang","OH","Ohio State University","Standard Grant","Yuanyuan Yang","06/30/2020","$285,000.00","","zhang@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8283","","$0.00","Solid-state data storage built upon NAND flash memory is fundamentally changing the memory and storage hierarchy for virtually the entire information technology infrastructure. Nevertheless, there have been several fundamental and challenging issues to be addressed before the industry can explore the flash memory to its full potential. First, as flash memory technology scales down, its reliability degradation approaches to an alarming level, leading to serious concerns and skepticism of storage system architects and users in many applications. Second, system and application development of solid-state storage has been independently conducted, resulting in isolation, duplicated operations, and an inefficient management among these layers. Due to the technology scaling and information loss in existing simple interface with storage devices, flash memory has not been efficiently and reliably utilized in practice, and the situation will become worse with the technology scaling. The PIs of this project will apply a holistic system design methodology to cohesively address the challenges preventing wider adoption of flash memory. By innovating well-orchestrated cross-layer information sharing and utilization, this design methodology enables seamless utilization of system-level workload and physical-level device characteristics across the entire software/hardware stack without complicating overall system design. An integrated software and hardware prototyping infrastructure will be developed to demonstrate the potential using major and widely used software systems, such as Hadoop, virtual machines, and database. This project will achieve a high broader impact by transforming basic research results into storage systems, and by training both undergraduate and graduate students with research activities, and by timely integrating new research results to classrooms.<br/><br/>Specifically, this project will carry out several closely related tasks: (1) It will develop techniques that can learn and predict the varying characteristics and their correlations of individual flash memory devices. This will provide run-time information that makes it possible to optimize the use of flash memory for alleviating the reliability crisis and adapting to varying system-level workload characteristics. (2) It will develop techniques that enable critical information exchange across the storage hierarchy in order to facilitate cross-layer information sharing. (3) It will further develop a set of techniques across the design hierarchy that can effectively utilize these runtime collections and predictions to improve the overall system reliability and performance."
"1560526","CAREER: Conflict Minimization in Distributed Software Development","CCF","SOFTWARE & HARDWARE FOUNDATION, SOFTWARE ENG & FORMAL METHODS","08/01/2015","04/23/2018","Anita Sarma","OR","Oregon State University","Continuing grant","Sol Greenspan","04/30/2020","$459,270.00","","anita.sarma@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7798, 7944","1045, 7798, 7944, 9102, 9150, 9251","$0.00","Software development is a complex socio-technical activity typically occurring concurrently, in distributed teams, and within the larger organizational goals and context. Current development tools are overwhelmed by the scale of software-intensive systems, and often end up contributing to, rather than minimizing, information overload, and coordination breakdowns, which ultimately lead to software conflicts and project delays.<br/> <br/>This research seeks to establish an understanding of how past development data and team practices can be used to proactively identify dependencies and constraints across tasks, and schedule tasks so as to minimize conflicting changes in parallel, distributed development. This work will contribute: (1) conflict typology formalizing software conflicts and their interplay with organizational context, (2) knowledge about how to achieve improvements in productivity, quality, and development speed, (3) a suite of analysis techniques, design principles, tool prototypes, and interaction methods for conflict minimization in distributed, parallel development. Evaluation includes deployment to real software development teams and controlled experiments of the efficacy of the resulting tools. The broader impacts of the work are ultimately to enable software teams to develop software in a conflict-free environment and train students on critical processes associated with collaboration competency."
"1542117","CAREER: Analyzing Program Changes and Versions for Bug Detection and Diagnosis","CCF","SOFTWARE & HARDWARE FOUNDATION","08/16/2014","05/23/2018","Wei Le","IA","Iowa State University","Continuing grant","Sol Greenspan","05/31/2020","$412,971.00","","weile@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7798","1045, 7944","$0.00","Software development is inherently incremental. Nowadays, many software companies adopt an agile process and a shorter release cycle, where software needs to be delivered faster but with quality assurances, and many versions of software may co-exist in the field and need to be maintained. While faster releases do not increase the user's tolerance of bugs, it is challenging to correctly introduce a change on top of existing programs. In recent studies, researchers found that when programming a change, the most important information a developer wants to know is whether this change breaks any code elsewhere, and in fact, 15-24% of the bug fixes are incorrect. <br/><br/>This project will develop program analyses to address quality assurance problems related to program changes and versions, specifically for the three challenges: efficiently verifying changes for reliable software releases, automatically diagnosing failures caused by changes, and effectively patching multiple versions of software.  The project will result in tools and techniques transferable to industry to improve the quality and productivity of software development. The results will also be disseminated via STEM volunteering, new course materials, and release of the software and data obtained through this research."
"1740225","E2CDA: Type I: Collaborative Research: Energy-Efficient Artificial Intelligence with Binary RRAM and Analog Epitaxial Synaptic Arrays","CCF","Energy Efficient Computing: fr","09/15/2017","03/22/2019","Jae-sun Seo","AZ","Arizona State University","Continuing grant","Sankar Basu","08/31/2020","$386,044.00","Shimeng Yu","jaesun.seo@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","015Y","7945","$0.00","In recent years, deep learning and artificial neural networks have been very successful in large-scale recognition and classification tasks, some even surpassing human-level accuracy. However, state-of-the-art deep learning algorithms tend to present very large network models, which poses significant challenges for hardware, especially for memory. Emerging resistive devices have been proposed as an alternative solution for weight storage and parallel neural computing, but severe limitations still exist for applying resistive random access memories (RRAMs) for practical large-scale neural computing. This proposal targets on addressing limitations in resistive device based neural computing through novel device engineering, new bitcell designs, new neuron circuits, energy-aware architecture, and a new circuit-level benchmark simulator. A successful completion of this research is likely to have consequences to our society, enabling wide adoption of dense and energy-efficient intelligent hardware to power-/area-constrained local mobile/wearable devices. Furthermore, a self-learning chip that learns in near real-time and consumes very low-power can be integrated in smart biomedical devices, personalizing healthcare. This project will have a strong effort on integrating the research outcomes with education and outreach through summer outreach programs for high school students, undergraduate/graduate student training, and organization of tutorials and workshops at conferences for knowledge dissemination.<br/><br/>The proposal will perform innovative and interdisciplinary research to address many limitations in today?s resistive device based neural computing and make a leap progress towards energy-efficient intelligent computing. Severe limitations of applying resistive random access memories (RRAMs) for practical large-scale neural computing include: (1) device-level non-idealities, e.g., non-linearity, variability, selector, and endurance, (2) inefficiency in representing negative weights and neurons, and (3) limited demonstration on simpler networks, instead of cutting-edge convolutional and recurrent neural networks. To address these limitations, novel technologies from devices to architectures will be investigated. First, new bitcell circuits will be designed for today's binary resistive devices, efficiently mapping XNOR functionality with (+1, -1) weights and neurons. Second, a novel epitaxial resistive device (EpiRAM) that exhibits many idealistic properties will be investigated, including linear programming for analog weights, suppressed variability, self-selectivity, and high endurance. Third, new neuron circuits will be explored for integration with new resistive devices for feedforward/feedback deep neural networks. Finally, new data-mapping techniques that efficiently map state-of-the-art deep neural networks onto the hardware framework with RRAM arrays will be developed, and the overall energy-efficiency will be verified with a new benchmark simulator ?NeuroSim?. With vertical innovations across material, device, circuit and architecture, tremendous potential and research needs will be pursued towards energy-efficient artificial intelligence in ubiquitous resource-constrained hardware systems."
"1617475","SHF: Small: Visual Architectures: Engaging Crowds in Design and Discovery for Custom Reconfigurable Devices","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","05/24/2018","Gayatri Mehta","TX","University of North Texas","Standard Grant","Sankar Basu","05/31/2020","$274,000.00","","Gayatri.Mehta@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","7798","7923, 7945, 9102, 9251","$0.00","Custom reconfigurable computing platforms offer low power, high performance, high efficiency, and increased redundancy tailored to the needs of a specific application domain. The ability to successfully create these highly customized domain-specific architectures offers tremendous advantages, including orders of magnitude power savings, longer battery lives, smaller, faster, more robust devices, and a shorter time to market. However, making extreme customization an integral part of the design process requires design to be significantly simpler to create novel, out of the box architectures that directly address the requirements of a specific domain. This challenge can be divided into three specific areas: (1) discover fast, efficient algorithms that allow for rapid design space exploration; (2) increase participation and widen the community by promoting computational thinking among non-scientists and non-engineers; and (3) educate the next generation of custom chip designers through innovative visual frameworks. Effective multiplayer and cooperative games will be developed in this project that allow teams of players to collaboratively solve mapping and design problems of much larger scope and complexity. In this respect the project also involves members if underrepresented groups in a major way. <br/><br/>This research is centered around the idea of developing Visual Architectures to address the challenges outlined above. More specifically, the project involves: (a) creating a highly visual, game-like design environment, thus making complex architecture and mapping problems accessible leading to greater engagement and significantly increasing participation; (b) analyze the vast amounts of data generated through the widening player community to discover better, more efficient automated algorithms; (c) employ the interactive platforms thus created to educate the next generation of chip designers and build their skills and intuitions."
"1619816","CAREER: Combating Dark Silicon through Specialization: Communication-Aware Tiled Many-Accelerator Architectures","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2015","05/23/2018","Mark Hempstead","MA","Tufts University","Continuing grant","Yuanyuan Yang","01/31/2020","$407,707.00","","mark@ece.tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7798","1045, 7941","$0.00","Researchers have predicted that, in the coming years, the computer industry?s steady gains in computing performance will slow and even cease all together because of a condition known as Dark Silicon. Dark Silicon is the result of an increase in power density that will make it necessary to leave sections of a microchip powered off. The consequences of Dark Silicon could be widespread, limiting the increasing benefits all aspects of society---from medicine, commerce to entertainment----have reaped from advances in computing. One solution is specialization. With the advent of high-level synthesis tools capable of generating circuits from high-level programming languages such as C, it is now possible to realize specialized accelerators for any application with 10-100x the efficiency of general purpose microprocessors. The computing industry needs new architecture paradigms to organize thousands of these accelerators. This project envisions the computing platform of the future as comprised of domain specific tiles, each with 10-100 specialized accelerators. The research team will advance this vision with innovations in two aspects. First, by inventing a new memory-interconnect subsystem capable of keeping these specialized cores fed with data. Second, canonical tiles will be invented for different application domains that can be studied by the wider research community.<br/> <br/>The team will study memory-interconnect systems for tiles in a manner that recognizes the unique features of many-accelerator architectures: intermittent usage of accelerators, bursts of communication, and the need to share memory between specialized cores. While designing tiles of many-accelerators, the team will develop a new description language that describes the computation and communication capabilities of a tile and matches them with a graph representation of the workload. To provide broader impact, the tools, canonical designs, and models developed by this project will be made available to the wider research community to enable the study and eventual commercialization of specialized architectures. As increasing the technological literacy of the general public is essential to cultivating responsible behavior, the integrated educational plan focuses on all age groups: it reaches K-12 students through workshops and presentations; undergraduate and graduate students through research opportunities and course offerings; and the greater Philadelphia community."
"1751356","CAREER: Information Theory of Dynamical Systems","CCF","COMM & INFORMATION FOUNDATIONS","02/01/2018","01/05/2018","Victoria Kostina","CA","California Institute of Technology","Continuing grant","Phillip Regalia","01/31/2023","$265,169.00","","vkostina@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7797","1045, 7935, 9102, 9251","$0.00","Modern technology aims for a massively and diversely connected world populated by a seamless network of intelligent, dynamically distributed systems engaged in a shared interaction with the physical world and each other through unreliable sensors, actuators, and noisy communication channels. Classical information theory, while it has long served as an enabler of high speed, long-distance communication for point-to-point, delay-tolerant communication systems using coding over long blocks of symbols, lacks ready solutions to the new challenges of the era of massive, dynamic connectivity. Evolving networks are rather delay sensitive, so coding over long blocks of observed data will not be feasible. Information exchanges frequently seek to maximizing payoff, rather than simply recover the information sent, and are often event-triggered, prompting new mathematical models and tools to gain insights into jointly optimal sensing/coding/control strategies for these systems. The project will also offer undergraduate research opportunities in conjunction with Caltech's Summer Undergraduate Research Fellowships (SURF) program, alongside outreach activities to middle and high school students via Caltech's Center for Teaching, Learning and Outreach, in order to encourage future scientists and engineers.<br/><br/>The proposed research will advance the state of the art in understanding the fundamental trade-offs between communication and performance in dynamical systems. The project will draw upon the tools from both information theory and control theory to achieve its objectives, which are (1) to establish the fundamental information-theoretic trade-offs in delay-constrained causal source coding for dynamical systems under a distortion constraint; (2) to elucidate the information-theoretic trade-offs of control over noisy channels and propose new coding schemes with theoretical performance guarantees; and (3) to gain insight into jointly optimal sampling and coding strategies for tracking and control."
"1618104","SHF: Small: Enhancing Memory System Dependability by Integrity Checking","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","05/15/2017","Arun Somani","IA","Iowa State University","Standard Grant","Yuanyuan Yang","06/30/2020","$443,500.00","Zhao Zhang","arun@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7798","7923, 7941, 9150, 9251","$0.00","Memory system dependability is increasingly a concern as memory cell density and total capacity continue to increase. Recent field studies have shown that memory error rates are rising and memory errors have demonstrated correlation patterns. With these two trends, current memory error protection schemes are no longer sufficient for server computers. This project explores a unique error protection scheme called MemGuard, which is based on memory integrity checking, to enhance memory error protections for server computers as well as to provide a cost- and energy-efficient solution for personal and mobile computers. The research may significantly improve the dependability of computer systems without incurring high cost or energy overhead. The education and outreach activities will encourage minority and women students to get involved in the research, and will include interactions with middle/high school students and teachers.<br/><br/>The MemGuard scheme checks the consistency between memory reads and memory writes using hash-based signatures to detect memory errors.  It can detect memory cell errors with a negligible rate of false negative. Compared to SECDED (single error correcting double error detection) ECC and SDDC (single data device correction) schemes, it is much stronger in multi-bit error detection and with negligible cost and energy overhead. It does not correct errors immediately as the other two schemes do; instead, it may reply on OS checkpointing or program restarting for error recovery. The project will fully investigate the design of MemGuard, evaluate the strength of MemGuard with realistic DRAM error modes, extend it to multiprocessors and I/O rich environments, develop a similar integrity-based scheme for processor/memory communication error protection, and combine MemGuard with existing error protection schemes. The project will also optimize the design and implementation of the hash functions of MemGuard, combine MemGuard with selection error protection, and explore efficient checkpointing strategies for improved efficiency."
"1924486","XPS: EXPL: FP: Collaborative Research: SPANDAN: Scalable Parallel Algorithms for Network Dynamics Analysis","CCF","Exploiting Parallel&Scalabilty","09/01/2018","03/15/2019","Sanjukta Bhowmick","TX","University of North Texas","Standard Grant","Aidong Zhang","08/31/2019","$43,707.00","","Sanjukta.Bhowmick@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","8283","9150","$0.00","The goal of SPANDAN project is to create a novel architecture-independent framework <br/>for designing efficient, portable and scalable parallel algorithms for analyzing <br/>large-scale dynamic networks. SPANDAN will not only provide an intuitive methodology <br/>for efficiently translating sequential algorithms into scalable parallel algorithms <br/>for dynamic networks, but also provide mechanisms for their analytical evaluation and <br/>serve as a mediatory layer between applications and system level tuning. To evaluate <br/>the effectiveness of SPANDAN framework in real-world applications, the PIs will <br/>collaborate with social scientists and biologists. They will also integrate research <br/>findings into various courses such as network analysis, parallel algorithms, and <br/>bioinformatics. They will further collaborate with high schools to develop summer courses <br/>with the goal of encouraging women and minority students to pursue IT-related careers. <br/><br/><br/>As the underlying methodology, the SPANDAN framework will exploit graph sparsification <br/>techniques to divide the network into sparse subgraphs (certificates) that form the <br/>leaves of a sparsification tree. This innovative approach will lead to the design and <br/>analysis of efficient parallel algorithms for updating dynamic networks, and reduction <br/>of memory latency associated with parallelizing unstructured data. Specifically parallel <br/>algorithms will be designed for maintaining network topological characteristics, and <br/>updating influential vertices and communities. To demonstrate portability and performance, <br/>the developed algorithms will be implemented on the distributed memory clusters, shared <br/>memory multicores, and massively multithreaded CRAY-XMT.<br/><br/>For further information see the project web site at:<br/> http://cs.mst.edu/labs/crewman/projects/SPANDAN/"
"1533881","XPS: EXPL: FP: Collaborative Research: SPANDAN: Scalable Parallel Algorithms for Network Dynamics Analysis","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/27/2015","Sanjukta Bhowmick","NE","University of Nebraska at Omaha","Standard Grant","Aidong Zhang","04/30/2019","$146,528.00","","Sanjukta.Bhowmick@unt.edu","6001 Dodge Street","Omaha","NE","681820210","4025542286","CSE","8283","9150","$0.00","The goal of SPANDAN project is to create a novel architecture-independent framework <br/>for designing efficient, portable and scalable parallel algorithms for analyzing <br/>large-scale dynamic networks. SPANDAN will not only provide an intuitive methodology <br/>for efficiently translating sequential algorithms into scalable parallel algorithms <br/>for dynamic networks, but also provide mechanisms for their analytical evaluation and <br/>serve as a mediatory layer between applications and system level tuning. To evaluate <br/>the effectiveness of SPANDAN framework in real-world applications, the PIs will <br/>collaborate with social scientists and biologists. They will also integrate research <br/>findings into various courses such as network analysis, parallel algorithms, and <br/>bioinformatics. They will further collaborate with high schools to develop summer courses <br/>with the goal of encouraging women and minority students to pursue IT-related careers. <br/><br/><br/>As the underlying methodology, the SPANDAN framework will exploit graph sparsification <br/>techniques to divide the network into sparse subgraphs (certificates) that form the <br/>leaves of a sparsification tree. This innovative approach will lead to the design and <br/>analysis of efficient parallel algorithms for updating dynamic networks, and reduction <br/>of memory latency associated with parallelizing unstructured data. Specifically parallel <br/>algorithms will be designed for maintaining network topological characteristics, and <br/>updating influential vertices and communities. To demonstrate portability and performance, <br/>the developed algorithms will be implemented on the distributed memory clusters, shared <br/>memory multicores, and massively multithreaded CRAY-XMT.<br/><br/>For further information see the project web site at:<br/> http://cs.mst.edu/labs/crewman/projects/SPANDAN/"
"1646999","EAGER:   Exploring the Use of Secure Multi-Party Computation in the Context of Organ Donation","CCF","Secure &Trustworthy Cyberspace","09/01/2016","03/07/2019","Giuseppe Ateniese","NJ","Stevens Institute of Technology","Standard Grant","Rahul Shah","08/31/2019","$125,669.00","","giuseppe.ateniese@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","8060","7434, 7916, 9102","$0.00","Informally speaking, Secure Multi-Party Computation (SMPC) allows two or more parties to jointly compute some function on their private inputs in a distributed fashion (i.e., without the involvement of a trusted third party) such that none of the parties learns anything beyond its dedicated output and what it can deduce from considering both this output and its own private input. Since its inception in 1982 by Yao, SMPC has advanced greatly and over the years a large body of work has been developed. To date, prominent applications for SMPC include private set intersection, auctions, and data mining.  However, despite all advances, there still are many areas of application for which the use of SMPC has not yet been explored.  Considering the fact that SMPC allows one to achieve strong security guarantees, the use of SMPC should be further advanced into fields of application which require the handling of highly-sensitive information of multiple parties in a centralized fashion and as such exhibit great promise to substantially benefit from the use of SMPC techniques.  Such an area of application is organ donation. Currently, more than 120,000 patients in the U.S. alone are waiting to receive a lifesaving organ transplant and the need by far outweighs the number of available organs. Increasing the pool of organ donors is challenging and reports of organ scandals have even resulted in a decline in the number of potential organ donors. On one hand, transparency and fairness in the allocation process was shown to influence the willingness to donate organs. In turn, it is argued that in the case of living donations (where a patient has a willing donor but the donor's medical characteristics are not compatible with those of the patient), the recipient of the organ donation should have the right for the transparency to be limited.  As such this project seeks to explore whether it is possible to effectively and efficiently introduce SMPC into the context of organ donation with the goal to ensure suitable transparency and privacy guarantees for donors and recipients alike. The potential impact of this work is substantial---for individual patients and society at large---in that addressing common attacks on traditional organ donation systems may not only help rebuild lost trust but may even lead to a greater buy-in than ever before.<br/><br/>For living donations, the project seeks to devise initial protocols which allow the determination of donors in a cyclic fashion such that (a) it does not require a trusted third party, (b) the attributes of all patients and donors are kept private at all times, (c) all parties are satisfied with the exchange, (d) application-specific requirements are met, and (e) it is secure even in the presence of adversaries. For post-mortem donations, the project will explore the suitability of traditional privacy-preserving matching approaches---recognizing that matching the characteristics of organs come with unique challenges and requirements.  Also, the project will investigate whether it is feasible to introduce a systemic change to how post-mortem organ donation is carried out today."
"1755800","CRII: AF: Theoretical Problems in Quantum Computation","CCF","QUANTUM COMPUTING","05/01/2018","03/09/2018","Xiaodi Wu","MD","University of Maryland College Park","Standard Grant","Almadena Chtchelkanova","04/30/2020","$175,000.00","","xwu@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7928","8228","$0.00","In anticipation of quantum computers being utilized in the near future, this project seeks to find quantum advantages in machine-learning-like tasks that are ubiquitous in our daily life. Future quantum computers will take advantage of entanglement, a quantum property, that digital computers do not use. It is important to understand the essential role of entanglement in the computational power of quantum computers to be able to use it efficiently. <br/><br/>This project consists of two sets of questions that touch upon central topics in quantum information (e.g., entanglement and its impact on complexity theory) and novel topics such as property testing. The first part of the project investigates the possibility of designing fast quantum algorithms for property testing of unknown classical distributions and quantum states, a well-motivated topic related to statistics, data analysis, and machine learning. The project aims to integrate techniques from classical property testing, quantum tomography, quantum walks, and so on. The second part of the project investigates the computational power of the absence of entanglement in the context of quantum Merlin-Arthur protocols with two provers (QMA(2)). It is a well-motivated problem due to its connection to the separability problem within quantum information as well as the Unique Games Conjecture in the field of approximation algorithm. Specific objectives include the study of (1) the k-extendible states and de Finetti theorems for the separability problem and (2) non-trivial error reduction schemes of QMA(2). The techniques used are primarily inspired by ideas from the sum-of-squares techniques in optimization, weak measurements and approximation theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1561144","CyberSEES: Type 1: Collaborative Research: Sustainability-aware Management of Interdependent Power and Water Systems","CCF","CyberSEES","09/15/2015","09/17/2015","Yihsu Chen","CA","University of California-Santa Cruz","Standard Grant","Rahul Shah","09/30/2019","$175,320.00","","yihsuchen@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","8211","8207, 8231","$0.00","While extensive attention has been given to sustainability in the energy systems, including the subsystems of electricity, petroleum, and natural gas, an oft-overlooked aspect is the interdependence between energy and other infrastructure systems, such as water and transportation systems, and the potential adverse impacts to economics, reliability, and sustainability caused by such interdependence. For example, regulations in the water sector to preserve freshwater may restrict water usage in the power sector, likely causing reduced available generation capacities and hence jeopardizing the reliability of power systems. On the other hand, environmental policies only focused on the power sector, such as those encouraging retrofitting or installing carbon dioxide capture and sequestration capabilities to existing and new coal plants would further constrain the water system as coal plants with carbon sequestration are among the heaviest users of water. Thus, there is a clear need to better understand and manage the interdependence of critical infrastructure systems to promote sustainability across all systems, while not undermining economic and reliability considerations. This proposed work aims to address this need through the theory, modeling and computation of large-scale, interdependent complex systems by way of distributed, highly scalable computing. The results will be widely disseminated through publications and seminars. Further, the project team will leverage established institutional outreach programs to the general public, especially to high-school students and teachers, such as through the Engineering Projects In Community Service program and Purdue?s Energy Academy. <br/><br/>The grand vision of this project is to promote sustainability across interdependent systems, as well as to achieve economic efficiency and to maintain reliability through decentralized yet coordinated management of individual systems by establishing a complete modeling, analytical, and computational framework based upon the general class of augmented Lagrangian methods originating from convex optimization. While the augmented Lagrangian method is not a new algorithm, the current implementation of such algorithms has not taken advantage of its distributed feature, which would be particularly suitable to deal with large-scale, interlinked systems. One of the major goals of this work is to establish the theoretical foundations of distributed Lagrangian methods and to implement the algorithms on supercomputer clusters to demonstrate the benefits of distributed computing. This research aims to pave the way for cloud computing such that the algorithms can be used by decision-makers even without access to supercomputers. Another contribution is that the augmented Lagrangian method algorithms will be extended to incorporate stochastic data, both in terms of theoretical issues such as algorithm convergence as well as practical implementation. The computational methods will be tested and validated through real-world models of interdependent power and water systems."
"1830746","CyberSEES: Type 1: Collaborative Research: Sustainability-aware Management of Interdependent Power and Water Systems","CCF","CyberSEES","02/01/2018","03/12/2018","Josue Medellin-Azuara","CA","University of California - Merced","Standard Grant","Rahul Shah","09/30/2019","$8,397.00","","jmedellin-azuara@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092598670","CSE","8211","8207, 8231","$0.00","While extensive attention has been given to sustainability in the energy systems, including the subsystems of electricity, petroleum, and natural gas, an oft-overlooked aspect is the interdependence between energy and other infrastructure systems, such as water and transportation systems, and the potential adverse impacts to economics, reliability, and sustainability caused by such interdependence. For example, regulations in the water sector to preserve freshwater may restrict water usage in the power sector, likely causing reduced available generation capacities and hence jeopardizing the reliability of power systems. On the other hand, environmental policies only focused on the power sector, such as those encouraging retrofitting or installing carbon dioxide capture and sequestration capabilities to existing and new coal plants would further constrain the water system as coal plants with carbon sequestration are among the heaviest users of water. Thus, there is a clear need to better understand and manage the interdependence of critical infrastructure systems to promote sustainability across all systems, while not undermining economic and reliability considerations. This proposed work aims to address this need through the theory, modeling and computation of large-scale, interdependent complex systems by way of distributed, highly scalable computing. The results will be widely disseminated through publications and seminars. Further, the project team will leverage established institutional outreach programs to the general public, especially to high-school students and teachers, such as through the Engineering Projects In Community Service program and Purdue?s Energy Academy. <br/><br/>The grand vision of this project is to promote sustainability across interdependent systems, as well as to achieve economic efficiency and to maintain reliability through decentralized yet coordinated management of individual systems by establishing a complete modeling, analytical, and computational framework based upon the general class of augmented Lagrangian methods originating from convex optimization. While the augmented Lagrangian method is not a new algorithm, the current implementation of such algorithms has not taken advantage of its distributed feature, which would be particularly suitable to deal with large-scale, interlinked systems. One of the major goals of this work is to establish the theoretical foundations of distributed Lagrangian methods and to implement the algorithms on supercomputer clusters to demonstrate the benefits of distributed computing. This research aims to pave the way for cloud computing such that the algorithms can be used by decision-makers even without access to supercomputers. Another contribution is that the augmented Lagrangian method algorithms will be extended to incorporate stochastic data, both in terms of theoretical issues such as algorithm convergence as well as practical implementation. The computational methods will be tested and validated through real-world models of interdependent power and water systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1652159","CAREER: Scalable Neuromorphic Learning Machines","CCF","SOFTWARE & HARDWARE FOUNDATION, IntgStrat Undst Neurl&Cogn Sys","02/15/2017","02/25/2019","Emre Neftci","CA","University of California-Irvine","Continuing grant","Sankar Basu","01/31/2022","$435,912.00","","eneftci@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7798, 8624","1045, 7945, 8089, 8091","$0.00","Machine learning algorithms based on artificial neural networks significantly advanced our ability to solve human-centric cognitive tasks at or above human proficiency. Neuromorphic hardware that emulate the biological processes of the brain on a physical, electronic substrate are emerging as ultra low-power alternatives for performing such tasks where adaptability and autonomy are critical. However, neuromorphic hardware lacks general and efficient inference and learning models of the type that empower artificial neural networks, while being compatible with the spatial and temporal constraints of the brain. This research will bridge isolated fields of machine learning and neuromorphic engineering, and address the energetic and performance merits of computing under physical constraints on communication, precision, retention and failures. The solutions sought to meet these challenges will outline the principles for designing continuously learning hardware, resistant to soft errors and failures of future and emerging computing and memory technologies.This interdisciplinary effort will bring a multifaceted skill set to students and researchers alike, and impact many domains of embedded computing, such as brain implants for detecting and alleviating neurological conditions, implantable prosthetics, assistive robots capable of learning and performing human-level cognitive tasks, as well as defense and surveillance related workloads. To encourage young generations to this approach, the PI will 1) organize hands-on workshops, 2) initiate a student-driven project for developing educational tools targeted for teaching K-12 students the building blocks of spike-based deep learning, and 3) offer public video and lab-based courses on neuromorphic intelligence, including hands-on experiments with cutting edge neuromorphic hardware for students.<br/><br/>The proposed approach will study the stochastic nature of biological neurons and synapses to provide a blueprint for inference and learning machines compatible with the digital and mixed-signal neuromorphic hardware. The goals of this vertically-integrated project will be achieved by devising: 1) Spike-based algorithms guided by statistical machine learning theory that operate on information that is locally available to the underlying physical and neural processes that achieve or surpass the performance of equivalent learning algorithms in deep artificial neural networks; 2) Dedicated scalable neuromorphic hardware architectures for ultra low-power, continuously learning, which are key to adaptive behavior in embedded real-time behaving systems; 3) Rules governing the organization of attention and working memory in the brain using insights obtained from neural networks models equipped with dynamic feedback loops. In tandem with the breakthroughs in deep recurrent neural networks, this project aims to create unprecedented transfer of knowledge, sparking the foundations for novel computers that proactively interpret and learn from real-world data, solve novel problems using what they learned, and operate with the efficiency and proficiency of the human brain."
"1713977","CIF: Small: Alignment for Secrecy: One-Time-Pads in the Air without Keys","CCF","COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace","09/01/2017","06/26/2017","Sennur Ulukus","MD","University of Maryland College Park","Standard Grant","Phillip Regalia","08/31/2020","$499,999.00","","ulukus@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7797, 8060","7923, 7937, 9102","$0.00","In today's communication networks, most data flow through wireless links which are particularly vulnerable to eavesdropping attacks. While currently available cryptographic algorithms provide useful protection against computationally limited eavesdroppers, the ever-increasing computational power of adversaries necessitates techniques that provide stronger forms of security guarantees. Information-theoretic physical layer security provides unconditional security guarantees that are valid even against computationally unlimited adversaries. The goal of this research is to advance information-theoretic physical layer security to address practical issues arising from user misbehavior, conflicting user interests, lack of complete network state information; and to devise practically implementable codes.<br/><br/>In the application of information-theoretic physical layer security to multi-user wireless channels, alignment emerges as a powerful tool to achieve the maximum possible secure degrees of freedom for the network. Optimal alignment for the purposes of security may be interpreted as a form of one-time-pad communication in the air with no need for explicit exchange of keys; security is bootstrapped from the randomness in the physical channel and from the willingness of the users to cooperate. The goal of this research is to develop techniques to make such secure alignment schemes more robust to user misbehavior and to the lack of channel state information; more practical by making connections to polar coding to develop explicit codes; and more widely applicable to modern wireless networks with multi-antenna nodes. Specifically, this research investigates the cases of 1) deviating users who do not follow agreed-upon optimum protocols; 2) combating helpers and selfish users who have conflicting interests; 3) vector channels arising from use of multiple antennas and the associated channel state information issues; and 4) explicit code design using polar codes and relating polar alignment and signal alignment."
"1917924","SHF:Small:Scalable and Precise Program Analyses via Linear Conjunctive Language Reachability","CCF","SOFTWARE & HARDWARE FOUNDATION","12/01/2018","02/11/2019","Qirun Zhang","GA","Georgia Tech Research Corporation","Standard Grant","Anindya Banerjee","09/30/2021","$481,902.00","","qrzhang@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798","7923, 7943","$0.00","Static program analysis provides foundational and practical techniques to help build reliable and secure software. Context-free language (CFL) reachability has been widely adopted for specifying program analysis problems. However, little foundational progress exists on advancing the CFL-reachability framework itself. To support precise and scalable program analyses, an expressive, accessible class of formal language reachability is needed to bridge fundamental formal language research and practical analysis-based tool development. The project's novelties are twofold: a new powerful formalism for specifying program analyses, and algorithms and techniques for realizing a practical framework based on this formalism. The project's impacts are deepened knowledge on and improved capabilities for building precise and scalable static analyses, as well as practical analyses for improving software reliability and security.<br/><br/>This project will explore linear conjunctive language (LCL) reachability as a new static analysis formalism.  In contrast to CFLs, LCLs are closed under all set-theoretic operations and can also be efficiently recognized in quadratic time.  A significant number of advanced program analyses need to match properties described by multiple CFLs simultaneously. LCLs can precisely express many such properties, while CFLs cannot because they are not closed under intersection. Thus, LCL reachability offers a novel perspective in specifying and realizing program analyses. The investigators' initial work on LCL-reachability has shown considerable promise, leading to both more precise and orders of magnitude more scalable alias and taint analysis, two widely-used analyses.  This project aims to fully exploit LCL-reachability's potentials by developing a unified solution for specifying program analysis problems in LCL and implementing novel data structures that support efficient LCL-reachability algorithms. It focuses on (1) theoretical development of the LCL-reachability formulation, (2) efficient algorithms for computing LCL-reachability, and (3) generalizing to practical program analyses. If successful, this project will significantly advance the state-of-the-art in software analysis and verification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1903951","CAREER: Scaling-up Resistive Synaptic Arrays for Neuro-inspired Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","11/15/2018","Shimeng Yu","GA","Georgia Tech Research Corporation","Continuing grant","Sankar Basu","01/31/2021","$123,512.00","","shimeng.yu@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798","1045, 7945, 8089","$0.00","Neuro-inspired deep learning algorithms have demonstrated their power in executing intelligent tasks such as image and speech recognition. However, training of such deep neural networks requires huge amount of computational resources that are not affordable for mobile applications. Hardware acceleration of deep learning, with orders of magnitude improvement in speed and energy efficiency, remains a grand challenge for the conventional hardware based on silicon CMOS technology and von-Neumann architecture. As the learning algorithms extensively involve matrix operations, neuro-inspired architectures that leverage the distributed computing in the neuron nodes and localized storage in the synaptic networks are very attractive. The ultimate goal of this project is to advance the neuro-inspired computing with emerging nano-device technologies towards a self-learning chip. A chip that learns in real-time and consumes low-power can be placed at frontend sensors, bringing broad benefits for a number of current applications. The PI will establish close collaboration with industry through student internships and technology transfer. The plan for integration of research and education will train students with interdisciplinary skills. The cross-layer nature of this project ranging from semiconductor device, circuit design, electronic design automation, and machine learning is expected to provide an ideal platform for this educational goal.<br/><br/>The technical goal of this project is to overcome the challenges that prevent scaling up of the crossbar array size for neuro-inspired architecture. Resistive devices with continuous multilevel states have been proposed to function as synaptic weights in the crossbar architecture. However, with the increase of the array size, issues associated with device yield, device variability, and array parasitics will arise and may degrade the system performance. The PI plans to tackle these challenges by exploiting hierarchical research efforts from devices, circuits and architectures. The outcome of the research includes device compact model, circuit-level benchmark simulator for estimating the area/latency/power of the crossbar array macro, and architectural tool for efficiently mapping the learning algorithms into the crossbar architecture.  The PI has established a custom fabrication channel for tape-out of resistive devices on top of CMOS peripheral circuits via his collaboration with academic partners. The prototype chip with measured data is expected to make a strong impact on this field, which previously relied on the simulations for predicting large-scale array performance."
"1629291","XPS: FULL: Collaborative Research: Maximizing the Performance Potential and Reliability of Flash-based Solid State Devices for Future Storage Systems","CCF","Exploiting Parallel&Scalabilty","07/01/2016","06/28/2016","Feng Chen","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Yuanyuan Yang","06/30/2020","$290,000.00","","fchen15@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","8283","","$0.00","Solid-state data storage built upon NAND flash memory is fundamentally changing the memory and storage hierarchy for virtually the entire information technology infrastructure. Nevertheless, there have been several fundamental and challenging issues to be addressed before the industry can explore the flash memory to its full potential. First, as flash memory technology scales down, its reliability degradation approaches to an alarming level, leading to serious concerns and skepticism of storage system architects and users in many applications. Second, system and application development of solid-state storage has been independently conducted, resulting in isolation, duplicated operations, and an inefficient management among these layers. Due to the technology scaling and information loss in existing simple interface with storage devices, flash memory has not been efficiently and reliably utilized in practice, and the situation will become worse with the technology scaling. The PIs of this project will apply a holistic system design methodology to cohesively address the challenges preventing wider adoption of flash memory. By innovating well-orchestrated cross-layer information sharing and utilization, this design methodology enables seamless utilization of system-level workload and physical-level device characteristics across the entire software/hardware stack without complicating overall system design. An integrated software and hardware prototyping infrastructure will be developed to demonstrate the potential using major and widely used software systems, such as Hadoop, virtual machines, and database. This project will achieve a high broader impact by transforming basic research results into storage systems, and by training both undergraduate and graduate students with research activities, and by timely integrating new research results to classrooms.<br/><br/>Specifically, this project will carry out several closely related tasks: (1) It will develop techniques that can learn and predict the varying characteristics and their correlations of individual flash memory devices. This will provide run-time information that makes it possible to optimize the use of flash memory for alleviating the reliability crisis and adapting to varying system-level workload characteristics. (2) It will develop techniques that enable critical information exchange across the storage hierarchy in order to facilitate cross-layer information sharing. (3) It will further develop a set of techniques across the design hierarchy that can effectively utilize these runtime collections and predictions to improve the overall system reliability and performance."
"1813637","CIF: Small: Contagion Processes in Multi-layer and Multiplex Networks","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/22/2018","Osman Yagan","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","09/30/2021","$500,000.00","","oyagan@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7923, 7935","$0.00","Online social networks are increasingly serving as a primary source of real time news for billions of people. In addition, online social networks often act as a medium to initiate and facilitate collective action. This project advances our understanding of contagion processes such as the spread of information and influence in online social networks. Unlike much of the current literature focusing on single and isolated networks, the project considers contagions taking place over multiple networks. By a combination of approaches involving mathematical modeling and analysis as well as real-life data sets, this project seeks a better understanding of i) how the participation of multiple networks affects the speed and extent that information propagates; ii) why different topics have different spreading characteristics over the same population; and iii) the ways misinformation spreads in social networks and how this might be countered by efficiently injecting correct information. The project will engage students from underrepresented groups and will include outreach activities to high-schools to encourage them to pursue a STEM education. <br/><br/>This project seeks to advance the state-of-the art in modeling and analysis of two major classes of contagion processes over multi-layer and multiplex networks: i) Information propagation (i.e., simple contagion), and ii) Influence propagation (i.e., complex contagion).  Advanced tools from random graph theory and percolation theory are leveraged to derive fundamental relations between network parameters (e.g., degree distribution, size, clustering, degree correlations, coupling strength, and transmissibility) and the threshold, probability, and expected size of information cascades. The analysis of contagion is extended to a wide range of network models including real-world topologies. The modeling of complex contagion is extended from binary-state dynamics to multi-stage dynamics and this will be utilized in studying the spread of misinformation. On a broader level, this project contributes to a largely unexplored field of analyzing random graph models formed by the union of two or more random graphs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1704041","SHF: MEDIUM: Collaborative Research: The Theory and Practice of Dependent Types in Haskell","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2017","09/19/2017","Richard Eisenberg","PA","Bryn Mawr College","Continuing grant","Anindya Banerjee","06/30/2021","$311,303.00","","rae@cs.brynmawr.edu","101 N. Merion Avenue","Bryn Mawr","PA","190102899","6105265298","CSE","7798","7924, 7943","$0.00","This project's overarching goal is to prevent bugs in software by extending the Haskell programming language to support dependent types. Haskell is used by researchers and programmers in industry to build a variety of software  systems, such as financial analysis tools, interactive websites, data visualizations, and automated vehicle software. Dependent types are an up-and-coming technology that allows programmers to avoid bugs in their software, allowing them to include mathematical proofs of correctness in their code. These proofs are checked before the software ever runs, ruling out the possibility of failures, such as a crashed website. The intellectual merits are insights into the integration of advanced mathematical theories with an industrial-strength development tool (Haskell) and a deeper understanding of the mathematical principles that underlie the creation of correct software. The project's broader significance and importance are to give the technology industry access to dependent types for the first time, while creating opportunities for students (including those at one principal investigator's undergraduate women's college) to engage with this technology.<br/><br/>The project includes both practical and foundational components. The  Haskell type system, as implemented in the Glasgow Haskell Compiler (GHC) version 8.0, is able to simulate dependent types through the use of many language extensions. However, this use requires awkward encodings, and the extensions that support them complicate the language. In contrast, the Haskell type system envisioned by this project is based on a uniform approach to dependently typed programming that subsumes prior extensions. Part of this project involves replacing the core language of GHC with one based on dependent type theory, using relevance annotations to ensure that these extensions are backwards compatible. Furthermore, the project also introduces matchable functions, a qualifier that determines whether function applications can be analyzed via pattern matching, enabling the integration of dependent types with GHC's current type inference algorithm.  Finally, this project includes an examination of the semantics of dependently typed programming languages with partiality."
"1617934","CIF: Small: Designing Secure, Reliable, and Resilient Wireless Sensor Networks","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/30/2016","Osman Yagan","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","06/30/2019","$500,000.00","Virgil Gligor","oyagan@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7923, 7936","$0.00","Wireless sensor networks (WSNs) are distributed collection of small sensor nodes that gather security-sensitive data and control security-critical operations in a wide range of industrial, home, and business applications. Their numerous applications, including those that concern the nation?s security, the health care system, and monitoring and protecting natural landscapes, clearly put the robust operation of WSNs at the core of technologies that are vital to our society. Using advanced tools from the theory of random graphs, this project develops methods for designing WSNs that are i) secure; ii) reliable against sensor and link failures; and iii) resilient against adversarial attacks. The research program is supported by several education and outreach activities including development of new courses, K-12 outreach, and dissemination of research results to academic and industrial audience.<br/><br/>In order to ensure a secure, reliable, and resilient WSN operation, investigators consider WSNs that employ a randomized key predistribution scheme and develop conditions so that the network i) is k-connected; and ii) is resilient against node capture attacks in the sense of unassailability and unsplittability. This is done by developing realistic models of secure WSNs constructed by intersecting two random graphs: a cryptographic graph induced by the random key predistribution scheme and a communication graph induced by the wireless communication media and random deployment of sensors. For several classical random key predistribution schemes and wireless communication models, investigators develop (i) zero-one laws for k-connectivity; (ii) Poisson approximation results to obtain the asymptotic probability of k-connectivity; and (iii) conditions on network parameters (e.g., number of nodes, link failure probability, number of keys per node, key pool size) that ensure the unassailability and unsplittability of the network."
"1703835","SHF: Medium: Collaborative Research: The Theory and Practice of Dependent Types in Haskell","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2017","07/21/2017","Stephanie Weirich","PA","University of Pennsylvania","Continuing grant","Anindya Banerjee","06/30/2021","$638,662.00","","sweirich@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7798","7924, 7943","$0.00","This project's overarching goal is to prevent bugs in software by extending the Haskell programming language to support dependent types. Haskell is used by researchers and programmers in industry to build a variety of software  systems, such as financial analysis tools, interactive websites, data visualizations, and automated vehicle software. Dependent types are an up-and-coming technology that allows programmers to avoid bugs in their software, allowing them to include mathematical proofs of correctness in their code. These proofs are checked before the software ever runs, ruling out the possibility of failures, such as a crashed website. The intellectual merits are insights into the integration of advanced mathematical theories with an industrial-strength development tool (Haskell) and a deeper understanding of the mathematical principles that underlie the creation of correct software. The project's broader significance and importance are to give the technology industry access to dependent types for the first time, while creating opportunities for students (including those at one principal investigator's undergraduate women's college) to engage with this technology.<br/><br/>The project includes both practical and foundational components. The  Haskell type system, as implemented in the Glasgow Haskell Compiler (GHC) version 8.0, is able to simulate dependent types through the use of many language extensions. However, this use requires awkward encodings, and the extensions that support them complicate the language. In contrast, the Haskell type system envisioned by this project is based on a uniform approach to dependently typed programming that subsumes prior extensions. Part of this project involves replacing the core language of GHC with one based on dependent type theory, using relevance annotations to ensure that these extensions are backwards compatible. Furthermore, the project also introduces matchable functions, a qualifier that determines whether function applications can be analyzed via pattern matching, enabling the integration of dependent types with GHC's current type inference algorithm.  Finally, this project includes an examination of the semantics of dependently typed programming languages with partiality."
"1618717","AF: Small: New classes of optimization methods for nonconvex large scale machine learning models.","CCF","COMPUTATIONAL MATHEMATICS, ALGORITHMIC FOUNDATIONS","09/01/2016","06/10/2016","Katya Scheinberg","PA","Lehigh University","Standard Grant","Rahul Shah","08/31/2019","$499,143.00","Frank Curtis, Martin Takac","kas410@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","1271, 7796","7923, 7929, 9263","$0.00","Intelligent systems that say, recommend music or movies based on past interests, or recognize faces or handwriting based on labeled samples, often learn from examples using ""supervised learning.""  The system tries to find a prediction function: a combination of feature values of the song, movie, image, or pen movements, that, on known inputs, produces score values that agree with known preferences. Some combinations may add with simple positive or negative weight parameters (The more guitar the better, or I really don't want accordion), while others can be more complex (nether too loud nor too soft).  If parameters for such a function can be found, then it can be hoped that, on a new input, the function will be a good approximation for the preference. <br/><br/>In scientific computing, there are many optimization techniques used to find the best parameters.  The type called ""gradient methods"" is like a group hike that gets caught in the hills after dark; the members want to go downhill to return to the valley quickly, but take small steps so as not to trip. With a little light, the group can discover more about its vicinity to 1) suggest the best direction, 2) take longer steps without tripping, or 3) send different members in different directions so that someone finds the best way. When there are many parameters (not just latitude and longitude) there are many more directions to step.  Simple combinations define simple (aka convex) valleys, and many optimization-based learning methods (including support vector machines (SVM),  least squares, and logistic regression) have been effectively applied to find the best parameters.  More complex combinations that sometime lead to better learning, may define non-convex valleys, so the known methods may get stuck in dips or have to take very small steps -- they often lack theoretical convergence guarantees and do not always work well in practice. <br/><br/>This project will explore non-convex optimization for machine learning with three techniques that are analogous to the hikers? use of the light: <br/>First, new techniques will be explored for exploiting approximate second-order derivatives within stochastic methods, which is expected to improve performance over stochastic gradient methods, avoid convergence to saddle points, and improve complexity guarantees over first-order approaches. Compared to other such techniques that have been proposed, these approaches will be unique as they will be set within trust-region frameworks, the exploration of which represents the second component of the project. Known for decades to offer improved performance for nonconvex optimization, trust region algorithms have not fully been explored for machine learning, and we believe that, when combined with second-order information, dramatic improvements (both theoretically and practically) can be achieved. Finally, for such methods to be efficient in large-scale settings, one needs to offer techniques for solving trust region subproblems in situations when all data might not be stored on a single computer. To address this,  parallel and distributed optimization techniques will be developed for solving trust region subproblems and related problems.  The three PIs work together with about a dozen students at Lehigh; their website is one way they disseminate research papers, software, and news of weekly activities. <br/><br/>This project is funded jointly by NSF CISE CCF Algorithmic Foundations, and NSF MPS DMS Computational Mathematics."
"1526750","SHF:  Small:  Virtualizing Coordinated Resource Management of Flows on Handhelds with VIADUCT","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/28/2015","Anand Sivasubramaniam","PA","Pennsylvania State Univ University Park","Standard Grant","Yuanyuan Yang","07/31/2019","$499,998.00","Chitaranjan Das, Mahmut Kandemir","anand@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7798","7923, 7941","$0.00","Handheld devices such as cell phones, tablets and wearables, are gaining<br/>rapid adoption in a ubiquitous world. Correspondingly, there is an<br/>explosion of very demanding and interactive applications such as media<br/>streaming, interactive games, video conferencing and social networking,<br/>that users are deploying on such devices. Despite tremendous<br/>improvements in the energy efficiencies of individual components found<br/>in these devices, piece-meal optimizations of hardware components are<br/>inadequate to accommodate the demanding needs of these applications that<br/>employ multiple CPU cores, accelerators, memory systems, system<br/>peripherals and sensors, concurrently in very sophisticated ways.  Lack<br/>of coordination among system resources leads to low resource utilization<br/>and poor system-level energy efficiencies when running such<br/>applications. Rather than throw more hardware at this problem, this<br/>project proposes VIADUCT, which allocates and orchestrates system-level<br/>resources to boost parallelism and resource utilization for these<br/>platforms in a more energy-efficient fashion.<br/><br/>Recognizing that many of these applications periodically process frames<br/>of data that need to flow in a pipelined manner through several hardware<br/>components within soft real-time bounds, VIADUCT creates a virtual<br/>channel per flow of such frames in an application. The channel holds<br/>resources allocated across all the hardware components, with runtime<br/>coordination to ensure seamless frame movements without the conventional<br/>interface inefficiencies across the components. Fine-grain scheduling<br/>and resource management leverages knowledge of high level semantics to<br/>provide Quality-of-Service (QoS) for each flow in the most<br/>energy-efficient fashion. The specific research contributions are in<br/>hardware interfaces, mechanisms and policies to chain the hardware<br/>components for creating a virtual channel for a flow, architecting the<br/>memory system as an efficient conduit to sustain this flow, and<br/>designing the system to sustain several concurrent flows. The project<br/>aims to provide a holistic evaluation testbed for such platforms amongst<br/>the broader research community, and also aims to involve<br/>under-represented groups in the research and enhance Penn State?s<br/>curriculum in related topics."
"1815899","SHF: Small: Energy Efficient Learning on Chip with Quantized Representations","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","06/19/2018","Diana Marculescu","PA","Carnegie-Mellon University","Standard Grant","Sankar Basu","09/30/2021","$450,000.00","Ronald Blanton","dianam@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","075Z, 7923, 7945, 9102","$0.00","Machine learning models, especially deep neural networks, have been adopted in many real-time applications, such as speech and image recognition or object detection. These applications typically must be fast and energy efficient so they are usable in the field. This project develops quantized representations and algorithmic transformations for neural networks that represent large models with significantly less storage, energy and area, and with little or no accuracy degradation - in other words, enabling the use of efficient neural networks on mobile devices. The results of this project are poised to change how designers approach modeling, analysis, and optimization methodologies for large-scale deep neural networks, and more generally, any statistical learning applications that rely on expensive compute operations, with large memory footprint. The project will have not only a technical impact, but also an important educational and mentoring component by potentially changing how engineers are trained in a multidisciplinary fashion for dealing with next generation technological advances in general, and the problem of energy efficient machine learning in silicon in particular. The extensive experience of the Carnegie Mellon team in outreach to underrepresented groups involving training a diverse student body will be leveraged in this work, while further expanding the project's outreach to high-school and middle-school students.<br/><br/>This project exploits the fact that using a quantized representation for on-chip training and inference can reduce the energy consumption and storage requirements of the associated hardware implementation. This is a crucial feature in achieving higher throughput and lower latency for real-time learning systems. This project addresses these challenges by developing novel quantization approaches for machine learning models that reduce both data movement and computation, and therefore reduce overall energy consumption. Furthermore, the research pursued herein relies on new algorithmic approaches for training quantized learning models so as to accelerate their training process without harming their accuracy, and learning quantized models on hardware (i.e., field programmable gate arrays) to verify their benefits and accelerate their adoption.  To demonstrate feasibility, the project features a hardware accelerator-based testbed for inference and training models in a quantized fashion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1439145","XPS: FULL: DSD: Asynchronous PDE Algorithms for Turbulent Flows at Exascale","CCF","Exploiting Parallel&Scalabilty","08/15/2014","08/12/2014","Diego Donzis","TX","Texas A&M Engineering Experiment Station","Standard Grant","Micah Beck","07/31/2019","$850,000.00","Nancy Amato, Lawrence Rauchwerger, Sharath Girimaji, Raktim Bhattacharya","donzis@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","8283","","$0.00","Future exascale computing systems will be available to study important,<br/>compute-intensive applications such as multi-physics multi-scale natural<br/>phenomena and engineering systems typically modeled accurately by partial<br/>differential equations (PDEs). A prime example is turbulence at high Reynolds<br/>numbers, typically found in natural and engineering systems, which comprise an<br/>extremely wide range of spatial and temporal scales and has thus became a Grand<br/>Challenge in scientific computing.<br/><br/>Many challenges exists that must be overcome before exascale systems<br/>can be utilized effectively. These include communication between<br/>processing elements as well as global synchronizations both of which will<br/>likely be a main bottleneck when millions of billions of processing elements<br/>are utilized in a simulation.<br/><br/>In this project, the PIs develop novel exascale numerical schemes for PDEs,<br/>especially those describing turbulent flows, that exploit asynchrony from the mathematical to<br/>the software level. These are based on widely used finite differences, compact differentiation and spectral schemes.<br/>Asynchrony offers better performance but also introduces errors in the solution. The new schemes will<br/>be able to trade-off accuracy and performance in a quantitative and predictable manner.<br/>The approach includes (i) rigorous mathematical studies of stability and<br/>accuracy based on numerical analysis and dynamical systems, which will also<br/>provide a framework for the development of new schemes and quantify its<br/>uncertainty, (ii) development of specific elements in a scalable library for parallel<br/>computing to enable portable implementations on current and future machines,<br/>and (iii) physics based modeling of numerical perturbations in<br/>realistic flows.<br/><br/>The tools, techniques and simulation data in this project<br/>will be integrated in the PIs' educational efforts through graduate mentoring, undergraduate<br/>research and as material for courses in high-performance computing, fluid dynamics and dynamical systems<br/>taught by the PIs."
"1439021","XPS: FULL:CCA: Extracting Scalable Parallelism by Relaxing the Contracts across the System Stack","CCF","Exploiting Parallel&Scalabilty","08/01/2014","08/06/2014","Mahmut Kandemir","PA","Pennsylvania State Univ University Park","Standard Grant","Yuanyuan Yang","07/31/2019","$850,000.00","Chitaranjan Das, Anand Sivasubramaniam","kandemir@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8283","","$0.00","Technology scaling trends have made parallelism the de-facto standard for enhancing performance across a spectrum of computing environments spanning from high-end computing  to embedded platforms. Yet, the software is woefully lagging in its ability to extract usable parallelism offered by the underlying hardware platforms primarily because of the compartmentalized contracts between the different layers of the system stack. Rigid contracts restrict the ability to leverage a rich design space of performance/power/correctness trade-offs within and across layers, that could be achievable by straying slightly from the contract. Although such a relaxed contract, referred to as approximate computing, has received attention recently, much of the work in this area is still compartmentalized and lacks a holistic cross-layer strategy to maximize parallelism, while adhering to power and correctness mandates.<br/>                           <br/>Thus, the motivation of this project is to explore a holistic cross-layer approach to approximate computing spanning application, runtime system, compiler and hardware, thereby breaking the rigidity of the contracts between the layers, while still allowing them to cooperate for extracting the achievable parallelism across a diverse set of applications in both the high-end and mobile computing environments. Specifically, it involves application-level analysis of the scope of approximation for computation, data access and synchronization, designing efficient hardware mechanisms that could facilitate and benefit from approximation, and developing compiler and runtime support for expressing, exploiting and evaluating/validating the approximations in an architecture-aware fashion. This cross-layer approach to approximate computing is expected to play a crucial role towards achieving scalable parallelism for the next decade and beyond, with a potentially high impact to the computing industry. In addition, the tools and models developed from this project are disseminated in the public domain to a broader research community, and the PIs engage in a variety of outreach activities such as recruiting women and minority and involvement of local high school students through Penn State Eberly College's Exploration-U initiatives."
"1704240","CIF: Medium: Collaborative Research: Low-Resolution Sampling with Generalized Thresholds","CCF","COMM & INFORMATION FOUNDATIONS","05/01/2017","05/07/2018","Jian Li","FL","University of Florida","Continuing grant","Phillip Regalia","04/30/2021","$193,710.00","","li@dsp.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7797","7924, 7936","$0.00","CIF: Medium: Collaborative Research:<br/>Low-Resolution Sampling with Generalized Thresholds<br/><br/>Jian Li, Lee Swindlehurst, and Mojtaba Soltanalian <br/><br/>Abstract<br/><br/>Quantization of signals of interest is a necessary first step in digital signal processing applications. When signals across a wide frequency band are of interest, a fundamental tradeoff between sampling rate, amplitude quantization precision, cost, and power consumption is encountered. The investigators study low resolution sampling techniques with general thresholds, which are affordable, technically feasible, easy to apply, energy-efficient, and consistent with technological trends. The enormous gains in capacity and spectral efficiency, for example, that could be provided by a successful millimeter wave (mm-wave) massive multiple-input multiple output implementation could have a revolutionary effect on the performance of wireless systems nearly everywhere we use them: at home, at work, at school, commuting via public transportation or by plane, shopping, at restaurants, recreational venues, sporting events, and so on. Besides consumer applications, there are many military- and security-related scenarios where our systems could be used.<br/><br/>This project involves advancing fundamental knowledge in developing dynamic energy-efficient and cost-effective sampling techniques and applies engineering principles to address the critical needs of several important and related applications. Specifically, this project involves  addressing significant open questions, including deterministic identifiability, performance bounds, and impact of thresholding pattern on spectrum sensing and array processing, radio frequency interference mitigation, and mm-wave communications to gain fundamental insights into the novel paradigm of low resolution sampling with general thresholds, devising novel signal processing algorithms, including effective and efficient sparse signal recovery techniques and parametric maximum likelihood methods for enhanced performance, and evaluating and demonstrating the performance using measured data. This project also involves preparing students for engineering in the 21st century through the incorporation of practical design and problem-solving techniques into both the education curriculum."
"1533917","XPS: EXPL: DSD: Portal: A Language and Compiler for Parallel N-body Computations","CCF","SOFTWARE & HARDWARE FOUNDATION, Exploiting Parallel&Scalabilty","07/01/2015","04/28/2017","Aparna Chandramowlishwaran","CA","University of California-Irvine","Standard Grant","Anindya Banerjee","06/30/2019","$316,000.00","","amowli@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7798, 8283","7943, 9251","$0.00","Title: XPS: EXPL: DSD: Portal: A Language and Compiler for Parallel N-body Computations<br/><br/>Modern machines are becoming increasingly more complex resulting in even the most advanced compilers failing to generate the best optimized code. As a result, there is a big gap between the algorithm one designs on paper and the code that runs efficiently on a billion-core system. This research project aims to bridge this gap by developing Portal, a new high-performance domain-specific language(DSL) and compiler, for the domain of N-body problems. Such problems have applications in various areas ranging from scientific computing simulations in molecular dynamics, astrophysics, acoustics, fluid dynamics all the way to big data problems. In Portal, domain scientists can write programs at a high level while obtaining the performance of highly tuned and optimized low level code written by experts on modern massively parallel machines. The intellectual merit is to show how a DSL with a high-level formulation can lead directly to both asymptotically fast algorithms and their efficient parallel implementations on a variety of distinct architectures. The project's broader significance and importance are freely available software to enable domain scientists to harness the performance power of parallel computing and enabling scientific discovery not only in scientific computing and machine learning but also in a number of related problems in domains such as statistics, computer graphics, computational geometry, and applied mathematics. These problems can be expressed in Portal to obtain an out-of-the-box parallel optimized implementation.<br/><br/>The goals of Portal are three-fold: (a) to implement scalable, fast, and asymptotically optimal tree-based N-body algorithms, (b) to design an intuitive language and API to enable rapid implementations of a variety of algorithms, and (c) to enable parallel large-scale problems to run on current and future exascale machines. More importantly, the language and intermediate algorithm representation are independent of the architecture, making this approach portable and easily extensible to different platforms from ARM/x86 CPUs to GPUs. The project aims to solve important software issues that will allow for interoperability and scalability of N-body problems on massive datasets."
"1526780","CIF: Small: Challenges and opportunities of timing mismatch in multi-user wireless networks","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2015","01/25/2017","Hamid Jafarkhani","CA","University of California-Irvine","Standard Grant","Phillip Regalia","09/30/2019","$553,072.00","","hamidj@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7797","7923, 7935, 9232","$0.00","The tremendous expansion of wireless communications has improved the quality of life, provided economical growth, and assisted the expansion of new businesses. At the same time, the new opportunities have created a huge demand for higher transmission data rates and a need for designing more efficient wireless communication networks. Understanding how to deal with timing mismatch and imperfect channel information in communication networks is essential for designing the wireless networks of the future.<br/><br/>Recently, interference management techniques have provided enormous improvements in reliability and bandwidth utilization. These improvements are achieved under ideal assumptions such as perfect channel knowledge and perfect synchronization. In this project, the PIs will revisit the availability of perfect channel information and more importantly the assumption of perfect synchronization in multi-user wireless networks. This project analyzes the effects of timing mismatch on the performance of multi-user wireless networks, from the viewpoint of sampling diversity, with the aim to explore its benefits in the presence of timing mismatch. This will lead to a theoretical derivation of optimal timing mismatch values and the corresponding optimal timing delays, followed by the practical design of communication systems that benefit from timing mismatch and approach the theoretical performance bounds, under a comprehensive set of channel models."
"1563924","SHF: Medium: Collaborative Research: Materials authentication using nuclear quadrupole resonance spectroscopy","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","07/15/2016","09/16/2018","Swarup Bhunia","FL","University of Florida","Continuing grant","Sankar Basu","06/30/2020","$400,000.00","","swarup@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","2878, 7798","7798, 7924, 7945","$0.00","Counterfeit and substandard pharmaceuticals, dietary supplements, and food items containing extremely harmful contaminants have emerged as a major problem worldwide for the health industry. High-value items such as packaged medicines, which are often sold online through untrusted supply chains, are particularly prone to fraud. This project aims to develop and test novel low-cost materials authentication techniques that would enable end users to easily and reliably verify the chemical composition of medicines and dietary supplements. The widespread adoption of the proposed authentication technology could have an impact on public health by significantly enhancing the security of the supply chain for pharmaceutical and food products. <br/><br/>The proposed authentication approach is based on comparing the Nuclear Quadrupole Resonance (NQR) spectra generated by the material under test with reference spectra stored in a secure database. The atoms of about half of all the elements in the periodic table contain so-called quadrupolar nuclei that generate NQR signals. This project will focus on the spectra of nitrogen, which is found in a large majority of pharmaceutical products. The NQR spectra are highly sensitive to chemical composition and physical properties, and thus act as unique ""chemical fingerprints"" that are difficult to emulate or falsify. Low power portable electronics and an integrated system resulting in instrumentation for noninvasive, nondestructive and quantitative testing will be developed for combining sensing, software, and data collection/analysis."
"1703635","CIF:Medium:Collaborative Research:Low Resolution Sampling with Generalized Thresholds","CCF","COMM & INFORMATION FOUNDATIONS","05/01/2017","05/11/2018","Arnold Swindlehurst","CA","University of California-Irvine","Continuing grant","Phillip Regalia","04/30/2021","$194,349.00","","swindle@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7797","7924, 7936","$0.00","CIF: Medium: Collaborative Research:<br/>Low-Resolution Sampling with Generalized Thresholds<br/><br/>Jian Li, Lee Swindlehurst, and Mojtaba Soltanalian <br/><br/>Abstract<br/><br/>Quantization of signals of interest is a necessary first step in digital signal processing applications. When signals across a wide frequency band are of interest, a fundamental tradeoff between sampling rate, amplitude quantization precision, cost, and power consumption is encountered. The investigators study low resolution sampling techniques with general thresholds, which are affordable, technically feasible, easy to apply, energy-efficient, and consistent with technological trends. The enormous gains in capacity and spectral efficiency, for example, that could be provided by a successful millimeter wave (mm-wave) massive multiple-input multiple output implementation could have a revolutionary effect on the performance of wireless systems nearly everywhere we use them: at home, at work, at school, commuting via public transportation or by plane, shopping, at restaurants, recreational venues, sporting events, and so on. Besides consumer applications, there are many military- and security-related scenarios where our systems could be used.<br/><br/>This project involves advancing fundamental knowledge in developing dynamic energy-efficient and cost-effective sampling techniques and applies engineering principles to address the critical needs of several important and related applications. Specifically, this project involves  addressing significant open questions, including deterministic identifiability, performance bounds, and impact of thresholding pattern on spectrum sensing and array processing, radio frequency interference mitigation, and mm-wave communications to gain fundamental insights into the novel paradigm of low resolution sampling with general thresholds, devising novel signal processing algorithms, including effective and efficient sparse signal recovery techniques and parametric maximum likelihood methods for enhanced performance, and evaluating and demonstrating the performance using measured data. This project also involves preparing students for engineering in the 21st century through the incorporation of practical design and problem-solving techniques into both the education curriculum."
"1619027","SHF: Small: Reconfigurability and Technology Integration  of Magnetic Energy Minimization Co-Processor (MEMCoP)","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","07/07/2016","Sanjukta Bhanja","FL","University of South Florida","Standard Grant","Sankar Basu","06/30/2019","$449,999.00","Greg Carman","bhanja@eng.usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","7798","7923, 7945","$0.00","This collaborative proposal been two universities - USF and UCLA - advocates a new form of computing that uses circular nanomagnets to solve quadratic optimization problems, orders of magnitude faster than that of a conventional computer. A wide range of application domains can be potentially accelerated through this research, e.g., computationally expensive and hard to parallelize problems of finding patterns in social media, error-correcting codes, big data applications etc. The project plans to engage undergraduate REU students, and the PIs will be involved with the local community through interaction with K-12 teachers and K7-K8 women students. As a part of Sloan University Center for Exemplary Mentoring (UCEM), the lead PI will recruit and retain undergraduate students. The UCLA team will work with a group of undergraduate students sponsored by Center for Excellence in Engineering and Diversity (CEED).<br/><br/>Core computational theme of this proposal is mapping quadratic energy minimization problem spaces into a set of interacting magnets such that the energy relationship between the problem variables is proportional to that of the energies between the corresponding magnets. The optimization is accomplished by the relaxation physics of the magnets themselves and solutions can be read-out in parallel. In essence, given a specific instance of the problem, the plan is to arrive at a specific magnetic layout, the relaxed state of which will be the solution of the original problem.  Reconfigurability is fundamental to the success of this form of high-payoff computing (one does not intend to fabricate a specific magnetic layout to solve one instance of a problem). Three alternative programming solutions are proposed, and trade-offs between the mechanisms will be evaluated. The project also plans to explore reconfigurability together with readability and system integration aspects."
"1616248","AF:SMALL:Sparse Geometric Graph Algorithms","CCF","ALGORITHMIC FOUNDATIONS","08/01/2016","05/31/2016","David Eppstein","CA","University of California-Irvine","Standard Grant","Rahul Shah","07/31/2019","$415,894.00","","eppstein@ics.uci.EDU","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7796","7923, 7929","$0.00","Many real-world problems can be modeled by geometric graphs: systems<br/>of vertices that represent geometric objects and edges that represent<br/>connections or interactions between pairs of these objects. For<br/>instance, the vertices of a road network may represent intersections<br/>or junctions of roads, while the edges represent the segments of road<br/>between two consecutive intersections. Although these graphs are not<br/>planar (a road can cross a bridge without intersecting the road beneath), such crossings are<br/>uncommon, and the project will investigate ways of exploiting that<br/>sparse crossing structure to efficiently solve problems such as route<br/>planning on road networks. Similarly, the project will investigate the<br/>structure of the underlying geometric graphs, and the use of that<br/>structure to develop efficient algorithms, for other problems that<br/>include the visualization of hierarchically clustered networks,<br/>pattern mining in social networks, splitting large scientific<br/>simulations into smaller subproblems with low amounts of interaction<br/>between the subproblems, the analysis of mechanical systems of rigid<br/>parts connected by hinges, the design of maps that distort geographic<br/>areas to display other types of quantitative information, and the<br/>visualization of overlaps between fragments of DNA sequences.<br/><br/><br/>The project proposes attacks on a large collection of problems from<br/>application areas where sparse geometric graphs naturally arise. The<br/>project suggests a model for real-world road networks in which a<br/>derived graph of edges and their crossings has bounded degeneracy, and<br/>seeks to investigate how sparsity properties of this crossing graph<br/>affect the underlying road graph. The project seeks to determine<br/>whether it is hard to recognize the graphs of 4-polytopes and simple<br/>4-polytopes, and whether a recognition algorithm of the investigator<br/>for a special class of 4-polytopes can be extended to a realization<br/>algorithm. The graphs in this class have small separators, and the<br/>project seeks to determine whether this is true more generally for<br/>certain graphs derived from planar clustered graph drawings. The<br/>project seeks efficient data structures that maintain low-degeneracy<br/>orientations of a dynamic graph and use these orientations to quickly<br/>find features in the graph, needed when fitting social networks to<br/>exponential random graph models. Finite element meshes with bounded<br/>aspect ratio for each element have small separators, but arbitrary<br/>tetrahedral meshes do not. The project seeks intermediate conditions<br/>on the shape of the elements in a mesh that would ensure the existence<br/>of small separators. The graphs arising in kinematic analysis can be<br/>characterized by linear bounds on the edges in each subgraph, and<br/>recognized in quadratic time by a pebbling algorithm. The project<br/>seeks more efficient algorithms to test the rigidity or degrees of<br/>freedom of mechanical structures in subquadratic time. Subdivisions of<br/>rectangles into smaller rectangles have applications in architectural<br/>design, cartographic information visualization, and VLSI design. An<br/>area-universal subdivision is one that can fit any assignment of areas<br/>to its rectangles. The project seeks efficient algorithms to construct<br/>area-universal subdivisions as well as subdivisions produced by a<br/>recursive splitting process. Unit interval graphs arise in modeling<br/>human preferences and in DNA physical mapping. Some problems that are<br/>hard on broader graph classes can be solved efficiently on unit<br/>interval graphs by representing the graph using a binary sequence and<br/>applying a finite automaton to the sequence. The project seeks a more<br/>general explanation of this phenomenon. In graph drawing, styles of<br/>graph drawing on the basis that are somewhere-dense allow all graphs<br/>to be drawn in that style with constant bends per edge while nowhere<br/>dense styles require an unbounded number of bends per edge. The<br/>project seeks a theory of sparsity distinguishing nowhere-dense styles<br/>that require many bends per edge from those that require few<br/>bends. Additional components of the project include questions on<br/>diameter graphs and book thickness."
"1814798","SHF: SMALL: DockerizeME: Automatic Inference and Repair of Computing Environments","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","12/11/2018","Christopher Parnin","NC","North Carolina State University","Standard Grant","Sol Greenspan","09/30/2021","$349,675.00","","cjparnin@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7923, 7944","$0.00","When developing or deploying software, programmers must ensure that any external libraries or services which are necessary to run an application are properly installed and configured. The process of preparing computing infrastructure to properly execute an application is referred to as software configuration. Currently, programmers often manually perform software configuration, which can result in errors and poor maintainability; as a result, improper software configuration can cost billions of dollars of loss for business, lead to unexpected downtime of services, and cause failure of critical infrastructure and loss of data. Unfortunately, the skills required for proper software configuration can be orthogonal to software development, meaning there are limited programmers who are trained in software configuration skills.<br/><br/>This project will develop techniques to automatically perform the software configuration necessary to run an arbitrary application. Two main research tasks will be investigated for this project. One task will be to develop an approach for automatically inferring a Dockerfile, a configuration script for the Docker container system, capable of executing an application. The approach will use automatic code analysis of existing software libraries to build an offline knowledge base capable of recovering the dependencies between them. The approach will augment the knowledge base with rules learned by mining existing Dockerfiles, configuration scripts, and developer resources like Stack Overflow. Further, the approach will apply minimization techniques on environment specifications extracted from this knowledge base to arrive at a minimal set of application dependencies. The second task will develop a system to detect when configuration scripts are incompatible with code, such as in the event of a dependency upgrade, and use search-based techniques to automatically repair these configuration scripts. In addition, transfer learning will be used to guide successful inferences and repairs. Finally, these approaches are applied in two applications: detecting when code snippets in community resources are incompatible with an API version, and building repair bots that can automatically create a pull request for repairing configuration scripts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845706","CAREER: Rethinking Abstractions in Virtualized Architectures and Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2019","01/31/2019","Jia Rao","TX","University of Texas at Arlington","Continuing grant","Yuanyuan Yang","06/30/2024","$81,367.00","","jia.rao@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","7798","1045, 7941","$0.00","Recent advances in cloud computing have led to a wide adoption of virtualization techniques in modern computer systems which allow better utilization of computer resources. There is also a steady trend towards building future data centers and high-performance computers with a software-defined architecture. However, challenges remain in adopting virtualization in many critical domains.  First, there still exists a large performance gap between virtualized and physical systems, especially for high-speed devices, scientific workloads and latency-sensitive applications. Second, performance isolation and resource elasticity are often contradictory goals under the current models, which leaves much of the economic benefit of virtualization unexploited. Third, the additional system complexity in virtualiation undermines stability and predictability. This research addresses these issues and seeks to improve the performance, cost-effectiveness, and predictability of virtualized systems. The research will be tightly integrated into teaching and further broaden its impacts through mentoring and recruiting minority students, and outreach activities in K12 schools.<br/><br/>Specifically, this project will identify gaps in the existing abstractions that cause performance degradation, inefficiencies and unpredictability, as well as pinpointing the essence of current abstractions that has enabled isolation, modularity and portability. The project entails three research thrusts: First, it will design and implement augmented abstractions for various types of virtualized systems, including virtual machines, containers and virtualized networks, to bridge the semantic gaps. Second, it will leverage the augmented abstractions to design efficient, effective and elastic resource management schemes while retaining much of the benefit of the existing abstractions. Third, it will increase the understanding of abstraction in multi-tenant systems and apply the knowledge to studying the inefficiencies of conventional systems and guide the design of new abstractions in emerging architectures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1439156","XPS: FULL: CCA: Collaborative Research: CASH: Cost-aware Adaptation of Software and Hardware","CCF","Exploiting Parallel&Scalabilty","09/01/2014","08/08/2014","Henry Hoffmann","IL","University of Chicago","Standard Grant","Yuanyuan Yang","08/31/2019","$300,000.00","","hankhoffmann@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8283","","$0.00","Programming for and leveraging the benefits of scalable and highly<br/>parallel computer architectures is becoming increasingly<br/>challenging. In order to ease programmer and user effort in achieving<br/>efficient use of parallel architectures and to enable optimal usage of<br/>parallel hardware both within a single chip and across a data center,<br/>this project explores the co-design of a fine-grain configurable (down<br/>to the ALU, FPU, Cache bank, fetch unit, etc) architecture along with<br/>a software optimizing runtime system which controls the architecture?s<br/>configuration. This runtime management system understands high-level<br/>goals and constraints such as optimizing for power, latency, cost, or<br/>complex mixed-goals and dynamically allocates fine-grain resources to<br/>meet the constraints. This project will evaluate and design a configurable<br/>manycore-inspired architecture and a self-adapting runtime system.<br/><br/>This project will investigate the creation of a complete, scalable,<br/>self-adaptive computing system; and will push the boundaries of<br/>adapting systems by utilizing hardware that is configurable and<br/>monitorable below the processor core level. By providing such a<br/>flexible and highly monitored architecture to the adaptation runtime,<br/>this project will explore the scalability of adaptive runtime<br/>algorithms to handle a game-changing increase in the number of<br/>controllable parameters. This project will explore the extent to<br/>which it is fruitful to configure a parallel architecture."
"1724784","CAREER:  Algorithms for Next-Generation Genomics","CCF","ALGORITHMIC FOUNDATIONS, COMPUTATIONAL BIOLOGY","09/01/2016","05/18/2017","Benjamin Raphael","NJ","Princeton University","Continuing grant","Mitra Basu","12/31/2019","$292,101.00","","braphael@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7796, 7931","1045, 1187, 7931, 9150, 9218","$0.00","CAREER: Algorithms for Next-Generation Genomics<br/>The objective of this project is to develop algorithms for new and emerging high-throughput DNA sequencing technologies.  These technologies are lowering the cost of DNA sequencing by orders of magnitude and thereby enabling a variety of new applications.  These new applications, combined with the varying characteristics of the DNA sequences produced by these technologies, are increasing demand for efficient algorithms to interpret the resulting large volumes of DNA sequence data. The PI will develop a new class of robust algorithms for genome assembly and discovery of DNA sequence variants.  Some of these algorithms will rely on the availability of a closely related reference genome sequence, while others will operate de novo directly from the individual DNA sequences (i.e. reads) produced by a DNA sequencing machine.  In the latter case, the PI will design algorithms   that exploit longer range DNA sequence information available in newer single-molecule and nanopore sequencing technologies.  These algorithms will retain high sensitivity and specificity while scaling to billions-trillions of nucleotides and thousands of genomes. Finally, the PI will introduce combinatorial algorithms for the study of genome rearrangements in heterogeneous mixtures of DNA sequences.  Such mixtures arise in metagenomics or cancer genomics, where the DNA that is sequenced is a mixture of genomes from different species, or from cells harboring different mutations, respectively.  The PI collaborates closely with biologists and technology developers to ensure relevance and applicability of the algorithms.  At the same time, some of algorithms and techniques from graph theory, combinatorial optimization, and probability that are developed in the proposal are applicable to problems outside of biology. <br/>Broader Impact<br/>The proposed research will be integrated with an educational component that includes the development of undergraduate seminar in personal genomics, a summer research experience in computational biology for high-school students, and the incorporation of a computational biology module into a summer computing camp for 9th grade girls. The PI will continue to actively mentor and recruit undergraduate and graduate students, including women and underrepresented minorities. Finally, software implementing the algorithms will be freely distributed to the scientific community through a public webserver."
"1717899","AF: Small: Duality-based tools for simple vs. optimal mechanism design and applications to cryptocurrency","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","04/25/2018","Seth Weinberg","NJ","Princeton University","Standard Grant","Tracy Kimbrel","08/31/2020","$466,000.00","","smweinberg@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7796","7923, 7932, 9251","$0.00","Traditionally, we think of algorithms as ""processing an input"" in order to ""produce an output."" For example, imagine a firm trying to allocate various resources among its employees: It might solicit from each employee how each resource affects their productivity (the ""input""), and allocate the resources in a way to maximize the total production (the ""output""). When each user has the same goal (maximize the firm's productivity), the traditional algorithmic paradigm perfectly captures the firm's objective.<br/><br/>What if instead we wish to model a cloud computing service allocating various resources among its users? Now, the goals of the individual users (maximize their own productivity) are misaligned with that of the service. Therefore, users may manipulate whatever algorithm is deployed in order to improve their own productivity, possibly at the cost of others'. Properly designing solutions for such domains inevitably requires mechanism design, which makes use of both algorithmic and game theoretic tools. As part of this project, the PI is developing a new undergraduate course ""Economics and Computation"" in order to provide the next generation of computer scientists with the ability to reason rigorously about the incentives of users who interact with their systems.<br/><br/>Much prior work prescribes wildly complex mechanisms which are unusable in practice, motivating prior work of the PI and others to investigate the theoretical properties of simple mechanisms. The main research focus of this project is to greatly expand our understanding of how to appropriately deploy simple mechanisms, via a rigorous theoretical foundation. As part of this project, the PI will continue giving talks and tutorials about the specific approach used to obtain these new results, referred to as a ""duality theory.""<br/><br/>A secondary focus of this project is to apply these theoretical foundations to resolve cryptocurrency incentive issues arising within Bitcoin, an emerging cryptocurrency. While Bitcoin has remained largely immune to traditional security breaches, numerous incentive issues have been discovered which could undermine its future security if not properly addressed. As part of this project, the PI will help broaden participation by other mechanism designers in this direction via tutorials.<br/><br/>In a little more detail, the main research focus of this project aims to answer questions such as ""what properties of a setting make simple mechanisms appropriate or inappropriate?"" or ""Quantitatively, how 'complex' does a mechanism need to be in order to be 'close' to optimal?"" The main technical ingredient in this approach is a novel duality framework recently developed by the PI and co-authors. The secondary focus aims specifically to make progress towards an incentive compatible cryptocurrency protocol: one where all users are incentivized to follow the protocol in earnest, even when their refusal to do so goes undetected. While the PI's focus will be on the theoretical foundations of such a protocol, any findings will be followed up by experimental (simulations) or empirical (data) research as well."
"1533837","XPS: EXPL: CCA: Verification and Optimization Tools for Heterogeneous Memory Consistency Models","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/12/2015","Margaret Martonosi","NJ","Princeton University","Standard Grant","Yuanyuan Yang","08/31/2019","$300,000.00","","martonosi@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8283","","$0.00","Over the past decade, the deceleration of Moore's Law and Dennard Scaling has required computing to make a dramatic shift towards on-chip parallelism in order to achieve computer performance scaling at acceptable power budgets.  In further response, the use of diverse processing elements and specialized accelerators has also increased; many smartphone processors or systems-on-chip (SoCs)  include 4-6 different instruction set architectures (ISAs) and memory consistency models (MCMs).  In the face of this increasing heterogeneity, this project's research aims to tame the architecture, verification, and software implications of this fast-growing complexity.<br/><br/>Ensuring that computations occur on the right data at the right time is fundamental to computing system reliability, and MCMs are intended to guarantee this in multi-threaded systems, but better verification and translation support is needed.  In particular, this work is developing a toolkit with elements including: (i) Grammars for specifying MCMs and hardware implementations, as well as tools to derive these specifications from existing design descriptions, appropriately annotated if needed. (ii) Modules for enumerating and checking implementation-level (i.e. microarchitecture-level) Happens-Before-Graphs to generate verifiers for arbitrary MCMs and implementations. (iii) Modules for automatically translating from one MCM to another. (iv) Tools that compose the above modules to automatically generate litmus tests, to do binary translation including MCM translation, and other useful examples. (v) A pedagogical tool (an MCM animator and illustrator) for teaching students in computer architecture and parallel programming classes.  To facilitate broad use of this work, basic modules and composed tools will be distributed as free software."
"1815033","SHF: Small: Collaborative Research: Energy Efficient Strain Assisted Spin Transfer Torque Memory","CCF","GRANT OPP FOR ACAD LIA W/INDUS, SOFTWARE & HARDWARE FOUNDATION","10/01/2018","12/13/2018","Jayasimha Atulasimha","VA","Virginia Commonwealth University","Standard Grant","Sankar Basu","03/31/2022","$402,392.00","Supriyo Bandyopadhyay","jatulasimha@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","1504, 7798","019Z, 7923, 7945","$0.00","Non-volatile random access memory (NV-RAM) is often built with a device called spin transfer torque random access memory (STT-RAM), the main constituent of which is a circular nano-magnet. A bit is ""written"" into the nano-magnet by passing a spin-polarized current whose polarity determines whether bit ""1"" or bit ""0"" is written. The energy barrier between these states prevents the magnetization from switching spontaneously due to thermal noise, making the device non-volatile. Unfortunately, the energy dissipated in the writing current is 100-1000 times more than the energy dissipated in today's CMOS devices, which is a large cost to pay for non-volatility. This project seeks to demonstrate that temporarily reducing the energy barrier between the ""up"" and ""down"" magnetization states with surface acoustic waves (SAW) can significantly lower the current needed to write a bit and reduce the energy dissipation by orders of magnitude. This would make the SAW-assisted STT-RAM ideal for embedded processors, internet of things, large data centers and cyber-physical systems requiring low energy memory. At least 3 PhD students would be trained on the techniques of complementary nano-fabrication, nano-characterization and computer modeling. The investigators will hold a nano-magnetism workshop for high school students and will host under-represented K-12 students in their labs for a summer month, as well as leverage the ""Nano-Days"" program to reach out to high school students. <br/><br/>The key technical approach in this research project will involve the following complementary experimental and modeling research directions: (i) Modeling the combined effect of strain and spin transfer torque (STT) on the magnetization switching in the presence of thermal noise (ii) Experimental demonstration of strain induced reduction in STT write current in optimized devices (guided by the modeling effort) (iii) Proof-of-concept demonstration of acoustic wave induced reduction in STT write current. The knowledge generated by this research would lead to better understanding of the combined effect of strain and spin torque on switching nano-magnetic memory elements and the switching probability in the presence of thermal noise. If successful, it would demonstrate a low energy paradigm for writing information in non-volatile nano-magnetic memory devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1733872","AiTF: Collaborative Research: Algorithms for Smartphone Peer-to-Peer Networks","CCF","Algorithms in the Field","09/01/2017","09/15/2018","Klara Nahrstedt","IL","University of Illinois at Urbana-Champaign","Standard Grant","Tracy Kimbrel","08/31/2020","$318,735.00","","klara@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7239","","$0.00","The growing ubiquity of smartphones, combined with the rapid improvement of operating system support for direct wireless communication between nearby devices, creates a compelling opportunity for the emergence of easy to deploy and widely used smartphone peer-to-peer applications. There are several use cases for such applications. For example, in many user environments, cellular data minutes are bought in small blocks and carefully conserved by users, generating an interest in networking operations that can avoid infrastructure.  In addition, smartphone peer-to-peer networks can bring connectivity to settings such as disaster zones, festivals, or wilderness where traditional cellular and WiFi coverage is compromised, overwhelmed, or non-existent. This project aims to develop network algorithms and tools that simplify the design of useful distributed systems on top of local peer-to-peer connections. The project has the potential for significant societal impact by enabling compelling new applications for smartphone peer-to-peer networks. The project is also expected to have significant educational impact.<br/> <br/>The project focuses on designing and analyzing provably correct and efficient network computation algorithms that can run on top of existing smartphone peer-to-peer services, and simplify the design of peer-to-peer systems that can be deployed on existing smartphone hardware. The project consists of two major research directions. The first research direction is experimental in nature. It will study and evaluate peer-to-peer services available in commodity smartphone operating systems and define a small number of validated abstractions that capture their capabilities and behavior. The second research direction is theoretical in nature. It will describe and analyze solutions to well-motivated network computation primitives using these abstractions. The problems studied will include consensus, leader election, rumor spreading, gossip and function computation. The project will seek both provably correct and efficient algorithms as well as lower bounds that establish fundamental limits for useful computation in this setting."
"1725447","SPX: Collaborative Research: Ula! - An Integrated Deep Neural Network (DNN) Acceleration Framework with Enhanced Unsupervised Learning Capability","CCF","SPX: Scalable Parallelism in t","09/01/2017","07/22/2017","Yuan Xie","CA","University of California-Santa Barbara","Standard Grant","Yuanyuan Yang","08/31/2021","$280,000.00","","yuanxie@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","042Y","026Z","$0.00","In light of very recent revolutions of unsupervised learning algorithms (e.g., generative adversarial networks and dual-learning) and the emergence of their applications, three PIs/co-PI from Duke and UCSB form a team to design Ula! - an integrated DNN acceleration framework with enhanced unsupervised learning capability. The project revolutionizes the DNN research by introducing an integrated unsupervised learning computation framework with three vertically-integrated components from the aspects of software (algorithm), hardware (computing), and application (realization). The project echoes the call from the BRAIN Initiative (2013) and the Nanotechnology-Inspired Grand Challenge for Future Computing (2015) from the White House. The research outcomes will benefit both Computational Intelligence (CI) and Computer Architecture (CA) industries at large by introducing a synergy between computing paradigm and artificial intelligence (AI). The corresponding education components enhance existing curricula and pedagogy by introducing interdisciplinary modules on the software/hardware co-design for AI with creative teaching practices, and give special attentions to women and underrepresented minority groups.<br/><br/>The project performs three tasks: (1) At the software level, a generalized hierarchical decision-making (GHDM) system is designed to efficiently execute the state-of-the-art unsupervised learning and reinforcement learning processes with substantially reduced computation cost; (2) At the hardware level, a novel DNN computing paradigm is designed with enhanced unsupervised learning supports, based on the novelties in near data computing, GPU architecture, and FGPA + heterogeneous platforms; (3) At the application level, the usage of Ula! is exploited in scenarios that can greatly benefit from unsupervised learning and reinforcement learning. The developed techniques are also demonstrated and evaluated on three representative computing platforms: GPU, FPGA, and emerging nanoscale computing systems, respectively."
"1524519","SHF: Small: Lambda Encodings Reborn","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","06/26/2018","Aaron Stump","IA","University of Iowa","Standard Grant","Nina Amla","01/31/2020","$543,918.00","","aaron-stump@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7798","7923, 8206, 9150, 9251","$0.00","Proof assistants are software tools that assist users in developing formal proofs of theorems. Proof assistants are now widely used to verify large  software systems. Hence a more trustworthy proof assistant could have significant impact on high assurance software. An important issue in the design of proof assistants is how to ensure they are logically sound. This project investigates a new foundation for proof assistants, based on a method of representing data using only functions which is known as lambda encodings.  <br/><br/>Lambda encodings are important for proof assistants because they eliminate the need for a datatype subsystem.  Such subsystems are complicated, and increase the difficulty of ensuring logical soundness of the proof assistant.  There are several technical problems in using lambda encoding for this purpose, including the fact that induction principles could not be derived using them.  This project develops new solutions to these problems, that enables the use of lambda encodings as a viable foundation for proof assistants. These new methods will be integrated into a new proof assistant, called Cedille, which has a simpler foundation than other similar tools, and increases its trustworthiness."
"1331390","CyberSEES: Type2: In-Situ, Wireless, Energy-Harvesting Soil Moisture/Nutrient Sensors for Managing Agricultural Resources & Environmental Impact","CCF","GRANT OPP FOR ACAD LIA W/INDUS, INFORMATION TECHNOLOGY RESEARC, Research Coordination Networks, CyberSEES","09/15/2013","11/21/2018","Ratnesh Kumar","IA","Iowa State University","Standard Grant","Eva E. Zanzerkia","08/31/2019","$1,050,000.00","Robert Weber, Liang Dong, Michael Castellano, Fernando Miguez","rkumar@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","1504, 1640, 1664, 8211","019Z, 9150","$0.00","The project will address Cyber-Enabled Nutrient Management for Sustainable Agriculture through precise spatial/temporal control of agricultural inputs for optimized resource utilization and minimized environmental impact. There is a critical need for the development of cyber technologies in production agriculture that allow automated collection of real-time spatial soil and crop data while advancing a deeper understanding of the fertilizer inputs and nitrogen (N) cycling. This project aims to develop: (1) In-situ, accurate, self-calibrating, self-localizing soil sensors for monitoring soil properties such as soil moisture and nitrates, (2) Small antenna technologies for underground communication so the sensors do not interfere with agricultural operations, (3) Effective techniques for broadband energy-harvesting from vibrational sources (eg., thunder and farming operations); (4) Modeling and analysis methods for understanding the underlying nitrogen cycling at high spatial/temporal resolutions, and information management and decision-making tools for precise agricultural control.<br/><br/>Managing the nitrogen Cycle is one of the grand challenges identified by the National Academies. Nitrogen fertilizers from farm fields are major source of water quality impairment and the leading contributor to hypoxia. The project is a step towards developing deeper understanding of agricultural N cycling process while developing precise controls over N fertilizer inputs that are key to sustainable agriculture. To enhance educational and workforce development efforts, the PIs will contribute to a graduate minor in Sustainable Agriculture by introducing new curriculum material on Bio-Chemical Sensors. PhD students of the funded research will enroll in the Sustainable Agriculture minor, thus developing a new generation of workforce trained in the aspects of cybersystems, agroecosystems, environmental monitoring, and sustainable cultivation. To achieve wider awareness and reception, PIs will work closely with industry and government organizations in the agricultural sector, several of whom have expressed interest through supporting letters."
"1740197","E2CDA: Type I: Collaborative Research: Energy-Efficient Artificial Intelligence with Binary RRAM and Analog Epitaxial Synaptic Arrays","CCF","Energy Efficient Computing: fr","09/15/2017","07/23/2018","Saibal Mukhopadhyay","GA","Georgia Tech Research Corporation","Continuing grant","Sankar Basu","08/31/2020","$162,412.00","","saibal@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","015Y","7945, 8089","$0.00","In recent years, deep learning and artificial neural networks have been very successful in large-scale recognition and classification tasks, some even surpassing human-level accuracy. However, state-of-the-art deep learning algorithms tend to present very large network models, which poses significant challenges for hardware, especially for memory. Emerging resistive devices have been proposed as an alternative solution for weight storage and parallel neural computing, but severe limitations still exist for applying resistive random access memories (RRAMs) for practical large-scale neural computing. This proposal targets on addressing limitations in resistive device based neural computing through novel device engineering, new bitcell designs, new neuron circuits, energy-aware architecture, and a new circuit-level benchmark simulator. A successful completion of this research is likely to have consequences to our society, enabling wide adoption of dense and energy-efficient intelligent hardware to power-/area-constrained local mobile/wearable devices. Furthermore, a self-learning chip that learns in near real-time and consumes very low-power can be integrated in smart biomedical devices, personalizing healthcare. This project will have a strong effort on integrating the research outcomes with education and outreach through summer outreach programs for high school students, undergraduate/graduate student training, and organization of tutorials and workshops at conferences for knowledge dissemination.<br/><br/>The proposal will perform  interdisciplinary research to address many limitations in today's resistive device based neural computing and make a leap progress towards energy-efficient intelligent computing. Severe limitations of applying resistive random access memories (RRAMs) for practical large-scale neural computing include: (1) device-level non-idealities, e.g., non-linearity, variability, selector, and endurance, (2) inefficiency in representing negative weights and neurons, and (3) limited demonstration on simpler networks, instead of cutting-edge convolutional and recurrent neural networks. To address these limitations, novel technologies from devices to architectures will be investigated. First, new bitcell circuits will be designed for today's binary resistive devices, efficiently mapping XNOR functionality with (+1, -1) weights and neurons. Second, a novel epitaxial resistive device (EpiRAM) that exhibits many idealistic properties will be investigated, including linear programming for analog weights, suppressed variability, self-selectivity, and high endurance. Third, new neuron circuits will be explored for integration with new resistive devices for feedforward/feedback deep neural networks. Finally, new data-mapping techniques that efficiently map state-of-the-art deep neural networks onto the hardware framework with RRAM arrays will be developed, and the overall energy-efficiency will be verified with a new benchmark simulator ""NeuroSim"". With innovations across material, device, circuit and architecture,  research needs will be pursued towards energy-efficient processing in ubiquitous resource-constrained hardware systems."
"1637385","AitF: The Fuzzy Log: A Unifying Abstraction for the Theory and Practice of Distributed Systems","CCF","Algorithms in the Field","10/01/2016","07/07/2018","Mahesh Balakrishnan","CT","Yale University","Standard Grant","Tracy J. Kimbrel","09/30/2019","$600,000.00","James Aspnes, Daniel Abadi","mahesh.balakrishnan@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7239","","$0.00","The Fuzzy Log project seeks to democratize the design and development of complex distributed systems, accelerating innovation by allowing developers to focus on high-level application functionality instead of low-level protocol details. Examples of such complex systems include Software Defined Network controllers for the network, filesystem namespaces for storage, schedulers and allocators for big data run-times, and general-purpose coordination services.  These distributed systems require large numbers of highly trained engineers and scientists to construct and operate them. Simplifying the design, development, deployment and debugging of such systems can drastically reduce the cost to create and operate massively scalable cloud services that are reliable and responsive. More broadly, the Fuzzy Log project will also act as an educational gestalt that combines distributed systems and theory to improve the state of the art in cloud computing.<br/><br/>A Fuzzy Log is a partially ordered shared log that multiple clients can append to and read from concurrently. As in other shared log designs, applications can extract properties such as consistency, durability, and concurrency control from the Fuzzy Log. However, unlike a conventional shared log, a Fuzzy Log does not impose a total order over all entries. When clients append to the log, they specify dependencies to define a partial order; when they read from the log, the system returns entries in some sequence satisfying the partial order. Fuzzy Log applications are simple to design, implement, and debug, with full-fledged distributed systems realized in hundreds of lines of code. Fuzzy Log applications are also fast and scalable, extracting parallelism from workloads while imposing order only when strictly necessary."
"1702596","SHF: Medium: Microbiology on a Programmable Biochip: An Integrated Hardware/Software Digital Microfluidics Platform","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2017","07/17/2017","Krishnendu Chakrabarty","NC","Duke University","Standard Grant","Sankar Basu","06/30/2020","$900,000.00","Richard Fair, Kristin Scott","krish@ee.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7798","7924, 7945","$0.00","The goal of this research is to develop miniaturized, low-cost, and smart microfluidic biochips that can perform ""epigenetics in a drop"". Such a platform will revolutionize data acquisition for molecular biology studies. The knowledge gained from applying molecular biology protocols to microfluidic biochips will facilitate the understanding of diseases such as cancer, and knowledge about epigenetic modifications in cells from inheritance will broaden the understanding of disease development. The results from this research will also be applicable to quantitative analysis protocols such as gene expression analysis. This project will foster multi-disciplinary education for engineering students and it has the potential to pave the way for new high-tech companies. <br/><br/>This research has been structured as an interdisciplinary collaboration between investigators with complementary expertise in design automation and system architecture for microfluidics, digital microfluidics technology, and molecular biology. It will lead to an integrated 5-layer system architecture for the seamless on-chip execution of complex biochemical protocols. Breakthroughs in design automation will enable real-time decision-making based on prescribed decision criteria; such a design will allow a diverse collection of protocol paths to be traversed. Research objectives include system design and optimization methods to support quantitative analysis protocols, demonstration of biomolecular protocols on microfluidic biochips, and evaluation of design automation solutions using test cases extracted from benchtop implementation of biomolecular protocols."
"1823034","SPX: Collaborative Research: Global Address Programming with Accelerators","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/30/2018","Katherine Yelick","CA","University of California-Berkeley","Standard Grant","Vipin Chaudhary","09/30/2021","$465,000.00","Aydin Buluc","yelick@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","042Y","026Z","$0.00","Large-scale computing today is dominated by parallel computing, where a large task is divided into many smaller tasks and those smaller tasks run at the same time. Traditionally each of those tasks run independently up to a common stopping point, then they halt, exchange information, and continue. This global stop-and-communicate step is quite expensive. This project instead pursues a different approach, where individual tasks directly communicate with other tasks asynchronously, without having to wait for a global stopping point. This approach is likely to yield better performance on large-scale computing tasks, specifically on what is becoming the dominant large-scale machine, a heterogeneous machine with many CPUs and other<br/>many-core processors. The project will deliver a set of high-performance, open-source data structures and algorithm implementations to support irregular patterns of communication, notably those that arise in biology, graph analytics, and sparse linear algebra for machine learning. These will not only be directly useful for end users but also demonstrate how to design and engineer primitives for accelerator-equipped distributed-memory machines. The project also engages application developers (both in our groups and externally) to make the outcomes broadly useful.<br/><br/>The project will develop a programming environment for accelerator-based HPC systems that integrates accelerators into a Partitioned Global Address Space (PGAS) model, which will allow direct communication between GPUs in a manner that is well suited to both applications and the underlying hardware. Specifically, GPU programming will be integrated with the UPC++ PGAS programming model (""GPUPC++""). The project will thus advance the state of the art in algorithms, programming models, and low-level support for the heterogeneous large-scale computers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740184","E2CDA: Type I: Collaborative Research: Energy-Efficient Artificial Intelligence with Binary RRAM and Analog Epitaxial Synaptic Arrays","CCF","Energy Efficient Computing: fr","09/15/2017","07/11/2018","Jeehwan Kim","MA","Massachusetts Institute of Technology","Continuing grant","Sankar Basu","08/31/2020","$243,624.00","","jeehwan@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","015Y","7945","$0.00","In recent years, deep learning and artificial neural networks have been very successful in large-scale recognition and classification tasks, some even surpassing human-level accuracy. However, state-of-the-art deep learning algorithms tend to present very large network models, which poses significant challenges for hardware, especially for memory. Emerging resistive devices have been proposed as an alternative solution for weight storage and parallel neural computing, but severe limitations still exist for applying resistive random access memories (RRAMs) for practical large-scale neural computing. This proposal targets on addressing limitations in resistive device based neural computing through novel device engineering, new bitcell designs, new neuron circuits, energy-aware architecture, and a new circuit-level benchmark simulator. A successful completion of this research is likely to have consequences to our society, enabling wide adoption of dense and energy-efficient intelligent hardware to power-/area-constrained local mobile/wearable devices. Furthermore, a self-learning chip that learns in near real-time and consumes very low-power can be integrated in smart biomedical devices, personalizing healthcare. This project will have a strong effort on integrating the research outcomes with education and outreach through summer outreach programs for high school students, undergraduate/graduate student training, and organization of tutorials and workshops at conferences for knowledge dissemination.<br/><br/>The proposal will perform innovative and interdisciplinary research to address many limitations in today?s resistive device based neural computing and make a leap progress towards energy-efficient intelligent computing. Severe limitations of applying resistive random access memories (RRAMs) for practical large-scale neural computing include: (1) device-level non-idealities, e.g., non-linearity, variability, selector, and endurance, (2) inefficiency in representing negative weights and neurons, and (3) limited demonstration on simpler networks, instead of cutting-edge convolutional and recurrent neural networks. To address these limitations, novel technologies from devices to architectures will be investigated. First, new bitcell circuits will be designed for today?s binary resistive devices, efficiently mapping XNOR functionality with (+1, -1) weights and neurons. Second, a novel epitaxial resistive device (EpiRAM) that exhibits many idealistic properties will be investigated, including linear programming for analog weights, suppressed variability, self-selectivity, and high endurance. Third, new neuron circuits will be explored for integration with new resistive devices for feedforward/feedback deep neural networks. Finally, new data-mapping techniques that efficiently map state-of-the-art deep neural networks onto the hardware framework with RRAM arrays will be developed, and the overall energy-efficiency will be verified with a new benchmark simulator ?NeuroSim?. With vertical innovations across material, device, circuit and architecture, tremendous potential and research needs will be pursued towards energy-efficient artificial intelligence in ubiquitous resource-constrained hardware systems."
"1733842","AiTF: Collaborative Research: Algorithms for Smartphone Peer-to-Peer Networks","CCF","Algorithms in the Field","09/01/2017","08/10/2017","Calvin Newport","DC","Georgetown University","Standard Grant","Tracy J. Kimbrel","08/31/2020","$319,578.00","","cnewport@cs.georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","7239","","$0.00","The growing ubiquity of smartphones, combined with the rapid improvement of operating system support for direct wireless communication between nearby devices, creates a compelling opportunity for the emergence of easy to deploy and widely used smartphone peer-to-peer applications. There are several use cases for such applications. For example, in many user environments, cellular data minutes are bought in small blocks and carefully conserved by users, generating an interest in networking operations that can avoid infrastructure.  In addition, smartphone peer-to-peer networks can bring connectivity to settings such as disaster zones, festivals, or wilderness where traditional cellular and WiFi coverage is compromised, overwhelmed, or non-existent. This project aims to develop network algorithms and tools that simplify the design of useful distributed systems on top of local peer-to-peer connections. The project has the potential for significant societal impact by enabling compelling new applications for smartphone peer-to-peer networks. The project is also expected to have significant educational impact.<br/> <br/>The project focuses on designing and analyzing provably correct and efficient network computation algorithms that can run on top of existing smartphone peer-to-peer services, and simplify the design of peer-to-peer systems that can be deployed on existing smartphone hardware. The project consists of two major research directions. The first research direction is experimental in nature. It will study and evaluate peer-to-peer services available in commodity smartphone operating systems and define a small number of validated abstractions that capture their capabilities and behavior. The second research direction is theoretical in nature. It will describe and analyze solutions to well-motivated network computation primitives using these abstractions. The problems studied will include consensus, leader election, rumor spreading, gossip and function computation. The project will seek both provably correct and efficient algorithms as well as lower bounds that establish fundamental limits for useful computation in this setting."
"1637516","AitF: Collaborative Research: Foundations of Intent-based Networking","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS","09/01/2016","06/26/2017","Loris D'Antoni","WI","University of Wisconsin-Madison","Standard Grant","Tracy J. Kimbrel","08/31/2019","$355,985.00","Srinivasa Akella","loris@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7239, 7796","7926, 9251","$0.00","Computer networks play an essential role in the day-to-day operations of businesses, organizations, and governments: they facilitate access to services and information as well as help protect against some types of cyberattacks.  Unfortunately, current networks require highly-skilled network operators to provide detailed specifications of how the network should behave.  This is a tedious and error prone process that limits how easily a network can evolve to meet emerging business needs and opens the door for subtle errors that can have a drastic impact on network availability, performance, and security.  The goal of this project is to automatically produce the detailed specifications required by networking hardware from a set of high-level security and performance objectives specified by individuals who may have limited networking background.  In other words, this project aims to allow administrators to focus on what the network should do rather than how it should be achieved.  The broader impact of this project is to pave the way for increased network stability and security, and also to aid in training the next generation of network professionals.<br/><br/>Automatically producing network configurations that satisfy a set of high-level policies and objectives (collectively referred to as ""intent"") requires both a language for network administrators to formally specify their intents and a mechanism for generating optimal and correct configurations for various types of networking hardware.  To satisfy these requirements, the PIs plan to explore how program synthesis techniques can be applied and extended to network configurations.  The project will lead to the design of synthesis techniques for generating specific types of intent implementations (e.g., traditional control plane configurations), as well as introduce domain-specific refinements to the chosen synthesis algorithms to ensure the time required for synthesis is practical and the resulting data and control planes are optimal (e.g., the configurations have minimal complexity).  The algorithms produced by this research will advance the state of the art of program synthesis and provide new insights into how to apply program synthesis to other domains."
"1637427","AitF: Collaborative Research: Foundations of Intent-based Networking","CCF","Algorithms in the Field","09/01/2016","08/23/2016","Aaron Gember-Jacobson","NY","Colgate University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$60,011.00","","agemberjacobson@colgate.edu","13 Oak Drive","Hamilton","NY","133461398","3152287457","CSE","7239","","$0.00","Computer networks play an essential role in the day-to-day operations of businesses, organizations, and governments: they facilitate access to services and information as well as help protect against some types of cyberattacks.  Unfortunately, current networks require highly-skilled network operators to provide detailed specifications of how the network should behave.  This is a tedious and error prone process that limits how easily a network can evolve to meet emerging business needs and opens the door for subtle errors that can have a drastic impact on network availability, performance, and security.  The goal of this project is to automatically produce the detailed specifications required by networking hardware from a set of high-level security and performance objectives specified by individuals who may have limited networking background.  In other words, this project aims to allow administrators to focus on what the network should do rather than how it should be achieved.  The broader impact of this project is to pave the way for increased network stability and security, and also to aid in training the next generation of network professionals.<br/><br/>Automatically producing network configurations that satisfy a set of high-level policies and objectives (collectively referred to as ""intent"") requires both a language for network administrators to formally specify their intents and a mechanism for generating optimal and correct configurations for various types of networking hardware.  To satisfy these requirements, the PIs plan to explore how program synthesis techniques can be applied and extended to network configurations.  The project will lead to the design of synthesis techniques for generating specific types of intent implementations (e.g., traditional control plane configurations), as well as introduce domain-specific refinements to the chosen synthesis algorithms to ensure the time required for synthesis is practical and the resulting data and control planes are optimal (e.g., the configurations have minimal complexity).  The algorithms produced by this research will advance the state of the art of program synthesis and provide new insights into how to apply program synthesis to other domains."
"1535897","AitF: EXPL: Collaborative Research: Approximate Discrete Programming for Real-Time Systems","CCF","Algorithms in the Field","09/01/2015","07/30/2015","Christoph Studer","NY","Cornell University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$200,000.00","","studer@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7239","013Z","$0.00","Discrete programming (DP) deals with optimization problems involving variables that range over a discrete (e.g., integer-valued) solution space. DP is an important tool in a variety of practical applications including digital communications, operations research, power grid optimization, and computer vision. While discrete programs are typically solved offline by sophisticated software using powerful computers, DP has recently emerged as an important tool in applications requiring real-time processing in embedded systems with stringent area, cost, and power constraints. Since existing DP solvers entail prohibitive complexity and power consumption when implemented on existing embedded hardware, novel algorithms and hardware architectures are necessary to unlock the potential of DP in real-time applications. This project fuses optimization theory, numerical methods, and circuit design to develop fast algorithms and suitable hardware architectures for real-time DP in embedded systems. Besides a thorough theoretical analysis of the proposed methods, the project includes extensive software and hardware benchmarking to reveal the efficacy of real-time DP in practice. To bridge the ever-growing gap between recent advances in numerical optimization and hardware design, the project also includes the development of undergraduate and graduate courses that build upon the vertically-integrated research approach of this project, in addition to offering summer research internships (REUs) to introduce young scientists to the field of discrete programming.<br/><br/>The project develops a set of computationally efficient and hardware-aware algorithms and corresponding dedicated very-large scale integration (VLSI) architectures that enable DP for real-time embedded systems.  The proposed DP algorithms rely on a variety of algorithmic transformations, ranging from semidefinite and infinity-norm-based relaxations to exact variable-splitting methods and non-convex approximations. These disparate approaches offer a wide range of tradeoffs between solution quality and hardware implementation complexity. The project studies these fundamental tradeoffs, as well as the effects of finite-precision arithmetic in VLSI, from both a theoretical and practical perspective. To carry out this investigation, three dedicated VLSI architectures will be developed that exploit the inherent parallelism of the proposed algorithms. These architectures target (i) data detection in multi-antenna (MIMO) wireless systems that is the key bottleneck in next-generation communication systems, (ii) signal recovery problems in hyperspectral imaging, and (iii) phase retrieval problems from x-ray crystallography. By investigating the domain-specific performance and complexity of various numerical solvers in a variety of conditions and hardware configurations, the project will reveal the efficacy and limits of DP for a broad range of real-time applications beyond the ones studied in this project."
"1525749","SHF: Small: Collaborative Research: Variation-Resilient VLSI Systems with Cross-Layer Controlled Approximation","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/29/2015","Jiang Hu","TX","Texas A&M Engineering Experiment Station","Standard Grant","Sankar Basu","07/31/2019","$210,000.00","","jianghu@ece.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7798","7923, 7945","$0.00","Applications driven by human-computer interactions through the five human senses are projected to underpin the next generation of computing. For many of these applications, occasional small errors are often not only acceptable but also bring opportunities for building lighter, cheaper, and more robust systems that use less energy and may have a longer battery life. This project will study how to advance computing technology by allowing deliberate imprecision in hardware implementations through the notion of approximate computing.  The outcomes of this project will be a set of design techniques for approximate computing that can become a key component of hardware computing technology, potentially benefiting systems ranging from high performance computing for big data analytics and low power implementation for internet of things. This project will also provide an opportunity for training students with the latest design and computing technology.<br/> <br/>The research goals of this project are to create new approximate computing techniques to optimize a system at all stages of its life, from design-time to runtime, which can enable cross-layer control of performance-power-precision trade-offs. The research agenda consists of several components. First, new error models with different accuracy-complexity trade-offs will be developed. Second, new design-time optimization techniques, especially hardware resource scheduling and binding in high-level synthesis, will be studied with consideration of approximation, variation, and runtime circuit reconfiguration. Third, compile-time and operating-system-level task mapping/scheduling algorithms will be investigated to make the best use of circuits with various precisions. Last but not least, runtime precision control techniques will be explored in conjunction with dynamic voltage and frequency scaling in order to achieve a smooth trade-off between power and user experience."
"1823037","SPX: Collaborative Research: Global Address Programming with Accelerators","CCF","SPX: Scalable Parallelism in t","10/01/2018","08/30/2018","John Owens","CA","University of California-Davis","Standard Grant","Vipin Chaudhary","09/30/2021","$386,025.00","","jowens@ece.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","042Y","026Z","$0.00","Large-scale computing today is dominated by parallel computing, where a large task is divided into many smaller tasks and those smaller tasks run at the same time. Traditionally each of those tasks run independently up to a common stopping point, then they halt, exchange information, and continue. This global stop-and-communicate step is quite expensive. This project instead pursues a different approach, where individual tasks directly communicate with other tasks asynchronously, without having to wait for a global stopping point. This approach is likely to yield better performance on large-scale computing tasks, specifically on what is becoming the dominant large-scale machine, a heterogeneous machine with many CPUs and other<br/>many-core processors. The project will deliver a set of high-performance, open-source data structures and algorithm implementations to support irregular patterns of communication, notably those that arise in biology, graph analytics, and sparse linear algebra for machine learning. These will not only be directly useful for end users but also demonstrate how to design and engineer primitives for accelerator-equipped distributed-memory machines. The project also engages application developers (both in our groups and externally) to make the outcomes broadly useful.<br/><br/>The project will develop a programming environment for accelerator-based HPC systems that integrates accelerators into a Partitioned Global Address Space (PGAS) model, which will allow direct communication between GPUs in a manner that is well suited to both applications and the underlying hardware. Specifically, GPU programming will be integrated with the UPC++ PGAS programming model (""GPUPC++""). The project will thus advance the state of the art in algorithms, programming models, and low-level support for the heterogeneous large-scale computers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1733873","AITF: Applied Algorithmic Foundation for Scheduling Multiprogrammed Parallelizable Workloads","CCF","Algorithms in the Field","10/01/2017","02/01/2018","Benjamin Moseley","MO","Washington University","Standard Grant","Tracy J. Kimbrel","09/30/2021","$650,000.00","Kunal Agrawal, I-Ting Lee","moseleyb85@gmail.com","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7239","7354, 9102, 9150","$0.00","Most modern computing systems contain multiprocessor hardware which is shared by many applications.  In these systems, good scheduling algorithms that decide how to allocate resources among applications are crucial to ensure good quality of service and efficient use of system resources.  This project will design foundational algorithms and prototype implementations of scheduling algorithms that provide guarantees of performance and resource utilization for these shared machines.  This work will shape the efficiency of critical computing infrastructure by improving performance of parallel systems from personal computers to data centers to supercomputers.   All results, including published articles and software artifacts, will be released to the public via world-wide web.  The PIs will integrate research with education by incorporating this research into the PIs' graduate courses and training PhD, MS, and BS students in applied and theoretical parallel computing research.<br/><br/>The project will involve designing practically efficient schedulers guided by theoretical foundations.  The PIs will design a theory of multi-programmed scheduling for parallel programs by considering a variety of scheduling objectives important to system designers.  In particular, the research will focus on optimizing latency objectives that are used in servers, clouds, and interactive systems.  The PIs will also explore efficient mechanisms that can be used to implement these algorithms in practice and perform empirical validations of their designs.  By combining theoretical analysis with feedback from empirical evaluations, the proposed work will gain insights that will advance the state of the art of both theory and practice of parallel job scheduling."
"1566589","CRII: SHF: Machine-Learning-Based Test Effectiveness Prediction","CCF","CRII CISE Research Initiation","05/15/2016","05/18/2016","Lingming Zhang","TX","University of Texas at Dallas","Standard Grant","Sol J. Greenspan","04/30/2019","$174,150.00","","lingming.zhang@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","026Y","7798, 7944, 8228","$0.00","Test effectiveness, which indicates the capability of tests in detecting potential software bugs, is crucial for software testing. More effective tests can detect more potential bugs and thus help prevent economic loss or even physical damage caused by software bugs. Therefore, a huge body of research efforts have been dedicated to test effectiveness evaluation during the past decades. Recently, mutation testing, a powerful methodology that computes the detection rate of artificially injected bugs to measure test effectiveness, is drawing more and more attention from both the academia and industry. Various studies have shown that artificial bugs generated by mutation testing are close to real bugs, demonstrating mutation testing effectiveness in test effectiveness evaluation. However, a major obstacle for mutation testing is the efficiency problem ? mutation testing requires the execution of each artificial buggy version (i.e., mutant) to check whether the test suite can detect that bug, and which is extremely time consuming. Therefore, a light-weight but precise technique for measuring test effectiveness is highly desirable.<br/><br/>The approach is to automatically extract test effectiveness information (e.g., mutation testing results) from various open-source projects  to directly predict the test effectiveness of the current project without any mutant execution. More specifically, the PI proposes to design a general classification framework based on a suite of static and dynamic features collected according to the PIE theory of fault detection. Furthermore, this research will explore judicious applications of advanced program analysis, machine learning, and software mining techniques for more powerful feature collection, more active learning, as well as more comprehensive training data preparation. The proposed approach will result in efficient but precise test effectiveness evaluation for projects developed using various programming languages and test paradigms, which is crucial for high-quality software. Furthermore, the training of the classification models will require to collect various basic testing, analysis, and mining information from a huge number of open-source projects, and thus may also benefit a large variety of software testing/analysis/mining  techniques that explore open-source software repositories."
"1535902","AitF: EXPL: Collaborative Research: Approximate Discrete Programming for Real-Time Systems","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS","09/01/2015","04/20/2018","Thomas Goldstein","MD","University of Maryland College Park","Standard Grant","Tracy J. Kimbrel","08/31/2019","$216,000.00","","tomg@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7239, 7796","013Z, 7926, 9251","$0.00","Discrete programming (DP) deals with optimization problems involving variables that range over a discrete (e.g., integer-valued) solution space. DP is an important tool in a variety of practical applications including digital communications, operations research, power grid optimization, and computer vision. While discrete programs are typically solved offline by sophisticated software using powerful computers, DP has recently emerged as an important tool in applications requiring real-time processing in embedded systems with stringent area, cost, and power constraints. Since existing DP solvers entail prohibitive complexity and power consumption when implemented on existing embedded hardware, novel algorithms and hardware architectures are necessary to unlock the potential of DP in real-time applications. This project fuses optimization theory, numerical methods, and circuit design to develop fast algorithms and suitable hardware architectures for real-time DP in embedded systems. Besides a thorough theoretical analysis of the proposed methods, the project includes extensive software and hardware benchmarking to reveal the efficacy of real-time DP in practice. To bridge the ever-growing gap between recent advances in numerical optimization and hardware design, the project also includes the development of undergraduate and graduate courses that build upon the vertically-integrated research approach of this project, in addition to offering summer research internships (REUs) to introduce young scientists to the field of discrete programming.<br/><br/>The project develops a set of computationally efficient and hardware-aware algorithms and corresponding dedicated very-large scale integration (VLSI) architectures that enable DP for real-time embedded systems.  The proposed DP algorithms rely on a variety of algorithmic transformations, ranging from semidefinite and infinity-norm-based relaxations to exact variable-splitting methods and non-convex approximations. These disparate approaches offer a wide range of tradeoffs between solution quality and hardware implementation complexity. The project studies these fundamental tradeoffs, as well as the effects of finite-precision arithmetic in VLSI, from both a theoretical and practical perspective. To carry out this investigation, three dedicated VLSI architectures will be developed that exploit the inherent parallelism of the proposed algorithms. These architectures target (i) data detection in multi-antenna (MIMO) wireless systems that is the key bottleneck in next-generation communication systems, (ii) signal recovery problems in hyperspectral imaging, and (iii) phase retrieval problems from x-ray crystallography. By investigating the domain-specific performance and complexity of various numerical solvers in a variety of conditions and hardware configurations, the project will reveal the efficacy and limits of DP for a broad range of real-time applications beyond the ones studied in this project."
"1422871","CIF:  Small:  Statistical Approach to Signal Processing for Long-Haul Fiber-Optic Communication Sytems","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2014","07/10/2014","Maite Brandt-Pearce","VA","University of Virginia Main Campus","Standard Grant","Phillip Regalia","07/31/2019","$368,444.00","","mb9q@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7797","7923, 7936","$0.00","Society depends on fiber-optic networks to carry its huge data demands. Recent literature on optical communications has predicted a plateau in the throughput increase of long-haul fiber-optic communication systems, estimating that the demand for throughput will exceed supply within the next ten years or so.  The long term solution to this problem is the systematic replacement of all long-haul fiber (millions of miles just in the US) to more effective fibers. This research project explores a less drastic shorter-term solution.  It seeks to answer the following question: if limiting assumptions made in deriving this throughput plateau are relaxed, by using more sophisticated channel modeling and signal processing, can the system capacity be increased sufficiently to delay or even obviate the costly fiber upgrade?  <br/><br/>In this project a full probabilistic description of the signal and noise co-propagation through optical fibers is developed so that statistical signal processing approaches can be effectively used on this difficult channel. A probabilistic representation of the fiber-optic system including its correlated, nonstationary, and nonGaussian qualities for wavelength division multiplexed systems and emerging elastic optical networks is first derived.  The research includes the derivation of the statistics of each mode of a Gaussian mixture model for the joint signal and noise processes as they propagate through a long-haul fiber. The models developed are used to derive accurate closed-form predictions of the system performance. Signal processing algorithms and receiver designs that optimize the above performance metrics are then developed.  The program leverages current models developed for deterministic signals. The focus is in generating accurate yet simple models to make them more generally applicable to both legacy and new optical networks."
"1763348","CIF: Medium: Collaborative Research: New Frontiers in Polar Coding: 5G and Beyond","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/08/2018","Hessam Mahdavifar","MI","University of Michigan Ann Arbor","Continuing grant","Phillip Regalia","09/30/2022","$130,133.00","","hessam@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7797","7924, 7935","$0.00","Since the first mobile phones became available in the 1980s, four distinct ""generations"" of wireless networks have been deployed to support reliable transmission of ever-increasing volumes of data to a growing number of users. What makes such reliable transmission of information possible are error-correcting codes, first conceived by Claude Shannon over 60 years ago. Polar coding is a key new error-correction technology introduced in the fifth wireless generation, known as 5G, that is currently being developed and standardized. Thus, soon enough, consumers the world over will all be using polar codes whenever making a phone call or accessing the Internet on a mobile device. Polar codes provably achieve the fundamental limits of communication established by Shannon in 1948, with low encoding and decoding complexity. Nevertheless, numerous challenges must be overcome in order to realize the full potential of polar coding in wireless communications. This project addresses these challenges to facilitate successful deployment of polar codes in 5G systems, while investigating fundamental problems in polar coding that lie beyond the 5G time horizon. These problems include polarization for time-varying channels and polar coding for channels with deletions. The results from this part of the investigation will contribute to the foundations of error-correction coding theory, and will also have an impact on adjacent scientific disciplines that are influenced by the polar-coding paradigm.<br/><br/>The discovery of channel polarization and polar codes is universally recognized as an historic breakthrough in coding theory. For short block lengths, polar codes under cyclic-redundancy-check-aided successive-cancellation list decoding are currently the best known coding scheme for binary-input Gaussian channels. Due to this and other considerations, 3GPP has decided to incorporate polar codes in the 5G wireless communications standard. The overarching goal in this project is to explore new frontiers in polar coding, thereby fundamentally advancing the current state-of-the-art in the field. Part of the research aims for immediately relevance to successful deployment of polar codes in 5G, whereas other parts focus on key theoretical problems in polar coding that lie beyond the 5G time-horizon. The specific objectives in this project are as follows: (1) Attain the gains of cyclic-redundancy-check-aided polar list-decoding with significantly lower complexity; (2) Construct practical universal polar codes that are not channel-dependent and provide near-optimal finite-length performance; (3) Develop code-domain multiple access techniques based on polar codes to enable massive connectivity; (4) Design and evaluate polar coding schemes for extended classes of channels, going well beyond the original memoryless and stationary set-up; (5) Extend the polarization paradigm and design polar codes for channels with deletions; (6) Develop a full-scale efficient, low-latency, and low-power system-on-chip implementation of polar list decoders. This project is a natural outgrowth of extensive and transformative prior work carried out by the investigators in polar coding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640970","INSPIRE: Assessing feasible regions of configuration spaces for macromolecular crystals","CCF","ANALYSIS PROGRAM, INFORMATION TECHNOLOGY RESEARC, ALGORITHMIC FOUNDATIONS, INSPIRE","09/01/2016","06/29/2018","Gregory Chirikjian","MD","Johns Hopkins University","Continuing grant","Rahul Shah","08/31/2020","$600,000.00","Bernard Shiffman","gregc@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1281, 1640, 7796, 8078","7929, 8653","$0.00","This INSPIRE project is jointly funded by Algorithmic Foundations in CISE/CCF, Analysis in MPS/DMS, and the NSF Office of Integrative Activities.<br/><br/>Knowledge of the 3-dimensional structure of protein molecules supports scientific understanding of how proteins perform their functions within cells.  Structures of over 100,000 proteins in the Protein Data Bank have been determined by macromolecular x-ray crystallography: measuring the diffraction of x-ray beams from crystal composed of many symmetrically arranged copies of one or more protein molecules gives partial information (the amplitudes of the Fourier transform) that must be filled in (solving the ""phase problem,"" often by molecular replacement -- taking phases from related molecules) to complete the 3d structure. Molecular replacement works quite well for simple single-domain proteins, but breaks down for multi-domain proteins and large complexes; one needs to explore the possible combinations of domains and their diffraction patterns as replacement candidates. <br/><br/>This cross-disciplinary project brings together experts in robotics and in pure mathematics to address the ''phase problem'' of macromolecular x-ray crystallography.  The mathematical and computational framework developed in this project will enable many more protein structures to be solved in a less laborious way than can be done now. The project also introduces Baltimore City high school students to mathematics and molecular biophysics through unique visualization activities.<br/><br/>The essence of combining domains is geometric.  The team can use articulated multi-rigid-body models from the field of robotics to combine rigid portions of structures, from domains with similar sequences. The relative rigid-body motions between the domains become the unknown degrees of freedom in these articulated models.  Crystal packing constraints will rule out the majority of possible configurations for these domains, and reduce the otherwise high-dimensional nature of the search space. The project will develop new algebro-geometric and computational methods for rapidly discarding the large collision regions in configuration space, so that searches will focus on the remaining small-volume feasible regions in this high-dimensional search space. Computer code will be prepared integrating the resulting methods into existing molecular crystallography software packages."
"1717842","CIF: Small: Fundamental limits and coding for massive wireless random-access","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/28/2017","Yury Polyanskiy","MA","Massachusetts Institute of Technology","Standard Grant","Phillip Regalia","06/30/2020","$450,000.00","","yp@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7797","7923, 7935","$0.00","The key expected innovation of the next generation of radio-access networks (such as cellular) is their ability to service vast numbers of active devices as envisioned in the so-called ""Internet-of-Things"". Unfortunately, current networks were designed with the human-type communication in mind, and this led to a focus on the operating regime of a (relatively) few simultaneously active users. More specifically, present systems employ centralized resource allocation, thus orthogonalizing the access from different users. This solution is not acceptable for machine-type communication, as it relies on a significant control-layer overhead thereby incurring a significant penalty in latency and energy efficiency. Consequently, there is a strong economical demand for a new solution in both the unlicensed spectrum (so called, low-power wide-area networks) and the licensed spectrum (5G).<br/><br/>The goal of this work is to provide theoretical guidance for the design of the multiple-access layer in the next generation of wireless networks. Classical work on the topic lacks several specific details, making it inadequate: ignoring the control-layer overhead in network analytic literature, and ignoring delay in information theory. Consequently, this work aims to provide necessary contemporary modifications: (a) a gigantic number of idle (inactive) users; (b) a still large number of active users; (c) short packets; (d) high energy-efficiency (low energy-per-bit). This project introduces a new paradigm of random-access coding that separates data communication from user identification, for which fundamental limits are going to be derived and the low-complexity practical solutions studied. Performance of the currently available solutions will be contrasted with the non-asymptotic fundamental limits and new solutions developed. In addition to information-theoretic and communication-theoretic parts, the work involves a combinatorial-theoretic component in the form of constructing Sidon sets, B2-sequences and superimposed codes."
"1421505","SHF: Small: Optimizing Compiler and Runtime for Concurrency-Oriented Execution Model","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2014","08/31/2018","Zheng Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Anindya Banerjee","08/31/2019","$426,141.00","","eddy.zhengzhang@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7798","7923, 7943, 9251","$0.00","Title: SHF:Small:Optimizing Compiler and Runtime for Concurrency-Oriented Execution Model<br/><br/>The ""dark silicon"" effect, where an increasing fraction of cores will have to be kept powered off (or, ""dark""), at every generation of transistor downsizing, has made it difficult to sustain further efficiency gains via the scaling of semiconductor technology.  However, the demands of applications and their data on storage and processing capabilities are rapidly growing, thus increasing the gap between the efficiency of the system stack and the needs of modern applications.  This research project aims to redesign the system stack based on a novel paradigm that combines throughput-processing architecture and a concurrency-centric compilation framework.  The system stack used in this research project consists of architecture specialized for throughput, which trades single-thread instruction level parallelism (ILP) exploitation units for throughput units.  The compiler is specialized for concurrency, which minimizes single thread latency by interleaved execution of a tremendous number of concurrent threads.<br/><br/>This research project reveals the implications of concurrent execution on throughput processors and how these implications affect compile-time decisions and the corresponding runtime optimization. The intellectual merits are two-fold: 1) it reveals that the existing mainstream CPU compilation techniques are concurrency-oblivious, which leaves both many challenging problems unanswered and many opportunities for performance improvement to be explored, and 2) it tackles these problems by addressing both the resource allocation and instruction/thread scheduling aspects of compile-time decision making, which is where the fundamental difference between the concurrent execution model and the traditional CPU execution model arises. The broader impacts of this project are that the research results will drive innovation in business, education, and computing applications by reinventing the system stack to enhance efficiency and to help achieve the next supercomputing milestone, namely, exascale-computing."
"1712119","AF:Small:Geometric and Combinatoric Algorithms for Contact and Intersection Representation of Graphs","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/31/2017","Stephen Kobourov","AZ","University of Arizona","Standard Grant","Rahul Shah","08/31/2020","$449,062.00","","kobourov@cs.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","7796","7923, 7926, 7929","$0.00","Networks of nodes connected by links are a useful abstraction in many areas -- from sociology and biology to engineering and transportation. Networks are used to model causal structures, hierarchies, and scheduling. Although networks are typically drawn as node-link diagrams, several areas use geometric representations of networks (molecules in chemistry or floor-plans in engineering). This project explores intersection and contact representation of networks, where nodes are geometric objects (e.g., rectangles, segments) and links are realized by intersections (e.g., segments crossing) or contacts between the objects (e.g., rectangular duals). A geometric representation is more than just a way to display the network; it reveals underlying combinatorial structures which can often be described only using geometry. The goal of this project is to leverage these connections and build new bridges between geometry and combinatorics, in order to develop algorithms for intersection and contact representations of networks, with broader impact in information visualization, network analysis, and computational cartography. Educational activities, such as new coursework development, graduate and undergraduate student advising and outreach are integral parts of this project.  Finally, continuing the investigator's tradition of implementing new algorithms and deploying software, dissemination of results will not only be accomplished by publishing in scientific journals, organizing workshops and seminars, but also by making software and data available.<br/><br/>The specific research agenda is to identify connections between geometry and combinatorics in the context of intersection and contact representations of graphs. As a result of studying the classes of graphs that can be represented as contacts between simple polygons (e.g., triangles and quadrilaterals), the combinatorial properties of these graph classes will be used to design algorithms for constructing such representations. Efficient algorithms will be designed and implemented for constructing proportional (value-by-area) contact representations and for creating static and dynamic cartograms. These problems are addressed on two fronts: (a) by finding necessary and sufficient conditions for specific types of intersection and contact representation, developing new representation algorithms, and establishing tight bounds for the problems; and (b) by directly applying the theoretical results to practical problems in information visualization and cartography, as well as implementing and experimentally evaluating the new methods, which are made available as fully functional online systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718698","CIF: Small: High-Dimensional Analysis of Stochastic Iterative Algorithms for Signal Estimation","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","07/03/2017","Yue Lu","MA","Harvard University","Standard Grant","Phillip Regalia","06/30/2020","$515,560.00","","yuelu@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7797","7923, 7935, 9251","$0.00","Optimization lies at the heart of modern signal and information processing. In recent years, the soaring quantity of information that is being acquired and becoming available makes computational and algorithmic issues increasingly important. This project contributes to an understanding of the fundamental limits of various stochastic optimization algorithms when dealing with high-dimensional data. Since such algorithms are the workhorse in many estimation, inference, and machine learning tasks, this research is well-posed to make significant and broad impact on many applications. Examples include real-time or low-latency medical image reconstructions, distributed computation on power grids, and the training of artificial neural networks for image understanding.<br/><br/>In this project, the PI studies a family of efficient stochastic iterative algorithms for solving large-scale convex and nonconvex optimization problems that arise in various signal estimation tasks. The broad goal in this project is to analyze the exact dynamics of these stochastic iterative algorithms in the high-dimensional limit. This asymptotic analysis provides a complete characterization of the typical behavior of the algorithms. The theoretical investigation draws upon techniques from the statistical physics of mean-field interactive particle systems, the weak convergence theory of stochastic processes, signal processing, information theory, and optimization. The theoretical analysis can be used to clarify the effectiveness of such stochastic methods for large-scale optimization and to establish their fundamental performance bounds. The insights obtained from the analysis can also be used to guide the design of new scalable algorithms to achieve optimal trade-offs between estimation accuracy, sample complexity, and computational complexity."
"1705077","CIF:Medium:Collaborative Research:An Information-theoretic approach to nanopore sequencing","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2017","09/19/2017","Suhas Diggavi","CA","University of California-Los Angeles","Continuing grant","Phillip Regalia","05/31/2021","$348,467.00","","suhas@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7797","7924, 7935","$0.00","Fast and inexpensive DNA sequencing technology is beginning to impact society through applications ranging from personalized medicine to understanding of ecological systems enabled by second generation sequencers. Wider applicability of second-generation sequencing technology is limited, however, by the short length of the DNA fragments that can be read. Nanopore sequencing has the potential to overcome several shortcomings of state-of-the art short-read sequencing.<br/><br/>The goal of this project is to create new foundational theory and algorithms enabling several applications of nanopore sequencing. This research will support nanopore technology to become a leading next-generation sequencing approach. This project also contains a unique inter-university education and research program, which will include joint and collaborative student advising and curricular development.<br/><br/>To realize the advantages of nanopore sequencing, methods for reducing and combating sequencing errors need to be developed. In a nanopore sequencer, DNA is transmigrated through a nanopore, and the ion current variations through the pore are measured to infer the DNA sequence. The mapping from the DNA sequence to the observed current trace, has several impairments (causing errors) including multiple nucleotides affecting each observation, random variations in nanopore response, dwelling time variations, synchronization errors, and noise. This project develops a holistic approach using tools from information theory and bio-informatics based on multiple interacting thrusts: (1) developing mathematical models and (2) information theory for nanopore sequencing, (3) decoding algorithms exploiting the structure of the nanopore channel, and (4) the theory and methodology for applications in DNA forensics, DNA phasing and DNA information storage. We couple this with an existing experimental nanopore sequencing research program, guiding models and the theory/algorithms with specific data as well as validating these ideas."
"1533656","XPS: FULL: FP: Design and Synthesis of New Energy-efficient Self-healing Computing Electronics with Real-time Configurability","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/07/2015","Jie Gu","IL","Northwestern University","Standard Grant","Yuanyuan Yang","08/31/2019","$548,614.00","Hai Zhou","jgu@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","8283","","$0.00","The energy consumption and material cost for modern data-centric computing systems have been rapidly expanding over the past decade.  Unfortunately, a significant amount of the energy and material expense is being wasted to create enough tolerance to manufacturing defects, process variation, reliability degradation, etc.  As the technology scaling is becoming prohibitively expensive, it is highly desirable to develop a novel computer system with real-time reconfigurability to cope with the variability stemming from both manufacturing and user demand.  Unfortunately, conventional CMOS technology has not provided us such a capability because once an integrated circuit is fabricated, it can no longer be modified.  However, the very recent development of non-volatile memory devices, specially the emerging memristor device, have opened the door for a new design paradigm with real-time reconfigurability.<br/><br/>Leveraging the memristor?s real-time tunability, large on-off resistive ratio and CMOS process compatibility, this project explores a new design and synthesis methodology of large scale integrated circuits with real-time reconfigurability.  By intelligently integrating novel memristor device based self-healing circuits with detection and tuning functionalities, a new computer system can perform real-time adjustment without allocating a large amount of design margin and thus achieve significant energy and cost saving.   With this in mind, this project will perform (1) device level modeling and design kit development for memristor integrated VLSI and mixed-signal design, (2) circuit level design to create robust self-healing digital and mixed-signal system, and (3) design automation development to enable large scale integration of the proposed reconfigurable design methodology.  For broader impact, the project provides a unique training opportunity to the students to obtain a broad knowledge base and out-of-box thinking capability through the cross-layer research activity in this project.  The developed design kit and methodology will be available to the community to inspire innovative VLSI and mixed-signal circuit design with memristor devices."
"1704401","CIF: Medium: Collaborative Research: Low-Resolution Sampling with Generalized Thresholds","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","05/01/2017","09/14/2018","Mojtaba Soltanalian","IL","University of Illinois at Chicago","Continuing grant","Phillip Regalia","04/30/2021","$297,664.00","","msol@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","2878, 7797","7797, 7924, 7936","$0.00","CIF: Medium: Collaborative Research:<br/>Low-Resolution Sampling with Generalized Thresholds<br/><br/>Jian Li, Lee Swindlehurst, and Mojtaba Soltanalian <br/><br/>Abstract<br/><br/>Quantization of signals of interest is a necessary first step in digital signal processing applications. When signals across a wide frequency band are of interest, a fundamental tradeoff between sampling rate, amplitude quantization precision, cost, and power consumption is encountered. The investigators study low resolution sampling techniques with general thresholds, which are affordable, technically feasible, easy to apply, energy-efficient, and consistent with technological trends. The enormous gains in capacity and spectral efficiency, for example, that could be provided by a successful millimeter wave (mm-wave) massive multiple-input multiple output implementation could have a revolutionary effect on the performance of wireless systems nearly everywhere we use them: at home, at work, at school, commuting via public transportation or by plane, shopping, at restaurants, recreational venues, sporting events, and so on. Besides consumer applications, there are many military- and security-related scenarios where our systems could be used.<br/><br/>This project involves advancing fundamental knowledge in developing dynamic energy-efficient and cost-effective sampling techniques and applies engineering principles to address the critical needs of several important and related applications. Specifically, this project involves  addressing significant open questions, including deterministic identifiability, performance bounds, and impact of thresholding pattern on spectrum sensing and array processing, radio frequency interference mitigation, and mm-wave communications to gain fundamental insights into the novel paradigm of low resolution sampling with general thresholds, devising novel signal processing algorithms, including effective and efficient sparse signal recovery techniques and parametric maximum likelihood methods for enhanced performance, and evaluating and demonstrating the performance using measured data. This project also involves preparing students for engineering in the 21st century through the incorporation of practical design and problem-solving techniques into both the education curriculum."
"1526106","SHF: Small: Online Detection and Recovery from Electrostatic Discharge Induced Transient Errors","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2015","07/15/2015","Elyse Rosenbaum","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sankar Basu","06/30/2019","$450,000.00","Shobha Vasudevan","elyse@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7923, 7945","$0.00","If energy from an electrostatic discharge (ESD) reaches the integrated circuit components inside a mobile device, failure may result. Multiple design cycles are generally needed before a product can go through ESD qualification testing with only sporadic, relatively benign, soft failures occurring. Manifestations of soft failures include a hanging application, an operating system crash, poor signal fidelity, data transmission errors, and momentary display flickering. These are referred to as soft failures because they are recoverable from, although operator intervention may be needed, e.g., one may have to turn the power off and on again. This project will develop techniques for demonstrating an operating system based approach for mitigating ESD-induced soft failures. It is expected to reduce the need for hardware to be redesigned to meet ESD specifications, thereby resulting in a shorter time-to-market for mobile devices and other electronic products. Such techniques can potentially subsequently be applied to mitigate a broad range of reliability and security hazards as well. The project will involve graduate and undergraduate students, include members of underrepresented groups, and will thus help enlarge the workforce in information and communication technologies.<br/><br/>The signature of ESD-induced noise can be detected in an integrated circuit (IC) and when such noise is detected, logic errors are likely to occur throughout the IC. It will be demonstrated that ESD detectors may be placed inside an IC; when activated, an interrupt signal will be sent to the operating system. In the interest of power and area efficiency, there will be no attempt to identify the affected registers inside the IC, nor to handle the different types of errors differently. Instead, the operating system will be designed to respond to the notice of impending errors and will subsequently rollback the system to a previous ""known good"" checkpoint. The proposed solution will be validated in hardware and this will allow ESD-induced errors to be characterized at the application level. To enable further research in ESD-induced failure analysis, diagnosis and recovery, fault modeling of ESD-induced errors will also be undertaken."
"1525902","SHF: Small: Deep Learning Software Repositories","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2015","07/20/2015","Denys Poshyvanyk","VA","College of William and Mary","Standard Grant","Sol J. Greenspan","08/31/2019","$400,000.00","","dposhyvanyk@wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7798","7923, 7944","$0.00","Improvements in both computational power and the amount of memory in modern computer architectures, have enabled new approaches to canonical machine learning tasks. Specifically, these architectural advances have enabled machines, which are capable of learning deep compositional representations of massive data repositories. The rise of deep learning has ushered tremendous advances in several fields, and, given the complexity of software repositories, our hypothesis is that deep learning has the potential to usher new analytical frameworks and methodologies for Software Engineering research as well practice.<br/><br/>The research program addresses three main goals by applying deep learning where conventional machine learning has been used before. First is the design of new models based on deep architectures for Software Engineering tasks. The project will develop deep software language models for sequence analysis tasks and deep information retrieval models for document analysis tasks. Second, the project will apply the internal representations to practical problems in Software Engineering by instantiating deep learning to support tasks such as code suggestion, improving software lexicons, model-based testing, code search and clone detection. Third, the project will conduct empirical evaluations designed to demonstrate ways of modeling software artifacts that will inform entirely novel suites of learned features that can be used from task to task. The move from traditional machine learning to deep learning will improve results in many software analysis tasks and in empirical Software Engineering research."
"1703403","CIF:Medium:Collaborative Research:An Information-theoretic approach to nanopore sequencing","CCF","SPECIAL PROJECTS - CCF, COMM & INFORMATION FOUNDATIONS","06/15/2017","09/13/2018","Sreeram Kannan","WA","University of Washington","Continuing grant","Phillip Regalia","05/31/2021","$557,790.00","Jens Gundlach","ksreeram@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","2878, 7797","7797, 7924, 7935","$0.00","Fast and inexpensive DNA sequencing technology is beginning to impact society through applications ranging from personalized medicine to understanding of ecological systems enabled by second generation sequencers. Wider applicability of second-generation sequencing technology is limited, however, by the short length of the DNA fragments that can be read. Nanopore sequencing has the potential to overcome several shortcomings of state-of-the art short-read sequencing.<br/><br/>The goal of this project is to create new foundational theory and algorithms enabling several applications of nanopore sequencing. This research will support nanopore technology to become a leading next-generation sequencing approach. This project also contains a unique inter-university education and research program, which will include joint and collaborative student advising and curricular development.<br/><br/>To realize the advantages of nanopore sequencing, methods for reducing and combating sequencing errors need to be developed. In a nanopore sequencer, DNA is transmigrated through a nanopore, and the ion current variations through the pore are measured to infer the DNA sequence. The mapping from the DNA sequence to the observed current trace, has several impairments (causing errors) including multiple nucleotides affecting each observation, random variations in nanopore response, dwelling time variations, synchronization errors, and noise. This project develops a holistic approach using tools from information theory and bio-informatics based on multiple interacting thrusts: (1) developing mathematical models and (2) information theory for nanopore sequencing, (3) decoding algorithms exploiting the structure of the nanopore channel, and (4) the theory and methodology for applications in DNA forensics, DNA phasing and DNA information storage. We couple this with an existing experimental nanopore sequencing research program, guiding models and the theory/algorithms with specific data as well as validating these ideas."
"1717532","SHF: Small: Enabling and Analyzing Accuracy-aware Reliable GPU Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2017","07/07/2017","Adwait Jog","VA","College of William and Mary","Standard Grant","Yuanyuan Yang","07/31/2020","$449,999.00","Evgenia Smirni","adwait@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7798","7923, 7941","$0.00","Graphics Processing Units (GPUs) are becoming the default choice for general-purpose hardware acceleration because of their ability to enable orders of magnitude faster and energy-efficient execution for large-scale high-performance computing applications. Since the majority of such applications executing on large-scale HPC systems are long-running, it is very important that they cope with a variety of hardware- and software-based faults. Many prior works have shown that real HPC systems are vulnerable to soft errors. An absence of essential protection and checkpointing mechanisms can lead to lower scientific productivity, operational efficiency, and even monetary loss. However, these protection mechanisms (e.g., error correction codes) are themselves not free -- they incur very high performance, energy, and area costs.<br/><br/>This project takes a holistic approach to explore the avenues to reduce these protection overheads by taking advantage of the fact that all errors do not lead to an unacceptable loss in the accuracy of application output. Prior results show that GPGPU applications are amenable to such accuracy-aware optimizations. In order to enable these optimizations, this project will address three major research questions: a) What hardware/software support and tools are necessary to determine which instructions are not vulnerable to soft errors, b) Based on this analysis, which hardware component(s) need not be protected and for how long, while not sacrificing application quality beyond the user's quality requirements, and c) What optimizations in terms of resource management and scheduling are necessary to make low-overhead but reliable computation more effective and efficient. These questions will be explored via a variety of GPGPU applications emerging from the areas of high-performance computing (HPC), big-data analytics, machine learning, and graphics. If successful, this project will generate several novel research insights that will play an important role in enabling low-cost reliable GPU computing. The results of this project will be integrated into the existing and new undergraduate and graduate courses on computer architecture and reliability, which will facilitate in training students, including women and students from diverse backgrounds and minority groups."
"1564044","CIF: Small: Collaborative Research: Communicating While Computing: Mobile Fog Computing Over Wireless Heterogeneous Networks","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","09/17/2015","Gesualdo Scutari","IN","Purdue University","Standard Grant","Phillip Regalia","08/31/2019","$224,829.00","","gscutari@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7923, 7935","$0.00","An increasing number of applications, including surveillance, medical monitoring, automatic translations and gaming, rely on the capability of mobile wireless devices to carry out computation-intensive tasks in a timely manner. This requirement conflicts with the expectation that mobile devices should run on a battery without needing frequent recharging. A promising solution to this challenge is mobile cloud computing, that is, the offloading of computation-intensive tasks to a cloud service provider by means of wireless transmission. However, the energy and latency entailed by wireless transmission may offset the potential gains of mobile cloud computing. This project proposes to tackle the outlined problem via the development of effective, low-complexity, scalable and flexible offloading strategies that operate over a mobile fog computing architecture, in which small-cell base stations are endowed with computing capabilities to offer proximate wireless access and computing. The insights gained from the successful completion of this project will be beneficial for a gamut of other exciting problem domains that require large-scale optimization, including big data mining, signal processing, machine learning, and smart grid. The research agenda is complemented by a multidisciplinary educational plan that targets both undergraduate and graduate students via hands-on learning and experimentation activities. Industrial collaboration is also envisaged through internship and co-op opportunities. <br/><br/>The inter-layer optimization of the computation and communication resources in a mobile fog computing network yields unstructured nonconvex mixed-integer problems, which are unexplored and challenging, and whose formulation depends on whether the mobile applications are splittable, i.e., divisible into subtasks that can be individually offloaded, or not. Since the problems at hand do not lend themselves to the application of existing iterative optimization techniques, such as Difference-of-Convex programming, a class of scalable and flexible solution methods with controllable convergence, complexity and overhead is introduced based on a novel successive convex approximation framework. In the case of splittable applications, the analytical and algorithmic framework is augmented by the application of message passing strategies that leverage the call graph representation of the mobile applications."
"1514531","CIF: Medium: Collaborative Research: On-demand Physical Layer Cooperation","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2015","06/12/2017","Suhas Diggavi","CA","University of California-Los Angeles","Continuing grant","Phillip Regalia","06/30/2019","$699,000.00","Christina Fragouli","suhas@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7797","7924, 7935, 9251","$0.00","Wireless access is fast becoming the primary portal to the Internet, causing an exponentially rising demand for wireless data.  This has pushed current wireless systems to their limits, despite significant investment in infrastructure to meet the ever-growing demands. Physical layer cooperation can enable near-optimal usage of the available wireless bandwidth.<br/><br/>This proposal fundamentally rethinks physical-layer cooperation by introducing an on-demand approach to wireless cooperation. It can be argued that the success of the (wired) Internet was made possible by its on-demand operation, using adaptation, local knowledge, feedback and a best-effort service model. Notably, the success of many peer-to-peer network protocols is closely tied to on-demand information propagation and adaptation of the network. The broad goal of this proposal is to bring this philosophy to wireless networks, enabling near-optimal usage of the wireless network bandwidth within the complexity constraints of implementable systems. This project aims to complement the theoretical work with proof-of-concept deployments on software radio testbeds, and also engage  industry partners to impact next-generation wireless network designs.  The project also promotes training of research engineers, through a plan to establish a unique inter-university education and research program, which will include joint and collaborative student advising and curricular development. <br/><br/>The underlying assumption of most network information theory works is that one can build architectures which tightly coordinate the estimation and sharing of information about the wireless channels, the user requirements and the network topology, at a very fast time-scale, without impacting the performance (rates, error). That is, the cost of learning the very dynamic network state is not accounted for. Another implicit assumption is complete network usage, that all available relays in a wireless network are used, with no adaptation to user demand. This can be very wasteful in many situations, and again, the cost of unnecessarily using relays is not accounted for. The many breakthrough ideas based on these assumptions have advanced a collective understanding, but bringing physical layer cooperation techniques closer to practical networks requires additional steps to move beyond these assumptions. This project puts together a program that develops the theoretical foundations and practice of an on-demand network operation, that dispenses with these assumptions. This entails operating specific subsets of the network relays (sub-networks) that fulfill target rates, as opposed to using all network relays to achieve the best possible performance. This project constructs a theoretical understanding of how to select, adapt and operate these sub-networks on demand, by using accountable partial network knowledge and using feedback mechanisms to enhance signal adaptation to the unknown. The theoretical formulations are tightly coupled to implementable protocols that will be validated in test beds."
"1703013","SHF: Medium: Collaborative Research: Machine Learning Enabled Network-on-Chip Architectures for Optimized Energy, Performance and Reliability","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2017","04/16/2018","Avinash Karanth","OH","Ohio University","Continuing grant","Yuanyuan Yang","05/31/2020","$524,000.00","Razvan Bunescu","kodi@ohio.edu","108 CUTLER HL","ATHENS","OH","457012979","7405932857","CSE","7798","7798, 7924, 7941, 9251","$0.00","Network-on-Chip (NoC) architectures have emerged as the prevailing on-chip communication fabric for multicores and Chip Multiprocessors (CMPs). However, as NoC architectures are scaled, they face serious challenges. A key challenge in addressing optimized NoC architecture design today is the plethora of performance enhancing, energy efficient and fault tolerant techniques available to NoC designers and the large design space that must be navigated to simultaneously reduce power, improve reliability, increase performance and maintain QoS. <br/><br/>This research proposes a new cross-layer, cross-cutting methodology spanning circuits, architectures, machine learning algorithms, and applications, aimed at designing energy-efficient, reliable and scalable NoCs. This research will result in (1) novel cross-layer design techniques that take a holistic approach of simultaneously reducing power consumption, while still achieving reliability and performance goals for NoCs, (2) a fundamental understanding of the use of hardware-amenable ML for NoC design optimization, (3) software and hardware techniques for monitoring and collecting critical data and key design parameters during network execution to optimize NoC design, and (4) modeling and simulation tools that will improve the architecture community's design methodologies for evaluating scalable NoCs. The proposed research bridges a very important gap between hardware architects who design power management and fault tolerant techniques at the circuit and architecture level and machine learning scientists who develop predictive and optimization techniques. Due to its cross-cutting nature, the proposed research has the potential to significantly transform the design of next-generation CMPs and System-on-Chips (SoCs) where complex decisions have to be made that affect the power, performance and reliability. The research will also play a major role in education by integrating discovery with teaching and training. The PIs are committed and will continue to expand on outreach activities as part of the proposed project by making the necessary efforts to attract and train minority students in this field."
"1513606","SHF: Medium: Collaborative Research: Scaling On-chip Networks to 1000-core Systems using Heterogeneous Emerging Interconnect Technologies","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","06/06/2017","Avinash Karanth","OH","Ohio University","Continuing grant","Yuanyuan Yang","07/31/2019","$496,000.00","Savas Kaya","kodi@ohio.edu","108 CUTLER HL","ATHENS","OH","457012979","7405932857","CSE","7798","7924, 7941, 9251","$0.00","Power dissipation has become a fundamental barrier to scaling computing performance across all platforms from handheld, embedded systems, to laptops, to servers to data centers. Technology scaling down to the sub-nanometer regime has aided the growth in transistors per chip that has made multi-core architectures a power-efficient approach to harnessing parallelism and improving performance. The computing capabilities of these multi-core architectures can be unleashed only if the underlying Network-on-Chip (NoC) connecting the cores can provide the required bandwidth within the power budget of the chip. However, the design of power-efficient, low-latency and high-bandwidth NoCs using traditional metallic interconnects that can scale to 1000 cores and beyond, is proving to be a significant challenge of enormous proportions. Research has shown that emerging technologies such as photonics and wireless have the potential to alleviate the critical bandwidth, power, and latency challenges of future NoCs. However, hybrid NoC designs taking advantages of both photonics and wireless technologies have not been explored.<br/> <br/>This research proposes to lay the groundwork for completely re-thinking the NoC design and proposes to explore heterogeneity of emerging interconnect technology for designing performance scalable, and power-efficient NoCs. The overall objective is to combine multiple technologies to achieve our challenging goals of (1) scalability to 1000 cores, (2) power efficiency of at least a 50% power reduction as compared to the state-of-the-art metallic interconnects, and (3) high bandwidth and low latency across a wide variety of applications. First, at the architecture level, optics will be deployed for short-range (< 100 cores) to improve local communication and wireless for long-range communication in order to scale the number of cores to 1000 by providing sufficient global bandwidth. Second, at the circuit level, hybrid transceiver architectures will be explored to integrate novel ultra-low power wireless circuits based on SiGe/BiCMOS technology with optical waveguides and ring-resonators to provide the large bandwidth desired for kilo-core designs. Furthermore, wireless communication requirements will be addressed by designing mm-wave/THz frequency broadband and directional antennas based on advanced 3D printing technology. This proposal describes a transformative and viable approach combining technology, architecture, algorithms and applications research for designing scalable and energy-efficient NoCs. The cross-cutting nature of this research will foster new research directions in several areas, spanning technology/energy-aware NoC design, novel computer architectures, and cutting-edge modeling and simulations tools for emerging technologies."
"1841148","CIF: Student Travel Support for the 2019 IEEE International Symposium on Information Theory","CCF","COMM & INFORMATION FOUNDATIONS","01/01/2019","07/21/2018","Alfred Hero","MI","University of Michigan Ann Arbor","Standard Grant","Phillip Regalia","12/31/2019","$25,000.00","","hero@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7797","7556, 7935","$0.00","The IEEE International Symposium on Information Theory (ISIT) is the premier international conference in information theory, the scientific discipline that studies the mathematical foundations of reliable communications, whether by telephony, television, wireless or optical systems, and the fundamental limits of data storage, such as CDs, DVDs, flash drives, or other semiconductor media. The symposium series has run since 1954, originally every two years, and yearly since 2000. This project funds the participation of US-based students in this event. Participation at this conference will enhance the research experiences for students and provide increased opportunities for new collaborations.<br/><br/>Students form an integral part of the Information Theory community, and the IEEE ISIT Conference continues to promote student participation through the ISIT Jack Kiel Wolf Best Student Paper Award, the creation of the ISIT tutorial series, and the Information Theory Society Student Committee. Students comprise about one-third of the participants at ISIT, and the travel funds offered through this award will support the attendance at the 2019 ISIT by students enrolled in a graduate or undergraduate program in the United States and whose papers have been accepted for presentation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1406739","SHF: Medium: Collaborative Research: Advanced Architectures for Hand-held 3D Ultrasound","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2014","05/20/2014","Thomas Wenisch","MI","University of Michigan Ann Arbor","Standard Grant","Yuanyuan Yang","05/31/2019","$599,182.00","Jeffrey Fowlkes, Jonathan Rubin","twenisch@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","7924, 7941","$0.00","Much as every medical professional listens beneath the skin with a stethoscope today, we foresee a time when hand-held medical imaging will become as ubiquitous; ?peering under the skin? using a hand-held imaging device.  Hand-held imaging is not only a matter of convenience; moving the imaging device to the patient (rather than a critical patient to an imaging suite) has been shown to improve clinical outcomes.  Moreover, the portability of hand-held systems can make advanced imaging available to traditionally underserved populations in the rural and developing world.  Today, hand-held imaging is possible with compact, battery-operated ultrasound devices.  However, existing hand-held ultrasound systems produce a low-resolution two-dimensional view within the patient, and fall far short of the image quality possible in state-of-the-art non-portable ultrasound systems.  These larger systems can produce real-time 3D ultrasound images, drastically improving system ease of use, and have already been demonstrated to improve diagnostic efficiency.  Moreover, direct 3D image acquisition enables new diagnostic capabilities that are difficult or impossible to accomplish with 2D, such as measuring volumetric blood flow.  However, forming 3D ultrasound images requires over 5000 times more computing horsepower than comparable 2D imaging.  Because it is in close contact with human skin, an ultrasound scan head must operate within a tight power budget (similar to that of a cell phone) to maintain safe temperatures.  Developing a 3D ultrasound imaging system within this tight power budget requires innovation both in signal processing and computer architecture techniques.<br/><br/>This project will develop a new hardware architecture for 3D hand-held ultrasound that leverages co-design of hardware and beamforming algorithms, three-dimensional die stacking, massive parallelism, and streaming data flow, to enable high-resolution 3D ultrasound imaging in a hand-held device.  The project focuses on three medical ultrasound applications areas: (i) image quality enhancements for general imaging applications, such as abdominal imaging; (ii) advanced 3D motion tracking; and (iii) high-frame-rate 3D flow tracking for cardiac applications.  The proposed research program focuses on hardware acceleration for specific, novel applications of diagnostic ultrasound targeting heart disease and Chronic Obstructive Pulmonary Disease, respectively the 1st and 3rd leading causes of death in the United States.  Project innovations will be demonstrated and evaluated using an FPGA prototype to reconstruct images of physical phantoms captured with existing ultrasound probes."
"1533918","XPS: EXPL: FP: Collaborative Research: SPANDAN: Scalable Parallel Algorithms for Network Dynamics Analysis","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/27/2015","Sajal Das","MO","Missouri University of Science and Technology","Standard Grant","Aidong Zhang","08/31/2019","$153,248.00","","sdas@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","8283","9150","$0.00","The goal of SPANDAN project is to create a novel architecture-independent framework <br/>for designing efficient, portable and scalable parallel algorithms for analyzing <br/>large-scale dynamic networks. SPANDAN will not only provide an intuitive methodology <br/>for efficiently translating sequential algorithms into scalable parallel algorithms <br/>for dynamic networks, but also provide mechanisms for their analytical evaluation and <br/>serve as a mediatory layer between applications and system level tuning. To evaluate <br/>the effectiveness of SPANDAN framework in real-world applications, the PIs will <br/>collaborate with social scientists and biologists. They will also integrate research <br/>findings into various courses such as network analysis, parallel algorithms, and <br/>bioinformatics. They will further collaborate with high schools to develop summer courses <br/>with the goal of encouraging women and minority students to pursue IT-related careers. <br/><br/><br/>As the underlying methodology, the SPANDAN framework will exploit graph sparsification <br/>techniques to divide the network into sparse subgraphs (certificates) that form the <br/>leaves of a sparsification tree. This innovative approach will lead to the design and <br/>analysis of efficient parallel algorithms for updating dynamic networks, and reduction <br/>of memory latency associated with parallelizing unstructured data. Specifically parallel <br/>algorithms will be designed for maintaining network topological characteristics, and <br/>updating influential vertices and communities. To demonstrate portability and performance, <br/>the developed algorithms will be implemented on the distributed memory clusters, shared <br/>memory multicores, and massively multithreaded CRAY-XMT.<br/><br/>For further information see the project web site at:<br/> http://cs.mst.edu/labs/crewman/projects/SPANDAN/"
"1812968","SHF: Small: Collaborative Research: Automatically Enhancing Quality of Social Communication Channels to Support Software Developers and Improve Tool Reliability","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/31/2018","Kostadin Damevski","VA","Virginia Commonwealth University","Standard Grant","Sol J. Greenspan","09/30/2021","$250,000.00","","kdamevski@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","2878","7923, 7944","$0.00","Social communication channels (e.g., Stack Overflow, Slack, and GitHub) play an increasingly important role in software developer productivity, as developers use these channels to collaborate and coordinate activities. Researchers have shown additional value in mining information from these sources to develop and improve recommendation systems, software integrated development environments, and other tools. Users of these systems complain about the varying and often poor quality of the information shared on these channels despite the intrinsic mechanisms for maintaining quality (e.g., voting, accepted answers). This project aims to address the need for automation in curating for increased quality of the information shared in developer communication channels. The envisioned system uses multiple software developers' social communication channels as parallel sources of evidence to establish quality of information in each channel.<br/><br/>This project will contribute to the state of the art by tackling three major challenges to bring automated curation of developer communication into practical use. The first aim is to develop analyses for automatically determining and improving the quality of developers' interactive communications on social media sites with regard to quality concerns including poor software properties, obsolescence, contradictions, unreliable sources, and duplication. The second aim is to explore different feedback mechanisms to indicate quality findings back to the communities. Third, the project will develop techniques to customize quality measurement and feedback to a particular developer's context. The resulting tools, data sets, and experimental infrastructure developed as part of the project will be released, enabling other researchers and practitioners to build on the project's results and ultimately advance the quality of the modern software development ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763848","SHF: Medium: Collaborative Research: Predictive Modeling for Next-generation Heterogeneous System Design","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","08/23/2018","Andreas Gerstlauer","TX","University of Texas at Austin","Standard Grant","Sankar Basu","09/30/2021","$679,411.00","Lizy John","gerstl@ece.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7924, 7945","$0.00","With semiconductor scaling reaching physical limits, performance and power consumption are ever more critical aspects in the design of emerging computer systems. Fast and accurate design models and tools are critical to support future computer system designers in evaluating design options before they can be built. Traditional simulation-based or analytical models are often too slow or inaccurate to effectively support design processes. This project instead develops novel machine learning-based, predictive methodologies to rapidly estimate the performance and power consumption of future generation products at early design stages using observations obtained on commercially available silicon today. Such techniques will allow efficient design cycles ensuring that the next-generation computing infrastructure meets the needs and expectations of consumers and continues to meet them over the product lifecycle. Along with research activities, course material on predictive modeling will be integrated into the university courses taught by the investigators, technology will be transferred to industrial partners through training and tutorials, and tools and models developed in this project will be released as open source software. In addition to training of graduate students, emphasis will be paid to undergraduate student training, towards including federally recognized under-represented groups, training of STEM teachers, and to run summer code camps to increase access for middle school and high school students. <br/><br/>This project specifically investigates use of advanced machine learning techniques for prediction of power and performance of any machine based on hardware-dependent and independent application characteristics obtained by running on any existing other machine, focusing on large-scale data center and accelerator technologies, namely multi-core CPUs, GPUs and FPGAs. Specific research tasks include the investigation of: (1) fast and accurate models for system designers and system programmers to perform rapid, early hardware and software design space exploration; (2) fast online prediction models that can be integrated into modern operating systems and virtual machine; and (3) fast yet accurate model training procedures that can create new predictive models while applications run. This research is expected to also allow semiconductor companies to better understand the scenarios under which predictive modeling is sufficiently accurate to be deployed during an industrial design process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763795","SHF: Medium: Collaborative Research: Predictive Modeling for Next-generation Heterogeneous System Design","CCF","SPECIAL PROJECTS - CCF, SOFTWARE & HARDWARE FOUNDATION","10/01/2018","08/23/2018","Philip Brisk","CA","University of California-Riverside","Standard Grant","Sankar Basu","09/30/2021","$322,350.00","","philip@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","2878, 7798","7924, 7945","$0.00","With semiconductor scaling reaching physical limits, performance and power consumption are ever more critical aspects in the design of emerging computer systems. Fast and accurate design models and tools are critical to support future computer system designers in evaluating design options before they can be built. Traditional simulation-based or analytical models are often too slow or inaccurate to effectively support design processes. This project instead develops novel machine learning-based, predictive methodologies to rapidly estimate the performance and power consumption of future generation products at early design stages using observations obtained on commercially available silicon today. Such techniques will allow efficient design cycles ensuring that the next-generation computing infrastructure meets the needs and expectations of consumers and continues to meet them over the product lifecycle. Along with research activities, course material on predictive modeling will be integrated into the university courses taught by the investigators, technology will be transferred to industrial partners through training and tutorials, and tools and models developed in this project will be released as open source software. In addition to training of graduate students, emphasis will be paid to undergraduate student training, towards including federally recognized under-represented groups, training of STEM teachers, and to run summer code camps to increase access for middle school and high school students. <br/><br/>This project specifically investigates use of advanced machine learning techniques for prediction of power and performance of any machine based on hardware-dependent and independent application characteristics obtained by running on any existing other machine, focusing on large-scale data center and accelerator technologies, namely multi-core CPUs, GPUs and FPGAs. Specific research tasks include the investigation of: (1) fast and accurate models for system designers and system programmers to perform rapid, early hardware and software design space exploration; (2) fast online prediction models that can be integrated into modern operating systems and virtual machine; and (3) fast yet accurate model training procedures that can create new predictive models while applications run. This research is expected to also allow semiconductor companies to better understand the scenarios under which predictive modeling is sufficiently accurate to be deployed during an industrial design process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833483","NSF Workshop on Programmable Networks","CCF","INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CCF, Networking Technology and Syst, ALGORITHMIC FOUNDATIONS","10/01/2018","08/18/2018","Anirudh Sivaraman Kaushalram","NY","New York University","Standard Grant","Tracy J. Kimbrel","04/30/2019","$70,000.00","Vladimir Braverman, Xin Jin","as11800@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","1640, 2878, 7363, 7796","073Z, 7556, 7926","$0.00","Programmable networks---networks whose functionality can be changed by a network operator---are emerging to fundamentally change the way computer networks are designed, built, and managed. Programmable networks have been a holy grail for networking research, and such devices (such as network interface cards and switches) as well as new advanced network applications are coming to market. Due to the requirement of providing fast packet processing, programmable network devices have a limited amount of high-speed memory and can perform only a small amount of computation on every packet. This award supports a workshop addressing a grand challenge in this space: to rethink the full software stack of networks, from developing new algorithms and data structures to efficiently realize network applications with limited resources, to developing high-level programming frameworks to simplify programming. This requires the efforts of multiple research communities: networking, algorithms, programming languages, and computer architecture.<br/><br/>This workshop aims to bring together these different research communities to survey the state of the art in programmable networks, to identify key research challenges going forward, and to exchange ideas. The primary goal is to stimulate discussion between different communities engaged in research on programmable networks and to broaden the research community working on programmable networks. As a concrete output from the workshop, participants will produce a report that will be published as an editorial in ACM SIGCOMM's online magazine, Computer Communication Review. This report will cohesively summarize the workshop's discussions, outline a set of research themes for future research in the area, and make recommendations on infrastructure support to aid research in programmable networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1337215","XPS: CLCCA: Allocating Heterogeneous Datacenter Hardware to Strategic Agents","CCF","INFORMATION TECHNOLOGY RESEARC, Exploiting Parallel&Scalabilty","09/15/2013","09/02/2013","Benjamin Lee","NC","Duke University","Standard Grant","Anindya Banerjee","08/31/2019","$700,000.00","Vincent Conitzer","benjamin.c.lee@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1640, 8283","7932","$0.00","With the democratization of cloud computing, increasingly diverse software tasks demand computation from cloud computing. Scalable datacenter computing is a 21st century imperative and energy efficiency is the most pervasive challenge. Increasing capability to extract value from the data deluge requires order-of-magnitude gains in efficiency. While heterogeneous hardware systems provide efficiency, they also complicate resource management. Managing new hardware architectures with economic mechanisms is a qualitatively new approach to mitigate this complexity.<br/><br/>The investigators propose resource allocation mechanisms that encourage truthful participation in datacenters with heterogeneous hardware. Resource allocation pairs hardware with software. Users may explicitly request specific amounts and types of hardware. Or users may define their value for performance, which agents then use to bid for hardware. Both allocation mechanisms rely on the truthful representation of preferences from users. The investigators will quantify inefficiencies arising from strategic behavior in datacenters and will design allocation mechanisms that are robust when strategic agents participate in the system.  Women and under-represented students will benefit from this research experience."
"1823562","FoMR: Collaborative Research: Single-Thread Multi-Accelerator Execution to Close the Dennard Scaling Gap","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/10/2018","Tony Nowatzki","CA","University of California-Los Angeles","Standard Grant","Yuanyuan Yang","09/30/2021","$37,000.00","","tjn@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","2878","021Z, 2878, 7798, 7941","$0.00","With ever-slowing scaling trends in microprocessor technologies, traditional techniques of improving processor performance are no longer viable, and achieving higher performance requires a dramatically different approach. This project develops a multicore chip microarchitecture using specialized accelerators and code-injection techniques without needing to modify user-level software, compilers and operating systems.  The impact of this research is to help steer microprocessor design in novel ways that can help sustain performance improvements, especially for datacenter and big-data computing.  <br/><br/>This project builds on a recent promising technique involves offloading program phases onto specialized processors (accelerators) which are tuned to execute programs with specific characteristics  (i.e., parallelism, control dependence, memory behavior) at extremely high efficiency. There are two main challenges which motivate the major thrusts of this work.  The first is the question of how to design a practical system for managing the execution of heterogeneous accelerators and dynamic translation.  The second is how to design a set of accelerators which provide integer factors of improvement over general purpose processors' performance and energy efficiency.  This work addresses the first challenge by designing a disaggregated translation subsystem, including region detection hardware at each core, a set of disaggregated compiler cores and a translation cache, and a hardware/software layer which dynamically re-maps logical threads to physical cores based on dynamic code properties and load balancing.  To design a set of balanced accelerators, this work is analyzing programs to identify key program behaviors, and developing targeted accelerators for each.  Finally, this includes the design of synthesis-time resource allocation algorithms which will co-optimize the choice of cache interface, general core attributes, and accelerator execution model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823447","FoMR: Collaborative Research: Single-Thread Multi-Accelerator Execution to Close the Dennard Scaling Gap","CCF","SPECIAL PROJECTS - CCF","10/01/2018","08/10/2018","Karthikeyan Sankaralingam","WI","University of Wisconsin-Madison","Standard Grant","Yuanyuan Yang","09/30/2021","$175,000.00","","karu@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","2878","021Z, 2878, 7798, 7941","$0.00","With ever-slowing scaling trends in microprocessor technologies, traditional techniques of improving processor performance are no longer viable, and achieving higher performance requires a dramatically different approach. This project develops a multicore chip microarchitecture using specialized accelerators and code-injection techniques without needing to modify user-level software, compilers and operating systems.  The impact of this research is to help steer microprocessor design in novel ways that can help sustain performance improvements, especially for datacenter and big-data computing.  <br/><br/>This project builds on a recent promising technique involves offloading program phases onto specialized processors (accelerators) which are tuned to execute programs with specific characteristics  (i.e., parallelism, control dependence, memory behavior) at extremely high efficiency. There are two main challenges which motivate the major thrusts of this work.  The first is the question of how to design a practical system for managing the execution of heterogeneous accelerators and dynamic translation.  The second is how to design a set of accelerators which provide integer factors of improvement over general purpose processors' performance and energy efficiency.  This work addresses the first challenge by designing a disaggregated translation subsystem, including region detection hardware at each core, a set of disaggregated compiler cores and a translation cache, and a hardware/software layer which dynamically re-maps logical threads to physical cores based on dynamic code properties and load balancing.  To design a set of balanced accelerators, this work is analyzing programs to identify key program behaviors, and developing targeted accelerators for each.  Finally, this includes the design of synthesis-time resource allocation algorithms which will co-optimize the choice of cache interface, general core attributes, and accelerator execution model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816003","CIF: Small: Collaborative Research: Blue-Noise Graph Sampling","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2018","05/21/2018","Daniel Lau","KY","University of Kentucky Research Foundation","Standard Grant","Phillip Regalia","05/31/2021","$199,997.00","","dllau@uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7797","7923, 7935, 9150","$0.00","This project presents a collaborative research and education effort in graph signal processing where interesting phenomena in nature can often be captured by graphs since objects and data are invariably interrelated in some sense.  Social networks, ecological networks, and the human brain are a few examples of such networks. A feature that these networks of interest have in common, is that they define very large graphs. Algorithms used to compute properties of complete graphs, rapidly become impractical when the graphs under study become very large. Graph sampling thus becomes essential. The research explores a somewhat radical departure from the prior work on graph sampling and is based on the notion of stochastic sampling in irregular sampling grids. In concert with the advancing the scientific goals of the project, the investigators will also jointly develop a short course on graph signal processing and its applications so as to introduce this emerging field to a broad set of students.<br/><br/>While fixed-time sampling is well known, this project focuses on stochastic sampling theory to graph signal processing and, in particular, graph sub-sampling with and without knowledge of the signal. In the case of sampling without signal knowledge, investigators intend to design optimal sub-sampling grids that minimize low frequency energy in a binary signal where investigators intend to develop low computational complexity dither algorithms that closely mimic the ideal patterns. In the case of sub-sampling with signal knowledge, the developed adaptive sub-sampling algorithms will adjust the sampling rate based on the local frequency content of the signal to, thereby, sample at just above the Nyquist rate of the sample within a small region of interest.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1815992","CIF: Small: Collaborative Research: Blue-Noise Graph Sampling","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2018","05/21/2018","Gonzalo Arce","DE","University of Delaware","Standard Grant","Phillip Regalia","05/31/2021","$300,000.00","Sebastian Cioaba","arce@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","7797","7923, 7935, 9150","$0.00","This project presents a collaborative research and education effort in graph signal processing where interesting phenomena in nature can often be captured by graphs since objects and data are invariably interrelated in some sense.  Social networks, ecological networks, and the human brain are a few examples of such networks. A feature that these networks of interest have in common, is that they define very large graphs. Algorithms used to compute properties of complete graphs, rapidly become impractical when the graphs under study become very large. Graph sampling thus becomes essential. The research explores a somewhat radical departure from the prior work on graph sampling and is based on the notion of stochastic sampling in irregular sampling grids. In concert with the advancing the scientific goals of the project, the investigators will also jointly develop a short course on graph signal processing and its applications so as to introduce this emerging field to a broad set of students.<br/><br/>While fixed-time sampling is well known, this project focuses on stochastic sampling theory to graph signal processing and, in particular, graph sub-sampling with and without knowledge of the signal. In the case of sampling without signal knowledge, investigators intend to design optimal sub-sampling grids that minimize low frequency energy in a binary signal where investigators intend to develop low computational complexity dither algorithms that closely mimic the ideal patterns. In the case of sub-sampling with signal knowledge, the developed adaptive sub-sampling algorithms will adjust the sampling rate based on the local frequency content of the signal to, thereby, sample at just above the Nyquist rate of the sample within a small region of interest.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1423624","NeTS: Small: Pareto-Optimized Heat Diffusion Protocol on Ollivier-Ricci Curvature Controlled Wireless Networks","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2014","07/24/2014","Edmond Jonckheere","CA","University of Southern California","Standard Grant","Phillip Regalia","07/31/2019","$499,995.00","Bhaskar Krishnamachari","jonckhee@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","7923, 7935","$0.00","While there are a variety of throughput-optimal wireless network protocols, more challenging is the problem of making the protocol optimal relative to such conflicting objectives as queue occupancy (related to latency) and routing cost (related to power management), while holding throughput-optimality. The first step in that direction is the design of a protocol mimicking heat diffusion, as the celebrated Dirichlet principle of heat calculus already endows the routing with minimum routing cost property. The next step requires a significant departure from heat diffusion in order to make the protocol Pareto-optimal relative to routing cost and queue occupancy, while enforcing interference restrictions, link directionality and capacity. From this point onwards, the research effort will be conducted along two different lines of investigation. First, since the classical heat calculus had to be modified in a nontrivial way to make it an implementable wireless network protocol, there is a need to understand the ?heat equation? in the context of directed graphs, subject to interference restrictions and capacity constraints. Classical heat calculus involves the classical Laplacian, which is a linear operator, while here the central mathematical object of concern of this new ""heat calculus"" is a nonlinear Laplacian in the fluid limit, which describes the rate-level, rather than packet-level, behavior of the stochastic wireless network. The second line of investigation is dedicated to finding the network invariant that could anticipate the potential for large queue occupancy and/or large routing cost. We will develop the Ollivier-Ricci curvature as a computationally implementable network parameter inversely proportional to queue occupancy and routing cost, even under directionality and other network constraints. Also, the Ollivier-Ricci curvature will be investigated as a predictor of the size of the capacity region. Finally, the research will culminate with some Ricci flow technique to optimize the network for maximum Ollivier-Ricci curvature, hence attaining the largest capacity region.  <br/><br/>The framework of this proposal is applicable to a wide family of stochastic problems with interdependent resources, where the resources are a collection of interdependent servers that can only be accessed under certain constraints, and the consumers are of random service time with asynchronous completion. This general model describes a wide variety of problems including queuing networks, product assembly systems, memory or processor managements, call centers, agent allocations, data switches, healthcare systems, and transmission planning or storage allocation in power systems just to name a few.  Specifically, the research will strive to achieve cross-fertilization among this broad set of problems with classical thermodynamics and Ohm?s law in circuit theory that open a new way to analyze and optimize these complicated problems."
"1617397","CIF: Small: Collaborative Research: Geometrical and Statistical Modeling of Space-Time symmetries for Human Action Analysis and Retraining","CCF","COMM & INFORMATION FOUNDATIONS","07/15/2016","07/08/2016","Anuj Srivastava","FL","Florida State University","Standard Grant","Phillip Regalia","06/30/2019","$216,883.00","","anuj@stat.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7797","7923, 7936","$0.00","This interdisciplinary research aims to advance current understanding and utilization of space-time symmetries in analyzing human movements, using fundamental tools from engineering, geometry, and statistics. Broader applications of this research include home or workplace-based self-reflection of daily activities, promotion of higher efficiency of human movements, and long-term management and/or prevention of movement disorders. While the need for comprehensively and statistically analyzing human kinematics is well chronicled, the current measures are often limited to simplistic quantities such as speeds and acceleration profiles of individual limbs. This project will focus on both spatial and temporal symmetries of limb movements, full body shapes, and complete dynamical actions, for assessment of movements ranging from daily activities to physiotherapeutic exercises. <br/><br/>Symmetry has been used in the past, in clinical biomechanics, but in a limited way. This project will develop a comprehensive theory, built on fundamental tools from differential geometry and statistical analysis of geometric objects, to represent, quantify, analyze, and classify motions according to their level of symmetry. The specific forms of symmetry will include spatial reflection, temporal reflection, and space-time glide symmetries. This formulation will incorporate data from various sensing modalities and features, including point trajectories and stick figures from motion capture systems, to shape silhouettes and dynamic textures obtained from video sensors. The project outcomes also include the development of a real-time media-system for movement re-training and reflection of common actions, such as sitting to standing (STS). The proposal brings together a strong and inter-disciplinary team of researchers with expertise in computer vision and action recognition (Turaga), differential geometry and statistics (Srivastava), and somatics and kinesiology (Coleman)."
"1527388","CIF: Small: Learning Signal Representations for Multiple Inference Tasks","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2015","07/27/2015","Maxim Raginsky","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","07/31/2019","$500,000.00","Pierre Moulin","maxim@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7936","$0.00","Rapid advances in high-performance computing and widespread availability of massive datasets are bringing about a paradigm shift in the theory and practice of signal representations, geared towards inference and learning. A signal representation is a compressed summary that only retains those features of the signal that are salient for a class of inference tasks. This project provides a comprehensive theoretical and algorithmic framework for signal representations, which is sufficiently broad to cover both the traditional types of signal representations, such as vector quantization and sparse codes, and the more modern types, inspired by recent advances in machine learning and signal processing for Big Data. <br/><br/>Under this framework, the statistical performance and the computational complexity of signal representations are addressed in a unified manner by imposing structural constraints on the encoding map, the decoding map, and the model space of the representation, while simultaneously tailoring these objects to the class of tasks of interest. This unification leads to new theoretical and algorithmic insights into highly structured internal representations that are a key factor in recent spectacular success of deep neural networks on challenging tasks in visual, audio, and speech analytics."
"1815559","NSF-BSF:CIF:Small: Searching for the Rare: an Active Inference and Learning Approach","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2018","05/22/2018","Qing Zhao","NY","Cornell University","Standard Grant","Phillip Regalia","06/30/2021","$490,078.00","","qz16@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","7923, 7935, 9102","$0.00","This project addresses the problem of searching for a few rare events of interest among a massive number of possibilities. The rare events may represent opportunities with exceptional returns or anomalies associated with high costs or potential catastrophic consequences. This problem arises in a broad range of applications, ranging from communications and infrastructure systems, cyber-security, to social-economic networks, and is particularly relevant in the era of increasing network size and abundance of data. The multidisciplinary nature of this project also provides a rich research experience for both undergraduate and graduate students.<br/><br/>The scientific objective is to develop general design methodologies for detecting rare events quickly and reliably when the total number of hypotheses is large, the observations are noisy, and the prior knowledge on the rare events may be as little as ""they are different from the nominal."" The project consists of three steps that represent a logical progression in scope and level of difficulty: (i) achieving optimal sample complexity with respect to detection accuracy through active hypothesis testing; (ii) achieving optimal sample complexity with respect to the dimension of the search space by exploiting inherent hierarchical structures; (iii) tackling unknown models by integrating online learning with active inference. The holistic treatment within a principled theoretic framework draws inspirations from and contributes to fundamental theories of active hypothesis testing and statistical and machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1420651","CIF:Small: Collaborative Research:Theoretical Foundation of Distributed Wireless Channel Access","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2014","07/25/2014","Anthony Ephremides","MD","University of Maryland College Park","Standard Grant","Phillip Regalia","08/31/2019","$250,000.00","","etony@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7797","7923, 7935","$0.00","Due to lack of fundamental understanding on how to share wireless media among spatially distributed users, the energy consumption and bandwidth efficiency of wireless networks remains defficient and highly suboptimal. Prior investigations of wireless channel access have followed two separate paths that reflect vastly different viewpoints; namely, the traditional information-theoretic approach assumes perfect user coordination and ignores the modularized network architecture, while the traditional network-theoretic approach largely focuses on access control protocols and ignores the impact of the physical layer. This project will bridge the gap of the classical theories by developing a theoretical foundation for channel access in distributed wireless systems. More specifically, it will extend classical information theory by developing a channel coding theory for physical layer distributed communication, where users do not jointly design channel codes. It will also extend classical network theory by developing a medium access control (MAC) framework for distributed networking, where physical layer properties, such as joint multiuser message decoding and flexible adaptation of communication parameters, are efficiently exploited at the link layer.<br/><br/>The project contains two parts which respectively address the physical and the data link layers of distributed wireless networks. In Part I of the project, the goal is to develop a rigorous coding theory to characterize the fundamental limitis of distributed communication systems. The coding theory will support extensive communication performance tradeoffs and structured coding schemes with low computational complexity. Part II of the project contains three steps. The objective of the first step is to characterize optimal link layer distributed channel sharing schemes, their fundamental properties and performance improvements over classical schemes. The objective of the second step is to develop a unified MAC framework to achieve asymptotic optimal channel sharing in distributed networks via the joint adaptation of communication parameters (e.g., rate, power, transmission probability). The objective of the third step is to develop MAC algorithms with fast convergence properties that ensure efficient network operation in transient environments.<br/><br/>By extending information theory to distributed communication models, the project will advance the integration of information and network theories, and significantly improve the energy and bandwidth efficiency of wireless systems. Unification of the two classical theories will also influence the way modern communication network subjects are taught in Higher Education and attract more talented students to this field of acute national importance."
"1525904","CIF:  Small:Toward a Stochastic Geometry for Cellular Systems","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2015","06/26/2015","Martin Haenggi","IN","University of Notre Dame","Standard Grant","Phillip Regalia","06/30/2019","$492,642.00","","haenggi.1@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7797","7923, 7935, 9251","$0.00","Demands for wireless Internet and voice access have continued to grow exponentially, while the available spectrum remains scarce. As a result, novel architectures and transmission techniques are needed for cellular networks to improve their spectral efficiency and provide consistent and high-speed wireless service for all users. The two key approaches to achieve this goal are increased network density and heterogeneous network architectures, where multiple tiers of base stations are deployed with different capabilities, depending on the user density and traffic demands. For such networks, new mathematical models and techniques are needed that capture their inherent randomness and heterogeneity. Stochastic geometry is a mathematical theory that is ideally suited for such problems. It provides both the models and the theory for the analysis of the network performance and user experience. This project focuses on the development of stochastic geometry-based tools tailored to the fifth generation of cellular systems (5G), which will result in novel design insights and help identify promising network architectures without the need for extensive and expensive simulations. Hence it will have a significant impact on the discussions on 5G that currently dominate the wireless industry and academic research and may even influence the standardization process.  In addition, the project devises novel analytical techniques and makes theoretical contributions that are applicable beyond cellular networks, and it helps train future generations of students in emerging wireless technologies and analysis techniques.<br/><br/>As cellular networks become denser and more heterogeneous, the locations of the base stations become more irregular due to restrictions on the placement and adaptation to users and traffic. As a result, classical network models such as lattices become outdated and need to be replaced by models that capture the inherent randomness in the base station locations. Recently, researchers have applied techniques from stochastic geometry for the analysis of some of the key metrics of cellular systems, most notably the signal-to-interference ratio, which determines the quality of the wireless connections. However, the underlying model was mostly restricted to the Poisson point process, which is analytically convenient but not very realistic. The analysis of more accurate models and of advanced transmission schemes such as base station cooperation and multi-antenna transmission has proven rather difficult. Hence there is an urgent need to devise new models that accurately describe current and future cellular networks and to significantly extend the set of tools for their analysis. This proposal aims at meeting this need by applying novel ideas and recent insights to develop new theoretical methods that expand the currently available ones in three main directions: (1) efficient ways to obtain highly accurate approximate results for diverse network models; (2) fine-grained and sharp results on the experience of individual users; (3) fundamental insight into the impact of the temporal dependence of the interference in cellular systems. The analytical methods used include Palm theory, Tauberian theorems, series and factorial moment expansions, and general probability theory, and the models will be validated with actual data."
"1719133","CCF: CIF: Small: Interactive Learning from Noisy, Heterogeneous Feedback","CCF","COMM & INFORMATION FOUNDATIONS","07/15/2017","07/07/2017","Kamalika Chaudhuri","CA","University of California-San Diego","Standard Grant","Phillip Regalia","06/30/2020","$499,999.00","Tara Javidi","kamalika@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7923, 7936, 9102","$0.00","The goal of this project is to develop interactive learning frameworks and methods that can learn predictors based on complex, imperfect feedback adaptively solicited in an on-line fashion from human annotators. Such predictors can significantly benefit the practice of machine learning by making it more accessible in domains where annotations are expensive. Currently, beyond a handful of heuristic studies, the only well-understood interactive learning setting is active binary classification, where a single annotator interactively provides labels to a learning algorithm. The main challenge in exploiting richer feedback is that human responses are inherently inconsistent and imperfect. This project will overcome this challenge by assuming that the responses come from unknown probability distributions with some mild yet realistic properties, which will be exploited to provide methods that can learn reliably from complex feedback.<br/><br/>Specifically, this project will introduce a general framework for interactive learning from imperfect, complex feedback, and develop methods for three common cases: (1) Active Learning with Abstention Feedback, where annotators can either provide a label or declare I Don't Know (2) Active Learning for Multiclass Classification, where the goal is to learn a classifier for a large number of classes and (3) Active Learning with Feedback from Multiple Annotators, where the goal is to combine feedback from many labelers with varying amounts of expertise subject to a budget. These problems will be approached through two main tools -- adaptive hypothesis testing and surrogate loss minimization. Combining these approaches will lead to principled algorithms for building accurate machine learning predictors with low annotation cost, which in turn, will benefit the practice of machine learning in domains where annotated data is expensive."
"1618078","CIF Small: Massive MIMO in the MM-Wave Range: The Theory of Making it Practical","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2016","06/27/2016","Andreas Molisch","CA","University of Southern California","Standard Grant","Phillip Regalia","08/31/2019","$496,285.00","","molisch@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7797","7923, 7935","$0.00","Fifth generation cellular communications will critically depend on making available large swaths of bandwidth in the mm-wave range. To enhance spectral efficiency, and compensate for the large pathloss of the mm-wave bands, large antenna arrays (massive MIMO) will be used at both the transmitter and receiver. This project aims to develop new techniques to reduce both the cost and energy consumption normally associated with such large arrays. Solving these problems will be an important step forward for 5G, which in turn will be a cornerstone of the presidential vision of a wireless broadband revolution in the coming years. It will allow consumers to benefit from higher data rates at lower costs, and network operators and equipment manufacturers to better leverage their investment in mm-wave technology.<br/><br/>The focus of this work is on hybrid analog/digital transceiver structures that combine analogue beam forming based on second-order channel statistics (CSI) with digital beam forming based on instantaneous CSI, a structure that has both practical and fundamental advantages. The key point of the project is to tackle research questions that, while inspired by practical requirements, are novel and deep challenges in communication theory and signal processing. This project considers setups involving hybrid transceivers at both the base station and the user equipment, thus requiring optimization over matrices that change on different timescales, with different dimensions, and are linked over channels that do not follow the convenient, but unrealistic, simplifications commonly used. Another important practically motivated challenge is the impact of the properties of the propagation channel, which will be investigated through novel measurement and parameter extraction methods."
"1528030","CIF: Small: Mobile Immersive Communication: View Sampling and Rate-Distortion Limits","CCF","COMM & INFORMATION FOUNDATIONS, EPSCoR Co-Funding","09/01/2015","03/31/2017","Jacob Chakareski","AL","University of Alabama Tuscaloosa","Standard Grant","Phillip Regalia","08/31/2019","$240,167.00","","jacob@ua.edu","801 University Blvd.","Tuscaloosa","AL","354870005","2053485152","CSE","7797, 9150","7923, 7935, 9150, 9251","$0.00","Free-viewpoint video is an emerging technology for visual communication that creates the sensation of 3D immersion in the remote scene by allowing the user to dynamically switch between arbitrary viewpoints. It has the potential to advance society by enabling virtual human transportation and boost the global economy and quality of life. At present, free-viewpoint video is limited to high-end computing environments and studio-type settings, due to its higher bandwidth and complexity expansion over single-view video. Furthermore, the fundamental questions of viewpoint sampling (camera location) and resource allocation across the captured views are largely unanswered, due to the nascency of the technology, and are addressed using suboptimal heuristic approaches, thus penalizing  system efficiency. These two characteristics would otherwise make free-viewpoint video impractical and preclude its broader deployment, in particular on mobile devices, due to their constrained bandwidth, battery power, and CPU capabilities. However, the latter have become a primary platform for computing and communication needs, anywhere and anytime, a trend that will only accelerate in the future. Thus, it is anticipated that only by enabling ubiquitous and seamless mobile free-viewpoint video may the full potential of immersive communication be achieved. This project seeks to achieve this goal via concerted advances in signal representation, wireless video communication, and user-action modeling that will be integrated holistically.  The advances delivered by this investigation will have broad impact across diverse fields that involve live video communication via multiple viewpoints, including telemedicine, telepresence and telecollaboration, remote monitoring and control, entertainment (3D and free-viewpoint TV), gaming and virtual worlds, people-centric sensing and connected-community applications.<br/><br/>The project will pursue the following technical thrusts: (i) Characterization of the fundamental trade-offs between viewpoint space sampling, rate allocation, and signal fidelity in immersive mobile communication; (ii) Derivation of the optimal sampling policy, at the view and data-unit levels, in uplink communication; (iii) Design of novel view and rate scalable coding for multi-view broadcast and derivation of the optimal view embedding policy, as a function of the broadcast rate, in downlink communication; (iv) Characterization of the optimal scheduling policy for local ad-hoc communication between mobile clients; and (v) Integration of energy conservation and characterization of the trade-offs between battery lifetime and multi-view application performance."
"1514538","CIF: Medium: Collaborative Research: On-demand Physical Layer Cooperation","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2015","06/21/2017","Ayfer Ozgur","CA","Stanford University","Continuing grant","Phillip Regalia","06/30/2019","$333,000.00","","aozgur@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7797","7924, 7935","$0.00","Wireless access is fast becoming the primary portal to the Internet, causing an exponentially rising demand for wireless data.  This has pushed current wireless systems to their limits, despite significant investment in infrastructure to meet the ever-growing demands. Physical layer cooperation can enable near-optimal usage of the available wireless bandwidth.<br/><br/>This proposal fundamentally rethinks physical-layer cooperation by introducing an on-demand approach to wireless cooperation. It can be argued that the success of the (wired) Internet was made possible by its on-demand operation, using adaptation, local knowledge, feedback and a best-effort service model. Notably, the success of many peer-to-peer network protocols is closely tied to on-demand information propagation and adaptation of the network. The broad goal of this proposal is to bring this philosophy to wireless networks, enabling near-optimal usage of the wireless network bandwidth within the complexity constraints of implementable systems. This project aims to complement the theoretical work with proof-of-concept deployments on software radio testbeds, and also engage  industry partners to impact next-generation wireless network designs.  The project also promotes training of research engineers, through a plan to establish a unique inter-university education and research program, which will include joint and collaborative student advising and curricular development. <br/><br/>The underlying assumption of most network information theory works is that one can build architectures which tightly coordinate the estimation and sharing of information about the wireless channels, the user requirements and the network topology, at a very fast time-scale, without impacting the performance (rates, error). That is, the cost of learning the very dynamic network state is not accounted for. Another implicit assumption is complete network usage, that all available relays in a wireless network are used, with no adaptation to user demand. This can be very wasteful in many situations, and again, the cost of unnecessarily using relays is not accounted for. The many breakthrough ideas based on these assumptions have advanced a collective understanding, but bringing physical layer cooperation techniques closer to practical networks requires additional steps to move beyond these assumptions. This project puts together a program that develops the theoretical foundations and practice of an on-demand network operation, that dispenses with these assumptions. This entails operating specific subsets of the network relays (sub-networks) that fulfill target rates, as opposed to using all network relays to achieve the best possible performance. This project constructs a theoretical understanding of how to select, adapt and operate these sub-networks on demand, by using accountable partial network knowledge and using feedback mechanisms to enhance signal adaptation to the unknown. The theoretical formulations are tightly coupled to implementable protocols that will be validated in test beds."
"1618425","SHF: Small: Efficient Parallel Execution of Irregular, Ordered Algorithms","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2016","05/13/2016","Keshav Pingali","TX","University of Texas at Austin","Standard Grant","Anindya Banerjee","05/31/2019","$449,570.00","Jayadev Misra","pingali@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7923, 7943","$0.00","Most computers today consist of a collection of individual processing units called cores that can execute an application co-operatively, reducing the time required to produce the output of that application. However, current programming languages were developed for sequential computers that have a single processing unit, and they are not ideal for programming multicore parallel processors, while existing languages and tools for parallel programming are very difficult to use, requiring expert understanding of the computer hardware and system software. This project's broader significance and importance is that it aims to simplify the parallel programming of an important class of applications that includes physical simulations, such as battle-field simulations, and analysis of graphs, such as social networks. The intellectual merit is that the abstractions and systems software needed to parallelize these applications efficiently on multicore processors goes well beyond the state of the art, and if successful, will lead to a significant improvement in our understanding of how multicore parallel computers can be exploited effectively.  <br/><br/>Traditionally programmers have relied upon an abstraction called Task Dependence Graph for exposing parallelism in applications. However, dependence graphs cannot be used for emerging applications such as discrete-event simulation of physical systems, e.g., colliding particles and modeling of deforming materials using asynchronous variational integrators. Parallelization of such applications is very challenging because of the complex behaviors exhibited by tasks in such applications: for example, tasks may <br/>create new tasks which must be executed before existing tasks due to ordering constraints based on simulation-time causality, and the execution of one task may change the dependences between existing tasks. The key insight behind the project is that a data structure called the Kinetic Dependence Graph (KDG) can be used to track dependencies in such applications, permitting safe parallel execution at the cost of some book-keeping expense to maintain the KDG. The programming constructs and systems implementations developed by the project will be released publicly as part of the Galois system from the University of Texas at Austin."
"1812777","SHF: Small: Collaborative Research: Harvesting Wasted Time and Existing Circuitry for Efficient Field Testing","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","05/18/2018","Kundan Nepal","MN","University of St. Thomas","Standard Grant","Sankar Basu","09/30/2021","$93,491.00","","kundan.nepal@stthomas.edu","2115 Summit Avenue","St. Paul","MN","551051096","6519626038","CSE","7798","7923, 7945","$0.00","Digital integrated circuits (ICs) are the ""brains"" in the electronic devices used throughout modern society.  It is essential that these devices work correctly and reliably - especially in critical applications such as autonomous vehicles, infrastructure, the financial system, and health care.  In such applications, highly effective and efficient testing should be done in the field to identify problems that may arise after the device has been sent to the customer.  Thus, in this project, new ways of using circuitry already present on modern ICs while efficiently creating, applying, and transporting field-based tests to different parts of the chip will be investigated. At the same time, educational opportunities made possible through the project will be pursued, including the use of research findings in courses, the involvement of graduate and undergraduate students in the research, and outreach to high school students through summer camps and research opportunities.  The inclusion of members of underrepresented groups in these educational opportunities will be a focus of the investigators, who already have a proven record in mentoring such students in research.  <br/><br/>To address the field-testing issues, three major tasks will be performed in this research.  First, methods that use existing error-detection circuitry in an IC to make the field testing process more efficient will be explored.  In particular, times when the IC is operating in normal functional mode will be used to test for the presence of some faults.  Testing for those same faults in dedicated test sessions can then be avoided, reducing test time and the amount of time a circuit must be taken offline for test.  Next, methods for effectively using otherwise wasted time during dedicated test sessions to detect faults will be explored.  Finally, approaches that optimize the internal test network to efficiently send data used for test throughout a chip will be investigated.   Taken together, these approaches should enable field-based test to achieve high fault coverage while minimizing test time, thus enhancing the reliability of devices that so many people depend upon every day.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617401","SHF: Small: Design and Inference of Choreography Types to Reduce Concurrency Programming Errors","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2016","05/19/2016","Gul Agha","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sol J. Greenspan","05/31/2019","$500,000.00","","agha@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7923, 7944","$0.00","With the growth of the web, cloud computing, sensor networks and<br/>multicore programming, concurrency has become critical to software<br/>applications in the real world.  Because the Actor model of concurrent<br/>computation provides scalable concurrency, commercial software is<br/>often written using the Actor model.  Many concurrency related errors<br/>such as unprocessable messages, deadlocks, and livelocks result from a<br/>mismatch of component actors in a system.  The research develops<br/>choreography types as a method to understand concurrency structures<br/>and detect concurrency related bugs.  The intuition behind the<br/>research is that combining symbolic execution with information from<br/>concrete traces obtained during unit testing can provide a way to<br/>infer and approximate choreography types.<br/> <br/>The research would make concurrent and distributed programming<br/>safer--increasing trust in applications such as cloud computing on<br/>which a large part of the US economy is dependent today.  Much as data<br/>types help programmers think about the interfaces of components in a<br/>sequential program, the research would provide ways in which<br/>programmers can think about the structure of parallel programs in<br/>terms of choreographing actors with evolving type structure.  The<br/>educational impact of the research is to facilitate teaching scalable<br/>parallel programming, creating tools that expose and facilitate the<br/>understanding of the structure of concurrent programs."
"1525925","SHF: Small: Collaborative Research:Variation-Resilient VLSI Systems with Cross-Layer Controlled Approximation","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/29/2015","Sachin Sapatnekar","MN","University of Minnesota-Twin Cities","Standard Grant","Sankar Basu","07/31/2019","$210,000.00","","sachin@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","7923, 7945","$0.00","Applications driven by human-computer interactions through the five human senses are projected to underpin the next generation of computing. For many of these applications, occasional small errors are often not only acceptable but also bring opportunities for building lighter, cheaper, and more robust systems that use less energy and may have a longer battery life. This project will study how to advance computing technology by allowing deliberate imprecision in hardware implementations through the notion of approximate computing.  The outcomes of this project will be a set of design techniques for approximate computing that can become a key component of hardware computing technology, potentially benefiting systems ranging from high performance computing for big data analytics and low power implementation for internet of things. This project will also provide an opportunity for training students with the latest design and computing technology.<br/> <br/>The research goals of this project are to create new approximate computing techniques to optimize a system at all stages of its life, from design-time to runtime, which can enable cross-layer control of performance-power-precision trade-offs. The research agenda consists of several components. First, new error models with different accuracy-complexity trade-offs will be developed. Second, new design-time optimization techniques, especially hardware resource scheduling and binding in high-level synthesis, will be studied with consideration of approximation, variation, and runtime circuit reconfiguration. Third, compile-time and operating-system-level task mapping/scheduling algorithms will be investigated to make the best use of circuits with various precisions. Last but not least, runtime precision control techniques will be explored in conjunction with dynamic voltage and frequency scaling in order to achieve a smooth trade-off between power and user experience."
"1423656","SHF: Small: Some Error Correcting Codes for Computer Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2014","07/21/2014","Bella Bose","OR","Oregon State University","Standard Grant","Sankar Basu","08/31/2019","$288,271.00","","bose@eecs.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7798","7923, 7945","$0.00","Error control codes play a major role in achieving high reliable operations of modern computer systems, DVDs, cameras, etc. In this project new classes of error correcting codes suitable for high density flash memories are to be developed. The market revenue of flash memories is estimated to be $100B per year. The PI has close collaboration with leading Flash memory industry (Micron). While this enables development of error correcting codes specifically in the practical context of applications, the results of this research will not only be easily transitioned to industry, it will also train students, postdocs and future industry researchers via lectures, seminars and course development at the PIs institution. <br/><br/>In this project, multi-level flash, in which each cell can have 4 or 8 voltage levels, will be considered. Experiments indicate that the bit error rate of flash memory worsens as the Program/Erasure count increases. In addition, it is observed that most of the errors are of limited magnitude and of decreasing type. Study of error statistics of error characteristics justify that the L1 distance codes are more suitable to overcome errors in flash memories. New classes of L1 distance codes are to be developed using the concepts of elementary symmetric functions. The project will also consider the design of error correcting codes for insertion and deletion of repeated symbols in high speed data bus and communication systems. Furthermore, the theory and codes to be developed for flash memories can potentially be extended to design efficient high-ordered-spectral-null codes for optical discs and magnetic memories."
"1550470","EAGER: HAWKEYE: A Cross-Layer Resilient Architecture to Tradeoff Program Accuracy and Resilience Overheads","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2015","08/11/2015","Omer Khan","CT","University of Connecticut","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$250,000.00","Henry Hoffmann","khan@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7798","7916, 7942","$0.00","With semiconductor technology scaling to sub-nanometer scales, minute perturbations in lithography patterns and manufacturing processes make the computing hardware vulnerable to unpredictable deviations in functionality. Further, with increasingly strict power constraints, hardware designs are now reliant on dynamic voltage scaling between nominal and near-threshold regions, which exacerbates the reliability problem. These trends motivate the need for system resiliency without increasing power consumption. This project proposes a new method to achieve efficient resiliency, where hardware's coverage is bounded with certain guarantees, while computational efficiency is traded off at the cost of affecting program accuracy. The strategy focuses on data analytic applications since they naturally offer tradeoffs in program accuracy due to the unstructured nature of inputs, and the approximate algorithms used to solve these otherwise intractable problems. At the software level, the project is developing methods to classify crucial versus non-crucial code regions in a program: the crucial code affects program correctness and outcome, its functionality cannot be altered in any way; non-crucial code affects program accuracy, therefore it could tolerate errors. The hardware subsequently implements stronger resiliency methods (e.g., redundant execution) only for crucial code. For the non-crucial code, lightweight resiliency schemes ensure program correctness. If successful, this project will be a major step towards a new rigorous framework to reason and guarantee system resiliency alongside a program's performance, power, and accuracy of computation. <br/><br/>The proposed research provides for significant broader impacts related to curriculum development and student training through integration of modules in existing computer architecture and system design courses, and outreach through established REU sites and enrichment programs. The project promises transparent resiliency guarantees and this will have a major societal impact: enterprises and mission agencies will be able to reason in a quantifiable way about the resiliency versus efficiency tradeoffs in their software and hardware infrastructure. This is a crucial step forward given the increased awareness of system reliability in many application domains, such as healthcare, defense, finance, transportation, and automotive."
"1714699","SHF: Small: Supporting Regular Expression Testing, Search, Repair, Comprehension, and Maintenance","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/10/2017","Kathryn Stolee","NC","North Carolina State University","Standard Grant","Sol J. Greenspan","07/31/2020","$499,996.00","","ktstolee@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7923, 7944, 9102","$0.00","In software development, regular expressions are a common programming construct used for many purposes, including querying databases, searching documents, validating user input, and parsing files. Most programming languages have standard libraries or built-in support for regular expression processing. Despite their frequent appearance in software development activities, regular expressions are prone to programming errors. When a regular expression is responsible for a software bug, the impact can be severe, possibly resulting in corrupted data, security vulnerabilities, denial of service attacks, or website outages. This research develops new techniques to test, understand, reuse, and maintain regular expressions, in an effort to improve developer comprehension and reduce related bugs.<br/><br/>The approach is to develop coverage criteria for test suites, similarity metrics, and semantics-preserving transformations for regular expressions. The coverage criteria apply to the automata representation of the regular expression and are used to automatically generate test inputs to help developers adequately test regular expressions. The similarity metrics allow developers to find regular expressions that are similar to a buggy regular expression, as well as explain how the behavior differs among them. The semantics-preserving transformations enhance comprehension and maintenance, and also support the migration of regular expressions between languages.The broader impacts come primarily from the goal of reducing bugs related to regular expressions, which creates more reliable software for all."
"1619455","SHF: Small: Collaborative Research: Helping Mobile App Developers Make Implementation Decisions Based on App Store Analytics","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","06/30/2016","William Halfond","CA","University of Southern California","Standard Grant","Sol J. Greenspan","06/30/2019","$256,872.00","","halfond@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","7923, 7944","$0.00","The growth of mobile apps in recent years has been aided by Frameworks, Services, and Third Party Libraries (FSTPL), which provide support for user interfaces, advertising, analytics, and other critical app functionality. These FSTPLs enhance developer productivity, improve security, make key functionality easily accessible, and modularize complex and error-prone components. Our prior work shows that there is a tradeoff between the benefits of using FSTPLs and the impact they have on end users; however, developers lack clear guidance on how to manage these tradeoffs. Best practices that can be found online are generally anecdotal and sometimes contradictory. For developers who wish to improve their apps while maintaining the use of FSTPLs, there is no way to quantify or estimate the magnitude or impact of their FSTPL related design and implementation decisions. This motivates us to investigate techniques that can help developers more accurately evaluate FSTPL tradeoffs.  The results of this investigation will advance the state of the art in software engineering methodology and education, benefiting society by leading to the development of apps with higher reliability and usability.  For methodology, this will result in techniques to help developers improve the quality of their apps.  For education it will enhance training of future software developers in using analytical techniques to drive software design and implementation decisions.<br/><br/>In this proposal the PIs will investigate techniques to help app developers evaluate their usage of FSTPLs and their impact on end users? experience. The proposed work will include the design of techniques and methodologies for quantifying the way developers use FSTPLs in their apps and correlating their usage with user ratings and reviews that will be mined from the app stores. Within this project, the PIs will focus on two thrusts. The first will be to design program analysis based techniques that can measure and quantify the usage patterns of FSTPLs in mobile apps. The second will be to perform empirical investigations by applying the program analysis based techniques on apps from app stores and using statistical analysis to understand relationships between the gathered data and various user feedback based metrics, such as the ratings of apps, to learn best and worst practices for using FSTPLs. The techniques and methodologies produced by these two thrusts will allow developers to analyze their apps and determine if their usage of FSTPLs could negatively or positively impact the user experience. The approach will provide objective and quantifiable guidance to developers to make refactoring choices, redesign components, and other such decisions about their apps. Therefore developers will be able to understand how their design and implementation choices affect the users? perception of their apps. More broadly, the proposed work will define a methodology for guiding developers in making design decisions and a way to tie these decisions to ratings based outcomes."
"1320545","SHF: Small: Dynamic Power Management in the Dark Silicon Era","CCF","COMPUTER ARCHITECTURE","08/01/2013","05/22/2018","David Albonesi","NY","Cornell University","Standard Grant","Yuanyuan Yang","07/31/2019","$499,161.00","Christine Shoemaker","albonesi@csl.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7941","7923, 7941","$0.00","Future heterogeneous multicore microprocessors are expected to be so<br/>power constrained that not all transistors will be able to be powered<br/>on at once.  The general-purpose cores will be required to nimbly<br/>adapt to severely constrained power allocations when power is diverted<br/>to accelerators, and vice versa.  In addition, energy-aware scheduling<br/>of threads to cores is becoming imperative as multicore architectures<br/>become more heterogeneous.  This research develops power gated<br/>multicore architectures and integrated scheduling and power allocation<br/>algorithms for maximizing throughput given varying and potentially<br/>stringent limits on allocated power.<br/><br/>One facet of the research is the design, synthesis, and evaluation of<br/>power gated general-purpose and data-parallel accelerator<br/>microarchitectures comprised of deconfigurable lanes--horizontal<br/>slices through the pipeline--that permit dynamic tailoring of each<br/>core to the application.  The goal is to demonstrate tolerable<br/>performance and power-gating overheads yet flexibility in adapting to<br/>workload behaviors.  A second aspect of the work is a new optimization<br/>approach that efficiently finds a near-global-optimum configuration of<br/>lanes and thread-to-core assignment without requiring offline training<br/>or foreknowledge of the workload.  The approach combines reduced<br/>sampling techniques, adaptation of response surface models to online<br/>optimization, incorporation of limited online profiling information,<br/>and heuristic online search.  The research will improve the<br/>computational capability of future severely power-constrained devices;<br/>involve undergraduate, graduate, and/or postdoc women engineers; and<br/>be incorporated into computer architecture and heuristic optimization<br/>classes."
"1438286","XPS: Full: CCA: Enhancing Scalability and Energy Efficiency in Extreme-Scale Parallel Systems through Application-Aware Communication Reduction","CCF","Exploiting Parallel&Scalabilty","09/01/2014","08/12/2014","David Lilja","MN","University of Minnesota-Twin Cities","Standard Grant","Yuanyuan Yang","08/31/2019","$666,000.00","John Sartori, Ulya Karpuzcu","lilja@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8283","","$0.00","The high-performance computing systems available today are far from meeting the performance and energy efficiency targets necessary to satisfy the needs of future computing systems.  With processor clock frequencies leveling off over the past several years, the only way to achieve the expected levels of performance within the same physical size and the same or lower energy requirements is to have more processors computing in parallel.  However, increasing concurrency among the processors increases inter-processor communication, which then becomes the critical bottleneck.  This work proposes an application-centric approach to minimize communication-induced overheads in order to enable extreme scalability.  The implicit fault tolerance of future parallel applications can be exploited to mitigate the degraded locality and increased communication costs of highly concurrent systems while ensuring an acceptable level of application output quality.  <br/><br/>This work specifically focuses on (i) how data to be transferred can be compressed, and possibly discarded, and (ii) how communication and synchronization can be avoided or relaxed in certain application contexts, all while maintaining an acceptable level of output quality for the specific application.  The intellectual merit of this work stems from its exploration of cross-layer communication and synchronization techniques that exploit application characteristics to significantly reduce the cost of communication and synchronization.  This approach will enable a wider array of applications to exploit parallelism and thereby increase their scalability.  This work will enable extreme scalability of domain-specific applications executing on large-scale computing systems by quantifying the costs and benefits associated with application-aware communication and synchronization, designing application-aware scalable communication and synchronization primitives, developing software to support application-aware scalable communication, and designing new micro-architectural and system supports for application-aware scalable communication.<br/><br/>The broader impact of this research lies in significantly reducing the costs of communication and synchronization for a wide spectrum of applications and consequently enabling new levels of parallel scalability and energy efficiency across a range of application programs.  This project will produce open source tools for analyzing and enhancing the scalability of parallel applications, and will provide new opportunities for both graduate and undergraduate students to participate in cutting-edge computer systems research."
"1409872","SHF: Medium: Automated Graphical User Interface Testing  with Learning","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2014","05/15/2018","Koushik Sen","CA","University of California-Berkeley","Standard Grant","Sol J. Greenspan","07/31/2019","$850,000.00","George Necula","ksen@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7798","7924, 7944","$0.00","Smartphones and tablets with rich graphical user interfaces (GUIs) are<br/>becoming increasingly popular.  Hundreds of thousands of specialized<br/>applications, called apps, are already available for these mobile<br/>platforms, and the number of newly released apps continues to<br/>increase.  The complexity of these apps lies often in the user<br/>interface, with data processing either minor, or delegated to a<br/>backend component. A similar situation exists in applications using<br/>the software-as-a-service architecture, where the client-side<br/>component consists mostly of user interface code.  Testing such<br/>applications predominantly involves GUI testing. Existing automatic<br/>techniques for testing these interfaces either require a priori models<br/>of the interface and are thus hard to use, or operate blindly by<br/>sending random user events to the application and are typically unable<br/>to test the application in satisfactory depth.<br/><br/>This project investigates automatic GUI testing techniques that<br/>systematically explore the state space of an application without<br/>requiring an a priori defined model.  One insight behind this project<br/>is that the automatic construction of a model of the user interface<br/>and the testing of the interface are tasks that can cooperate in a<br/>mutually beneficial way. Furthermore, a guiding principle throughout<br/>this research is to design algorithms that operate with abstractions<br/>and heuristics that are simple enough to be understood by humans who<br/>do not necessarily understand the internals of the tested app. Such<br/>algorithms are easier to comprehend and to incorporate into a<br/>wholistic test process that includes automated techniques, such as the<br/>ones developed in this project, and manual testing and guidance.  The<br/>techniques developed in this project benefit directly programmers for<br/>these apps, and indirectly the numerous users of mobile and web<br/>applications."
"1817154","CIF: Small: Fundamental Limits of Caching Networks with General Topologies","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","07/02/2018","Mingyue Ji","UT","University of Utah","Standard Grant","Phillip Regalia","09/30/2020","$300,000.00","Rong-Rong Chen","mingyue.ji@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7797","7923, 7935, 9102","$0.00","Today, Internet data traffic is experiencing a tremendous growth and the majority will be content-oriented application such as video-on-demand services. Conventional technologies, however, are severely limited towards the goal of achieving such a dramatic throughput gain. Coded caching is an effective way to smooth out network traffic during peak traffic hours. If one jointly designs cache placement and coded delivery schemes, coded caching has the potential to turn (relatively) cheap memory into expensive bandwidth, i.e., the total traffic load in the network becomes inversely proportional to the per user memory. Despite a significant amount of work in the past few years, most studies on coded caching focus on relatively simple symmetric network topologies such as shared link, device-to-device, and hierarchical networks, and mainly exploit the global multiplicative caching gain due to the aggregate memory over the network. In practice, users and servers may communicate under general network topologies, e.g., via intermediate switches/routers. It is critical to study general topology caching networks because network topologies add a valuable new dimension to caching designs. This enables new global caching gains from network topologies beyond those from the aggregate memory. Due to the technical challenges associated with general topologies, the study of fundamental limits of coded caching for general networks has very few attempts in the literature. The information theoretic converse for these works are either missed or are based on highly restrictive assumptions. Hence, the fundamental limits of general topology caching networks are largely unknown. This project takes the first major step to tackle the challenges of general caching networks by studying their fundamental limits. The main goal is to develop topology-based coded caching schemes and the information theoretic converse to exploit the additional global multiplicative gain as a function of network topology.<br/><br/>This project has two main research thrusts: 1) Topology based design of achievable schemes for general wireline caching networks; 2) Characterization of information theoretic converse for general wireline caching networks. For Thrust 1, our methodology is a joint design of cache placement, coded multicast message generation, and delivery based on specific network topology via novel combinatorial design methods. The proposed approaches will lead to unique achievable coding schemes that can exploit the coded caching gain from both caches and network topologies. These will significantly improve existing designs that separate caching, message generation, and delivery, and can achieve the information theoretic outer bounds under certain parameter regimes. For Thrust 2, our methodology is establishing information theoretic outer bounds (impossibilities) based on novel information theoretic inequalities derived from network topologies and allowing joint generation and delivery of coded multicast messages depending on network topologies. While this project focuses on developing new methodologies towards a deep understanding of the fundamental limits for general topology wireline caching networks, it also lays a strong technical foundation for further studies of general topology wireless caching networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1453806","CAREER: Exo-Core: An Architecture to Detect Malware as Computational Anomalies","CCF","Secure &Trustworthy Cyberspace","03/01/2015","06/13/2018","Mohit Tiwari","TX","University of Texas at Austin","Continuing grant","Yuanyuan Yang","02/29/2020","$420,489.00","","tiwari@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8060","1045, 7434, 7941","$0.00","Applications that run on billions of mobile devices backed by enormous<br/>datacenters hold the promise of personal, always-on healthcare; of intelligent<br/>vehicles and homes; and thus of a healthier, more efficient society.  It is<br/>imperative to make such applications secure by protecting their integrity and<br/>keeping their data confidential. However, malicious programs (``malware'')<br/>today can subvert the best software-level defenses by impersonating benign<br/>processes on mobile devices or by attacking victim processes through the<br/>hardware on shared datacenter servers.  Grappling with such intelligent malware<br/>requires fundamental advances in the hardware-software organization of computer<br/>systems.<br/><br/>The key observation behind the research project here is that, while seemingly<br/>disparate, intelligent malware relies on hiding its hardware-level behaviors<br/>from operating system-level monitors. By exposing instruction-level and<br/>micro-architectural behaviors to software analysis, the proposed Exo-core<br/>architecture enables a new class of malware detectors.  In addition, Exo-core<br/>introduces programmable hardware accelerators to synthesize run-time program<br/>traces into robust models of benign programs. The project will also<br/>integrate research in hardware-software foundations of computer security<br/>into a new two-semester research course for undergraduate students."
"1563920","SHF: Medium: Contract-Based Black-Box Assurance","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","07/25/2017","Mats Per Erik Heimdahl","MN","University of Minnesota-Twin Cities","Continuing grant","Sol J. Greenspan","05/31/2019","$1,004,019.00","Stephen McCamant, Sanjai Rayadurgam","heimdahl@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","7924, 7944","$0.00","Systems in diverse domains such as transportation, medicine, aviation and space exploration, increasingly rely on third-party components with embedded software. Due to the lack of visibility into the components' implementation and design, traditional techniques for verifying that the components perform as advertised are not feasible; novel alternatives are needed. This research addresses the assurance of such ""black-box"" components by developing and evaluating (1) techniques to discover and capture contracts - the do's and the don'ts - that the component must satisfy to operate safely in its intended environment, and (2) rigorous metric-driven criteria that can be used to guide as well as to assess the thoroughness of the component testing.<br/><br/>The central hypotheses are that (1) novel test coverage adequacy criteria can be defined over the component's contracts as well as its object code; criteria that when satisfied will ensure the level of confidence needed for a critical system, and (2) object-code symbolic execution techniques can be developed for both automatic discovery of component behaviors as well as for verifying conformance to contracts. The expected advances from this research are: (a) new contract definition and discovery techniques suitable for contract-based testing, (b) coverage criteria for contracts and object-code of software, (c) enhanced object-code symbolic execution techniques, and (d) empirical evaluation of the approach on realistic systems. The anticipated broader impacts of this work are that: (a) observations from the empirical studies conducted provide guidance for regulatory agencies and standards organizations in their efforts; (b) commercial tool vendors adopt these techniques in their software development tools and thus help reduce the very high costs of testing critical systems; (c) extensive automation of the costly manual testing processes enabled by these new techniques provide a powerful incentive to formally specify and model requirements, thereby greatly improving quality in all aspects of software development projects; and (d) open-access to tools, case examples, experimental infrastructure, data, and curricular materials developed, advances the state of software engineering research, education and industrial practice."
"1409284","SHF: Medium: Collaborative Research: Micro Virtual Machines for Managed Languages: Abstraction, defined and contained","CCF","Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","06/15/2014","03/28/2017","J. Eliot Moss","MA","University of Massachusetts Amherst","Standard Grant","Anindya Banerjee","05/31/2019","$683,681.00","Timothy Richards","moss@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7354, 7798","7354, 7924, 7943, 9251","$0.00","Title: Micro Virtual Machines for Managed Languages: Abstraction, contained<br/><br/>A large fraction of today's software is written in managed languages. These languages increase software productivity by supporting programming at a very high level, hiding the complexity of modern computer hardware and operating systems. Examples include JavaScript, PHP, Objective-C, Java, C#, Python, and Ruby. These languages have great economic importance. Unfortunately, most of these languages are inefficient, imposing overheads as large as a factor of fifty compared to orthodox language choices such as C. The broader impacts of this research are two-fold: (1) Since managed languages are increasingly becoming economic linchpins, deployed in the engine rooms of global e-commerce platforms and Internet applications (including those of key software companies such as Amazon, Google, Facebook, Apple, Oracle, and Microsoft), improving the reliability and performance of these managed language platforms will lead to economic efficiencies as well as resource efficiencies in the form of reduced power costs to data centers and improved battery lifetime on mobile devices;(2) Concerning human diversity in the computing field, the research will engage undergraduates from under-represented groups, and encourage them to pursue graduate school and careers in computing and research.<br/><br/>This research project is defining, developing, evaluating, and refining the essential components of a new foundation layer for managed language implementation. In doing so, it addresses a key source of systemic inefficiency, by pioneering the ""micro virtual machine"" (microVM) as an efficient high-performance substrate for managed language implementation. The relationship between a microVM and existing managed language implementations is analogous to that between an operating system micro kernel and monolithic operating systems such as Linux. The microVM captures the insight that there exists a well-defined foundation common to most modern languages that can take responsibility for fundamental abstractions over hardware, concurrency, and memory. By isolating and exposing this substrate, the microVM embodies state-of-the-art base technology available to language implementers while isolating them from the pernicious complexities of these abstractions, freeing them to focus on all-important language-specific optimizations. The research is enabling more efficient software and a distinctly sharper focus for language implementation research and development. The intellectual merits of the research are: (1) Researching and identifying, via prototyping and evaluation, the right microVM abstractions, allowing clean separation of low-level language-neutral and high-level language-specific concerns, capturing the core behaviors, while also admitting high-performance language implementations; (2) Generating secondary research challenges, especially with regard to the effective and efficient combination of abstractions for hardware, concurrency, and memory; and (3) Creating a viable target for formal verification because the microVM has a relatively small code base, thus taking one step on the way towards trustworthy subsystems for assurance of managed software."
"1764104","CIF: Medium: Collaborative Research: New Frontiers in Polar Coding: 5G and Beyond","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/08/2018","Alexander Vardy","CA","University of California-San Diego","Continuing grant","Phillip Regalia","09/30/2022","$141,873.00","Warren Gross","vardy@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7924, 7935","$0.00","Since the first mobile phones became available in the 1980s, four distinct ""generations"" of wireless networks have been deployed to support reliable transmission of ever-increasing volumes of data to a growing number of users. What makes such reliable transmission of information possible are error-correcting codes, first conceived by Claude Shannon over 60 years ago. Polar coding is a key new error-correction technology introduced in the fifth wireless generation, known as 5G, that is currently being developed and standardized. Thus, soon enough, consumers the world over will all be using polar codes whenever making a phone call or accessing the Internet on a mobile device. Polar codes provably achieve the fundamental limits of communication established by Shannon in 1948, with low encoding and decoding complexity. Nevertheless, numerous challenges must be overcome in order to realize the full potential of polar coding in wireless communications. This project addresses these challenges to facilitate successful deployment of polar codes in 5G systems, while investigating fundamental problems in polar coding that lie beyond the 5G time horizon. These problems include polarization for time-varying channels and polar coding for channels with deletions. The results from this part of the investigation will contribute to the foundations of error-correction coding theory, and will also have an impact on adjacent scientific disciplines that are influenced by the polar-coding paradigm.<br/><br/>The discovery of channel polarization and polar codes is universally recognized as an historic breakthrough in coding theory. For short block lengths, polar codes under cyclic-redundancy-check-aided successive-cancellation list decoding are currently the best known coding scheme for binary-input Gaussian channels. Due to this and other considerations, 3GPP has decided to incorporate polar codes in the 5G wireless communications standard. The overarching goal in this project is to explore new frontiers in polar coding, thereby fundamentally advancing the current state-of-the-art in the field. Part of the research aims for immediately relevance to successful deployment of polar codes in 5G, whereas other parts focus on key theoretical problems in polar coding that lie beyond the 5G time-horizon. The specific objectives in this project are as follows: (1) Attain the gains of cyclic-redundancy-check-aided polar list-decoding with significantly lower complexity; (2) Construct practical universal polar codes that are not channel-dependent and provide near-optimal finite-length performance; (3) Develop code-domain multiple access techniques based on polar codes to enable massive connectivity; (4) Design and evaluate polar coding schemes for extended classes of channels, going well beyond the original memoryless and stationary set-up; (5) Extend the polarization paradigm and design polar codes for channels with deletions; (6) Develop a full-scale efficient, low-latency, and low-power system-on-chip implementation of polar list decoders. This project is a natural outgrowth of extensive and transformative prior work carried out by the investigators in polar coding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1702496","SHF: Medium: Collaborative Research: Machine Learning Enabled Network-on-Chip Architectures for Optimized Energy, Performance and Reliability","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2017","07/25/2017","D Brian Ma","TX","University of Texas at Dallas","Continuing grant","Yuanyuan Yang","05/31/2020","$250,000.00","","d.ma@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7798","7924, 7941","$0.00","Network-on-Chip (NoC) architectures have emerged as the prevailing on-chip communication fabric for multicores and Chip Multiprocessors (CMPs). However, as NoC architectures are scaled, they face serious challenges. A key challenge in addressing optimized NoC architecture design today is the plethora of performance enhancing, energy efficient and fault tolerant techniques available to NoC designers and the large design space that must be navigated to simultaneously reduce power, improve reliability, increase performance and maintain QoS. <br/><br/>This research proposes a new cross-layer, cross-cutting methodology spanning circuits, architectures, machine learning algorithms, and applications, aimed at designing energy-efficient, reliable and scalable NoCs. This research will result in (1) novel cross-layer design techniques that take a holistic approach of simultaneously reducing power consumption, while still achieving reliability and performance goals for NoCs, (2) a fundamental understanding of the use of hardware-amenable ML for NoC design optimization, (3) software and hardware techniques for monitoring and collecting critical data and key design parameters during network execution to optimize NoC design, and (4) modeling and simulation tools that will improve the architecture community's design methodologies for evaluating scalable NoCs. The proposed research bridges a very important gap between hardware architects who design power management and fault tolerant techniques at the circuit and architecture level and machine learning scientists who develop predictive and optimization techniques. Due to its cross-cutting nature, the proposed research has the potential to significantly transform the design of next-generation CMPs and System-on-Chips (SoCs) where complex decisions have to be made that affect the power, performance and reliability. The research will also play a major role in education by integrating discovery with teaching and training. The PIs are committed and will continue to expand on outreach activities as part of the proposed project by making the necessary efforts to attract and train minority students in this field."
"1816314","AF:Small: Algorithms for Fast Simulation of Macromolecular Interaction Systems","CCF","COMPUTATIONAL BIOLOGY","06/15/2018","06/07/2018","Dmytro Kozakov","NY","SUNY at Stony Brook","Standard Grant","Mitra Basu","05/31/2021","$460,000.00","","dmytro.kozakov@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7931","7923","$0.00","Macro-molecules, such as proteins and nucleic acids, are the major building blocks of the cell. Many macro-molecules perform their function by interacting with each other. Characterizing these interactions helps elucidate how living organisms function at the molecular level, contributes towards the development of treatments against diseases such as cancer and facilitates the design of novel bio-inspired materials. Detailed understanding of macro-molecular interaction mechanisms requires determining the three-dimensional structures of their complexes. These structures are very difficult to obtain using experimental techniques, thus, computational approaches, called macro-molecular docking, can be very useful. The investigator has developed fast and effective algorithms and software that, according to the worldwide evaluation experiment CAPRI (Critical Assessment of Predicted Interactions), are among the best for predicting the structures of protein-protein complexes. These methods have been implemented in the fully automated docking server ClusPro, which is free for academic use, and has over 18,000 regular users. However, the current macro-molecular docking tools are effective for mostly rigid macro-molecules that do not significantly change conformation upon binding. This severely limits applicability of the approach. The goal of this project is to develop new algorithms for docking flexible molecules. Expanding the scope of the docking approaches will lead to better understanding of fundamental biological questions and will facilitate biochemical, biomedical, and biotechnology research. In addition, the methods will be used in training graduate students and teaching undergraduate and high school students.<br/><br/>The docking problem is to computationally determine the 3-dimensional (3D) structure of the complex formed by two unbound macro-molecules, given their individual structures. Solving this problem requires detailed sampling of an energy-based scoring function over a complex search space. Due to the high cost of energy function evaluation and the extremely rugged energy landscape the sampling is computationally challenging. The goal of this proposal is to develop algorithms for fast energy evaluation and corresponding sampling methods for the modeling of interactions among flexible macro-molecules with many degrees of freedom. The basic idea is to represent the molecular system as a forest (i.e., a set of disjoint trees) of rigid clusters connected by hinges, calculating the interaction energies for all relative orientations of all rigid clusters on grids, and storing the calculated energy grids on the manifold of all rotational and translational states. The energy of the system can be then obtained by summing up the interaction energies extracted from the pre-calculated lookup tables of cluster interaction energies. The key advantage of proposed approach is a significant reduction in the cost of energy evaluation. In addition, the search is performed on the manifold of appropriate dimension. The major challenge for making the above approach applicable in practice is storing the large interaction energy tables of rigid clusters in memory. To solve this problem, the investigators will develop approaches and algorithms to compress interaction energy data using wavelets, resulting in good accuracy, high level of compression, and fast lookup speed. In addition, specialized sampling algorithms using wavelet compressed energy grids on the search manifold will be developed, and their application to flexible macro-molecular docking problems will be studied. The developed algorithms will be evaluated for performance and behavior. The approaches will be released as an open source software library, as well as made available to end users of docking by means of the ClusPro server.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832624","SHF: Medium: Collaborative Research: Micro-Virtual Machines for Managed Languages: Abstraction, contained","CCF","Computer Systems Research (CSR, SOFTWARE & HARDWARE FOUNDATION","08/12/2017","03/30/2018","Antony Hosking","MA","University of Massachusetts Amherst","Standard Grant","Anindya Banerjee","05/31/2019","$261,554.00","Antony Hosking","hosking@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7354, 7798","7354, 7924, 7943","$0.00","Title: Micro Virtual Machines for Managed Languages: Abstraction, contained<br/><br/>A large fraction of today's software is written in managed languages. These languages increase software productivity by supporting programming at a very high level, hiding the complexity of modern computer hardware and operating systems. Examples include JavaScript, PHP, Objective-C, Java, C#, Python, and Ruby. These languages have great economic importance. Unfortunately, most of these languages are inefficient, imposing overheads as large as a factor of fifty compared to orthodox language choices such as C. The broader impacts of this research are two-fold: (1) Since managed languages are increasingly becoming economic linchpins, deployed in the engine rooms of global e-commerce platforms and Internet applications (including those of key software companies such as Amazon, Google, Facebook, Apple, Oracle, and Microsoft), improving the reliability and performance of these managed language platforms will lead to economic efficiencies as well as resource efficiencies in the form of reduced power costs to data centers and improved battery lifetime on mobile devices;(2) Concerning human diversity in the computing field, the research will engage undergraduates from under-represented groups, and encourage them to pursue graduate school and careers in computing and research.<br/><br/>This research project is defining, developing, evaluating, and refining the essential components of a new foundation layer for managed language implementation. In doing so, it addresses a key source of systemic inefficiency, by pioneering the ""micro virtual machine"" (microVM) as an efficient high-performance substrate for managed language implementation. The relationship between a microVM and existing managed language implementations is analogous to that between an operating system micro kernel and monolithic operating systems such as Linux. The microVM captures the insight that there exists a well-defined foundation common to most modern languages that can take responsibility for fundamental abstractions over hardware, concurrency, and memory. By isolating and exposing this substrate, the microVM embodies state-of-the-art base technology available to language implementers while isolating them from the pernicious complexities of these abstractions, freeing them to focus on all-important language-specific optimizations. The research is enabling more efficient software and a distinctly sharper focus for language implementation research and development. The intellectual merits of the research are: (1) Researching and identifying, via prototyping and evaluation, the right microVM abstractions, allowing clean separation of low-level language-neutral and high-level language-specific concerns, capturing the core behaviors, while also admitting high-performance language implementations; (2) Generating secondary research challenges, especially with regard to the effective and efficient combination of abstractions for hardware, concurrency, and memory; and (3) Creating a viable target for formal verification because the microVM has a relatively small code base, thus taking one step on the way towards trustworthy subsystems for assurance of managed software."
"1718235","SHF: Small: Stabilizing Numeric Programs Against Platform Uncertainties","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2017","07/14/2017","Thomas Wahl","MA","Northeastern University","Standard Grant","Nina Amla","06/30/2020","$498,021.00","","wahl@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7798","7923, 8206","$0.00","Most computer programs process vast amounts of numerical data. Unfortunately, due to space and performance demands, computer arithmetic comes with its own rules. Making matters worse, different computers have different rules: while there are standardization efforts, efficiency considerations give hardware and compiler designers much freedom to bend the rules to their taste. As a result, the outcome of a computer calculation depends not only on the input, but also on the particular machine and environment in which the calculation takes place. This makes programs brittle and un-portable, and causes them to produce untrusted results. This project addresses these problems, by designing methods to detect inputs to computer programs that exhibit too much platform dependence, and to repair such programs, by making their behavior more robust.<br/><br/>Technical goals of this project include: (i) automatically warning users of disproportionately platform-dependent results of their numeric algorithms; (ii) repairing programs with platform instabilities; and (iii) proving programs stable against platform variations. Platform-independence of numeric computations is a form of robustness whose lack undermines the portability of program semantics. This project is one of the few to tackle the question of non-determinism in the specification (IEEE 754) of the theory (floating-point arithmetic) that machines are using today. This work requires new abstractions that soundly approximate the set of values of a program variable against a variety of compiler and hardware behaviors and features that may not even be known at analysis time. The project involves graduate and undergraduate students."
"1618039","SHF: Small: Latency Tolerance Aware Runtime Optimization for General-Purpose GPU Architectures","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","05/15/2017","Carole-Jean Wu","AZ","Arizona State University","Standard Grant","Yuanyuan Yang","07/31/2019","$346,000.00","","carole-jean.wu@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7798","7923, 7941, 9251","$0.00","Computing touches almost all aspects of our modern day lives -- from gene sequencing to physics simulations to powering the Internet, from real-time image and voice recognition to predicting stock market trends. The programmability advancement of high-performance accelerators, such as graphics processors, has enabled a large, diverse set of general-purpose algorithms to enjoy performance acceleration on GPUs. However, because the unique latency tolerance GPU design feature is often not taken into account, the state-of-the-art solutions lead to sub-optimal performance improvement. The proposed Latency Tolerance Aware runtime optimization framework (LATTE) can help the computing industry realize its high performance and high energy efficiency vision. Performance acceleration of important general-purpose algorithms with large and diverse input data sets has a profound impact on the advancement of all research domains and on society. The research agenda is complemented by an education agenda focusing on heterogeneous computing.<br/><br/>The LATTE framework proposed in this project aims to explore and propose architectural solutions and to create system supports to accelerate the execution of important general-purpose applications on GPUs. The proposed LATTE runtime optimization framework revolves around identifying and designing optimization techniques that are fully latency tolerance aware in order to maximize performance improvement for GPGPUs. The pitfalls for directly adapting CPU-centric optimization to GPU architectures are analyzed and used to motivate the need to proactively manage the highly time-multiplexed computation and memory resources considering the critical latency tolerance characteristic of GPUs. The findings can serve as foundations for future performance optimization research to avoid blind replication, leading to important scientific research contributions for GPGPU acceleration."
"1816397","CIF:Small:Deadline Scheduling and Sensor Fusion","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2018","05/22/2018","Lang Tong","NY","Cornell University","Standard Grant","Phillip Regalia","08/31/2021","$490,173.00","","ltong@ece.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","7923, 7935","$0.00","The deadline scheduling problem, in its most generic setting, is the scheduling of jobs with different workloads, costs/rewards, and deadlines for completion.  Today, computerized deadline scheduling permeates in all types of applications, from computer architecture to cyber-physical systems, from communication networks to the Internet of Things. It is not an overstatement that deadline scheduling is at the foundation of real time system theory and applications.  Yet, much is unknown, and the state of the art falls far short of solving an expanding range of real-time deadline scheduling problems.  Missing in particular are a fundamental understanding of the tradeoff between complexity and performance, algorithms that deliver near optimal solutions for very large problems, and techniques capable of dealing with model, data and operational uncertainties. This project seeks to advance the state of the art by addressing these missing elements.<br/><br/>This research advances the real-time system theory and practice in three directions. First, it pursues a novel ranking-based scheduling methodology that breaks the curse of dimension of scheduling complexity while guaranteeing asymptotically optimal performance for high dimensional deadline scheduling problems. Second, this research advances the frontier of sensor fusion for real-time monitoring and control by developing a decentralize scheduling solution for collecting information with varying degrees of priority and urgency. Third, this research develops online learning techniques to cope with unknown system dynamics and job arrival statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816406","SHF: Small: Collaborative Research: Energy Efficient Strain Assisted Spin Transfer Torque Memory","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","05/21/2018","Bin Ma","MN","University of Minnesota-Twin Cities","Standard Grant","Sankar Basu","09/30/2021","$140,000.00","","bmagn@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","7923, 7945","$0.00","Non-volatile random access memory (NV-RAM) is often built with a device called spin transfer torque random access memory (STT-RAM), the main constituent of which is a circular nano-magnet. A bit is ""written"" into the nano-magnet by passing a spin-polarized current whose polarity determines whether bit ""1"" or bit ""0"" is written. The energy barrier between these states prevents the magnetization from switching spontaneously due to thermal noise, making the device non-volatile. Unfortunately, the energy dissipated in the writing current is 100-1000 times more than the energy dissipated in today's CMOS devices, which is a large cost to pay for non-volatility. This project seeks to demonstrate that temporarily reducing the energy barrier between the ""up"" and ""down"" magnetization states with surface acoustic waves (SAW) can significantly lower the current needed to write a bit and reduce the energy dissipation by orders of magnitude. This would make the SAW-assisted STT-RAM ideal for embedded processors, internet of things, large data centers and cyber-physical systems requiring low energy memory. At least 3 PhD students would be trained on the techniques of complementary nano-fabrication, nano-characterization and computer modeling. The investigators will hold a nano-magnetism workshop for high school students and will host under-represented K-12 students in their labs for a summer month, as well as leverage the ""Nano-Days"" program to reach out to high school students. <br/><br/>The key technical approach in this research project will involve the following complementary experimental and modeling research directions: (i) Modeling the combined effect of strain and spin transfer torque (STT) on the magnetization switching in the presence of thermal noise (ii) Experimental demonstration of strain induced reduction in STT write current in optimized devices (guided by the modeling effort) (iii) Proof-of-concept demonstration of acoustic wave induced reduction in STT write current. The knowledge generated by this research would lead to better understanding of the combined effect of strain and spin torque on switching nano-magnetic memory elements and the switching probability in the presence of thermal noise. If successful, it would demonstrate a low energy paradigm for writing information in non-volatile nano-magnetic memory devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814928","SHF: Small: Collaborative Research: Harvesting Wasted Time and Existing Circuitry for Efficient Field Testing","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","05/18/2018","Jennifer Dworak","TX","Southern Methodist University","Standard Grant","Sankar Basu","09/30/2021","$391,769.00","Theodore Manikas","jdworak@lyle.smu.edu","6425 BOAZ","Dallas","TX","752750302","2147682030","CSE","7798","7923, 7945, 9102","$0.00","Digital integrated circuits (ICs) are the ""brains"" in the electronic devices used throughout modern society.  It is essential that these devices work correctly and reliably - especially in critical applications such as autonomous vehicles, infrastructure, the financial system, and health care.  In such applications, highly effective and efficient testing should be done in the field to identify problems that may arise after the device has been sent to the customer.  Thus, in this project, new ways of using circuitry already present on modern ICs while efficiently creating, applying, and transporting field-based tests to different parts of the chip will be investigated. At the same time, educational opportunities made possible through the project will be pursued, including the use of research findings in courses, the involvement of graduate and undergraduate students in the research, and outreach to high school students through summer camps and research opportunities.  The inclusion of members of underrepresented groups in these educational opportunities will be a focus of the investigators, who already have a proven record in mentoring such students in research.  <br/><br/>To address the field-testing issues, three major tasks will be performed in this research.  First, methods that use existing error-detection circuitry in an IC to make the field testing process more efficient will be explored.  In particular, times when the IC is operating in normal functional mode will be used to test for the presence of some faults.  Testing for those same faults in dedicated test sessions can then be avoided, reducing test time and the amount of time a circuit must be taken offline for test.  Next, methods for effectively using otherwise wasted time during dedicated test sessions to detect faults will be explored.  Finally, approaches that optimize the internal test network to efficiently send data used for test throughout a chip will be investigated.   Taken together, these approaches should enable field-based test to achieve high fault coverage while minimizing test time, thus enhancing the reliability of devices that so many people depend upon every day.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813160","CCF-BSF: AF: Small: Algorithms for Interactive Learning","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/18/2018","Sanjoy Dasgupta","CA","University of California-San Diego","Standard Grant","Rahul Shah","05/31/2021","$500,000.00","","dasgupta@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7796","7923, 7926","$0.00","Machine learning classifiers are core components of many of the technologies we use routinely: search engines, speech recognition engines, language translators, assisted driving systems, and so on. These classifiers are typically built by a process of 'supervised learning', in which a computer is given a collection of (input, output) pairs that illustrate a desired behavior (e.g. if the input is this English sentence, the output should be this Spanish sentence) and is told to produce a function that replicates such behavior. This is a rigid form of learning that is known to suffer from a variety of fundamental hurdles; for instance, there are classes of concepts that cannot efficiently be learned in this way. This project will study how such hurdles can be overcome by moving to a more natural learning setup, in which the learning machine is allowed to interact with a human while learning, and receives feedback that is richer than just output values. This research has the potential to influence the way in which machine learning is performed and to broaden its scope of applicability. It is inherently multidisciplinary, and thus part of the project includes community-building activities that will bring together different groups of relevant researchers. There is also an educational component to the project, centered on bringing knowledge of algorithms and machine learning to various student groups that have traditionally been under-represented in STEM disciplines. Interactive learning is a field with great promise in which most of the work to date has consisted of one-off systems geared towards specific applications. This project will aim to bring rigor, formalism, and algorithms with provable guarantees to parts of this field that are currently lacking them.<br/><br/>This project will aim to formalize forms of human feedback (to a learning machine) than are richer than those traditionally studied, such as: simple explanations (e.g. this bird is not a canary because it has the wrong type of beak); attention-focusing; and similarity judgments. The investigators will design algorithms that are able to use these kinds of feedback and have rigorous guarantees, both on correctness and on statistical rates of convergence. The project is particularly focused on overcoming fundamental hardness barriers in learning: learning concept classes that would be intractable to learn in the usual supervised framework; learning with dramatically fewer examples than would normally be needed; adapting to situations in which the distribution of the data is constantly shifting; and improving the results of unsupervised learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835090","NSF Student Travel Grant for 2018 Computational Modeling in Biology Network (COMBINE) Forum","CCF","COMPUTATIONAL BIOLOGY","08/01/2018","05/17/2018","Chris Myers","UT","University of Utah","Standard Grant","Mitra Basu","07/31/2019","$10,000.00","","myers@ece.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7931","7556","$0.00","Standards for data exchange are critical to the development of any field.  They enable researchers and practitioners to exchange information reliably, apply a variety of tools to their problems, and reproduce scientific results.  The COmputational MOdeling in Biology NEtwork (COMBINE) was created in 2010 to organize standardization efforts for systems biology.  This effort includes, among others, the systems biology markup language (SBML) for mathematical models created by systems biologists; and the synthetic biology open language (SBOL) for describing genetic designs created by synthetic biologists. While systems biology and synthetic biology have many unique requirements, there are also a lot of overlapping features such as the need to construct mathematical models, describe pathways, and provide visual representations.  The COMBINE meetings bring together representatives from both fields to enable these communities to share existing standards, software, and other infrastructure. This project will help enable US-based students from these two communities, particularly those new to these efforts or from underrepresented groups, to meet together at the 2018 COMBINE Forum to be held at Boston University in Boston, Massachusetts on October 8th to 12th (http://co.mbine.org/events/COMBINE_2018). <br/><br/>Workshops like this one are instrumental in making progress in the development of these standards. Currently, most standardization efforts are either unfunded or only have very limited support.  Indeed, the success of these standards has been due to the donation of time from many individuals to participate in discussions, write specifications, serve as standard editors, and attend workshops.  Many of these individuals working on the development of these standards are students based in the US who would otherwise find it difficult to find the funds to attend this meeting.   Therefore, this project would help support travel expenses for these US-based students working on these standards to travel to Boston for COMBINE 2018.  Not only would this be a valuable experience for these students, but it would also greatly enhance the synthetic biology community's goal of bringing together researchers in synthetic biology with those working in systems biology leading to integrated standards and shared software and infrastructure.  The goals of this workshop will be to continue to (1) develop synergistic relationships between systems and synthetic biology researchers; (2) provide a tighter integration of standard development for systems and synthetic biology; (3) perform outreach to modelers and experimentalists; and (4) discuss a coordinated effort to advocate the use of standards in publications and archival databases to enhance reproducibility of scientific results.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617708","SHF: Small: Word-level Abstraction of Arithmetic Gate-level Circuits","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/10/2016","Maciej Ciesielski","MA","University of Massachusetts Amherst","Standard Grant","Sankar Basu","05/31/2019","$450,000.00","","ciesiel@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7798","7923, 7945","$0.00","With an ever-increasing complexity of integrated circuits, hardware verification has become the dominating factor of the overall electronic design flow. Particularly critical and challenging is the verification of complex arithmetic components present in almost every design, from microprocessors to medical devices to a communication equipment.  In contrast to logic circuits, for which effective Boolean methods have been developed, the difficulty of arithmetic hardware verification lies in the size and the amount of data that needs to be analyzed. Different mathematical models, based on higher abstraction level than logic bits need to be developed to deal with this complexity. This project addresses this problem by developing new techniques for abstracting arithmetic structures from physical circuit implementations. In addition to verification, abstracting higher-level information from a design is important in hardware trust and security applications, where it can be used to analyze the design to isolate malicious hardware. Successful implementation of this work will contribute to the development of the state-of-the-art tools for electronic design automation and will increase design productivity. The project will also train undergraduate and graduate students, postdocs for future workforce in this technical area.<br/><br/>The project will develop a new method to abstract high-level information from arithmetic circuits using computer algebra approach. In this approach, circuit components, such as logic gates, are modeled in algebraic domain as pseudo-Boolean polynomials. Rewriting polynomials from circuit outputs to inputs makes it possible to extract arithmetic function embedded in the circuit. During the rewriting, the intermediate pseudo-Boolean expressions are examined in order to identify possible arithmetic structures. The identification is done using a novel ""spectral analysis"" technique, which matches the polynomial expressions against the reference ""spectra"" of basic arithmetic blocks, such as multipliers, adders, and multiply-and-accumulate operators. This approach will abstract word components from polynomial expressions to reason about the word-level structure from the internal expressions. By representing logic and arithmetic functions as pseudo-Boolean polynomials, it is possible to mitigate the size explosion typically encountered in Boolean based methods."
"1821525","EAGER: Statistical Modeling of Linguistic Change in Open Source Software","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2018","04/25/2018","Anas Mahmoud","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Sol J. Greenspan","04/30/2019","$63,067.00","","mahmoud@csc.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7798","7916, 7944, 9150","$0.00","The project explores a theory of open source software (OSS) evolution based on statistical natural language processing techniques. Based on the emerging recognition that software code is, in many ways, as ""natural"" as natural language (e.g., English), there is a trend to apply statistical models for software development tasks such as code analysis, comprehension, and programmer support. This grant extends the ""naturalness of code"" theory by studying how the code lexicon evolves in open source software as different developers work on a software project and features are added, modified, deleted.  The goal is to learn the extent to which the evolution of a developer's lexicon follows the laws of natural language evolution.<br/><br/>To create the needed demonstration, large datasets of code lexicons are being collected from a large number of OSS projects and their revisions (on GitHub and SourceForge). The main constructs of the frequency model of natural language evolution will be applied to track and identify the main patterns of language changes (e.g., birth, propagation, death of terms in the lexicon) throughout OSS projects life cycle. Part of the challenge is to better understand how events that instigate code evolution, such as maintenance activities and team formation, are fundamentally different from the events that instigate change in natural language, such as war and migration. The research should lead to new ways to predict software project outcomes and to improve software productivity and quality. The project will make available the data, tools, and algorithms that will be produced by the project, which will support future work to understand the dynamics of code evolution in open source software ecosystems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1629218","XPS: FULL: Collaborative Research: Maximizing the Performance Potential and Reliability of Flash-based Solid State Devices for Future Storage Systems","CCF","Exploiting Parallel&Scalabilty","07/01/2016","06/28/2016","Tong Zhang","NY","Rensselaer Polytechnic Institute","Standard Grant","Yuanyuan Yang","06/30/2019","$300,000.00","","tzhang@ecse.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8283","","$0.00","Solid-state data storage built upon NAND flash memory is fundamentally changing the memory and storage hierarchy for virtually the entire information technology infrastructure. Nevertheless, there have been several fundamental and challenging issues to be addressed before the industry can explore the flash memory to its full potential. First, as flash memory technology scales down, its reliability degradation approaches to an alarming level, leading to serious concerns and skepticism of storage system architects and users in many applications. Second, system and application development of solid-state storage has been independently conducted, resulting in isolation, duplicated operations, and an inefficient management among these layers. Due to the technology scaling and information loss in existing simple interface with storage devices, flash memory has not been efficiently and reliably utilized in practice, and the situation will become worse with the technology scaling. The PIs of this project will apply a holistic system design methodology to cohesively address the challenges preventing wider adoption of flash memory. By innovating well-orchestrated cross-layer information sharing and utilization, this design methodology enables seamless utilization of system-level workload and physical-level device characteristics across the entire software/hardware stack without complicating overall system design. An integrated software and hardware prototyping infrastructure will be developed to demonstrate the potential using major and widely used software systems, such as Hadoop, virtual machines, and database. This project will achieve a high broader impact by transforming basic research results into storage systems, and by training both undergraduate and graduate students with research activities, and by timely integrating new research results to classrooms.<br/><br/>Specifically, this project will carry out several closely related tasks: (1) It will develop techniques that can learn and predict the varying characteristics and their correlations of individual flash memory devices. This will provide run-time information that makes it possible to optimize the use of flash memory for alleviating the reliability crisis and adapting to varying system-level workload characteristics. (2) It will develop techniques that enable critical information exchange across the storage hierarchy in order to facilitate cross-layer information sharing. (3) It will further develop a set of techniques across the design hierarchy that can effectively utilize these runtime collections and predictions to improve the overall system reliability and performance."
"1652132","CAREER: Maximizing Energy Efficiency with Statistical Performance and Skin Temperature Quality of Service Guarantee for Handheld Platforms","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2017","03/02/2018","Carole-Jean Wu","AZ","Arizona State University","Continuing grant","Yuanyuan Yang","01/31/2022","$162,326.00","","carole-jean.wu@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7798","1045, 7941","$0.00","Smartphones reached almost 4 billion world-wide subscriptions in 2015 and have become our daily companion providing both high capacity computing, as well as personalized computing. For smartphones, user satisfaction determines the success or failure for a particular platform. The top ranked factor of user satisfaction is performance, which can be experienced by users through computation performance and battery performance. Therefore, smartphone designs must achieve balance between performance, temperature and energy management in a coordinated manner to maximize user satisfaction. Performance as measured by mobile application execution time has long been assumed to be a deterministic quantity. In reality, execution times vary substantially, depending on the characteristics of data inputs and the varying states of the system. Furthermore, the device surface temperature is a unique constraint for handheld devices. It is tightly coupled with the location of the major heat-generating source within a smartphone. Thus, a smartphone's energy efficiency is intricately related to both performance quality and skin temperature management, making optimization for system energy efficiency a complex task for battery-powered platforms. With the prevalence of portable electronics, the research outcome has a profound impact on the advancement of the relevant research domains and on society.The research agenda is complemented by an education agenda focusing on the design of handheld platforms, such as smartphones and other high-performance wearable electronics. The educational agenda includes (1) new graduate and undergraduate curricula that incorporates processor and handheld temperature and energy management techniques, (2) enhancing computer architecture and mobile computing courses through lab activities on the proposed skin temperature management for mobile devices, (3) mentoring undergraduate and graduate students in research, and (4) attracting and retaining underrepresented groups of students in STEM fields.<br/><br/>This research tackles the problem of performance, temperature and energy efficiency co-optimization from the vantage point of managing the hardware resources. It builds on the PI?s prior work in system and hardware architecture by proposing an optimizing user satisfaction (OPUS) framework for holistic management and coordination of performance quality, skin temperature, and energy efficiency for handhelds. Through accurate execution time models, OPUS dynamically adjusts the mobile platforms to meet the different quality of service goals. The research investigates the dynamic voltage-frequency scaling and temperature-aware computation acceleration algorithms, as well as emerging dynamic cooling mechanisms, suitable for handheld platforms. Optimizing for performance, energy efficiency, or temperature is not new, but optimizing devices in the context of smartphone user satisfaction gives rise to a set of opportunities that are non-existent in conventional computing platforms. The findings can serve as foundations for future user satisfaction optimization research for handheld platforms."
"1834480","Midwest Programming Languages Summit 2018","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2018","08/17/2018","Loris D'Antoni","WI","University of Wisconsin-Madison","Standard Grant","Anindya Banerjee","02/29/2020","$5,000.00","","loris@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7798","7556, 7793, 7943","$0.00","This award will support student travel to the seventh Midwest Programming Languages Summit, to be held at University of Wisconsin-Madison in September 2018.  This research workshop will serve as a valuable forum for researchers and students to share in-progress research ideas and receive timely feedback to influence subsequent work. This will be particularly valuable because of the high concentration of programming languages researchers in the Midwest, and because it will provide graduate student researchers opportunities to network and further develop their research and presentation skills. <br/><br/>The funds from this award will help support U.S. based students, focusing on graduate students at an advanced stage in their program and students who would otherwise not be able to attend this workshop. The impacts include training the next generation of researchers in this important research area. Higher priority will be assigned to students giving presentations and a strong emphasis will be put on supporting students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1656877","CRII: SHF: Toward Sustainable Software for Science - Implementing and Assessing Systematic Testing Approaches for Scientific Software","CCF","CRII CISE Research Initiation, CI REUSE, EPSCoR Co-Funding","03/01/2017","03/20/2017","Upulee Kanewala","MT","Montana State University","Standard Grant","Sol Greenspan","02/29/2020","$157,006.00","","upulee.kanewala@cs.montana.edu","309 MONTANA HALL","BOZEMAN","MT","597172470","4069942381","CSE","026Y, 6892, 9150","7798, 7944, 8228, 9150","$0.00","Scientific software is widely used in science and engineering. In<br/>addition, results obtained from scientific software are used as evidence in<br/>research publications. Despite the critical usage of such software, many<br/>studies have pointed out a lack of systematic testing of scientific software.<br/>As a result, subtle program errors can remain undetected. There are numerous<br/>reports of subtle faults in scientific software causing losses of billions of<br/>dollars and the withdrawal of scientific publications. This research aims to develop<br/>automated techniques for test oracle creation, test case selection, and develop<br/>methods for test oracle prioritization targeting scientific software. The intellectual<br/>merits of this research are the following: (1) It advances the understanding of<br/>the scientific software development process and investigates methods to<br/>incorporate systematic software testing activities into scientific software<br/>development without interfering with the scientific inquiry, (2) It forges new<br/>approaches to develop automated test oracles for programs that produce complex<br/>outputs and for programs that produce outputs that are previously unknown, (3)<br/>It develops new metrics to measure the effectiveness of partial test oracles<br/>and uses them for test oracle prioritization, (4) It extends the boundaries of<br/>current test case selection to effectively work with partial or approximate<br/>test oracles. The project's broader significance and importance are (1)<br/>produces a publicly available, easy to use testing tool that can be incorporated<br/>into the scientific software development culture such that the testing<br/>activities will not interfere with ?doing science,? (2) recruits Native Americans<br/>and women into computer science research, (3) develops a new higher level<br/>undergraduate course titled ?Software development methods for Scientists?<br/>targeting senior level undergraduate students in non-CS disciplines.<br/><br/>This project develops METtester: an automated testing framework that can effectively<br/>test scientific software. This testing framework analyzes the source code of<br/>the program under test and utilizes machine learning techniques in order to<br/>identify suitable test oracles called metamorphic relations (MRs). Then, it automatically<br/>generates effective test cases to conduct automated testing based on the<br/>identified MRs using a mutation based approach. After that, it creates a<br/>prioritized order of MRs to be used with testing in order to identify faults as<br/>early as possible during the testing process. Finally, METtester conducts<br/>testing on the program under test using the prioritized order of MRs with the generated<br/>test cases."
"1617730","AF: SMALL: Frontiers in Algorithmic Game Theory","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","05/31/2016","Constantinos Daskalakis","MA","Massachusetts Institute of Technology","Standard Grant","Tracy Kimbrel","06/30/2019","$500,000.00","","costis@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7923, 7927, 7932","$0.00","Recent years have seen much of the world's economic activity transferred to the Internet through old markets that obtained online presence and new markets that are directly inspired and enabled by the Internet, such as sponsored search and ad auctions. Driven by the increasing importance of online economic activity, there has been much interest in investigating its joint computational and economic characteristics through research at the interface of Computer Science and Economics. Economics brought to the table quantitative models and methodology for studying economic systems.  But it soon became clear that these models and methodology required adaptation, since the intention was to study systems that are computational while involving thousands or millions of participants. This thinking gave rise to the field of Algorithmic Game Theory (AGT), which has in the past fifteen years grown into a mature research discipline.<br/><br/>This project will identify and investigate new frontiers of research in this field, focusing on the following:<br/><br/>- Furthering our understanding of multi-item mechanisms by (i) weakening the distributional assumptions required for optimizing important objectives such as revenue; (ii) overcoming well-established computational intractability barriers for welfare maximization by moving away from truthful mechanisms to mechanisms solvable via online learning; and (iii) using optimal transport theory to characterize the structure of optimal  mechanisms.<br/><br/>- Discerning the role of learning and signaling in auction settings, by understanding how to (i) learn a revenue-optimal auction from weak forms of sample access to the bidders' distributions, (ii) test the near-optimality of a given auction from samples, and (iii) further a line of research going back to classical works of Akerlof and Milgrom-Weber by investigating how to design optimal signaling schemes for the auctioneer to use in common settings, such as sponsored search, where the auctioneer owns information about the properties of the items that the bidders do not have.<br/><br/>- Finally, revisiting the computational foundations of the field, investigating the relation between Factoring and Nash equilibrium.<br/><br/>The work in Algorithmic Game Theory is inherently interdisciplinary. This project will further the interaction between Computer Science and Economics through the publication of the research outcomes in high-impact Economics journals, organization of interdisciplinary research meetings, interdisciplinary research collaborations, and research surveys. The educational component of the project will involve the mentoring and education of researchers who intend to pursue their own careers in research or industry, and the design of pioneering classes in the interface of Computer Science and Economics. Furthering the educational and outreach impact of the project, the PI will be mentoring high school students in Computer Science research through the MIT PRIMES program, will be engaged in  Computer Science Unplugged activities at local elementary schools, and will engage with the broader public through public lectures."
"1645824","CPS: Breakthrough: A science of CPS robustness","CCF","CYBER-PHYSICAL SYSTEMS (CPS)","09/01/2016","08/18/2016","Paulo Tabuada","CA","University of California-Los Angeles","Standard Grant","Anindya Banerjee","08/31/2019","$425,000.00","","tabuada@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7918","7918","$0.00","Cyber-Physical Systems (CPS) offer the promise for radical changes to our everyday life by enabling the physical world to be programmed in the same way that a computer is programmed. The physical world, however, is far less predictable than a computer and this renders the design of CPS very challenging. In order to reduce the impact of unforeseen events arising from the physical world, or even from the cyber world, this project develops a science of CPS robustness. A robust CPS will only modestly deviate from its desired behavior upon the occurrence of unforeseen circumstances and has the ability to recover once these disrupting circumstances subside. The intellectual merit of this project is the development of a science of CPS robustness that harnesses the intricate interactions between cyber and physical components to obtain CPS that are able to operate in a wide range of unpredictable environments. The project?s broader significance and importance is the enablement of vast number of applications requiring CPS to operate seamlessly in unpredictable environments such as the internet-of-things or smart and connected communities.<br/><br/>At the technical level, this project leverages existing notions of robustness for cyber systems, such as self-stabilizing algorithms, and for physical systems, such as input-to-state stability, to create a science of CPS robustness. Expected outcomes include new temporal logics to specify CPS robustness, verification and synthesis algorithms for CPS robustness, as well as compositional design flows."
"1716563","AF: Small: Geometric  Complexity Theory","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","06/28/2017","Ketan Mulmuley","IL","University of Chicago","Standard Grant","Tracy J. Kimbrel","08/31/2020","$450,000.00","","mulmuley@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7796","7923, 7927","$0.00","Geometric complexity theory is an approach towards the most foundational outstanding conjecture of computer science, P vs. NP, which implies that theorem-proving cannot be automated.  Its current focus is on the algebraic variants of this conjecture.  The approach was initiated by the PI in projects supported by two earlier NSF grants.  It has revealed deep connections between the algebraic variants of the P vs. NP conjecture and foundational problems of algebraic geometry and representation theory, the two fields of mathematics most relevant to this project.  The goal of this project is to study, strengthen, and exploit these connections to advance this approach further.<br/><br/>The boundary between the tractable and intractable problems in mathematical and physical sciences is defined by the P vs.  NP problem.  Hence the study undertaken in this project is of central intellectual relevance to computer science, as well as to several other areas of mathematical and physical sciences.  Since geometric complexity theory reveals deep connections among the foundational problems of computer science, algebraic geometry, and representation theory, this study may help in bringing these fields together to advance simultaneously on their foundational problems.  Effort would also be made to mentor and train post-doctoral researchers in geometric complexity theory, and to disseminate the work in this emerging field through journals, conferences, and workshops.<br/><br/>The algebraic variant of the P vs.  NP conjecture that this project focuses on is the VP vs. VNP conjecture, which says that the permanent cannot be computed by algebraic circuits of polynomial size and degree.  The project supported by the earlier NSF grant CCF-1017760 revealed a foundational difficulty, called the geometric complexity theory chasm, at the interface of algebraic geometry and complexity theory, which needs to be overcome by any realistic approach to this conjecture.  This chasm exists because we do not know at present if the complexity class VP is equal to its closure.  Hence the most urgent problem in the context of the VP vs.  VNP conjecture now is to either show that VP is equal to its closure, or else construct specific candidate families in the closure of VP that are not in VP.  This project seeks to investigate this problem using the techniques of geometric complexity theory developed in the earlier project, in conjunction with the advanced techniques of algebraic geometry.<br/><br/>The project also seeks to revise the existing geometric complexity theory approach to the VP vs. VNP conjecture, using a refined notion of multiplicity-based obstructions, to take into account the recent negative results.  These obstructions are gadgets in algebraic geometry and representation theory that serve as proof certificates of hardness of the permanent.  The project will investigate the problem of proving their existence, through a sequence of intermediate problems at the interface of algebraic geometry, representation theory, and complexity theory."
"1837127","FMitF: Collaborative Research: RedLeaf: Verified Operating Systems in Rust","CCF","FMitF: Formal Methods in the F","09/15/2018","09/07/2018","Anton Burtsev","CA","University of California-Irvine","Standard Grant","Samee Khan","08/31/2022","$350,000.00","","aburtsev@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","094Y","062Z, 071Z","$0.00","An operating system kernel provides a foundation for isolation and security in every computer system used today. Operating system kernels are trusted to provide the first line of defense for numerous mission critical systems in the face of targeted security attacks. Unfortunately, despite decades of evolution modern operating systems are faulty and vulnerable. Inheriting their core engineering technology from the first time-sharing machines, modern operating system kernels are  still developed with a legacy software engineering techniques---a combination of an unsafe programming language, rudimentary concurrency primitives, and virtually no testing or verification tools. Today these systems are faulty and vulnerable. Lacking verification support, industry standard kernels make nearly every computer system on the planet vulnerable. <br/><br/>This project will develop RedLeaf, a new operating system, and associated formal verification tools for implementing provably secure and reliable systems in the Rust programming language. RedLeaf brings together state-of-the-art results from verification, programming-language, and systems research communities in order to enable unprecedented security and reliability guarantees in low-level systems software. To achieve complete verification of the entire software stack, i.e., operating system and applications, the RedLeaf team will develop a set of new tools, a collection of techniques and engineering disciplines, and a methodology focused on rapid development of verified systems software. The RedLeaf OS will run on an embedded CPU of a medical sensor, implement a network function virtualization framework aimed at line-rate network processing, and provide a general platform for a broad range of verifiably secure systems. The operating system and associated tools will be open source, directly benefiting the broader community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836724","FMitF: A Framework for Synthesis of Efficient, Reliable, and Secure Operating System Components","CCF","FMitF: Formal Methods in the F","10/01/2018","09/10/2018","Emina Torlak","WA","University of Washington","Standard Grant","Nina Amla","09/30/2022","$980,043.00","Xi Wang","emina@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","094Y","062Z, 071Z, 9102","$0.00","The operating system is a critical part of every computing device, from mobile phones to cloud servers. It consists of core software components, such as the kernel and the file system, that mediate the interaction between user applications and the underlying hardware. Bugs in these components have wide-ranging impact on systems in use every day, from causing crashes and slowdowns to allowing attackers to take over the entire system. This project develops Synix, a transformative new approach to building operating system components that eliminates entire classes of such bugs. Synix is based on automated program synthesis, and it is the first effort to synthesize a broad range of key operating system components, providing formal guarantees of efficiency, reliability, and security. By extending the scalability and reach of program synthesis to the domain of operating systems, Synix advances the state-of-the-art in formal methods and in the design of software components that underpin our computing infrastructure.<br/><br/>Synix takes the form of a novel framework for synthesis-aided development of efficient, reliable, and secure operating system components. The PIs prior work on push-button verification of kernels and file systems has demonstrated that it is feasible to verify the safety and security of these components fully automatically and with low specification burden. The enabling idea behind push-button verification is to design component interfaces to be finite so that the semantics of each interface procedure is expressible as a set of traces of bounded length. The main insight behind Synix is that finite interfaces are also an ideal target for syntax-guided synthesis. The research goal of this project is thus to develop new techniques for synthesizing efficient implementations of three classes of core operating system components with finite interfaces: (1) a just-in-time compiler for a given in-kernel interpreter, (2) a crash-safe file system for a given storage interface, and (3) a security monitor for a given application-level isolation policy. The driving idea underpinning the proposed solutions is to use self hosting, write-before relations, and narrow finite interfaces to decompose the target synthesis problems into more tractable synthesis tasks. The practical and educational goals of this project are to apply Synix to synthesize real operating system configurations, thus facilitating adoption; release the resulting tools as open-source software; actively support the tools users; and disseminate key results through papers, lectures, and tutorials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837129","FMitF: Opening Up the Black Box of Probabilistic Program Inference","CCF","FMitF: Formal Methods in the F","12/01/2018","09/07/2018","Todd Millstein","CA","University of California-Los Angeles","Standard Grant","Weng-keen Wong","11/30/2022","$947,397.00","Guy Van den Broeck","todd@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","094Y","062Z, 071Z","$0.00","Probabilistic programming languages are an expressive means for creating, maintaining, and understanding a wide range of machine-learning models, and they have been successfully used by both researchers and major technology companies.  However, today's probabilistic programming languages impose strong limitations on the kinds of programs for which they are effective, thereby precluding their use for many machine-learning applications.  This project adapts and generalizes techniques from the formal methods community for reasoning about traditional programs, in order to develop general-purpose algorithms for probabilistic inference, which is the key task that a probabilistic programming language must perform.  The project is implementing these algorithms in the context of a new imperative probabilistic programming language and is providing educational opportunities in the burgeoning area of probabilistic programming for graduate, undergraduate, and high-school students.<br/><br/>This project has three main technical thrusts.  First, the project is developing exact inference algorithms for discrete probabilistic programs by exploiting the connection to techniques for symbolic model checking and weighted model counting.  Second, the project is developing techniques to automatically decompose a probabilistic inference query into multiple simpler sub-queries, each of which can be solved using the most appropriate inference method. The key enabler of this decomposition is a novel abstraction process for probabilistic programs.  Third, the project is investigating the use of abstractions as proposal distributions for probabilistic inference, resulting in new abstraction-guided approximate inference algorithms.  The results of this project will make probabilistic programming more effective by making probabilistic inference and learning tractable for a wider class of programs.  The artifacts that result from this research will be released open source, including a new probabilistic programming language that leverages the newly developed inference techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836712","FMitF: Verifying Concurrent System Software with Cspec","CCF","FMitF: Formal Methods in the F","11/01/2018","08/31/2018","Nickolai Zeldovich","MA","Massachusetts Institute of Technology","Standard Grant","Anindya Banerjee","10/31/2022","$750,000.00","Adam Chlipala, M. Frans Kaashoek","nickolai@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","094Y","062Z, 7943","$0.00","Most computer systems involve concurrency---multiple threads, cores, or computers executing at the same time and interacting with one another. Developing concurrent software is error-prone, because the concurrent threads can interact in many unexpected ways, and it is easy for developers to overlook subtle corner-case interactions. This project will tackle this problem through the use of machine-checked proofs for concurrent software: that is, it will focus on helping developers to provide a mathematical proof that the software will execute according to its specification, for any possible interaction among the concurrent threads.  As a driving example, this project will develop a verified concurrent multicore file system. This project's novelty lies in its approach for specifying and proving concurrent software that reduces the number of thread interactions that the developer must reason about, while still providing a full proof of correctness.  The expected impact of this project will be a set of ideas, techniques, and tools for specifying and proving concurrent systems software.  Since concurrent software plays such a critical role in computer systems, being able to verify correctness of key software components will improve reliability and security of many systems.<br/><br/>The specific research contributions of this project will center around CSPEC, a framework for formal verification of concurrent software, which will help developers ensure that no corner cases are missed.  The key challenge faced by CSPEC is to reduce the number of interactions, or interleavings, that developers must consider in their proofs.  CSPEC addresses this challenge using so-called mover types to reorder commutative operations.  Mover types enable developers to reason about largely sequential executions rather than all possible interleavings.  CSPEC also provides a library of proof patterns for common styles of concurrency, including retry loops, state partitioning, and cooperative enforcement of rules between threads or processes.  In the process of developing a verified concurrent file system on top of CSPEC, the investigators will address additional research challenges, such as formulating a specification for a POSIX file system under concurrency, integrating reasoning about crash safety and concurrency, managing the complexity of proving a sophisticated concurrent file system, and generating efficient concurrent code to achieve high performance while preserving correctness.  As part of this project, the research team will also develop a course focused on formal reasoning about computer systems, which includes concurrency, reliability, and fault-tolerance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837051","FMitF: Collaborative Research: RedLeaf: Verified Operating Systems in Rust","CCF","FMitF: Formal Methods in the F","09/15/2018","09/07/2018","Zvonimir Rakamaric","UT","University of Utah","Standard Grant","Samee Khan","08/31/2022","$399,931.00","","zvonimir@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","094Y","062Z, 071Z","$0.00","An operating system kernel provides a foundation for isolation and security in every computer system used today. Operating system kernels are trusted to provide the first line of defense for numerous mission critical systems in the face of targeted security attacks. Unfortunately, despite decades of evolution modern operating systems are faulty and vulnerable. Inheriting their core engineering technology from the first time-sharing machines, modern operating system kernels are still developed with a legacy software engineering techniques---a combination of an unsafe programming language, rudimentary concurrency primitives, and virtually no testing or verification tools. Today these systems are faulty and vulnerable. Lacking verification support, industry standard kernels make nearly every computer system on the planet vulnerable. <br/><br/>This project will develop RedLeaf, a new operating system, and associated formal verification tools for implementing provably secure and reliable systems in the Rust programming language. RedLeaf brings together state-of-the-art results from verification, programming-language, and systems research communities in order to enable unprecedented security and reliability guarantees in low-level systems software. To achieve complete verification of the entire software stack, i.e., operating system and applications, the RedLeaf team will develop a set of new tools, a collection of techniques and engineering disciplines, and a methodology focused on rapid development of verified systems software. The RedLeaf OS will run on an embedded CPU of a medical sensor, implement a network function virtualization framework aimed at line-rate network processing, and provide a general platform for a broad range of verifiably secure systems. The operating system and associated tools will be open source, directly benefiting the broader community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836813","FMitF: Formal Verification of Accessibility","CCF","FMitF: Formal Methods in the F","01/01/2019","08/10/2018","Michael Ernst","WA","University of Washington","Standard Grant","Nina Amla","12/31/2022","$738,125.00","Andrew Ko, Jennifer Mankoff, Zachary Tatlock","mernst@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","094Y","062Z, 8206","$0.00","Most of the web is not designed for accessibility for the nearly one billion people that have a disability such as blindness, low vision, motorphysical impairments, or dyslexia, and no comprehensive, precise verification tools currently exist for checking accessibility. Existing testing tools are limited, imprecise, and incomplete, and even when they are used, they give guarantees only about one particular web browser configuration such as window size, default fonts and colors. This project aims to enable the formal verification of web accessibility. The research to be pursued involves the automatic identification of a broader class of accessibility problems that is currently possible and is intended to give guarantees of absence of such problems for all possible web browser configurations. The software tools developed in this project are intended to give web developers and content producers targeted, concrete feedback on who is affected by an accessibility issue, and why, and how to fix any problems.  <br/><br/>The project develops a user interface logic for specifying accessibility properties, and formalizes a large fragment of browser rendering algorithms using novel finitization reductions. The project builds software tools that translates web pages, accessibility rules, and the browser algorithm to quantifier-free linear real arithmetic, using an SMT solver to verify it or produce a concrete, inaccessible rendering of the webpage. To make the results of these verifications useful and usable to developers, content producers, and web users, the investigators develop new classes of concrete, comprehensible, and  actionable warning explanations and new techniques for patching accessibility at run time. To evaluate all of this work, the project is partnering with Adobe, Instructure, and Wikimedia.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837023","FMitF: Transplanting Syntax-Guided Synthesis to Computer Networks","CCF","FMitF: Formal Methods in the F","01/01/2019","08/10/2018","Xiaokang Qiu","IN","Purdue University","Standard Grant","Darleen L. Fisher","12/31/2022","$742,001.00","Sanjay Rao","xkqiu@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","094Y","062Z","$0.00","Computer networks are difficult to manage since there exists a wide gulf between the high-level goals that operators have for their networks (e.g., security, Quality of Service etc.), and the low-level configurations in heterogeneous devices in which such intent must be expressed. While the advent of Software-Defined Networking (an emerging technology that centralizes control decisions regarding how traffic must be handled, and separates them from the devices that actually forward each packet) helps, the process of designing networks is still ad-hoc, leading to high operational costs, design faults that account for a large fraction of network downtime, and costly security breaches. This project is motivated by the vision of design automation for networking, inspired by the success of the approach in other domains such as chip design. The project will develop methods for network architects to express their intent at higher levels of abstraction, and techniques to automatically synthesize low-level switch configurations that realize this intent correctly and efficiently.<br/><br/>The project will tackle network automation through syntax-guided synthesis, which has been a popular paradigm embodied in many program synthesis systems. Rather than directly synthesize low-level switch flow-table rules and switch configurations like current network synthesis efforts, the project will explore how to synthesize code from user-provided sketches and specifications, corresponding to a network programming language, which may then be translated into low-level switch configurations. This approach will enable human experts to convey their insights and hints, which are critical for applying modern synthesis techniques. The project will develop a network programming language that is expressive yet has the necessary formal semantics to enable synthesis, explore the design space of sketches for network synthesis, and develop ways to improve the performance of synthesis for computer networks. The broader impact of this project is to raise the level of abstraction for managing large networks, leading to much lower management costs, better performance, security and reliability. The project will extensively involve graduate and undergraduate students, and incorporate new curriculum material in programming language and networking classes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816615","SHF: Small: Science and Tools for Intelligent Developer Testing","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","05/30/2018","Tao Xie","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sol J. Greenspan","09/30/2021","$500,000.00","","taoxie@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7923, 7944","$0.00","Software dependability plays a critical role in United States businesses, government, and society. Although much progress has been made in software verification and validation, software testing remains by far the most widely used technique for improving software dependability. Among various types of testing, developer testing has been widely recognized as an important and valuable means of improving software dependability. The popularity and benefits of developer testing have been well witnessed in industry; however, manual developer testing is known to be labor intensive, and is often insufficient in comprehensively exercising behavior of the software under test to expose its hidden bugs. To address the issue, one of the common ways is to use testing infrastructures and tools to reduce or complement manual testing effort to achieve higher software dependability. In the past decade, the software testing research community has made significant progress in automatic test generation. With various recent scientific advances by the research community, a question naturally arises: what would be the audacious goal for the field of developer testing in the upcoming decade to bring a much higher testing effectiveness and efficiency to developers? To address this question, this project investigates the science and tools of intelligent developer testing, fundamentally advancing knowledge and understanding in foundations, techniques, and tools for intelligent developer testing. The project improves software dependability by revealing more bugs during software development before they manifest in deployed software.<br/><br/>The project focuses on science and tools for instilling intelligence from two major ways (natural language interfacing and continuous learning) into developer testing tools as part of the efforts for realizing the vision of intelligent software engineering. The project develops novel and practical techniques and tools of intelligent developer testing that have high potential to impact the industry. In particular, the project focuses on parameterized unit tests, which are an improvement over conventional unit tests because they can easily be extended by automated tools to increase code coverage while reusing developer-written oracles. The PI plans to develop intelligent tools that will make it significantly easier for developers to write parameterized unit tests with the goal of automatically translating developer intents into parts of executable test cases. This project advances the science by exploring a series of questions, e.g., how to define or determine levels of intelligence in developer testing, how to bring high levels of intelligence in developer testing tools, how to synergistically integrate machine intelligence and human intelligence (e.g., domain knowledge or insight) to effectively tackle challenging tasks in developer testing. The project involves research collaborations with industrial partners and involves participation of students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1521584","Collaborative Research: Expeditions in Computing: The Science of Deep Specification","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","07/24/2017","Adam Chlipala","MA","Massachusetts Institute of Technology","Continuing grant","Anindya Banerjee","11/30/2020","$1,148,325.00","","adamc@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7723","7723","$0.00","In our interconnected world, software bugs and security vulnerabilities pose enormous costs and risks.  The Deep Specification (""DeepSpec"", deepspec.org) project addresses this problem by showing how to build software that does what it is supposed to do, no less and (just as important) no more: No unintended backdoors that allow hackers in, no bugs that crash your app, your computer, or your car.  ""What the software is supposed to do"" is called its specification.  The DeepSpec project will develop new science, technology, and tools--for specifying what programs should do, for building programs that conform to those specifications, and for verifying that programs do behave exactly as intended.  The key enabling technology for this effort is modern interactive proof assistants, which support rigorous mathematical proofs about complex software artifacts.  Project activities will focus on core software-systems infrastructure such as operating systems, programming-language compilers, and computer chips, with applications such as elections and voting systems, cars, and smartphones.<br/><br/><br/>Better-specified and better-behaved software will benefit us all.  Many high-profile security breaches and low-profile intrusions use software bugs as their entry points.  Building on decades of previous work, DeepSpec will advance methods for specifying and verifying software so they can be used by the software industry.  The project will include workshops and summer schools to bring in industrial collaborators for technology transfer.  But the broader use of specifications in engineering also requires software engineers trained in specification and verification--so DeepSpec has a major component in education: the development of introductory and intermediate curriculum in how to think logically about specifications, how to use specifications in building systems-software components, or how to connect to such components.  The education component includes textbook and on-line course material to be developed at Princeton, Penn, MIT, and Yale, and to be available for use by students and instructors worldwide.  There will also be a summer school to train the teachers who can bring this science to colleges nationwide.<br/><br/><br/>Abstraction and modularity underlie all successful hardware and software systems: We build complex artifacts by decomposing them into parts that can be understood separately. Modular decomposition depends crucially on the artful choice of interfaces between pieces. As these interfaces become more expressive, we think of them as specifications of components or layers. Rich specifications based on formal logic are little used in industry today, but a practical platform for working with them will significantly reduce the costs of system implementation and evolution by identifying vulnerabilities, helping programmers understand the behavior of new components, facilitating rigorous change-impact analysis, and supporting maintainable machine-checked verifications that components are correct and fit together correctly.   This Expedition focuses on a particularly rich class of specifications, ""deep specifications."" These impose strong functional correctness requirements on individual components such that they connect together with rigorous composition theorems. The Expedition's goal is to focus the efforts of the programming languages and formal methods communities on developing and using deep specifications in the large.  Working in a formal proof management system, the project will concentrate particularly on connecting infrastructure components together at specification interfaces: compilers, operating systems, program analysis tools, and processor architectures."
"1851695","NSF Student Travel Grant for Sixteenth Conference on the Foundations of Nanoscience (FNANO 2019)","CCF","COMPUTATIONAL BIOLOGY","03/01/2019","02/20/2019","John Reif","NC","Duke University","Standard Grant","Mitra Basu","02/29/2020","$12,000.00","","reif@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7931","7556, 7946","$0.00","This award supports student travel for between 10 - 20 participants to the Conference on Foundations of Nanoscience, Snowbird, Utah in April 2019. The 16th Conference on Foundations of Nanoscience will have a mixture of invited talks by distinguished speakers as well as contributed posters and open discussion periods to enhance attendee interaction with the goal of creating vibrant intellectual community in the area of self-assembly. This project will provide a substantial multidisciplinary impact to nanoscience, biochemistry and chemistry, which will profit from the introduction of key methodologies derived from mainstream computer science, such as mathematical modeling, software engineering, algorithms and modular design methodologies.<br/><br/><br/>The Foundations of Nanoscience meeting (FNANO) was established by the International Society for Nanoscale Science, Computation, and Engineering in 2004 as a venue for the wide range of researchers interested in various aspects of self-assembly as it relates to computer science, nanoscience and nanotechnology. The meeting features multiple tracks covering recent work in different types of self-assembled architectures and devices, at scales ranging from nano-scale to meso-scale. Methodologies include both experimental as well as theoretical approaches. The conference spans traditional disciplines including chemistry, biochemistry, physics, computer science, mathematics, and various engineering disciplines. The emphasis is on basic, rather than applied research. The FNANO 2019 meeting is projected to attract 150 registered participants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1906903","Symposium on Discrete Algorithms Science (SODA) 2019 Travel Grant","CCF","ALGORITHMIC FOUNDATIONS","01/01/2019","12/20/2018","Clifford Stein","NY","Columbia University","Standard Grant","Balasubramanian Kalyanasundaram","12/31/2019","$15,000.00","","cliff@ieor.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7556, 7926","$0.00","This award will support student and postdoctoral attendance at the Annual ACM/SIAM Symposium on Discrete Algorithms Science (SODA) 2019, in San Diego, CA on January 6  to 9, 2019.  SODA is co-sponsored by the Association for Computing Machinery (ACM) and the Society for Industrial and Applied Mathematics (SIAM).  SODA is the premier annual research conference in the field of discrete algorithms and one of the three premier conferences in theoretical computer science. SODA has been meeting annually since 1990 and in a typical year has approximately 4000 attendees.  It is co-located with three smaller workshops, ALENEX (Meeting on Algorithm Engineering and Experimentation), ANALCO (Meeting on Analysis of Algorithms), and SOSA (Symposium on Simplicity in Algorithms).  SODA is attended by researchers all over the world.  The field of algorithms is a vibrant one, with high participation rates from young researchers, and many papers with student authors. For these student authors and student attendees, the conference serves as a valuable educational experience, both for the technical content of the talks and for the opportunities for networking that it provides.<br/><br/>The award will provide partial support to approximately thirty students, partly defraying the cost of travel and lodging.  Efforts will be made to support students from under represented groups<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1553288","CAREER:  Structure and Analysis of Low Degree Polynomials","CCF","ALGORITHMIC FOUNDATIONS","04/01/2016","02/15/2019","Daniel Kane","CA","University of California-San Diego","Continuing grant","Tracy Kimbrel","03/31/2021","$384,242.00","","dakane@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7796","1045, 7926, 7927","$0.00","The primary goal of complexity theory is to design fast algorithms for solving various computational problems and to understand when such algorithms will not exist. In order to do this, a number of sophisticated mathematical tools are often required. One of the more prominent tools used in this way are polynomials. Polynomials are both varied enough to express or approximate a number of objects of interest, and yet are also simple enough that properties of them can be easily understood. Thus, relating properties of the objects in question to properties of polynomials is a well-known technique that shows up in many areas of computer science.<br/><br/>Unfortunately, our understanding of these fundamental objects is far from complete. In fact, there are a number of problems for which our lack of understanding can be seen as a bottleneck for proving further results about problems of interest. This project will focus on attempts to remedy this gap. In particular, the PI plans to follow up on several recent advances in understanding of this area and to attempt to leverage any new results to gain insight into other important questions within computer science. In addition to leading to new algorithms of practical import, the research promises to have potential impacts in other fields such as probability theory and<br/>algebraic geometry.<br/><br/>More specifically, the PI intends to improve upon existing tools for understanding low degree polynomials in many variables. Of particular interest would be work relating to results on the distribution of the values of such a polynomial on random inputs, with particular focus on recent structural results that allow one to decompose arbitrary polynomials in terms of better behaved ones. Having attained such results, the project will continue by making use of these improvements in order to make progress on other important problems in theoretical computer science. In particular, there are a number of specific problems in the areas of explicit pseudorandom generators, machine learning, and circuit complexity for which such technical improvements show promise for providing substantial new results.<br/><br/>In addition to the research component of this project, the PI intends to help provide new educational opportunities particularly with regard to learning mathematical problem solving skills which are a key component of mathematics and computer science research."
"1521539","Collaborative Research: Expeditions in Computing:  The Science of Deep Specification","CCF","EXPERIMENTAL EXPEDITIONS, SOFTWARE & HARDWARE FOUNDATION","12/15/2015","07/24/2017","Stephanie Weirich","PA","University of Pennsylvania","Continuing grant","Anindya Banerjee","11/30/2020","$3,375,811.00","Benjamin Pierce, Stephan Zdancewic","sweirich@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7723, 7798","7723, 7943, 9251","$0.00","In our interconnected world, software bugs and security vulnerabilities pose enormous costs and risks.  The Deep Specification (""DeepSpec"", deepspec.org) project addresses this problem by showing how to build software that does what it is supposed to do, no less and (just as important) no more: No unintended backdoors that allow hackers in, no bugs that crash your app, your computer, or your car.  ""What the software is supposed to do"" is called its specification.  The DeepSpec project will develop new science, technology, and tools--for specifying what programs should do, for building programs that conform to those specifications, and for verifying that programs do behave exactly as intended.  The key enabling technology for this effort is modern interactive proof assistants, which support rigorous mathematical proofs about complex software artifacts.  Project activities will focus on core software-systems infrastructure such as operating systems, programming-language compilers, and computer chips, with applications such as elections and voting systems, cars, and smartphones.<br/><br/><br/>Better-specified and better-behaved software will benefit us all.  Many high-profile security breaches and low-profile intrusions use software bugs as their entry points.  Building on decades of previous work, DeepSpec will advance methods for specifying and verifying software so they can be used by the software industry.  The project will include workshops and summer schools to bring in industrial collaborators for technology transfer.  But the broader use of specifications in engineering also requires software engineers trained in specification and verification--so DeepSpec has a major component in education: the development of introductory and intermediate curriculum in how to think logically about specifications, how to use specifications in building systems-software components, or how to connect to such components.  The education component includes textbook and on-line course material to be developed at Princeton, Penn, MIT, and Yale, and to be available for use by students and instructors worldwide.  There will also be a summer school to train the teachers who can bring this science to colleges nationwide.<br/><br/><br/>Abstraction and modularity underlie all successful hardware and software systems: We build complex artifacts by decomposing them into parts that can be understood separately. Modular decomposition depends crucially on the artful choice of interfaces between pieces. As these interfaces become more expressive, we think of them as specifications of components or layers. Rich specifications based on formal logic are little used in industry today, but a practical platform for working with them will significantly reduce the costs of system implementation and evolution by identifying vulnerabilities, helping programmers understand the behavior of new components, facilitating rigorous change-impact analysis, and supporting maintainable machine-checked verifications that components are correct and fit together correctly.   This Expedition focuses on a particularly rich class of specifications, ""deep specifications."" These impose strong functional correctness requirements on individual components such that they connect together with rigorous composition theorems. The Expedition's goal is to focus the efforts of the programming languages and formal methods communities on developing and using deep specifications in the large.  Working in a formal proof management system, the project will concentrate particularly on connecting infrastructure components together at specification interfaces: compilers, operating systems, program analysis tools, and processor architectures."
"1822007","EAGER: Automated Watermark and Moldmate Identification","CCF","COMM & INFORMATION FOUNDATIONS","03/01/2018","02/26/2018","Charles Johnson","NY","Cornell University","Standard Grant","Phillip Regalia","02/29/2020","$150,000.00","","crj2@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7797","7916, 7935","$0.00","Identification of watermarks in antique (pre-1750) laid paper is a vital forensic in determining the date and origin of its production. For artworks on paper, watermarks are a key element in dating and authentication. Classifying and distinguishing watermarks is a difficult, time-consuming task even for paper analysis experts. Recent research has shown that a computer-based decision tree can reduce dramatically the time expended in the task of watermark identification and the concomitant identification of sheets of paper made on the same mold, while at the same time increasing the confidence in the result. The application of digital image processing to this task is one example of the new field of computational art history, which has exploded over the past decade. Through this project, automation using digital image processing can scale up the task of watermark and moldmate identification to handle the universe of the hundreds of individual watermarks in the papers of prints and drawings of Rembrandt and his pupils and allow expansion of this identification beyond these artworks in major museums to the thousands in smaller museums and private collections. This is expected to energize the expansion of this valuable forensic to the thousands of watermarks in laid paper used in artworks on paper by other pre-1750 artists. Advances in computational art history stemming from this project will offer a path for the general population, which is growing quite comfortable with the expansion of computer usage in everyday life, to engage with art in a new and appealing way. Art history is one subfield within cultural heritage that has delayed the longest in joining the emergence of the digital science and humanities. The success of this project will be a major enabler for further trans-disciplinary activities between the humanities and the sciences, thereby decreasing the separation of their two cultures, which is vital as the digitization of all aspects of daily life gains dominance at a scorching pace.<br/><br/>This project continues the pioneering efforts in the new field of computational art history that is broadening acceptance of digital image processing as a vital tool in art history research as it tremendously expands the intellectual reach of art historians. As the first tools are developed in this nascent field and more scientists and engineers are drawn to the challenges of these problems, the specific needs of art historians will undoubtedly reveal new tasks requiring advances in the underlying science of digital image processing. This connection between computation and the visual humanities also offers a powerful link to combat the widening gulf in education at all levels between engineering science and the humanities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1521523","Collaborative Research:  Expeditions in Computing:  The Science of Deep Specification","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","07/24/2017","Zhong Shao","CT","Yale University","Continuing grant","Anindya Banerjee","11/30/2020","$2,046,445.00","","zhong.shao@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7723","7723","$0.00","In our interconnected world, software bugs and security vulnerabilities pose enormous costs and risks.  The Deep Specification (""DeepSpec"", deepspec.org) project addresses this problem by showing how to build software that does what it is supposed to do, no less and (just as important) no more: No unintended backdoors that allow hackers in, no bugs that crash your app, your computer, or your car.  ""What the software is supposed to do"" is called its specification.  The DeepSpec project will develop new science, technology, and tools--for specifying what programs should do, for building programs that conform to those specifications, and for verifying that programs do behave exactly as intended.  The key enabling technology for this effort is modern interactive proof assistants, which support rigorous mathematical proofs about complex software artifacts.  Project activities will focus on core software-systems infrastructure such as operating systems, programming-language compilers, and computer chips, with applications such as elections and voting systems, cars, and smartphones.<br/><br/><br/>Better-specified and better-behaved software will benefit us all.  Many high-profile security breaches and low-profile intrusions use software bugs as their entry points.  Building on decades of previous work, DeepSpec will advance methods for specifying and verifying software so they can be used by the software industry.  The project will include workshops and summer schools to bring in industrial collaborators for technology transfer.  But the broader use of specifications in engineering also requires software engineers trained in specification and verification--so DeepSpec has a major component in education: the development of introductory and intermediate curriculum in how to think logically about specifications, how to use specifications in building systems-software components, or how to connect to such components.  The education component includes textbook and on-line course material to be developed at Princeton, Penn, MIT, and Yale, and to be available for use by students and instructors worldwide.  There will also be a summer school to train the teachers who can bring this science to colleges nationwide.<br/><br/><br/>Abstraction and modularity underlie all successful hardware and software systems: We build complex artifacts by decomposing them into parts that can be understood separately. Modular decomposition depends crucially on the artful choice of interfaces between pieces. As these interfaces become more expressive, we think of them as specifications of components or layers. Rich specifications based on formal logic are little used in industry today, but a practical platform for working with them will significantly reduce the costs of system implementation and evolution by identifying vulnerabilities, helping programmers understand the behavior of new components, facilitating rigorous change-impact analysis, and supporting maintainable machine-checked verifications that components are correct and fit together correctly.   This Expedition focuses on a particularly rich class of specifications, ""deep specifications."" These impose strong functional correctness requirements on individual components such that they connect together with rigorous composition theorems. The Expedition's goal is to focus the efforts of the programming languages and formal methods communities on developing and using deep specifications in the large.  Working in a formal proof management system, the project will concentrate particularly on connecting infrastructure components together at specification interfaces: compilers, operating systems, program analysis tools, and processor architectures."
"1521602","Collaborative Research: Expeditions in Computing: The Science of  Deep Specification","CCF","EXPERIMENTAL EXPEDITIONS","12/15/2015","07/24/2017","Andrew Appel","NJ","Princeton University","Continuing grant","Anindya Banerjee","11/30/2020","$3,453,419.00","","appel@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7723","7723","$0.00","In our interconnected world, software bugs and security vulnerabilities pose enormous costs and risks.  The Deep Specification (""DeepSpec"", deepspec.org) project addresses this problem by showing how to build software that does what it is supposed to do, no less and (just as important) no more: No unintended backdoors that allow hackers in, no bugs that crash your app, your computer, or your car.  ""What the software is supposed to do"" is called its specification.  The DeepSpec project will develop new science, technology, and tools--for specifying what programs should do, for building programs that conform to those specifications, and for verifying that programs do behave exactly as intended.  The key enabling technology for this effort is modern interactive proof assistants, which support rigorous mathematical proofs about complex software artifacts.  Project activities will focus on core software-systems infrastructure such as operating systems, programming-language compilers, and computer chips, with applications such as elections and voting systems, cars, and smartphones.<br/><br/><br/>Better-specified and better-behaved software will benefit us all.  Many high-profile security breaches and low-profile intrusions use software bugs as their entry points.  Building on decades of previous work, DeepSpec will advance methods for specifying and verifying software so they can be used by the software industry.  The project will include workshops and summer schools to bring in industrial collaborators for technology transfer.  But the broader use of specifications in engineering also requires software engineers trained in specification and verification--so DeepSpec has a major component in education: the development of introductory and intermediate curriculum in how to think logically about specifications, how to use specifications in building systems-software components, or how to connect to such components.  The education component includes textbook and on-line course material to be developed at Princeton, Penn, MIT, and Yale, and to be available for use by students and instructors worldwide.  There will also be a summer school to train the teachers who can bring this science to colleges nationwide.<br/><br/><br/>Abstraction and modularity underlie all successful hardware and software systems: We build complex artifacts by decomposing them into parts that can be understood separately. Modular decomposition depends crucially on the artful choice of interfaces between pieces. As these interfaces become more expressive, we think of them as specifications of components or layers. Rich specifications based on formal logic are little used in industry today, but a practical platform for working with them will significantly reduce the costs of system implementation and evolution by identifying vulnerabilities, helping programmers understand the behavior of new components, facilitating rigorous change-impact analysis, and supporting maintainable machine-checked verifications that components are correct and fit together correctly.   This Expedition focuses on a particularly rich class of specifications, ""deep specifications."" These impose strong functional correctness requirements on individual components such that they connect together with rigorous composition theorems. The Expedition's goal is to focus the efforts of the programming languages and formal methods communities on developing and using deep specifications in the large.  Working in a formal proof management system, the project will concentrate particularly on connecting infrastructure components together at specification interfaces: compilers, operating systems, program analysis tools, and processor architectures."
"1553751","CAREER: The Geometry of Polynomials in Algorithms and Combinatorics","CCF","ALGORITHMIC FOUNDATIONS","02/01/2016","09/19/2017","Nikhil Srivastava","CA","University of California-Berkeley","Continuing grant","Rahul Shah","01/31/2021","$270,054.00","","nikhil@math.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7796","1045, 7926","$0.00","Graphs and matrices are central objects in computer science. Two of the most successful paradigms for manipulating and optimizing over them are spectral methods, which study eigenvalues and eigenvectors, and semidefinite programming, which allows efficient optimization over certain convex sets defined by putting constraints on these eigenvalues. The goal of this proposal is to investigate a new technique for controlling eigenvalues which relies on tools from an area known as the geometry of polynomials. This alternate perspective was shown to be very powerful in recent work of the PI on Ramanujan expander graphs and the Kadison-Singer problem, and the PI believes that a more complete understanding of it will be fruitful both for theoretical computer science and for mathematics.<br/><br/>At a technical level, the  PI will focus on two points of contact between the geometry of polynomials and computer science: (A) The method of interlacing families of polynomials, a new analogue of the probabilistic method recently introduced by the PI and collaborators, based on studying random polynomials with real roots.  (B) Hyperbolic programming, a generalization of semidefinite programming which is intimately related to the polynomials appearing in (A).<br/><br/>More specifically, the proposal has the following goals: (i) Develop a more systematic and general understanding of (A) and use this to attack central linear-algebraic and combinatorial problems in TCS and applied math. (ii) Find efficient algorithms for producing the useful objects guaranteed to exist by (A), which currently require exponential time.  (iii) Explore the power of (B) as an algorithmic primitive in combinatorial optimization, in particular as an approach to graph partitioning problems, and understand whether it is actually more powerful than semidefinite programming.<br/><br/>The problems outlined in the proposal have the potential to impact various fields, such as signal processing, quantum computing, pseudorandomness, complexity theory, and operator theory. The subject is accessible to students of from diverse backgrounds across math and computer science, so this project uses to integrate research and education for both graduate and undergraduate students working between these fields."
"1253715","CAREER: Leveraging Three-Dimensional Integration Technology for Highly Heterogeneous Systems-on-Chip","CCF","SOFTWARE & HARDWARE FOUNDATION, DES AUTO FOR MICRO & NANO SYST","03/01/2013","03/03/2017","Emre Salman","NY","SUNY at Stony Brook","Continuing grant","Sankar Basu","08/31/2019","$453,809.00","","emre.salman@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7798, 7945","1045, 7945, 9251","$0.00","Heterogeneous three-dimensional (3D) integration is an emerging paradigm for developing multifunctional systems at a lower cost. The dominant practice targets primarily high performance applications with limited heterogeneity such as logic-on-logic and memory-on-logic integration. On the contrary, this proposal embodies a significantly different vision, in which the focus is on low power highly heterogeneous 3D systems-on-chip (SoCs) with promising applications to the life sciences, energy efficient mobile computing, and environmental control. Despite the recent and growing efforts to develop hybrid 3D ICs, a systematic design and analysis framework does not yet exist. This research fills this need by addressing three interrelated issues of critical importance to this framework: system-wide power, signal, and sensing integrity. Simultaneously satisfying these three design objectives is a significant challenge in tightly coupled 3D SoCs. By considering multiple planes as a single entity, novel circuit- and physical-level design and analysis methodologies will be developed to alleviate this fundamental challenge. The traditional understanding of signal integrity will be extended to consider sensing integrity, an important issue in heterogeneous 3D SoCs, where multiple sensors and sensing electronics are coordinated. The proposed methodologies will be evaluated by developing and fabricating a heterogeneous 3D test chip with application to bio-electronics.<br/><br/><br/>The synergy between 3D integration and heterogeneous SoCs provides a paradigm changing vision for integrated electronics with substantial benefits to science, industry, and society at large. At a time when the fundamental limits of traditional scaling are approaching, this synergy brings new opportunities to a variety of applications including More-than-Moore systems. The proposed research enhances the feasibility of this cooperation by targeting critical circuit- and physical-level issues. These research activities will be tightly integrated with educational initiatives at the secondary, undergraduate, and graduate levels with participation from minority and underrepresented groups. These efforts will include both traditional activities (such as introducing new courses and promoting undergraduate research) and novel methods that interact with recent pedagogical practices. The PI will also continue to participate in the departmental and local outreach activities such as an engineering summer camp and a science fair, both targeting high school students within the greater New York City area including Long Island. The intertwined objectives of this proposal will guarantee power, signal, and sensing integrity in hybrid 3D SoCs, while contributing toward the advancement of science and diversity in education."
"1822097","Student Travel Support for CCC 2018","CCF","INFORMATION TECHNOLOGY RESEARC, ALGORITHMIC FOUNDATIONS","04/01/2018","03/22/2018","Rocco Servedio","NY","Columbia University","Standard Grant","Tracy Kimbrel","03/31/2020","$15,000.00","","rocco@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1640, 7796","7556, 7927","$0.00","This award will support student attendance at the 33rd Annual Computational Complexity Conference (CCC), in San Diego, CA, June 22 to 24, 2018.  The field of computational complexity forms the backbone of theoretical computer science.  The conference aims to foster research in all areas of computational complexity theory, studying the absolute and relative power of computational models under resource constraints.  The search for efficient algorithms has important ramifications in many areas of business, technology, and science.  Complexity theory has helped with our understanding of fundamental difficulty underlying algorithmic issues and in turn has been able to motivate algorithm design for some of the most challenging computational problems.<br/><br/>This award will provide partial support to approximately 15 students to attend this top conference in the area of complexity theory.  Efforts will be made to support students from under-represented groups and geographic diversity among students will be taken into account.  The award will encourage and facilitate the student participation in this top-tier computer science conference.  It will contribute towards attracting more under-represented groups to computer science research giving students an opportunity to engage in the important technical, professional, and social exchanges that CCC fosters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1714215","CIF: Small: CQIS: Recoverability and Markovianity in Quantum Information","CCF","OFFICE OF MULTIDISCIPLINARY AC, QIS - Quantum Information Scie, QUANTUM COMPUTING","09/01/2017","06/15/2017","Mark Wilde","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Dmitri Maslov","08/31/2020","$400,482.00","","mwilde@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1253, 7281, 7928","7203, 7928, 9150","$0.00","This NSF award aims to address foundational questions in quantum information regarding entropy inequalities such as the monotonicity of quantum relative entropy and strong subadditivity. These entropy inequalities are at the core of quantum information and more generally physics, underlying various capacities of quantum communication channels, entropic uncertainty principles, and certain formulations of the second law of thermodynamics. PI Wilde and collaborators have recently established the strongest refinements of these entropy inequalities in the 40 years since Lieb, Ruskai, and Lindblad proved them in the 1970s and in the 30 years since Petz characterized their equality conditions in the 1980s. These refinements have interpretations in terms of recoverability and Markovianity in certain setups to which the original entropy inequalities apply.<br/><br/>The PI, armed with these new theorems and other tools, will address fundamental questions regarding recoverability and Markovianity in quantum systems. These questions have been awaiting answers before the refined entropy inequalities were available. Since the use of quantum relative entropy is so widespread all throughout quantum information, there is a vast territory to explore in terms of recoverability and Markovianity, which may lead to fundamental insights into the nature of quantum information. The work developed with this NSF award may also influence other areas of physics, given that Markovianity and recoverability are such fundamental notions permeating all of science.<br/><br/>The broader impact of this proposal is in line with the NSF Mission ""to promote the progress of science; to advance the national health, prosperity, and welfare; to secure the national defense; and for other purposes."" Indeed, research at the intersection of physics, computer science, and mathematics has advanced science in remarkable ways, pushing the limits of what is possible in principle and in practice. A major goal of this proposal is to enhance the training of graduate students at LSU."
"1743684","Support graduate students to attend the Heidelberg Laureate Forum","CCF","SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF, IIS SPECIAL PROJECTS","09/01/2017","08/23/2017","Michelle Goodson","TN","Oak Ridge Associated Universities","Standard Grant","Sankar Basu","08/31/2019","$80,000.00","","Michelle.Goodson@orau.org","100 ORAU Way","OAK RIDGE","TN","378306218","8652412722","CSE","1714, 2878, 7484","7945","$0.00","Oak Ridge Associated Universities, Inc. (ORAU) requests National Science Foundation (NSF) support for selection and attendance of US graduate students and postdoctoral researchers to attend the Heidelberg Laureate Forum (HLF) in 2017. The Heidelberg Laureate Forum takes place annually in Heidelberg, Germany, and brings 200 young researchers in the computer and mathematical sciences together from all over the world with a group of typically 30 laureates of the Abel Prize, Fields Medal, Turing Award, and Nevanlinna Prize, and is patterned after the Lindau Meeting of Nobel Laureates in the physical sciences. The ORAU has formalized a partnership with the HLF to organize a U.S. delegation for participation in the meetings. This proposal requests support for a total of 9 graduate students and postdoctoral researchers in computer science. <br/><br/>The ORAU proposes to support this program by promoting and advertising the opportunity; recruiting from and selecting a diverse pool of applicants; developing and maintaining a program Web site for recruiting, selecting, information and dissemination; coordinating travel and orientation activities for participants, and conducting a follow-up survey to document the program's impact."
"1350572","CAREER: New Directions in Arithmetic Computation","CCF","ALGORITHMIC FOUNDATIONS","02/01/2014","03/21/2018","Shubhangi Saraf","NJ","Rutgers University New Brunswick","Continuing grant","Tracy Kimbrel","01/31/2020","$494,406.00","","shubhangi.saraf@gmail.com","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7796","1045, 7927","$0.00","The overarching goal of the project is to understand the power and limitations of algebraic computation, and the main foci are the problems of showing lower bounds for arithmetic circuits and polynomial identity testing. These problems have played a central role in various areas of Theoretical Computer Science, and they form the algebraic analog of the P vs NP question. This project will develop several new approaches and techniques for advancing our understanding of both questions. Algebraic techniques have also seen several unexpected connections in pseudorandomness and coding theory. The potential of these methods is still far from understood. In this project the PI will develop these methods, in particular the method of multiplicities and partial derivatives, to give more powerful lower bounds, and new strengthened constructions of pseudorandom objects. <br/><br/>The research direction proposed in this project brings together ideas and tools from a broad array of disciplines and witnesses a lot of fruitful interaction between mathematics, computational complexity, as well as practical applications to information storage and retrieval. The PI will disseminate research findings by giving lectures, talks and developing new courses and making the material available online. The PI will be actively involved in mentoring young researchers, including high school students and minorities, who want to pursue research in Theoretical Computer Science. The PI will also organize a specialized workshop for women in Theoretical Computer Science, which will bring together women researchers from all over the world."
"1618895","AF:Small:Collaborative Research:Kinetics and Thermodynamics of Chemical Computation","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2016","05/31/2016","David Soloveichik","TX","University of Texas at Austin","Standard Grant","Mitra Basu","05/31/2020","$249,928.00","","david.soloveichik@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7923, 7946","$0.00","Biology is replete with smart molecular systems that perform nanoscale assembly, sense environmental stimuli, create chemical signals, and produce physical motion - each of these tasks coordinated by information processing circuits implemented with chemical reactions. Learning how to build artificial chemicals that compute autonomously in complex environments would bring about groundbreaking advances in manufacturing, chemical sensing, and medicine. The theory of computation has proven invaluable in enabling information processing in electronic man-made systems, and much-studied algorithms underlie the behavior of everything from communication networks to video games. However, a thorough understanding of the principles of chemical computation is still lacking. The goal of this proposal is to use rigorous mathematical models to investigate the capabilities and limitations of chemical information processing.<br/><br/>The proposed research will bring the fields of physics, chemistry, biology, and computer science closer intellectually and mutually enrich them. For example, conceptual frameworks and mathematical tools capturing the manipulation of information at the molecular level may yield critical insights into the design principles of evolved biological regulatory networks. Further, understanding how information processing is possible in the disordered world of chemistry could result in error-robust electronic computing. The project will also contribute to the development of undergraduate and graduate courses, which will train students to apply the principles of computer science and electrical engineering in traditionally incompatible domains. This will encourage the next generation of scientists to break through traditional disciplinary barriers and create the scientific and engineering fields of tomorrow. <br/><br/>This proposal will answer foundational questions about the computational power of chemical kinetics (chemical reaction networks). How can chemicals be programmed to have desired behaviors? How much molecular energy does such computation consume? How much ""more computation"" does every additional chemical reaction enable? Recent advances in DNA nanotechnology (strand displacement cascades) demonstrate that molecular systems of complex functionality can be designed and constructed from the ground up. This proposal will help precisely delineate the capabilities and limitations of this technology, resulting in smaller, simpler DNA-based circuits. This proposal also introduces a new paradigm, based on the laws of thermodynamics, for programming DNA-DNA interactions. As chemical and biological systems are comprised of molecules that are inherently information-rich and programmable, principles of computer science will help design smart molecules."
"1619343","AF:Small:Collaborative Research:Kinetics and Thermodynamics of Chemical Computation","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2016","06/28/2016","David Doty","CA","University of California-Davis","Standard Grant","Mitra Basu","05/31/2020","$266,000.00","","doty@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7798","7923, 7946, 9251","$0.00","Biology is replete with smart molecular systems that perform nanoscale assembly, sense environmental stimuli, create chemical signals, and produce physical motion - each of these tasks coordinated by information processing circuits implemented with chemical reactions. Learning how to build artificial chemicals that compute autonomously in complex environments would bring about groundbreaking advances in manufacturing, chemical sensing, and medicine. The theory of computation has proven invaluable in enabling information processing in electronic man-made systems, and much-studied algorithms underlie the behavior of everything from communication networks to video games. However, a thorough understanding of the principles of chemical computation is still lacking. The goal of this proposal is to use rigorous mathematical models to investigate the capabilities and limitations of chemical information processing.<br/><br/>The proposed research will bring the fields of physics, chemistry, biology, and computer science closer intellectually and mutually enrich them. For example, conceptual frameworks and mathematical tools capturing the manipulation of information at the molecular level may yield critical insights into the design principles of evolved biological regulatory networks. Further, understanding how information processing is possible in the disordered world of chemistry could result in error-robust electronic computing. The project will also contribute to the development of undergraduate and graduate courses, which will train students to apply the principles of computer science and electrical engineering in traditionally incompatible domains. This will encourage the next generation of scientists to break through traditional disciplinary barriers and create the scientific and engineering fields of tomorrow. <br/><br/>This proposal will answer foundational questions about the computational power of chemical kinetics (chemical reaction networks). How can chemicals be programmed to have desired behaviors? How much molecular energy does such computation consume? How much ""more computation"" does every additional chemical reaction enable? Recent advances in DNA nanotechnology (strand displacement cascades) demonstrate that molecular systems of complex functionality can be designed and constructed from the ground up. This proposal will help precisely delineate the capabilities and limitations of this technology, resulting in smaller, simpler DNA-based circuits. This proposal also introduces a new paradigm, based on the laws of thermodynamics, for programming DNA-DNA interactions. As chemical and biological systems are comprised of molecules that are inherently information-rich and programmable, principles of computer science will help design smart molecules."
"1658971","REU Site: Research and Development of Algorithms in a Software Factory","CCF","RSCH EXPER FOR UNDERGRAD SITES, EPSCoR Co-Funding","03/15/2017","03/15/2017","Clemente Izurieta","MT","Montana State University","Standard Grant","Rahul Shah","02/29/2020","$288,000.00","Brendan Mumey","clemente.izurieta@montana.edu","309 MONTANA HALL","BOZEMAN","MT","597172470","4069942381","CSE","1139, 9150","9150, 9250","$0.00","The objectives of Montana State University's (MSU) Computer Science (CS) Department REU project are 1) to expose undergraduate students to real-world, innovative and interdisciplinary algorithms research founded in modern software engineering techniques; 2) to encourage more undergraduates to continue their academic careers and seek graduate degrees in computer science and related disciplines; 3) to develop research skills and improve communication and collaborative skills; and 4) to foster collaboration and entrepreneurship in the development of prototype software through our innovative Software Factory environment. The REU program will aim to broaden impact and participation by recruiting high-quality undergraduate students from regional tribal colleges, students from small colleges with limited resources, as well as women engineering students to increase the gender and cultural diversity of students. The REU projects encourage students to take a systems-level perspective in solving problems from an ever-growing number of diverse commercial, educational and environmental domains. This REU will increase student awareness of cultural diversity and bolster academic confidence and interest among underrepresented groups to aid in retention and advancement, as well as preparing students in real world development environments. The program will leverage highly successful MSU campus-wide and College of Engineering programs already in place to recruit and retain underrepresented students from Montana tribal colleges.<br/><br/>By using a realistic software development environment - the Software Factory, the PIs will heighten undergraduate student participation in innovative research of algorithm development. Project topics have been carefully selected by each mentor to make sure they include a significant development component. It seeks to increase the number and diversity of students entering graduate programs and research careers in computer science and related fields. The program will target a variety of topics in interdisciplinary research aligned with every mentor's area of expertise. The students will acquire key general research skills, learn how to design algorithms and develop and use tools to tackle complex problems."
"1921047","AF: Small: Quantum Computational Pseudorandomness with Applications","CCF","QUANTUM COMPUTING","09/01/2018","02/12/2019","Fang Song","TX","Texas A&M Engineering Experiment Station","Standard Grant","Dmitri Maslov","09/30/2021","$268,826.00","","fsong@pdx.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7928","7923, 7928","$0.00","Pseudo-randomness, an efficient approximation for true randomness, has become indispensable in algorithm design, coding theory, cryptography and complexity theory. This project aims to develop a comprehensive theory of computational pseudo-randomness in the setting of quantum information processing. These pseudo-random objects and tools can be useful in quantum algorithm design and quantum complexity theory. The computational approach of this project to some problems outside the conventional territory of computing could stimulate further collaboration between computer scientists, quantum information theorists and physicists.  Course development, assisting the development of local ``Women in CS'' chapter and ``Women in Tech'' events, establishing interest groups in quantum computing at the university to attract underrepresented students, as well as outreach to high school students are an integral part of this award.<br/><br/>This specific focus is on computational pseudorandomness, which is indistinguishable from true randomness as far as efficient observers are concerned. There are three major objectives: 1) formalize and design pseudorandom quantum states and quantum operators, in analogy to two basic classical pseudorandom objects -- pseudorandom generators and pseudorandom functions; 2) investigate their applications in computer science, especially in quantum cryptography such as constructing quantum money, quantum authentication, and a novel primitive of tokenized cryptography. This requires developing appropriate quantum security models and designing new schemes; 3) develop other quantum pseudorandom objects and explore applications beyond computer science such as understanding black holes and thermalization in physics. The proposed pseudorandom objects and techniques to be developed can provide more efficient solutions to some proposed applications or even overcome some no-go results in the information-theoretical setting. This study complements the work on quantum state and unitary designs, which are statistical approximations to the quantum Haar randomness. Together, they can reveal more insights to the fundamental properties of quantum information. The computational lens of studying problems beyond computer science can be fruitful elsewhere.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816869","AF: Small: Quantum Computational Pseudorandomness with Applications","CCF","QUANTUM COMPUTING","10/01/2018","05/24/2018","Fang Song","OR","Portland State University","Standard Grant","Dmitri Maslov","03/31/2019","$268,826.00","","fsong@pdx.edu","1600 SW 4th Ave","Portland","OR","972070751","5037259900","CSE","7928","7923, 7928","$0.00","Pseudo-randomness, an efficient approximation for true randomness, has become indispensable in algorithm design, coding theory, cryptography and complexity theory. This project aims to develop a comprehensive theory of computational pseudo-randomness in the setting of quantum information processing. These pseudo-random objects and tools can be useful in quantum algorithm design and quantum complexity theory. The computational approach of this project to some problems outside the conventional territory of computing could stimulate further collaboration between computer scientists, quantum information theorists and physicists.  Course development, assisting the development of local ``Women in CS'' chapter and ``Women in Tech'' events, establishing interest groups in quantum computing at the university to attract underrepresented students, as well as outreach to high school students are an integral part of this award.<br/><br/>This specific focus is on computational pseudorandomness, which is indistinguishable from true randomness as far as efficient observers are concerned. There are three major objectives: 1) formalize and design pseudorandom quantum states and quantum operators, in analogy to two basic classical pseudorandom objects -- pseudorandom generators and pseudorandom functions; 2) investigate their applications in computer science, especially in quantum cryptography such as constructing quantum money, quantum authentication, and a novel primitive of tokenized cryptography. This requires developing appropriate quantum security models and designing new schemes; 3) develop other quantum pseudorandom objects and explore applications beyond computer science such as understanding black holes and thermalization in physics. The proposed pseudorandom objects and techniques to be developed can provide more efficient solutions to some proposed applications or even overcome some no-go results in the information-theoretical setting. This study complements the work on quantum state and unitary designs, which are statistical approximations to the quantum Haar randomness. Together, they can reveal more insights to the fundamental properties of quantum information. The computational lens of studying problems beyond computer science can be fruitful elsewhere.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1741615","CAREER: Common Links in Algorithms and Complexity","CCF","ALGORITHMIC FOUNDATIONS","01/20/2017","02/15/2019","Ryan Williams","MA","Massachusetts Institute of Technology","Continuing grant","Tracy Kimbrel","11/30/2020","$386,297.00","","rrw@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","1045, 7926, 7927","$0.00","The field of algorithm design builds clever programs that can quickly solve computational problems of interest. The field of complexity theory mathematically proves ""lower bounds,"" showing that no such clever program exists for (other) core problems. Intuitively, it appears that these two fields work on polar-opposite tasks. The major goal of this project is to discover counter-intuitive new connections between algorithm design and complexity theory, and to study the scientific consequences of the bridges built by these connections. It is hard to overestimate the potential impact---societal, scientific, and otherwise---of a theoretical framework which would lead to a fine-grained understanding of what computers can and cannot do. This project is focused on exploring concrete steps towards a better understanding, via studying links between the seemingly opposite tasks of algorithms and lower bounds. Another goal of the project is to bring complexity research closer to real-world computing, and to introduce practitioners to aspects of complexity that will impact their work. A final goal is educational outreach, through online forums dedicated to learning computer science, teaching summer school courses, and collaboration with the media on communicating theoretical computer science (including links between algorithms and lower bounds) to the public.<br/><br/>The PI seeks common links between algorithms and complexity: counter-intuitive similarities and bridges which will lead to greater insight into both areas. A central question in computer science is the famous P versus NP open problem, which is about the difficulty of combinatorial problems which admit short solutions. Such problems can always be solved via ?brute force?, trying all possible solutions. Can brute force always be replaced with a cleverer search method? This question is a major one; no satisfactory answers are known, and concrete answers seem far away. The conventional wisdom is that in general, brute force cannot be entirely avoided, but it is still mathematically possible that most natural search problems can be solved extremely rapidly, without any brute force. <br/><br/>Computational lower bounds are among the great scientific mysteries of our time: there are many conjectures and beliefs about them, but concrete results are few. Moreover, the theory is hampered by ?complexity barriers? which show that most known proof methods are incapable of proving strong lower bounds. The PI's long-term objective is to help discover and develop new ways of thinking that will demystify lower bounds, and elucidate the limits of possibilities of computing. The PI hypothesizes that an algorithmic perspective on lower bounds is the key: for example, earlier work of the PI shows that algorithms for the circuit satisfiability problem (which slightly beat brute force search) imply circuit complexity lower bounds. The PI has developed several new links within the past few years, and has proposed many more to be investigated. Among the various angles explored in this project, the potential scientific applications are vast, ranging from logical circuit design, to network algorithms, to improved hardware and software testing, to better nearest-neighbor search (with its own applications in computer vision, DNA sequencing, and machine learning), and to cryptography and security."
"1850443","CRII: AF: Markov Chain Monte Carlo Algorithms for Spin Systems","CCF","ALGORITHMIC FOUNDATIONS","07/01/2019","01/30/2019","Antonio Blanca Pimentel","PA","Pennsylvania State Univ University Park","Standard Grant","Rahul Shah","06/30/2021","$175,000.00","","azb1015@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7796","7796, 7926, 8228, 9102","$0.00","Generating samples from probability distributions is a fundamental computational task in science, engineering, and technology. Efficient and unbiased sampling algorithms have a significant impact in a variety of fields, notably including statistics, biology, physics, and computer science. The Markov chain Monte Carlo (MCMC) method provides a powerful class of sampling algorithms used by an active community of researchers in diverse applications, typically relying on heuristics for their design and analysis.  This project focuses on the theoretical study of MCMC algorithms, aiming to increase their reliability and to reduce computational costs in practice. Two application areas will be most relevant: statistical physics and machine learning. Most of the work will be carried out in close collaboration with graduate students; the training provided will be conducive to their development as researchers. <br/><br/>The PI will consider the problem of sampling from Gibbs (or Boltzmann) distributions in the context of spin systems, a general framework for modeling interacting systems of simple elements. In this context, the PI intends to rigorously analyze the efficiency of popular MCMC algorithms. Two specific foci will be the Alternating Scan dynamics for bipartite systems and the Swendsen-Wang algorithm for the classical Ising/Potts model from statistical physics. The former is actively used to train Restricted Boltzmann Machines and build sophisticated deep learning architectures, while the latter is a standard method for sampling from the Ising/Potts distribution. The PI will also study the interplay between the efficiency of MCMC algorithms and the phase transitions of the underlying probabilistic model.  Additional computational implications of these phase transitions will be explored in the context of structure learning, a closely related supervised learning problem. As such, the connections between theoretical computer science, statistical physics, and machine learning will play a central role in this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844434","EAGER: Collaborative Research: Tensor Network Methods for Quantum Simulations","CCF","CYBERINFRASTRUCTURE, QUANTUM COMPUTING","10/01/2018","07/27/2018","Eduardo Mucciolo","FL","University of Central Florida","Standard Grant","Dmitri Maslov","09/30/2020","$109,222.00","","mucciolo@physics.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7231, 7928","057Z, 7916, 7928","$0.00","Recent advances in quantum technologies have made small, high-quality quantum computers with tens of qubits finally available.  While these machines are still too small to make an impact on areas such as cryptography, they are big enough to study physical and chemical systems of relevance to materials science and chemistry, thus helping us better understand the origins of certain materials properties and how certain important chemical reactions take place. This project aims at developing novel simulation techniques to help benchmark these quantum machines.  These simulations will run on ordinary computers, but will make use of cloud computing services to scale up the system sizes as far as possible. The simulation techniques will also be used to investigate other quantum systems of relevance to physics and materials science that are not yet accessible to quantum hardware. Advancing knowledge in those areas is essential for developing better and stronger materials, as well as faster and smaller electronics. These projects will have the additional societal benefit of training graduate students in a very interdisciplinary area of research, at the interface between computer science and physics, thus helping bring highly-sought skills into the workforce.<br/><br/>The project consists of developing and deploying a novel method to simulate quantum many-body systems using tensor networks.  The method is based on the state history representation of the quantum dynamical evolution, as expressed in the Keldysh-Schwinger formalism.  Thus, rather than using the tensor network to represent the evolution of the probability amplitude of a state vector over time, the method uses the tensor network to represent the evolution itself, such that the full contraction of the network directly calculates quantities such as the expectation value of an observable or a two-point correlation function.  In this approach, entanglement is kept low, resulting in low bond dimensions on the network links, making contractions more amenable to exact computations.  For the contraction, a two-step contraction-decimation scheme is used to collapse the network.  More specifically, the scheme consists of the removal of local entanglement by compressing the information via singular value decomposition, followed by the decimation of the network by selectively removing rows or columns of the network.  This contraction scheme will be coded to optimally utilize the resources available on the largest instances of commercial cloud computing services.  The codes developed during the project will be made available through public repositories.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1533926","XPS: FULL: FP: Collaborative Research: Advancing autovectorization","CCF","Exploiting Parallel&Scalabilty","08/01/2015","07/20/2015","Alexander Veidenbaum","CA","University of California-Irvine","Standard Grant","Anindya Banerjee","07/31/2019","$338,006.00","Alexandru Nicolau","alexv@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","8283","","$0.00","Title: XPS:FULL:FP:Collaborative Research:Advancing autovectorization<br/><br/>The goal of this project is to advance the state of the art in autovectorization. This is a technique applied by compilers to automatically transform computer programs so that they can take advantage of the vector devices found in most processors. Today, most compilers have autovectorization capabilities, but their effectiveness is limited. The intellectual merit of this project lies in its potential to advance an important and beautiful core area of computer science, compiler technology, by creating new techniques and extending our understanding of programming patterns, program analysis, and transformation techniques. Beyond computer science, the project's broader significance and importance is that its results aim at increasing the fraction of code segments that, without human intervention, make use of vector devices. The effect of this increase is the acceleration of computer programs and the reduction of the energy that they consume.  Faster programs are of great importance in all application areas, but are particularly important in science and engineering where computing speed is an enabler of discoveries and better designs.  <br/><br/>The research strategy is to develop and evaluate a prototype autovectorizer based on the exploration of the space of equivalent versions of a program guided by an intelligent search engine. The space of equivalent versions is obtained with a source-to-source restructurer. A repository of codelets is planned in order to train the search engine so that it becomes capable of guiding the selection in the space of possibilities in order to identify a highly efficient version of the code."
"1527193","AF: Small: Subdivision Methods: Correctness and Complexity","CCF","ALGORITHMIC FOUNDATIONS","09/01/2015","08/20/2015","Michael Burr","SC","Clemson University","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2019","$246,411.00","","burr2@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","7796","7923, 7933, 9150","$0.00","Subdivision-based algorithms can solve problems from a wide variety of applications in mathematics, computer science, and the sciences.  For example, these types of algorithms are used in computer graphics, mathematical biology, computational geometry, mathematical modeling, robotics, machine learning, and mathematical computation.  Subdivision-based algorithms are popular because they are relatively easy to describe and implement on a computer, and they are often efficient in practice.  The work in this project is to quantify and improve the effectiveness of these types of algorithms.  By studying the efficiency and providing algorithms to approximate solutions to problems which are typically considered intractable, the results of this project provide techniques which can be applied to practical problems throughout the sciences.<br/><br/>Subdivision-based algorithms recursively and adaptively subdivide a given domain into smaller regions until, in each smaller region, the behavior of a problem-specific feature can be determined.  Subdivision-based algorithms are frequently used because they are parallelizable, recursive, and adaptive.  More precisely, they use weak local tests and perform more subdivisions only near difficult features.  These features that make subdivision-based algorithms practical, however, also make them challenging to study.  For example, local tests make global topological correctness difficult and adaptive (non-uniform) subdivisions make the number of subdivisions difficult to bound.  This project addresses both of the important questions of complexity and correctness for subdivision-based algorithms in the following two ways: (1) Using continuous amortization as a uniform method to compute the complexity of subdivision-based algorithms.  (2) Developing topologically certified subdivision-based algorithms for geometric applications on algebraic varieties.  This project extends the technique of continuous amortization to many different types of algorithms including iterative and two-dimensional subdivisions; additionally, the project develops subdivision-based algorithms to approximate previously intractable problems such as the medial axis and intersections of surfaces."
"1613905","AF: Small: General Linear Multimethods for the Time Integration of Multiscale Multiphysics Problems","CCF","CI REUSE, ALGORITHMIC FOUNDATIONS","08/01/2016","08/01/2016","Adrian Sandu","VA","Virginia Polytechnic Institute and State University","Standard Grant","Balasubramanian Kalyanasundaram","07/31/2019","$500,000.00","","sandu@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","6892, 7796","7433, 7923, 7933","$0.00","Many fields in science and engineering rely on computer simulations of time-dependent multiscale multiphysics systems. These fields include mechanical and chemical engineering, aeronautics, astrophysics, plasma physics, meteorology and oceanography, finance, environmental sciences, and urban modeling.<br/><br/>Multiscale problems, by definition, have interacting components that evolve at different temporal or spatial scales. For example, the coupled Earth includes the atmosphere (evolving at time scales of minutes) interacting with oceans (time scales of hours) and with polar ice caps (time scales of days). Computer simulations track the evolution of systems at discrete time steps -- the short time steps needed to track rapid atmospheric changes are inefficient for modeling the slow-changing ice. Even within the atmosphere models for local mixing chemical reactions (miliseconds) will be coupled to models of long distance flow of pollutants (weeks).  No single time discretization algorithm can efficiently model all processes. Improvements in simulation capabilities require the development of new flexible time integration algorithms that preserve the accuracy and stability of the component models.<br/><br/>This project will develop a rigorous approach to the construction and analysis of multirate multimethod discretizations for time-dependent partial differential equations (PDEs)in the framework of General Linear Methods (GLMs). GLMs extend traditional integration schemes such as Runge-Kutta and Linear Multistep, and enjoy favorable mathematical properties that make them very well-suited for the construction and study of multimethods. The new time stepping algorithms will enjoy the following properties:  (1) different time steps can be used in different subdomains to achieve efficiency; (2) different discretizations can be applied to operators modeling different physical processes; (3) they will have high order of temporal accuracy, and allow for efficient time step and error control;   and (4) linear and nonlinear stability constraints impose only local restrictions on the step size.  The new methods will be implemented in high quality software, and will be applied to real-life simulations arising in the prediction of atmospheric pollution.<br/><br/>The outcomes of this research will advance the field of large, multiscale, multiphysics simulations, which will benefit several major fields in science and engineering. The development of novel computational high performance computing algorithms and their application to real problems provide an excellent opportunity for training postdocs and graduate students. The PI will broadly disseminate the algorithms and software developed during this research."
"1714779","AF: Small: Lower Bounds for Computational Models, and Relations to Other Topics in Computational Complexity","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","07/10/2017","Ran Raz","NJ","Princeton University","Standard Grant","Tracy Kimbrel","08/31/2020","$450,000.00","","ranr@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7796","7923, 7927","$0.00","Computational complexity theory is a mathematical field that studies the limits of computers and the resources needed to perform computational tasks. A mathematical theory of computation is crucial in our information age, where computers are involved in essentially every part of our life. Computational complexity is also essential in designing efficient communication protocols, secure cryptographic protocols and in understanding human and machine learning. Studying the limits of computational models is among the most exciting, most challenging, and most important topics in theoretical computer science and is essential for understanding the power of computation and for the development of a theory of computation.<br/><br/>The project will study lower bounds for the resources required by different computational models, as well as related topics in computational complexity. The project will focus on three main research directions:<br/><br/>Time-Space lower bounds for learning: In a sequence of recent works, the PI and his coauthors proved that some of the most extensively studied learning problems require either a super-linear memory size or a super-polynomial number of samples. The project will further study memory/samples lower bounds for learning and their relations to other topics in complexity theory. Lower bounds for learning under memory constraints demonstrate the importance of memory in learning and cognitive processes. They may be relevant to understanding human learning and may have impact on machine learning, artificial intelligence and optimization. They also have applications in cryptography.<br/><br/>Lower bounds for arithmetic circuits: The project will study lower bounds for arithmetic circuits and formulas, as well as for subclasses of arithmetic circuits and formulas. Lower bounds for arithmetic circuits may have a broader impact within theoretical computer science, because of the centrality of polynomials in theoretical computer science.<br/><br/>Lower bounds for communication complexity: In a sequence of recent works, the PI and his coauthors proved the first gaps between information complexity and communication complexity. These results show that compression of interactive communication protocols to their information content is not possible, and hence show that interactive analogs to Shannon's source coding theorem and Huffman coding are not possible. Separation results of communication complexity and information complexity may be relevant to electrical engineering and in particular to the design of efficient communication protocols. The project will further study these topics, and more generally, lower bounds for communication complexity and their relations to other topics in computational complexity."
"1564025","AF: Medium: Collaborative Research: Top-down algorithmic design of structured nucleic acid assemblies","CCF","SOFTWARE & HARDWARE FOUNDATION, COMPUTATIONAL BIOLOGY","04/01/2016","07/20/2017","Mark Bathe","MA","Massachusetts Institute of Technology","Continuing grant","Mitra Basu","03/31/2020","$431,302.00","","mark.bathe@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7798, 7931","7924, 7946","$0.00","The past decade has witnessed dramatic growth in ability to ""print"" complex nanometer-scale structures and patterns using self-assembling nucleic acids. These structures can be used as templates to synthesize inorganic materials on the 1-100 nanometer-scale, or employed directly in applications such as DNA-based memory storage, therapeutic delivery, single-molecule structure-determination, and nanoscale excitonic materials. While various computational strategies are available to forward design these complex 3D structures manually from underlying DNA or RNA sequence and topology, the inverse problem of autonomously generating linear nucleic acid sequences from target geometry alone remains an unsolved computational challenge. In this project, fully automatic, top-down computer-aided design (CAD) algorithms are explored to generate topological sequence designs for broad classes of programmed DNA and RNA assemblies in an autonomous manner using target geometry alone. These assemblies can be ""printed"" via self-assembly in vitro or in vivo to form target nanoscale geometries using either synthetic or transcribed nucleic acids. The approach will offer a broadly accessible, high-level programming language to realize sequence-based programming of arbitrary 1D/2D/3D nanoscale structured materials based on nucleic acids with diverse applications in basic science and nanotechnology.<br/><br/><br/>The proposed computational algorithms will be distributed freely online as open source software as well as integrated into a variety of software packages to broadly enable the top-down design of DNA and RNA assemblies. These algorithms and software will provide the broader scientific and industrial communities with easy-to-use, high-level design strategies that will accelerate the broad participation of groups in the use of nucleic acid nanotechnology for diverse applications in biomolecular and materials science and technology. The tools will open up opportunities for high school students and undergraduates to gain hands-on experience in nucleic acid nanostructure design. Curriculum developments at ASU and MIT will employ the use of this sequence design software for participation by undergraduate and graduate students in its use and application to basic questions in computer science and nanotechnology research.<br/><br/><br/>Foundational aspects of the design of nanoscale structured materials using DNA and RNA will be explored. Algorithmic approaches to rendering diverse CAD-based geometric primitives using DNA and RNA will be investigated, including wireframe lattices in 2D and 3D, single-layer surfaces that may contain arbitrary curvatures, as well as 3D solid objects. Meshing algorithms will be used to discretize geometric objects in 1D, 2D, and 3D, and topological routing and sequence design will be applied to position nucleic acid strands within CAD objects. Continuous and discontinuous single stranded nucleic acids will be routed through duplexes using anti-parallel and parallel crossover configurations to exploit distinct modes of programmed self-assembly. Sequence design and routing will be validated experimentally to explore principles for obtaining optimal folding, self-assembly, and positioning of specific base pairs in 3D space. Self-assembly of nanostructures from RNA will additionally be explored, utilizing staple-free designs from single long continuous scaffold strands. Close interaction between experiment and computation will help to distill fundamental yet practical approaches to programming structured nucleic acid assemblies."
"1563799","AF: Medium: Collaborative Research: Top-down algorithmic design of structured nucleic acid assemblies","CCF","ALGORITHMIC FOUNDATIONS, SOFTWARE & HARDWARE FOUNDATION","04/01/2016","07/20/2017","Hao Yan","AZ","Arizona State University","Continuing grant","Mitra Basu","03/31/2020","$393,155.00","","hao.yan@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7796, 7798","7924, 7946","$0.00","The past decade has witnessed dramatic growth in ability to ""print"" complex nanometer-scale structures and patterns using self-assembling nucleic acids. These structures can be used as templates to synthesize inorganic materials on the 1-100 nanometer-scale, or employed directly in applications such as DNA-based memory storage, therapeutic delivery, single-molecule structure-determination, and nanoscale excitonic materials. While various computational strategies are available to forward design these complex 3D structures manually from underlying DNA or RNA sequence and topology, the inverse problem of autonomously generating linear nucleic acid sequences from target geometry alone remains an unsolved computational challenge. In this project, fully automatic, top-down computer-aided design (CAD) algorithms are explored to generate topological sequence designs for broad classes of programmed DNA and RNA assemblies in an autonomous manner using target geometry alone. These assemblies can be ""printed"" via self-assembly in vitro or in vivo to form target nanoscale geometries using either synthetic or transcribed nucleic acids. The approach will offer a broadly accessible, high-level programming language to realize sequence-based programming of arbitrary 1D/2D/3D nanoscale structured materials based on nucleic acids with diverse applications in basic science and nanotechnology.<br/><br/><br/>The proposed computational algorithms will be distributed freely online as open source software as well as integrated into a variety of software packages to broadly enable the top-down design of DNA and RNA assemblies. These algorithms and software will provide the broader scientific and industrial communities with easy-to-use, high-level design strategies that will accelerate the broad participation of groups in the use of nucleic acid nanotechnology for diverse applications in biomolecular and materials science and technology. The tools will open up opportunities for high school students and undergraduates to gain hands-on experience in nucleic acid nanostructure design. Curriculum developments at ASU and MIT will employ the use of this sequence design software for participation by undergraduate and graduate students in its use and application to basic questions in computer science and nanotechnology research.<br/><br/><br/>Foundational aspects of the design of nanoscale structured materials using DNA and RNA will be explored. Algorithmic approaches to rendering diverse CAD-based geometric primitives using DNA and RNA will be investigated, including wireframe lattices in 2D and 3D, single-layer surfaces that may contain arbitrary curvatures, as well as 3D solid objects. Meshing algorithms will be used to discretize geometric objects in 1D, 2D, and 3D, and topological routing and sequence design will be applied to position nucleic acid strands within CAD objects. Continuous and discontinuous single stranded nucleic acids will be routed through duplexes using anti-parallel and parallel crossover configurations to exploit distinct modes of programmed self-assembly. Sequence design and routing will be validated experimentally to explore principles for obtaining optimal folding, self-assembly, and positioning of specific base pairs in 3D space. Self-assembly of nanostructures from RNA will additionally be explored, utilizing staple-free designs from single long continuous scaffold strands. Close interaction between experiment and computation will help to distill fundamental yet practical approaches to programming structured nucleic acid assemblies."
"1714275","AF: Small: Classification Program for Counting Problems","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/10/2017","Jin-Yi Cai","WI","University of Wisconsin-Madison","Standard Grant","Tracy J. Kimbrel","08/31/2020","$450,000.00","","jyc@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7796","7923, 7926, 7927","$0.00","This project is a study of the computational complexity of counting problems.  The PI aims to classify the complexity of problems known as Sum-of-Product computations.  These counting problems come from all parts of computer science, and even other fields of study.  They are naturally defined and include such counting problems as vertex covers, graph colorings, and graph matchings.  There is also a strong connection to problems studied in statistical physics.<br/><br/>In computational complexity theory, there is no higher aim than to achieve a complete classification of a wide class of computational problems.  This is usually done in terms of the P and NP theory, where P and NP denote problems computable in polynomial time by deterministic and nondeterministic algorithms, respectively.  There has been strong interest in the PI's classification program, especially with the concept of holographic algorithms.  There is also a significant amount of computational experimentation in the search for the right formulation of the classification, providing an opportunity to engage undergraduate students in research.<br/><br/>A sharper delineation between what is or is not efficiently computable will have broader impact within computer science and beyond.  Within CS, a substantial body of work in AI is centered around similar models called partition functions.  Outside computer science, there is a long tradition in statistical physics to study partition functions, and this study informs the so-called exactly solved models.<br/><br/>In more technical terms, these are computations defined as sum_sigma prod_f f | sigma, where the f's are local constraint functions, and the sigmas are assignments to local variables.  There are three related frameworks to study these problems.<br/><br/>(1) Spin systems  or graph homomorphisms,<br/>(2) Counting CSP problems, and <br/>(3) Holant problems.<br/><br/>Over the past several years, the following thesis has gained considerable evidence, namely a large family of Sum-of-Product computations can be classified into exactly three categories with an explicit criterion on the constraint function set:<br/><br/>(I) Computable in P;<br/>(II) #P-hard for general graphs, but solvable in P for planar graphs; and<br/>(III) #P-hard even for planar graphs.<br/><br/>Furthermore, for Spin systems and Counting CSP, category (II) corresponds precisely to those problems which can be solved by holographic algorithms with matchgates.  But for Holant problems, there are additional novel tractable classes of problems.  The PI plans to prove classification theorems that apply to asymmetric constraint functions. If this can be settled for asymmetric as well as symmetric constraint functions, it will be a unifying result, answering questions that are open at least since the time of Kasteleyn in the 1960's."
"1524417","CIF: Small: Networks: Evolution, Learning and Social Norms","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2015","08/07/2015","Mihaela van der Schaar","CA","University of California-Los Angeles","Standard Grant","Phillip Regalia","09/30/2019","$482,627.00","","mihaela@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7797","7923, 7935","$0.00","Much of society is organized in networks, be they enterprise, scientific, education, societal, social, economic, etc. This project aims to study how networks evolve, which networks emerge, how information about the individuals in the network affect the evolution and the eventual shape of the network and conversely, how the evolution of the network affect learning about the individuals in the network.  Closely related issues are how the evolution and the shape of the network affect the functioning of the network, and what social norms promote the formation and maintenance of well-functioning of networks. This research project will enable us to better understand, monitor, design, operate and secure networks of many kinds.  In particular, this research will help design new policies for creating a safer and more creative Internet.<br/><br/>A sizeable network science literature studies networks that have already formed; a smaller literature, mainly in microeconomics, studies the formation of networks---but makes heroic assumptions (e.g., homogeneous agents/entities, complete information about other agents) that deviate from reality. Neither the network science literature nor the microeconomics literature takes into account that agents behave strategically in deciding what information to consume, produce and share (in addition to deciding what links to form/maintain/break) and that agents begin with incomplete information about others (i.e., they must learn about others). As a result, neither network science nor microeconomics provides a useful methodology for understanding, predicting and guiding the formation (and evolution) of real networks and the consequences of network formation. The overarching goal of this research project is to develop such a methodology. The research plan will take into account that agents behave strategically and that they begin with incomplete information about each other and thus must learn over time what information to produce and consume, and which connections to form and maintain and which to break. Thus, agents? learning and the network structure coevolve over time. A distinguishing feature of the current project is the application of social norms to understand and promote the evolution and well-functioning of networks."
"1839232","RAISE-TAQS: Entanglement and information in complex networks of qubits","CCF","OFFICE OF MULTIDISCIPLINARY AC","10/01/2018","09/13/2018","Zhexuan Gong","CO","Colorado School of Mines","Standard Grant","Dmitry Maslov","09/30/2021","$985,926.00","Lincoln Carr, Meenakshi Singh, Cecilia Diniz Behn","gong@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","1253","049Z, 057Z, 7928","$0.00","Quantum computers are now approaching a size that will soon perform tasks surpassing the power of today's fastest classical computers. To attain the full power of a quantum computer, qubits inside the quantum computer should be well connected with each other, so that information can be transferred, and entanglement can be generated between any two qubits as fast as possible. The qubit interactions will form a complex and time-varying network and their dynamics will be too complicated for classical computers to predict. This project will provide a key step in understanding quantum systems of a rapidly increasing level of complexity and find out how such complexity can be employed to massively speed up quantum computing over systems with sparse and simple qubit connections. The project will expand the fields of quantum information science and condensed matter physics into the territory of complexity science, via concrete ways to quantify complexity of quantum states. As the quantum information frontier is fostering a new technological revolution around the world, the project will train a new generation of undergraduate and graduate students with expertise in quantum technologies and develop a new 12-credit graduate certificate program in quantum engineering to accommodate the pressing need from industry professionals in obtaining quantum expertise.<br/><br/>The first half of this project aims to find out how high qubit connectivity can be used to speed up quantum information processing, focusing on mathematical quantum speed limits akin to Lieb-Robinson bounds, optimal entangling protocols for very small or very large number of qubits, and experimental demonstrations of entangling speed limit using solid-state qubits. The second half of this project will focus on states created by Hamiltonians with dense and complex interactions, quantifying their complexity and understanding their entanglement structure. Various network measures borrowed from complexity science will be employed to study the experimentally measurable quantum mutual information network and the recently developed quantum neural network, in order to bring new insight to quantum critical phenomena, entanglement area laws, and nonequilibrium many-body dynamics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836965","RAISE: Software for Making: Programmable Geometry for Mathematics Education, Classical Stringed Instrument Design, and its Material Culture","CCF","AISL, SOFTWARE & HARDWARE FOUNDATION","10/01/2018","09/04/2018","Harry Mairson","MA","Brandeis University","Standard Grant","Anindya Banerjee","09/30/2021","$420,761.00","","mairson@brandeis.edu","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","7259, 7798","049Z, 7943","$0.00","This Research Advanced by Interdisciplinary Science and Engineering (RAISE) project is supported by the Division of Research on Learning in the Education and Human Resources Directorate and by the Division of Computing and Communication Foundations in the Computer and Information Science and Engineering Directorate. This interdisciplinary project integrates historical insights from geometric design principles used to craft classical stringed instruments during the Renaissance era with modern insights drawn from computer science principles.  The project applies abstract mathematical concepts toward the making and designing of furniture, buildings, paintings, and instruments through a specific example: the making and designing of classical stringed instruments.  The research can help instrument makers employ customized software to facilitate a comparison of historical designs that draws on both geometrical proofs and evidence from art history.  The project's impacts include the potential to shift in fundamental ways not only how makers think about design and the process of making but also how computer scientists use foundational concepts from programming languages to inform the representation of physical objects.  Furthermore, this project develops an alternate teaching method to help students understand mathematics in creative ways and offers specific guidance to current luthiers in areas such as designing the physical structure of a stringed instrument to improve acoustical effect.<br/><br/>The project develops a domain-specific functional programming language based on straight-edge and compass constructions and applies it in three complementary directions. The first direction develops software tools (compilers) to inform the construction of classical stringed instruments based on geometric design principles applied during the Renaissance era. The second direction develops an analytical and computational understanding of the art history of these instruments and explores extensions to other maker domains. The third direction uses this domain-specific language to design an educational software tool. The tool uses a calculative and constructive method to teach Euclidean geometry at the pre-college level and complements the traditional algebraic, proof-based teaching method. The representation of instrument forms by high-level programming abstractions also facilitates their manufacture, with particular focus on the arching of the front and back carved plates --- of considerable acoustic significance --- through the use of computer numerically controlled (CNC) methods. The project's novelties include the domain-specific language itself, which is a programmable form of synthetic geometry, largely without numbers; its application within the contemporary process of violin making and in other maker domains; its use as a foundation for a computational art history, providing analytical insights into the evolution of classical stringed instrument design and its related material culture; and as a constructional, computational approach to teaching geometry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1423414","Design Automation for Paper Microfluidics","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2014","05/05/2017","Philip Brisk","CA","University of California-Riverside","Standard Grant","Sankar Basu","09/30/2019","$348,000.00","William Grover","philip@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7923, 7945, 9251","$0.00","Low-cost and disposable paper microfluidic devices are poised to make radical changes in applications such as point-of-care diagnostics, biothreat detection, and food/water supply and environmental monitoring. Scientists presently must design new devices for each application by hand, which is time-consuming, tedious, and error-prone. This proposal will adapt the semiconductor industry design methodologies to paper microfluidic technologies, while leveraging standard equipment such as inkjet printers and paper cutters for fabrication. This project will lower the barrier-to-entry for users of paper microfluidic devices, and will promote their widespread use among scientists.<br/> <br/>The broader impacts of existing paper microfluidic devices include products such as pregnancy tests, HIV tests, drug tests, and others, each of which is a one-off device designed by hand. The productivity enhancements attained through this project will reduce the design cost of paper microfluidic devices, and these savings can be passed directly to the consumer. At the University of California, Riverside, the PI and Co-PI will introduce an interdisciplinary undergraduate course on paper microfluidics targeting students from departments such as Bioengineering, Biology, Biomedical sciences, Computer Science/Engineering, Chemical and Environmental Engineering, Environmental Sciences, Neuroscience, and the School of Medicine. The PI and co-PI will continue their ongoing successful efforts to include women and underrepresented minority students in this project, and will disseminate tutorials, open source software, and educational materials developed in the course of this project to the wider scientific/academic community."
"1533663","XPS: FULL: CCA: Collaborative Research: Automatically Scalable Computation","CCF","Exploiting Parallel&Scalabilty","08/01/2015","07/27/2015","Steven Homer","MA","Trustees of Boston University","Standard Grant","M. Mimi McClure","07/31/2019","$350,000.00","Jonathan Appavoo, Ajay Joshi","homer@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","8283","","$0.00","For over thirty years, each generation of computers has been faster than the one that preceded it. This exponential scaling transformed the way we communicate, navigate, purchase, and conduct science. More recently, this dramatic growth in single processor performance has stopped and has been replaced by new generations of computers with more processors on them; for example, even the cell phones we carry have multiple processors in them.  Writing software that effectively leverages multiple processing elements is difficult, and rewriting the decades of accumulated software is both difficult and costly. This research takes a different approach -- rather than converting sequential software into parallel software, this project develops ways to store and reuse computation. Imagine computing only when computer time and energy are cheap and plentiful, storing that computation, and then using it later, when computation might be limited or expensive.  The approach used involves making informed predictions about computation likely to happen in the future, proactively executing likely computations in parallel with the actual computation, and then ""jumping forward in time"" if the actual execution arrives at any of the predicted computations that have already been completed.  This research touches many areas within Computer Science, architecture, compilers, machine learning, systems, and theory.  Additionally, exploiting massively parallel computation will produce immediate returns in multiple scientific fields that rely on computation.<br/><br/>The approach used in this research views computational execution as moving a system through the enormously high dimensional space represented by its registers and memory of a conventional single-threaded processor.  It uses machine learning algorithms to observe execution patterns and make predictions about likely future states of the computation.  Based on these predictions, the system launches potentially large numbers of speculative threads to execute from these likely computations, while the actual computation proceeds serially.  At strategically chosen points, the main computation queries the speculative executions to determine if any of the completed computation is useful; if it is, the main thread uses the speculative computation to immediately begin execution where the speculative computation left off, achieving a speed-up over the serial execution.  This approach has the potential to be extremely scalable: the more cores, memory, and communication bandwidth available, the greater the potential for performance improvement. The approach also scales across programs -- if the program running today happens upon a state encountered by a program running yesterday, the program can reuse yesterday's computation. This project has the potential to break new ground for research in many areas in Computer Science touched by it."
"1533737","XPS: FULL: CCA: Collaborative Research: Automatically Scalable Computation","CCF","Exploiting Parallel&Scalabilty","08/01/2015","07/27/2015","Margo Seltzer","MA","Harvard University","Standard Grant","M. Mimi McClure","07/31/2019","$525,000.00","David Brooks, Ryan Adams","margo@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","8283","","$0.00","For over thirty years, each generation of computers has been faster than the one that preceded it. This exponential scaling transformed the way we communicate, navigate, purchase, and conduct science. More recently, this dramatic growth in single processor performance has stopped and has been replaced by new generations of computers with more processors on them; for example, even the cell phones we carry have multiple processors in them.  Writing software that effectively leverages multiple processing elements is difficult, and rewriting the decades of accumulated software is both difficult and costly. This research takes a different approach -- rather than converting sequential software into parallel software, this project develops ways to store and reuse computation. Imagine computing only when computer time and energy are cheap and plentiful, storing that computation, and then using it later, when computation might be limited or expensive.  The approach used involves making informed predictions about computation likely to happen in the future, proactively executing likely computations in parallel with the actual computation, and then ""jumping forward in time"" if the actual execution arrives at any of the predicted computations that have already been completed.  This research touches many areas within Computer Science, architecture, compilers, machine learning, systems, and theory.  Additionally, exploiting massively parallel computation will produce immediate returns in multiple scientific fields that rely on computation.<br/><br/>The approach used in this research views computational execution as moving a system through the enormously high dimensional space represented by its registers and memory of a conventional single-threaded processor.  It uses machine learning algorithms to observe execution patterns and make predictions about likely future states of the computation.  Based on these predictions, the system launches potentially large numbers of speculative threads to execute from these likely computations, while the actual computation proceeds serially.  At strategically chosen points, the main computation queries the speculative executions to determine if any of the completed computation is useful; if it is, the main thread uses the speculative computation to immediately begin execution where the speculative computation left off, achieving a speed-up over the serial execution.  This approach has the potential to be extremely scalable: the more cores, memory, and communication bandwidth available, the greater the potential for performance improvement. The approach also scales across programs -- if the program running today happens upon a state encountered by a program running yesterday, the program can reuse yesterday's computation. This project has the potential to break new ground for research in many areas in Computer Science touched by it."
"1533912","XPS: FULL: FP:  Collaborative Research:Advancing autovectorization","CCF","Exploiting Parallel&Scalabilty","08/01/2015","07/20/2015","David Padua","IL","University of Illinois at Urbana-Champaign","Standard Grant","Anindya Banerjee","07/31/2019","$506,994.00","Gerald DeJong, Maria Garzaran","padua@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8283","","$0.00","Title: XPS:FULL:FP:Collaborative Research:Advancing autovectorization<br/><br/>The goal of this project is to advance the state of the art in autovectorization. This is a technique applied by compilers to automatically transform computer programs so that they can take advantage of the vector devices found in most processors. Today, most compilers have autovectorization capabilities, but their effectiveness is limited. The intellectual merit of this project lies in its potential to advance an important and beautiful core area of computer science, compiler technology, by creating new techniques and extending our understanding of programming patterns, program analysis, and transformation techniques. Beyond computer science, the project's broader significance and importance is that its results aim at increasing the fraction of code segments that, without human intervention, make use of vector devices. The effect of this increase is the acceleration of computer programs and the reduction of the energy that they consume.  Faster programs are of great importance in all application areas, but are particularly important in science and engineering where computing speed is an enabler of discoveries and better designs.  <br/><br/>The research strategy is to develop and evaluate a prototype autovectorizer based on the exploration of the space of equivalent versions of a program guided by an intelligent search engine. The space of equivalent versions is obtained with a source-to-source restructurer. A repository of codelets is planned in order to train the search engine so that it becomes capable of guiding the selection in the space of possibilities in order to identify a highly efficient version of the code."
"1617713","AF: Small: Randomness in Computation - Old Problems and New Directions","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","05/25/2016","Xin Li","MD","Johns Hopkins University","Standard Grant","Rahul Shah","06/30/2019","$374,815.00","","lixints@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7796","7923, 7927","$0.00","A major goal of computer science is to study how to compute more efficiently using limited resources. The understanding of this question has had profound influence on our daily life, in a variety of areas ranging from e-commerce, cloud computing, to travel planning and weather forecast. In this project the PI seeks to understand how to efficiently use the valuable resource of randomness (such as coin flips) in computation.<br/><br/>Randomness is extremely useful in computation and widely used in practice. Simulation of complex models such as those used for weather forecast and economy prediction relies on the use of random processes, and modern computer security will be lost completely without randomness. In this context, the project studies the fundamental questions of the power and limitations of randomness in computation. From a theoretical aspect, it can lead to breakthroughs towards solving long standing open questions in computer science, such as whether randomness is really necessary for algorithms. From a practical aspect, it can lead to improvements in several areas important to society, such as designing streaming and scalable computation protocols for massive datasets, enhancing computer security in an adversarial environment, and tolerating errors in communication protocols. Based on the research activities, the educational component in this project plans to train several Ph.D. students, publish online surveys for free access, integrate research outcomes into courses the PI is or will be teaching, and provide research opportunities for minority students through a joint effort with Johns Hopkins University.<br/><br/>The questions that will be addressed in this project include how to generate high quality randomness for computation, how to use randomness in the presence of information leakage or tampering by an adversary, and how to use randomness to detect and correct errors introduced in communications. Two fundamental objects and tools for studying these questions are pseudorandom generators and randomness extractors. A pseudorandom generator is an algorithm that stretches a small number of random bits into a large number of bits that appear to be perfectly random to a certain class of functions. A randomness extractor is an algorithm that converts low quality random sources into very high quality random bits. The project will explore new ways of constructing these objects, as well as the connections between these objects and other areas in computer science, such as cryptography, error correcting codes, and computational complexity. Through this the PI seeks to establish new connections between different areas, and thus leading to possible new breakthroughs."
"1539527","CyberSEES: Type 2: Human-centered systems for cyber-enabled sustainable buildings","CCF","CyberSEES","09/01/2015","08/27/2015","Panagiota Karava","IN","Purdue University","Standard Grant","Bruce Hamilton","08/31/2019","$1,200,000.00","Robert Proctor, Jianghai Hu, Athanasios Tzempelikos, Ilias Bilionis","pkarava@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8211","8208, 8231","$0.00","In the U.S., the building sector accounts for about 40% of primary energy usage, 71% of electricity and 38% of carbon dioxide emissions. For this reason, development of efficient solutions to reduce energy consumption and environmental impact of buildings is of critical societal importance. Occupants play a significant role in energy use of office buildings, affecting up to 30% of the energy use. To manage occupants' energy impact due to their presence and behavior, environmental control systems (e.g., HVAC, shading, lighting) have been automated based on the use of ""widely acceptable"" visual and thermal comfort metrics. However, occupants have a strong preference for customized indoor climate, and there is a strong relationship between occupants' perception of control over their environment and productivity, health and well-being. To address the challenge of customized control of the environment, the objective of this project is to realize a new paradigm for human-centered sustainable buildings, enabled by conducting research with computing innovations in probabilistic methods and machine learning, linked to sustainability, and with broader impacts in multiple domains of science and engineering. Broader impacts are: (1) New computing methods and algorithms on probabilistic classification, inference and optimal control that may impact a number of scientific communities, including Architectural, Mechanical, Electrical, Computer and Industrial/Human Factors Engineering, Computer and Psychological Sciences. Potential application areas include genomics, traffic flow prediction, infrastructure systems including power, transportation, etc. (2) Integration of the project's modeling, simulation, and experimental platforms into new teaching modules and experiential learning activities that support the curriculum and workforce development in four engineering schools (Civil, Mechanical, Electrical and Computer, Industrial Engineering) and the Department of Psychological Sciences. (3) Dissemination of research outcomes to the academic community and to the industry through publications, workshops, conferences and a customized external evaluation process. (4) Creation of outreach and engagement initiatives for K-12 teachers and students in STEM learning and research. <br/><br/>This project takes a multidisciplinary approach that is grounded in (1) new algorithms for automated identification of the relevant human perception-attributes of buildings; and (2) new concepts for intelligent and self-tuned comfort delivery systems for customized thermal and visual environments in buildings. The research includes: (1) Laboratory and field studies with human test-subjects that map indoor environment conditions, thermal and visual perception and comfort, occupant-building interactions and control actions, as well as corresponding space performance for perimeter building zones. (2) Probabilistic classification of human perception, comfort, and satisfaction profiles for a typical population. (3) Computationally-efficient inference algorithms for online learning of individual and population-level human preferences. (4) Optimal control algorithms and simulation tools for implementation in building management systems. The research outcomes will be integrated into a new cyber-enabled technological solution for self-tuned comfort delivery devices (thermostats, shading and lighting actuators). The experimental prototypes and field demonstrations will achieve improved performance with quantified building energy use reduction and occupant satisfaction, as well as robustness to uncertainty due to the reduction in the frequency of overrides."
"1901624","AF: Medium: Collaborative Research: Quantum-Secure Cryptography and Fine-Grained Quantum Query Complexity","CCF","QUANTUM COMPUTING","09/01/2018","10/22/2018","Fang Song","TX","Texas A&M Engineering Experiment Station","Continuing grant","Almadena Chtchelkanova","07/31/2021","$91,289.00","","fsong@pdx.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7928","7924, 7928","$0.00","Secure Internet communication faces a real threat in the form of a new breed of computer that harnesses the laws of quantum mechanics. The technical community is currently hard at work attempting to construct such ""quantum"" computers. While many mysteries about these devices remain, it is certain that a large-scale quantum computer would easily break all current public-key cryptography that underpins the current Internet. In certain attack models, important examples of private-key cryptography would also be rendered insecure. This 3-institution collaborative project studies the basic theoretical issues underlying these urgent threats to the security infrastructure. It seeks to understand the cryptography-breaking power of quantum computers, concentrating on two interweaving themes: quantum security for 1) authenticating, and 2) constructing quantum-secure cryptography from new primitives.  The project activities also include course development and mentorship at the graduate and undergraduate level. The project also involves specific outreach activities intended to broaden participation in Computer Science, including establishment and development of ""women in computer science"" chapters, outreach to local high schools, workshops for high-school STEM teachers, and development of computer science courses for a general audience at the three partner institutions.<br/><br/>Authentication-proofs, for example, that an e-mail really did originate from you--is a basic and well-studied cryptographic challenge. In the setting of quantum adversaries, it is not clear how to appropriately formulate this essential notion, let alone produce specific cryptographic tools that achieve it. This project is addressing both of the challenges noted above, focusing on development of strong formulations of authentication and new cryptographic constructions that offer secure authentication. Finding ""hidden"" algebraic structures--like the fact that two lists of numbers are merely cyclic shifts of each other--is an emblematic theme in the study of the computing power of quantum computers. Certain variants of this problem have resisted decades of concerted effort by the quantum algorithms community, and appear to be quite difficult. This project studies applications of these problems to constructing new private-key cryptographic tools with quantum security guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763736","AF: Medium: Collaborative Research: Quantum-Secure Cryptography and Fine-Grained Quantum Query Complexity","CCF","QUANTUM COMPUTING","08/01/2018","04/26/2018","Gorjan Alagic","MD","University of Maryland College Park","Continuing grant","Almadena Chtchelkanova","07/31/2021","$91,884.00","","galagic@gmail.com","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7928","7924, 7928","$0.00","Secure Internet communication faces a real threat in the form of a new breed of computer that harnesses the laws of quantum mechanics. The technical community is currently hard at work attempting to construct such ""quantum"" computers. While many mysteries about these devices remain, it is certain that a large-scale quantum computer would easily break all current public-key cryptography that underpins the current Internet. In certain attack models, important examples of private-key cryptography would also be rendered insecure. This 3-institution collaborative project studies the basic theoretical issues underlying these urgent threats to the security infrastructure. It seeks to understand the cryptography-breaking power of quantum computers, concentrating on two interweaving themes: quantum security for 1) authenticating, and 2) constructing quantum-secure cryptography from new primitives.  The project activities also include course development and mentorship at the graduate and undergraduate level. The project also involves specific outreach activities intended to broaden participation in Computer Science, including establishment and development of ""women in computer science"" chapters, outreach to local high schools, workshops for high-school STEM teachers, and development of computer science courses for a general audience at the three partner institutions.<br/><br/>Authentication-proofs, for example, that an e-mail really did originate from you--is a basic and well-studied cryptographic challenge. In the setting of quantum adversaries, it is not clear how to appropriately formulate this essential notion, let alone produce specific cryptographic tools that achieve it. This project is addressing both of the challenges noted above, focusing on development of strong formulations of authentication and new cryptographic constructions that offer secure authentication. Finding ""hidden"" algebraic structures--like the fact that two lists of numbers are merely cyclic shifts of each other--is an emblematic theme in the study of the computing power of quantum computers. Certain variants of this problem have resisted decades of concerted effort by the quantum algorithms community, and appear to be quite difficult. This project studies applications of these problems to constructing new private-key cryptographic tools with quantum security guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1412958","AF: Large: Theory of Computation - Pushing the State-of-the-Art","CCF","CYBERINFRASTRUCTURE, ALGORITHMIC FOUNDATIONS","09/01/2014","08/01/2018","Avi Wigderson","NJ","Institute For Advanced Study","Continuing grant","Tracy J. Kimbrel","08/31/2020","$2,000,003.00","TONI PITASSI, Ran Raz","avi@math.ias.edu","EINSTEIN DRIVE","PRINCETON","NJ","085404907","6097348000","CSE","7231, 7796","7925, 7927","$0.00","This project is aimed at understanding a variety of fundamental questions in the theory of computation.  It will be carried out via the postdoctoral mentoring program at the Institute for Advanced Study, and as such the specific topics of focus will evolve with the postdocs present each year. Current foci include, among others, the following:<br/><br/>- The power of formulas. <br/>Formulas is the most basic mathematical and computational descriptive mechanisms. Understanding their minimal length for natural problems captures at once limitation on the space requirements, as well as the potential parallelism inherent in the problem. The award will focus on the most challenging direction, which has so far resisted attack - proving limitation of formulas. More generally, the researchers will pursue proving limitations of other natural computational models, especially arithmetic computation.<br/><br/>- The power of relaxations<br/>The ""meta-algorithms"": Linear and semi-definite relaxations of integer programs, are among the most fruitful and powerful techniques for solving (or finding approximate solutions) to optimization problems. The award will focus on understanding the limits of these techniques. This work ties in naturally to understanding the limitations of natural proof systems and of natural strategies for search algorithms.<br/><br/>- Peeking inside the ""black-box""<br/>One of the most useful paradigms in programming and learning is the encapsulation of objects as black-boxes, to which only input-output access is allowed.  With this utility come limitations which can hopefully overcome if we are allowed some access into the internal workings of the black box.  Modeling such access, in scientific experiments, machine learning and computational complexity is a challenge the researchers plan to pursue.<br/><br/><br/>Computational complexity, a foundational core of computer science, has proved itself a remarkably deep and fruitful fountain of problems, ideas and techniques over the past decades.  The research agenda is expected to be a driver of innovation in Theoretical Computer Science and related disciplines. Some of the areas of study have potential implications outside theory, especially machine learning, coding theory, scientific discovery and more. <br/><br/>On the educational side, the mentoring program furthers the quality of IAS postdocs to serve as outstanding teachers, graduate advisors and academic leaders and innovators in one of the most exciting branches of science today.  Whether IAS alumni pursue a career in academia or industry their impact on technology and on the training of new generations undergraduate and graduate education is immense."
"1844190","EAGER: Collaborative: Tensor Networks Methods for Quantum Simulations","CCF","CYBERINFRASTRUCTURE","10/01/2018","07/27/2018","Claudio Chamon","MA","Trustees of Boston University","Standard Grant","Dmitry Maslov","09/30/2020","$188,824.00","Andrei Ruckenstein","chamon@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7231","057Z, 7916, 7928","$0.00","Recent advances in quantum technologies have made small, high-quality quantum computers with tens of qubits finally available.  While these machines are still too small to make an impact on areas such as cryptography, they are big enough to study physical and chemical systems of relevance to materials science and chemistry, thus helping us better understand the origins of certain materials properties and how certain important chemical reactions take place. This project aims at developing novel simulation techniques to help benchmark these quantum machines.  These simulations will run on ordinary computers, but will make use of cloud computing services to scale up the system sizes as far as possible. The simulation techniques will also be used to investigate other quantum systems of relevance to physics and materials science that are not yet accessible to quantum hardware. Advancing knowledge in those areas is essential for developing better and stronger materials, as well as faster and smaller electronics. These projects will have the additional societal benefit of training graduate students in a very interdisciplinary area of research, at the interface between computer science and physics, thus helping bring highly-sought skills into the workforce.<br/><br/>The project consists of developing and deploying a novel method to simulate quantum many-body systems using tensor networks.  The method is based on the state history representation of the quantum dynamical evolution, as expressed in the Keldysh-Schwinger formalism.  Thus, rather than using the tensor network to represent the evolution of the probability amplitude of a state vector over time, the method uses the tensor network to represent the evolution itself, such that the full contraction of the network directly calculates quantities such as the expectation value of an observable or a two-point correlation function.  In this approach, entanglement is kept low, resulting in low bond dimensions on the network links, making contractions more amenable to exact computations.  For the contraction, a two-step contraction-decimation scheme is used to collapse the network.  More specifically, the scheme consists of the removal of local entanglement by compressing the information via singular value decomposition, followed by the decimation of the network by selectively removing rows or columns of the network.  This contraction scheme will be coded to optimally utilize the resources available on the largest instances of commercial cloud computing services.  The codes developed during the project will be made available through public repositories.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816695","AF: Small: Provable Quantum Advantages in Optimization","CCF","OFFICE OF MULTIDISCIPLINARY AC, CYBERINFRASTRUCTURE, QIS - Quantum Information Scie","10/01/2018","07/16/2018","Xiaodi Wu","MD","University of Maryland College Park","Standard Grant","Almadena Y. Chtchelkanova","09/30/2021","$450,000.00","","xwu@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","1253, 7231, 7281","057Z, 7203, 7281, 7923, 7928","$0.00","The project aims to investigate the landscape of provable quantum advantages in optimization and machine learning, which is ubiquitous in our daily life, and to build a solid theoretical foundation for applications of quantum computing, especially with near-term quantum devices and in the establishment of quantum supremacy. By integrating modern tools in both optimization and quantum algorithm design, the project aims to design quantum algorithms for convex optimization and various semidefinite program classes, by quantizing the state-of-the-art classical optimization algorithms. The results obtained will be disseminated through a variety of venues, including conferences, new course materials, expository writings, and high school open days aimed at exposing young computer scientists to the frontiers of quantum information research. This project is jointly supported by the Algorithmic Foundations (AF) Program in the Division of Computing and Communications Foundations in the Directorate for Computer and Information Science and Engineering, and the Quantum Information Science (QIS) Program in the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This research project investigates provable quantum advantages in solving convex optimization, general and positive semidefinite programs, as well as variational optimization methods executable on near-term quantum devices. Specific targets include: (1) the sampling-based approach and the membership-to-separation approach for convex optimization; (2) optimal semidefinite program solvers based on the width-dependent and width-independent approaches as well as the interior point method. The investigator also aims to design new quantum optimization algorithms on near-term quantum devices as well as to provide theoretical justifications of quantum optimization algorithms based on the variational method and the quantum approximate optimization algorithm (QAOA).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751765","AF: EAGER: Homomorphism Problems in Digraphs (Dichotomies)","CCF","ALGORITHMIC FOUNDATIONS","09/15/2017","09/05/2017","Arash Rafiey","IN","Indiana State University","Standard Grant","Tracy J. Kimbrel","08/31/2019","$141,056.00","Geoffrey Exoo, Jeffrey Kinne, Laszlo Egri","Arash.Rafiey@indstate.edu","200 N 7TH STREET","TERRE HAUTE","IN","478091902","8122373088","CSE","7796","7916, 7926, 7927","$0.00","Graph coloring is one of the most important problems in theoretical computer science.  Many combinatorial optimization problems can be viewed as graph coloring problems.  For a given graph G and integer k, the question is whether there exists a coloring of its vertices with k colors such that any two adjacent vertices receive different colors.  The Graph (or Directed Graph) Homomorphism Problem is a generalization of graph coloring.  In the Graph Homomorphism Problem, the goal is to find a mapping from an input graph (or digraph) to a fixed target graph (or digraph) H that preserves adjacency.<br/><br/>Homomorphism problems, and the equivalent formulation as so-called constraint satisfaction problems (CSPs), enjoy a wide variety of applications as optimization problems that must be solved in practice.  Such applications can be seen in scheduling, planning, databases, artificial intelligence, and many other areas.<br/><br/>The Digraph Homomorphism Problem and CSPs have been two very active research areas in Theoretical Computer Science over the last two decades.  Several  tools (mostly algebraic) have been developed for solving CSPs, and very recently a number of proposed solutions (including our solution) to the main conjecture in the area (known as the CSP Conjecture) have arisen.  The present project aims to verify in detail each approach to distill the most elegant proof and most efficient algorithms.  The approach is purely combinatorial, using techniques from graph theory.<br/><br/>The project will also tackle problems closely related to the newly proposed solutions to the CSP Conjecture.  For example, the PIs seek forbidden obstruction characterizations for the types of digraphs H that make homomorphism problems feasible. This would help to improve the running time of the current algorithm.<br/><br/>The project aims also to have a high educational impact, through training graduate students in theoretical computer science, producing <br/>freely available and high quality lecture notes and survey material on the field, seeking connections between the research and other important areas of research across computing, and utilizing novel teaching and dissemination methods."
"1833230","Foundations of Computer Science (FOCS) Conference Student and Postdoc Travel Support","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/17/2018","Shanghua Teng","CA","University of Southern California","Standard Grant","Rahul Shah","05/31/2019","$20,000.00","","shanghua@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7796","7556, 7926","$0.00","This award supports student attendance at the 59th IEEE Annual Symposium on the Foundations of Computer Science (FOCS) in Paris, France from October 7 through October 9, 2018, as well as to support attendance by some qualified postdoctoral fellows who do not have other sources of travel funding. FOCS and its sister conference, the ACM STOC meeting, are the premier broad-based conferences on the Theory of Computing. FOCS has a record of strongly encouraging participation by students, for whom the conference serves as a valuable educational experience, both for the technical content of the talks and for the opportunities for networking that it provides. In recent years, the annual FOCS conference has been attended by eighty to one hundred students who have comprised more than a third of all attendees. This award will provide partial support to about 20 students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763773","AF: Medium: Collaborative Research: Quantum-Secure Cryptography and Fine-Grained Quantum Query Complexity","CCF","QUANTUM COMPUTING","08/01/2018","04/26/2018","Alexander Russell","CT","University of Connecticut","Continuing grant","Almadena Y. Chtchelkanova","07/31/2021","$91,165.00","","acr@cse.uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7928","7924, 7928","$0.00","Secure Internet communication faces a real threat in the form of a new breed of computer that harnesses the laws of quantum mechanics. The technical community is currently hard at work attempting to construct such ""quantum"" computers. While many mysteries about these devices remain, it is certain that a large-scale quantum computer would easily break all current public-key cryptography that underpins the current Internet. In certain attack models, important examples of private-key cryptography would also be rendered insecure. This 3-institution collaborative project studies the basic theoretical issues underlying these urgent threats to the security infrastructure. It seeks to understand the cryptography-breaking power of quantum computers, concentrating on two interweaving themes: quantum security for 1) authenticating, and 2) constructing quantum-secure cryptography from new primitives.  The project activities also include course development and mentorship at the graduate and undergraduate level. The project also involves specific outreach activities intended to broaden participation in Computer Science, including establishment and development of ""women in computer science"" chapters, outreach to local high schools, workshops for high-school STEM teachers, and development of computer science courses for a general audience at the three partner institutions.<br/><br/>Authentication-proofs, for example, that an e-mail really did originate from you--is a basic and well-studied cryptographic challenge. In the setting of quantum adversaries, it is not clear how to appropriately formulate this essential notion, let alone produce specific cryptographic tools that achieve it. This project is addressing both of the challenges noted above, focusing on development of strong formulations of authentication and new cryptographic constructions that offer secure authentication. Finding ""hidden"" algebraic structures--like the fact that two lists of numbers are merely cyclic shifts of each other--is an emblematic theme in the study of the computing power of quantum computers. Certain variants of this problem have resisted decades of concerted effort by the quantum algorithms community, and appear to be quite difficult. This project studies applications of these problems to constructing new private-key cryptographic tools with quantum security guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755955","CRII: AF: Practical Auction Design Using the Deferred-Acceptance Framework","CCF","ALGORITHMIC FOUNDATIONS","02/01/2018","04/06/2018","Vasilis Gkatzelis","PA","Drexel University","Standard Grant","Tracy Kimbrel","01/31/2020","$182,930.00","","gkatz@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7796","7796, 7932, 8228, 9251","$0.00","As the world grows increasingly interconnected, a more effective utilization of its scarce resources becomes possible through an unprecedented number of auctions. On a daily basis, human and software agents compete for an extremely diverse set of resources, ranging from the advertising space on web sites and the goods sold on online auction sites, to the electrical power supply that supports large cities and the landing slots of the world's busiest airports. Were it not for the auctions that regulate these resource allocation processes, massive amounts of social utility would be wasted; hence, it is imperative that these auctions are designed to the highest standard. Some of the optimization problems involved in designing such resource allocation mechanisms can be highly demanding from a computational standpoint. In reality, the design process is even more challenging, as the agents competing for these resources may be strategic and have their own interests at heart. <br/><br/>This project approaches resource allocation problems from the perspective of the designer who chooses the rules of the auction aiming to maximize her own objectives, such as efficiency, revenue, or fairness, despite the strategic behavior of the participants. The long literature on game theory and mechanism design has produced several celebrated auctions that maximize these objectives while at the same time providing attractive theoretical guarantees regarding the incentives of the participating agents, but many of these auctions are rarely used in practice. Two important drawbacks that render these auctions impractical are that: i) it may be non-trivial for the participating bidders to verify these auctions' incentive guarantees, and ii) these auctions often require that the bidders reveal their private preferences to the auctioneer and trust the implementation of the auction.<br/><br/>The long-term goal of this project is to develop a deeper understanding of the performance guarantees that an auction designer can achieve using practical auctions that avoid these drawbacks. Recent work by economists has proposed refined incentive properties that even non-experts can verify, as well as a framework for designing auctions that satisfy these incentive properties. This project aims to evaluate the performance of these auctions, and to extend the auction design framework to capture a much wider family of problem instances. As a result, it can lead to the design of novel and practical auctions that maximize the desired objectives, while contributing the algorithmic perspective of computer science to a line of research initiated by economists. <br/><br/>The proposed research combines techniques from both approximation algorithms and mechanism design. Specifically, the auction design framework that this project aims to analyze and generalize crucially depends on the design of backward greedy algorithms for deciding how the available resources should be allocated. Unlike forward greedy algorithms, the approximation guarantees achievable by backward greedy algorithms are not well understood even within the computer science literature, so this project will also contribute toward a deeper understanding of this interesting class of algorithms."
"1814603","CIF: Small: New Coding Techniques for Synchronization Errors","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/11/2018","Venkatesan Guruswami","PA","Carnegie-Mellon University","Standard Grant","Phillip Regalia","09/30/2020","$472,229.00","Bernhard Haeupler","guruswami@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7797","7923, 7926, 7927, 7935","$0.00","Coding theory has advanced our understanding of how to efficiently correct symbol corruptions and erasures. The error-correcting codes developed by this theory have had a tremendous practical and theoretical impact on technology and engineering as well as mathematics, theoretical computer science, and other fields. Correcting closely related synchronization errors, such as insertions and deletions, however, while also studied since the 1960s, has largely resisted progress so far. The goal of this proposal is to close this gap and develop a better understanding of and new coding techniques for synchronization errors. In addition to resolving fundamental questions on natural and basic error models, the investigators believe that the study has the potential to guide the design of systems which use efficient coding techniques to address synchronization and noise issues jointly, instead of spending significant resources on ensuring very tight controls on synchronization.<br/><br/>The project will build on the recent work in this area by the investigators and their students, and investigate new coding approaches for coping with insertions/deletions. For the case of large finite alphabets, the project will investigate codes based on synchronization strings. Synchronization strings isolate and directly tackle the synchronization aspect which distinguishes insertion and deletion errors from symbol corruptions and erasures, yielding an efficient way to reduce them to regular errors. Such a transformation can then leverage the tremendous progress made on regular error-correcting codes toward the design of insertion-deletion codes. The project will also investigate new approaches for the setting of binary or very small alphabets with the goal of better understanding the potential of insertion-deletion codes together with efficient constructions. Beyond simple insertions and deletions, the project will also study more general synchronization error models stemming from practical applications such as tandem repeats or block corruptions. The educational component will infuse appropriate concepts from the project into courses taught by the investigators, and take advantage of the accessible and attractive nature of the topic to engage undergraduates in research, in addition to the substantial involvement of graduate students. The project will aim to forge stronger intellectual ties between the computer science and information theory communities which are both actively engaged in study of codes in various models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1758006","REU Site: Computational Modeling Serving the City","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/01/2018","02/27/2018","Christof Teuscher","OR","Portland State University","Standard Grant","Rahul Shah","02/28/2021","$302,400.00","Jay Gopalakrishnan","teuscher@pdx.edu","1600 SW 4th Ave","Portland","OR","972070751","5037259900","CSE","1139","9250","$0.00","The focus area of this Research Experiences for Undergraduates (REU) site is computational modeling to serve and enhance the Portland metropolitan region as it grows and evolves. Students will be involved in cutting-edge, multi-disciplinary research projects and trained in computational thinking across different disciplines and communities. In doing so, they will gain an understanding of the potential and limits of these tools and how they can serve diverse urban communities. Portland State University (PSU), with its newly-funded Portland Institute for Computational Science (PICS), has developed a reputation as a national model for urban universities that enhance their region by working with partners to solve problems. The activities of this REU site will involve a 2-week training to teach students the foundations of computational modeling, followed by an 8-week research project completed in a research lab under a mentor's guidance and with the involvement of a community partner. The community-oriented aspect is aligned with PSU's motto ""Let knowledge serve the city."" The investigator will recruit a very diverse population of undergraduate students across disciplines, in particular from institutions with limited Science, Technology, Engineering and Math (STEM) research opportunities, such as 2-year community colleges. The REU site will allow students to enrich their educational experiences by providing them with an intellectual toolbox that has the potential to have a lasting impact on their diverse careers.<br/><br/>The intellectual merit of the project is rooted in exposing students to community-based computational modeling projects as a way of developing their computational thinking well beyond the computing disciplines. Interdisciplinary projects across disciplines and communities together with weekly site meetings will provide students a unique, multi-faceted perspective. The intended impact of the REU site is to teach students the fundamental skills of computational modeling and thinking that will help them understand and solve complex problems in a complex world. At the end of the internship, the student will present the research outcomes to the community partner. Weekly site meetings, seminars, site visits, and a final symposium will enrich the internship. The hands-on nature of the cutting-edge research projects has the potential to attract and motivate a diverse population of students to pursue a STEM career.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718342","AF:Small: Continuous Perspectives on Accelerated Methods for Combinatorial Optimization","CCF","ALGORITHMIC FOUNDATIONS","07/01/2017","06/22/2017","Lorenzo Orecchia","MA","Trustees of Boston University","Standard Grant","Rahul Shah","06/30/2020","$497,504.00","Alina Ene","orecchia@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7796","7923, 7926","$0.00","<br/>Efficient algorithms are at the heart of any modern computing systems. In the classical study of algorithms, the notion of efficiency has long been taken to equal polynomial running time in the size of the input. However, the recent rise of massive datasets, for which even quadratic running times may be practically infeasible, has led to a rethinking of this assumption. This effort has led to a number of breakthroughs on fundamental problems, such as computing the maximum flow through a network, which can now be solved in essentially linear time in many cases. These advances are largely based on novel algorithmic tools stemming from convex and numerical optimization, which heavily rely on continuous mathematics. Based on this paradigm, the PIs aim to provide improved algorithms for two classes of problems that are important both in theory and practice: maximum flow problems and submodular optimization problems. These problems are central to computer science and have wide applicability to many real-life problems. The project will support and train two Ph.D. students and a postdoc in algorithm design and optimization at Boston University. The proposed research requires a useful exchange of ideas between theoretical computer science and continuous optimization, strengthening existing ties as well as forging new connections between the two areas. The continuous viewpoint provides a new perspective on algorithm design that the PIs plan to disseminate broadly and to incorporate into their optimization courses at Boston University.<br/><br/><br/>In this project, the PIs will delve deeply into the connection between efficient discrete optimization and continuous mathematics by considering the following continuous approach to algorithm design: algorithms are initially conceived as continuous trajectories through the space of solutions to the problems, e.g., as described by a set of differential equations; these trajectories are discretized to yield true discrete-time algorithms that are provably fast. We plan to exploit the new understanding of algorithms obtained through this framework to provide improved algorithms for two classes of problems that are important both in theory and practice: maximum flow problems and submodular optimization problems. The main technical focus of the project will be understanding and leveraging the idea of ""acceleration"", which plays a central role in convex optimization, as it yields optimal gradient-descent algorithms for a large class of functions. A very recent line of work initiated the study of accelerated methods from a continuous-time perspective using ordinary differential equations (ODEs) and classical discretization tools. More precisely, these works aim to represent a class of accelerated methods via ODEs whose dynamics describe the continuous-time limits of the discrete algorithms, while the discrete algorithms can be interpreted as appropriate discretizations of the continuous-time curves described by the ODEs.<br/><br/>The project will build on this recently established viewpoint to make progress on central combinatorial optimization problems involving graphs and submodular functions. In particular, the project aims to exploit the rich combinatorial structure present in these problems to construct improved discretization procedures for known dynamical systems corresponding to accelerated algorithms. Specific problems of interest include: (a) Maximum s-t flows and connectivity problems in graphs and networks - The emphasis will be on leveraging convex optimization techniques such as acceleration in order to obtain faster approximate solutions for these problems in undirected and directed graphs. (b) Constrained submodular maximization problems - A particular area of focus will be on designing new continuous algorithms and discretization for solving known continuous relaxations of submodular objectives under structured constraints, such as packing and covering constraints.<br/><br/>"
"1514128","AF: Medium: Collaborative Research: Information Compression in Algorithm Design and Statistical Physics","CCF","ALGORITHMIC FOUNDATIONS","06/15/2015","03/14/2019","Dimitris Achlioptas","CA","University of California-Santa Cruz","Standard Grant","Tracy Kimbrel","05/31/2020","$452,000.00","","optas@cs.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7796","7924, 7926","$0.00","The existence of connections between probabilistic algorithms, statistical physics and information theory has been known for decades and has yielded a number of unexpected breakthroughs. Recent discoveries of the PIs and other researchers give clear indications that these connections go much deeper than previously thought. A key new idea is the realization that stochastic local search algorithms can be judged by their capacity to compress the randomness they consume, with convergence following as a consequence of compressibility. Further exploration of this idea is expected to have significant impact, both conceptual and technical, in multiple scientific fields. This includes algorithm design by information theoretic methods, the study of phase transitions in statistical mechanical systems based on information bottleneck arguments, and non-constructive proofs of existence of combinatorial objects. The project will offer a wide range of research opportunities at various levels of sophistication for graduate and undergraduate students in three state universities.<br/><br/>Information compression arguments have recently found striking applications in computer science and combinatorics. A glowing example is Moser's proof of the algorithmic Lovasz Local Lemma, which suggested an entirely new way of reasoning about randomized algorithms. Inspired by the work of Moser, one of the PIs with a collaborator has very recently created a general framework for analyzing stochastic local search algorithms using information compression. The framework is purely algorithmic, completely bypassing the Probabilistic Method. Besides helping to analyze the running times of existing algorithms it can also be used as a powerful new tool for designing novel, non-obvious randomized algorithms. The proposed research further develops this framework with the aim of unearthing completely new applications in computer science and combinatorics, while establishing mathematically rigorous connections to statistical physics. Concrete examples of such applications to be investigated include new tools for bounding the mixing time of Markov chains and algebraic connections between randomized algorithms and the classical theory of phase transitions in statistical physics."
"1317694","Collaborative Research: Molecular Programming Architectures, Abstractions, Algorithms, and Applications","CCF","INFORMATION TECHNOLOGY RESEARC, ALGORITHMIC FOUNDATIONS","10/01/2013","08/12/2016","Erik Winfree","CA","California Institute of Technology","Continuing grant","Mitra Basu","09/30/2019","$5,128,796.00","Richard Murray, Jehoshua Bruck, Niles Pierce, Paul W.K. Rothemund","winfree@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1640, 7796","7723, 7931, 7946","$0.00","The computing revolution began over two thousand years ago with the advent of mechanical devices for calculating the motions of celestial bodies. Sophisticated clockwork automata were developed centuries later to control the machinery that drove the industrial revolution, culminating in Babbage's remarkable design for a programmable mechanical computer. With the electronic revolution of the last century, the speed and complexity of computers increased dramatically. Using embedded computers we now program the behavior of a vast array of electro-mechanical devices, from cell phones and satellites to industrial manufacturing robots and self-driving cars. The history of computing has taught researchers two things: first, that the principles of computing can be embodied in a wide variety of physical substrates from gears to transistors, and second, that the mastery of a new physical substrate for computing has the potential to transform technology.  Another revolution is just beginning, one whose inspiration is the incredible chemistry and molecular machinery of life, one whose physical computing substrate consists of synthetic biomolecules and designed chemical reactions.  Like the previous revolutions, this ""molecular programming revolution"" will have the principles of computer science at its core.  By systematically programming the behaviors of a wide array of complex information-based molecular systems, from decision-making circuitry and molecular-scale manufacturing to biomedical diagnosis and smart therapeutics, it has the potential to radically transform material, chemical, biological, and medical industries.  With molecular programming, chemistry will become a major new information technology of the 21st century.<br/><br/>This Expeditions-in-Computing project aims to establish solid foundations for molecular programming.  Building on advances in DNA nanotechnology, DNA computing, and synthetic biology, the project will develop methods for programmable self-assembly of DNA strands to create sophisticated 2D and 3D structures, dynamic biochemical circuitry based on programmable interactions between DNA, RNA, and proteins, and integrated behaviors within spatially organized molecular systems and living cells. These architectures will provide systematic building blocks for creating programmable molecular systems able to sense molecular input, compute decisions about those inputs, and act on their environment. To manage system complexity and to provide modularity, the project will establish abstraction hierarchies with associated high-level languages for programming structure and behavior, compilers that turn high-level code into lists of synthesizable DNA sequences, and analysis software that can predict the performance of the sequences.  This will allow molecular programmers to specify, design, and verify the correctness of their systems before they are ever synthesized in the laboratory. In addition to these software tools, the project will study the theory of molecular algorithms in order to understand the potential and limitations of information-based molecular systems, what makes them efficient at the tasks they can perform, and how they can be effectively designed and analyzed. Putting the products of this fundamental research to the test, the project will pursue real-world applications such as molecular instruments for probing biological systems and programmable fabrication of nanoscale devices.<br/><br/>This project will expand the network of scientists and engineers working in molecular programming by building a diverse community of students, teachers, researchers, scientists, and engineers. This community will be fostered through the creation of publicly accessible software tools, courses, textbooks, workshops, tutorials, undergraduate research competitions, and popular science videos to teach the principles and methods of molecular programming and to engage young researchers and the public in this exciting new field. Industrial partnerships with relevant biotechnology and other high-tech companies will ensure fast transfer of knowledge generated into real-world products.  Perhaps most importantly, as molecular programming becomes a widespread technology, it has the potential to transform industry with new complex nanostructured materials, to transform chemistry with integrated and autonomous control of reactions, to transform biology with advanced molecular instruments, and to transform health care with more sophisticated diagnostics and therapeutics."
"1844939","CAREER: New Algorithmic Foundations for Online Scheduling","CCF","ALGORITHMIC FOUNDATIONS","07/01/2019","02/01/2019","Sungjin Im","CA","University of California - Merced","Continuing grant","Balasubramanian Kalyanasundaram","06/30/2024","$85,813.00","","sim3@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092598670","CSE","7796","1045, 7926","$0.00","As massive low-cost computing resources become increasingly available, harnessing their power is crucial in modern science and engineering. One particular issue involves scheduling:  what is the most effective way to assign resources, say computing cycles, to tasks in order to ensure good performance?  The scheduling problem is especially acute when little to nothing is known in advance about the tasks, including when they might arrive and how much compute time they may need; in such cases, dynamic allocation of resources is required.  Over the past two decades, exciting advances in approaches for addressing these so-called on-line scheduling problems have emerged, but the field is still struggling to address the increasingly challenging scheduling environments found in modern computing clusters.  This project aims to develop new methods to design and analyze online scheduling algorithms systematically with the aid of widely used optimization techniques, and as a result to potentially resolve some key open questions in online scheduling.  The research findings will likely provide an alternative method of educating students on scheduling in a broad context, which will have a significant impact on the computer science curriculum. This project will also involve mentoring students and disseminating the research outcomes through workshops, writing tutorials, and developing new course materials.<br/> <br/>At a more technical level, this project intends to investigate the effectiveness of online scheduling techniques for a variety of problems.  The project's first objective is to develop new gradient-descent methods to design and analyze online-scheduling. The second objective is to use bin-packing to study fundamental admission-control problems, and to develop new algorithmic tools when pre-emption is allowed. The third research problem to be studied involves the development of fine-grained scheduling algorithms for low-dimensional scheduling environments. Surprisingly, despite recent advances, many existing algorithms are no match even for the simplest greedy algorithms in the low-dimensional case, which is common in practice. The fourth research goal is to refine the behavior of the online algorithms as the workload approaches the system limit, which is related to fundamental questions regarding the underlying analysis models. The last goal is to explore new models for scheduling jobs with inter-dependencies by taking advantage of large-scale scheduling environments to circumvent the intractability results that are commonly found in the traditional models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1563122","AF: Medium: Collaborative Research: Circuit Lower Bounds via Projections","CCF","ALGORITHMIC FOUNDATIONS","04/01/2016","09/19/2017","Li-Yang Tan","IL","Toyota Technological Institute at Chicago","Continuing grant","Tracy Kimbrel","03/31/2019","$331,250.00","","liyang@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7924, 7927","$0.00","Computers play a central role in how we work, play, and communicate with each other in today's world.  It is a truism that  computers have grown steadily more powerful over the years, but equally important (if not more so) than the amount of sheer computing power available is how efficiently we are able to harness that power.  Finding an efficient strategy to solve a given problem (in the language of computer science, an efficient algorithm) can often spell the difference between success and failure.  (As an illustrative analogy, consider the task of assembling a large jigsaw puzzle.  A poor choice of strategy such as a brute-force approach of trying each pair of pieces against each other may be infeasibly slow, while a cleverer approach such as grouping pieces by their color can be radically more efficient and lead to a feasible solution.)  But in order to fully understand the abilities of efficient  algorithms, it is crucial to also understand their limits: what is it that efficient algorithms *cannot* do? The field of ""computational  complexity"", which is the subject of the PIs' project, seeks to mathematically prove that certain computational problems do not admit *any* efficient algorithm no matter how long and hard we try to develop one.  Such results can have both practical value (by guiding algorithm development away from ""dead ends"") and deep theoretical significance, as they play a profound role in shaping our fundamental understanding of the phenomenon of computation.<br/><br/>The 1980s witnessed exciting progress on a range of Boolean circuit models (a mathematical abstraction of the digital circuits that modern computers are built from) in computational complexity; researchers succeeded in proving many lower bounds establishing that various computational problems have no efficient algorithms in these models.  However, further progress slowed significantly after the 1980s. Many of the landmark results obtained in this era were based on the ""method of random restrictions"", which roughly speaking uses probabilistic arguments to show that Boolean circuits can be dramatically simplified by making certain random substitutions of constant values for input variables.  In this project the PIs will intensively investigate an extension of the method of random restrictions which they call the ""method of random projections."" Rather than simply substituting constant values for input variables, the random projection method additionally identifies groups of variables, ""projecting"" them all to the same new variable so that they must all take the same value.  While the underlying idea is simple, it turns out that this identification of variables helps ""maintain useful structure"" which is extremely useful for proving lower bounds.  In recent work the PIs have successfully used this new ""method of random projections"" to solve several decades-old problems in Boolean circuit lower bounds and related areas (which in some cases had notoriously resisted progress since the 1980s or 1990s).  As the main intellectual goals of the project, the PIs will continue to develop and apply the method of random projections to attack important open problems in Boolean circuit complexity.<br/><br/>In addition to the technical goals described above, other central goals of the project are to educate, communicate, and inspire.  The PIs will train graduate students through research collaboration, disseminate research results through seminar talks, survey articles and other publications, and continue ongoing outreach activities aimed at increasing interest in and awareness of theoretical computer science topics in a broader population, including presentations at elementary schools."
"1514164","AF: Medium: Collaborative Research: Information Compression in Algorithm Design and Statistical Physics","CCF","ALGORITHMIC FOUNDATIONS","06/15/2015","04/13/2018","Mario Szegedy","NJ","Rutgers University New Brunswick","Standard Grant","Tracy Kimbrel","05/31/2020","$461,342.00","","szegedy@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7796","7924, 7926","$0.00","The existence of connections between probabilistic algorithms, statistical physics and information theory has been known for decades and has yielded a number of unexpected breakthroughs. Recent discoveries of the PIs and other researchers give clear indications that these connections go much deeper than previously thought. A key new idea is the realization that stochastic local search algorithms can be judged by their capacity to compress the randomness they consume, with convergence following as a consequence of compressibility. Further exploration of this idea is expected to have significant impact, both conceptual and technical, in multiple scientific fields. This includes algorithm design by information theoretic methods, the study of phase transitions in statistical mechanical systems based on information bottleneck arguments, and non-constructive proofs of existence of combinatorial objects. The project will offer a wide range of research opportunities at various levels of sophistication for graduate and undergraduate students in three state universities.<br/><br/>Information compression arguments have recently found striking applications in computer science and combinatorics. A glowing example is Moser's proof of the algorithmic Lovasz Local Lemma, which suggested an entirely new way of reasoning about randomized algorithms. Inspired by the work of Moser, one of the PIs with a collaborator has very recently created a general framework for analyzing stochastic local search algorithms using information compression. The framework is purely algorithmic, completely bypassing the Probabilistic Method. Besides helping to analyze the running times of existing algorithms it can also be used as a powerful new tool for designing novel, non-obvious randomized algorithms. The proposed research further develops this framework with the aim of unearthing completely new applications in computer science and combinatorics, while establishing mathematically rigorous connections to statistical physics. Concrete examples of such applications to be investigated include new tools for bounding the mixing time of Markov chains and algebraic connections between randomized algorithms and the classical theory of phase transitions in statistical physics."
"1422012","Collaborative RUI: Quadrilateral Surface Meshes with Provable Quality Guarantees","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","08/15/2014","Dianna XU","PA","Bryn Mawr College","Standard Grant","Rahul Shah","08/31/2019","$149,432.00","","dxu@cs.brynmawr.edu","101 N. Merion Avenue","Bryn Mawr","PA","190102899","6105265298","CSE","7796","7923, 7929, 9229","$0.00","The representation and reconstruction of complex three-dimensional objects is critical in a wide range of applications in computing today. Polygonal meshes have become the industry standard for the representation of surfaces with highly complex geometry and arbitrary genus in computer graphics and geometry processing applications. While triangle meshes are the most popular type of mesh representation for surfaces, quadrilateral meshes are better suited than triangle meshes in several applications such as character animation, texture mapping, spline-based surface modeling, mesh compression, and some specific finite element analysis applications. Provably good algorithms for generating triangle meshes from surfaces given by parametric or implicit functions or as point point clouds are widely available. However, algorithms to generate quadrilateral meshes with provable quality guarantees for such surfaces are not as prevalent, in part because the problem of generating a quadrilateral mesh from a given surface is intrinsically harder than its triangular counterpart. The goal of this project is to develop algorithms for quadrilateral meshes for various surface representations with provable guarantees on element quality as measured by commonly used metrics such as angle bounds or aspect ratio, and mesh quality as measured by mesh size or anisotropy. Direct and indirect methods (which generate a quad mesh from a triangle mesh), as well as parameterization guided methods will be utilized. Techniques from computational geometry and graph theory will play a central role in the design and development of algorithms. <br/><br/>The automated generation of provably good quadrilateral meshes for surfaces is a fundamental problem that is of interest both in theory, as it raises several geometric, combinatorial, and graph-theoretic questions, as well as practice, where the computational pipeline from designing a model for the surface to the end stage of simulation or animation is frequently dominated by the meshing process. A formal understanding of these questions is critical not only for the theoretical underpinnings of automated mesh generation, but also for the sound practice of utilizing the meshes in a wide range of applications. <br/><br/>As a collaborative effort between PIs at three undergraduate institutions, involvement of undergraduate students in research projects is an integral part of this project. An important and particular goal for this project is the creation of a larger peer group for female and minority undergraduate Computer Science majors by providing opportunities for collaboration and joint research projects between students across all three institutions. Through early and active involvement of undergraduates in the project, the PIs also seek to create a pipeline of female and minority students bound for graduate school in Computer Science."
"1552097","CAREER: Pursuing New Tools for Approximation Algorithms","CCF","ALGORITHMIC FOUNDATIONS","03/01/2016","02/07/2019","Shayan Gharan","WA","University of Washington","Continuing grant","Rahul Shah","02/28/2021","$311,776.00","","shayan@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","1045, 7926","$0.00","Many of the large-scale industries are now employing sophisticated algorithms to solve variants of fundamental optimization problems. For example, Amazon uses a variant of the Traveling Salesman Problem (TSP) known as the vehicle routing problem for routing Amazon-Fresh Trucks. Uber solves a variant of TSP to route its shared-ride services. Many of the social networks including Facebook and Google+ solve variants of constraints satisfaction problems for their social targeting tasks. These optimization problems have ubiquitous applicability but they are computationally challenging in the sense that many are known to be NP-hard. This means that under standard assumptions they cannot be solved optimally by algorithms which terminate in reasonable time. The field of approximation algorithms attempts to develop efficient algorithms that find solutions close to the optimum. These approximation algorithms have found many applications in the real world. The project will advance state-of-the-art in approximation algorithms which will not only have impact on industry but also contribute to our fundamental understanding of P vs NP issue, which is at the core of computer science.<br/><br/>This project aims to develop new tools and techniques to obtain improved approximation algorithms for fundamental optimization problems, including the TSP and Constraint Satisfaction problems. The project intends to prove new algebraic properties of stable polynomials and use them to study graphs from an algebraic point of view. These tools will lead to design a new class of approximation algorithms. These new tools coming out of this project will be incorporated in the next generation of courses in approximation algorithms that focus on algebraic techniques. Although grounded in computer science theory, the project will also attract many graduate students outside of theory from applied fields like machine learning and artificial intelligence forming basis for interdisciplinary research."
"1618657","AF: Small: A-Hypergeometric Solutions of Linear Differential Equations","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","06/28/2016","Mark van Hoeij","FL","Florida State University","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2019","$449,999.00","","hoeij@math.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7796","7923, 7933","$0.00","Many scientific laws are captured as differential equations --<br/>formulas that relates a quantity (such as position) to its derivatives<br/>(velocity and acceleration). Differential equations are common in<br/>science, mathematics and engineering.  They can be solved numerically,<br/>but may also have a closed-form solution -- an exact solution written<br/>in terms of familiar functions.<br/><br/>Except for small equations, closed-form solutions were thought to be<br/>rare.  Algorithms for second-order equations, which were developed by<br/>the PI and his students, have demonstrated that closed form solutions <br/>of linear differential equations with polynomial coefficients<br/>are actually common; solutions can often be written in terms of<br/>Gauss's hypergeometric function, a familiar function in differential<br/>equations.<br/><br/>This project aims to find out if closed form solutions are also common<br/>for higher order equations.  The PI will work with two graduate<br/>students to develop algorithms to search for solutions in terms of<br/>A-hypergeometric functions, which were introduced by Gel'fand,<br/>Kapranov and Zelevinski.<br/> <br/>There is a wide variety of A-hypergeometric functions, which<br/>correspond to polytopes. A-hypergeometric functions can be<br/>multivariate, so several tools that currently only exist for<br/>univariate equations need to be generalized.  A key motivating<br/>question is if convergent integer-series solutions can always be<br/>written in terms of A-hypergeometric functions. If true, then<br/>A-hypergeometric solutions would be common in areas of mathematics and<br/>science that involve combinatorial structures or integrals, such as the Ising model<br/>or Feynman diagrams in physics. This question leads to several others<br/>on A-hypergeometric functions, some of which can be settled by the<br/>algorithms to be developed in this project."
"1422603","AF: Small: Self-Organizing Particle Systems","CCF","ALGORITHMIC FOUNDATIONS","08/01/2014","05/04/2018","Andrea Richa","AZ","Arizona State University","Standard Grant","Rahul Shah","07/31/2019","$490,000.00","","aricha@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7796","7923, 7926, 7934, 9102, 9251","$0.00","The goal of this project is to lay the foundations for algorithmic research on self-organizing particle systems. Particle systems are physical systems of simple computational particles that can bond to other particles and that can use these bonds in order to communicate with neighboring particles and to move from one spot to another (non-occupied) spot. These particle systems are supposed to be able to self-organize in order to adapt to a desired shape without any central control. Self-organizing particle systems have many interesting applications like coating objects for monitoring and repair purposes and the formation of nano-scale devices for surgery and molecular-scale electronic structures. While there has been quite a lot of systems work in this area, especially in the context of modular self-reconfigurable robotic systems, only very little theoretical work has been done in this area so far.<br/><br/>This project will prepare the ground for rigorous algorithmic research on self-organizing particle systems by proposing basic models and solving some basic algorithmic problems in this area. More specifically, the main objectives of this three-year project are (i) to refine an amoeba-inspired modelfor particle systems in 2D, and to develop appropriate models for particle systems in 3D; and (ii) to develop self-organizing algorithms for the smart paint problem, covering and bridging problems, shape formation problems, and the macrophage problem in 2D and 3D. A transformative, novel thinking approach will be needed if one indeed wants to capture the essential nature of these systems, in some ways mimicking those that already exist in nature.<br/><br/>The proposed research will have an impact in several respects, such as: (i) bridging the gap between theory and practice in the area of self-organizing particle systems, with impact on many application areas such as micro-fabrication and cellular engineering; (ii) international collaboration; (iii) multidisciplinary activities, since the topics in this proposal will foster collaboration with researchers in multiple areas such as nano-scale micro-fabrication, cellular engineering, nano-scale medical applications, biochemistry, and computer science;  and (iv) enhancing diversity at Arizona State University and at the Computer Science Theory\Algorithms community at large."
"1714844","AF:   Small:  Collaborative Research:  Scalable, high-order mesh-free algorithms applied to bulk-surface biomechanical problems","CCF","ALGORITHMIC FOUNDATIONS","08/01/2017","08/01/2017","Varun Shankar","UT","University of Utah","Standard Grant","Balasubramanian Kalyanasundaram","07/31/2020","$88,619.00","Grady Wright","shankar@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7796","7923, 7933","$0.00","Modeling material processes, like interaction of a drug molecule docking with a receptor site, must capture interactions, often by systems of partial differential equations (PDEs), both in the bulk and on the surface. These PDEs must be calculated numerically, since they often have non-linear couplings between bulk and surface whose geometries often evolve in time. This project develops and implements scalable, high-order, meshfree algorithms -- based on radial basis functions (RBFs) -- for complex multi-scale bulk-surface biomechanical modeling in three-dimensional evolving geometries. The algorithms and software developed under this grant will directly enable researchers to explore scientific questions in lipid membrane morphology and physiology of platelets in the clotting process. The algorithms will have broad applicability to other coupled bulk-surface processes -- in biology, material science, and many industrial problems. The grant will help bolster the research portfolio of the new Computational Science and Engineering (CSE) PhD program at Boise State University, and will support one of the first graduate students in this program. The PIs will continue to build upon their successful track record of recruiting graduate students in computational mathematics from underrepresented groups as part of this project by working with the LSAMP program at Boise State University.<br/><br/>A specific focus of this proposal is on developing RBF algorithms for two biomechanical and physiological problems that are at the forefront of what current numerical techniques can handle and that have features common  to general bulk-surface problems: morphology of the lipid bilayer and platelet aggregation and coagulation. These problems will drive the development of the following advances in numerical discretizations and algorithms for RBFs: 1) Scale-free kernels for high-order solutions of surface PDEs; 2) Stable, scalable meshfree schemes for advection in geometrically complex domains without any tuning parameters; 3) High-order meshfree geometric modeling techniques with optimal computational complexity; 4) SIMD-friendly algorithms for automatic scattered node generation with variable spatial; 5) Algorithms for low-cost automatic stencil selection for upwinding and adaptive node refinement; 6) Preconditioning strategies for implicit discretizations of bulk-surface; 7) Consistent, accuracy preserving meshfree techniques for visualizing solutions from RBF-based high-order methods. The developments will be made publicly available through an open-source software package.  "
"1525130","AF: Small: Quantum Algorithms Arising from Ideas in Physics","CCF","ALGORITHMIC FOUNDATIONS","08/01/2015","08/24/2018","Peter Shor","MA","Massachusetts Institute of Technology","Standard Grant","Dmitry Maslov","07/31/2019","$327,440.00","Edward Farhi","shor@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7923, 7928","$0.00","Currently, quantum computers are able to manipulate no more than a few handfuls of qubits. However, it is widely expected that in the next few decades, quantum computers that are large enough to do interesting things will be built. Currently, the most lucrative application of these appear to be in the areas of cryptography and the areas of simulation of quantum mechanics, which it is hoped will have great impact in the areas of physics, chemistry, and pharmaceutical research. However, a toolbox of techniques for designing algorithms could give quantum computers a much greater impact, if algorithms could be developed for them that speed up other classes of problems, especially ones that speed up finding either the optimal or approximation solutions for combinatorial search problems.<br/>The PIs intend to investigate several algorithms in quantum computing, approaching them by using ideas from physics, computer science, and mathematics. The PIs view quantum information science as a branch of all of these disciplines, and believe that techniques from each area can open insights into the others.  The questions the PIs intend to address are related to algorithms and protocols: (1) exploration instances of adiabatic quantum algorithms where they outperform classical algorithms, (2) an in-depth study of knot-based quantum money schemes,  (3) an attempt to solve quantum versions of the satisfiability problem, and (4) investigate quantum approximate optimization algorithms."
"1319755","SHF: Small: Taming the Combinatorial Explosion of Power Management for Future Manycore Systems","CCF","COMPUTER ARCHITECTURE","08/01/2013","04/26/2018","Abhishek Bhattacharjee","NJ","Rutgers University New Brunswick","Standard Grant","Yuanyuan Yang","07/31/2019","$450,000.00","Abhishek Bhattacharjee","abhib@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7941","7923, 7941","$0.00","We will soon enter the manycore era, in which systems will include<br/>hundreds of cores, large multi-level cache structures, sophisticated<br/>networks-on-chip, many memory controllers, and huge amounts of main<br/>memory.  These systems will also embody a rich array of power<br/>management mechanisms and will strive to achieve high performance<br/>under various power, energy, and thermal constraints.  Unfortunately,<br/>the uncoordinated power management of a system's components can<br/>produce oscillating behavior, higher power/energy/temperature, and/or<br/>excessive performance degradation.  Given their large number of<br/>hardware components and the wide spectrum of available mechanisms,<br/>manycore systems will have to coordinate the actions of their various<br/>power managers.  Moreover, to decide on a (coordinated) course of<br/>action, these systems will have to comprehensively and rigorously<br/>reason about various performance and power/energy/temperature<br/>tradeoffs.<br/><br/>This project will develop global power coordination (GPC) to manage the<br/>combinatorial explosion of possible power management configurations in<br/>manycore systems.  GPC will be realized using an engine that analyzes<br/>the space of possible power state configurations for the entire<br/>system. It will then decide which configurations are most appropriate,<br/>and use per-component power managers to actuate the proper settings.<br/>To optimize the search for good configurations, GPC will consider<br/>greedy search heuristics that prune the space by relying on novel<br/>techniques that estimate the benefit of power state changes, group<br/>resources with similar power management hooks, and leverage prior<br/>observed behavior and a hierarchical organization.<br/><br/>This project will broadly impact society in many ways.  First,<br/>addressing the power, energy, and temperature problems of manycore<br/>systems can greatly impact the datacenters that run our Internet<br/>services and the high-performance systems that advance our science.<br/>Second, tackling these problems will address one of the key<br/>technological barriers in the computing industry.  Third, the project<br/>will educate graduate, undergraduate, and high-school students, while<br/>broadening the participation of underrepresented groups in computer<br/>science."
"1810758","NSF-BSF: AF: Small: An Algorithmic Theory of Brain Networks","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/18/2018","Nancy Lynch","MA","Massachusetts Institute of Technology","Standard Grant","Rahul Shah","05/31/2021","$450,000.00","","lynch@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7923, 7934, 8089, 8091, 9102","$0.00","Understanding how the brain works, as a computational device, is a central challenge of modern neuroscience and AI.  Different research communities approach this challenge in different ways, including examining neural network structure as a clue to computational function, using functional imaging to study neural activation patterns, developing theory based on simplified models of neural computation, and engineering of neural-inspired machine learning  architectures. This project will approach the problem using techniques from distributed computing theory and other branches of theoretical computer science. This project has the potential to improve our understanding of computation in the brain, by identifying key problems that are solved in the brain and key mechanisms that may be used to solve them.  This work can also have impact on theoretical computer science, by contributing a new and fruitful direction for theoretical study. This collaboration between MIT and the Weizmann Institute in Israel will increase the participation of women and minority participants in this field and will seek to bridge the gap between computer scientists and biology researchers.<br/><br/>Specifically, the project develops an algorithmic theory for brain networks, based on novel stochastic Spiking Neural Network models with general interconnection patterns. It defines a collection of abstract problems to be solved by these networks, inspired by problems that are solved in actual brains, such as problems of focus, recognition, learning, and memory.  The project designs algorithms (networks) that solve the problems, and analyze them in terms of static costs such as network size, and dynamic costs such as time to converge to a correct solution.  The investigators consider tradeoffs between the various costs, and will prove corresponding lower bound results. The models, problems, and solutions should be simple enough to enable theoretical analysis, yet realistic enough to provide insight into the behavior of real neural networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717556","AF:  Small:   Collaborative Research:  Scalable, high-order mesh-free algorithms applied to bulk-surface biomechanical problems","CCF","ALGORITHMIC FOUNDATIONS","08/01/2017","08/01/2017","Grady Wright","ID","Boise State University","Standard Grant","Balasubramanian Kalyanasundaram","07/31/2020","$244,417.00","","gradywright@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","7796","7923, 7933, 9150","$0.00","Modeling material processes, like interaction of a drug molecule docking with a receptor site, must capture interactions, often by systems of partial differential equations (PDEs), both in the bulk and on the surface. These PDEs must be calculated numerically, since they often have non-linear couplings between bulk and surface whose geometries often evolve in time. This project develops and implements scalable, high-order, meshfree algorithms -- based on radial basis functions (RBFs) -- for complex multi-scale bulk-surface biomechanical modeling in three-dimensional evolving geometries. The algorithms and software developed under this grant will directly enable researchers to explore scientific questions in lipid membrane morphology and physiology of platelets in the clotting process. The algorithms will have broad applicability to other coupled bulk-surface processes -- in biology, material science, and many industrial problems. The grant will help bolster the research portfolio of the new Computational Science and Engineering (CSE) PhD program at Boise State University, and will support one of the first graduate students in this program. The PIs will continue to build upon their successful track record of recruiting graduate students in computational mathematics from underrepresented groups as part of this project by working with the LSAMP program at Boise State University.<br/><br/>A specific focus of this proposal is on developing RBF algorithms for two biomechanical and physiological problems that are at the forefront of what current numerical techniques can handle and that have features common  to general bulk-surface problems: morphology of the lipid bilayer and platelet aggregation and coagulation. These problems will drive the development of the following advances in numerical discretizations and algorithms for RBFs: 1) Scale-free kernels for high-order solutions of surface PDEs; 2) Stable, scalable meshfree schemes for advection in geometrically complex domains without any tuning parameters; 3) High-order meshfree geometric modeling techniques with optimal computational complexity; 4) SIMD-friendly algorithms for automatic scattered node generation with variable spatial; 5) Algorithms for low-cost automatic stencil selection for upwinding and adaptive node refinement; 6) Preconditioning strategies for implicit discretizations of bulk-surface; 7) Consistent, accuracy preserving meshfree techniques for visualizing solutions from RBF-based high-order methods. The developments will be made publicly available through an open-source software package.  "
"1540634","BSF:2014359:Invariance in error correcting codes and computational complexity","CCF","SPECIAL PROJECTS - CCF","09/01/2015","08/07/2015","Swastik Kopparty","NJ","Rutgers University New Brunswick","Standard Grant","Tracy J. Kimbrel","08/31/2019","$40,000.00","Shubhangi Saraf","swastik.kopparty@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","2878","2878","$0.00","Information can be digitally encoded (as 0s and 1s) so that even if some of these bits are lost or modified, the information can be recovered.  This foundational idea, which is the basis for such commonplace technologies as DVDs and digital broadcast, is explored in theoretical computer science, where it plays a key role in probabilistically checkable proofs and property testing.  Exploring the limits of this idea in these abstract settings can lead to new understanding, which enables technological innovation. <br/><br/>It has been long known in mathematics that studying the invariances (symmetries) of an object has an important role in understanding the object. In recent years, the study of invariances has led to some significant  advances in various topics in theoretical computer science, such as property testing, error-correcting codes and complexity theory.  For example, codes with affine invariance and codes with a transitive invariance group  have been shown to have important applications in complexity theory, such as constructing PCPs.  This project will support the travel of the PIs and their students for research collaborations on these topics with collaborators in Israel. This research will deepen our understanding of objects with invariances, and develop new applications of this theory in complexity theory."
"1718671","SHF:Small:Device/Circuit Co-design of Negative Capacitance Transistors","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2017","08/31/2017","Sung Lim","GA","Georgia Tech Research Corporation","Standard Grant","Sankar Basu","08/31/2020","$450,000.00","Asif Khan","limsk@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798","7923, 7945","$0.00","When the supply voltage of a transistor drops, its power consumption reduces significantly but at the cost of delay increase. A scientific breakthrough found in 2008 named negative capacitance states that if a ferroelectric material is used for the gate of a transistor, its delay does not degrade even when the voltage reduces. This remarkable effect has a potential to offer very low power yet high performance electronic switches. Existing studies have demonstrated such benefits at individual transistor-level, but its impact on large-scale circuits and systems has not been well studied. The goal of the proposed research is to quantify these benefits first and then to develop ways to further the benefits. The project, by design, is interdisciplinary in nature and will cover a large range of topics from material science, device engineering to circuit design. The project is expected to provide a unique and interdisciplinary experience for the participating graduate and undergraduate students. The STEM outreach and education programs described will help high school and lower-level college students to seek further studies and careers in semiconductor science and engineering.<br/><br/>The team proposes a holistic approach to negative capacitance transistor technology that incorporates experimental research at the most fundamental material-device level, going all the way up to the full chip level circuit simulations. They will develop physics-based compact models for their experimental devices and use these experimentally calibrated models to investigate the performance of negative capacitance transistors corresponding to advance technology nodes such as 10 nm and 7 nm. These models will serve as the intermediary that will translate the results from the basic experiments for chip-level simulations. Large-scale full-chip designs targeting internet-of-things to high-performance applications will be built and optimized to maintain the device superiority all the way up to system and circuit-level. For negative capacitance technology to successfully resolve the power dissipation bottleneck in computing and spur new paradigms in electronics, disjoint research in each individual levels in the hierarchy will not bring about the necessary breakthroughs. The team will develop an approach where all aspects of physics, materials, devices, compact models and circuits are optimized in a self-consistent manner."
"1563155","AF: Medium: Collaborative Research:  Circuit Lower Bounds via Projections","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","04/01/2016","09/14/2018","Rocco Servedio","NY","Columbia University","Continuing grant","Tracy J. Kimbrel","03/31/2020","$841,512.00","","rocco@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","2878, 7796","7924, 7927","$0.00","Computers play a central role in how we work, play, and communicate with each other in today's world.  It is a truism that  computers have grown steadily more powerful over the years, but equally important (if not more so) than the amount of sheer computing power available is how efficiently we are able to harness that power.  Finding an efficient strategy to solve a given problem (in the language of computer science, an efficient algorithm) can often spell the difference between success and failure.  (As an illustrative analogy, consider the task of assembling a large jigsaw puzzle.  A poor choice of strategy such as a brute-force approach of trying each pair of pieces against each other may be infeasibly slow, while a cleverer approach such as grouping pieces by their color can be radically more efficient and lead to a feasible solution.)  But in order to fully understand the abilities of efficient  algorithms, it is crucial to also understand their limits: what is it that efficient algorithms *cannot* do? The field of ""computational  complexity"", which is the subject of the PIs' project, seeks to mathematically prove that certain computational problems do not admit *any* efficient algorithm no matter how long and hard we try to develop one.  Such results can have both practical value (by guiding algorithm development away from ""dead ends"") and deep theoretical significance, as they play a profound role in shaping our fundamental understanding of the phenomenon of computation.<br/><br/>The 1980s witnessed exciting progress on a range of Boolean circuit models (a mathematical abstraction of the digital circuits that modern computers are built from) in computational complexity; researchers succeeded in proving many lower bounds establishing that various computational problems have no efficient algorithms in these models.  However, further progress slowed significantly after the 1980s. Many of the landmark results obtained in this era were based on the ""method of random restrictions"", which roughly speaking uses probabilistic arguments to show that Boolean circuits can be dramatically simplified by making certain random substitutions of constant values for input variables.  In this project the PIs will intensively investigate an extension of the method of random restrictions which they call the ""method of random projections."" Rather than simply substituting constant values for input variables, the random projection method additionally identifies groups of variables, ""projecting"" them all to the same new variable so that they must all take the same value.  While the underlying idea is simple, it turns out that this identification of variables helps ""maintain useful structure"" which is extremely useful for proving lower bounds.  In recent work the PIs have successfully used this new ""method of random projections"" to solve several decades-old problems in Boolean circuit lower bounds and related areas (which in some cases had notoriously resisted progress since the 1980s or 1990s).  As the main intellectual goals of the project, the PIs will continue to develop and apply the method of random projections to attack important open problems in Boolean circuit complexity.<br/><br/>In addition to the technical goals described above, other central goals of the project are to educate, communicate, and inspire.  The PIs will train graduate students through research collaboration, disseminate research results through seminar talks, survey articles and other publications, and continue ongoing outreach activities aimed at increasing interest in and awareness of theoretical computer science topics in a broader population, including presentations at elementary schools."
"1652824","CAREER: Robust Molecular Computation: Error-Correcting Reaction Networks and Leakless DNA Circuits","CCF","COMPUTATIONAL BIOLOGY","09/01/2017","02/21/2017","David Soloveichik","TX","University of Texas at Austin","Continuing grant","Mitra Basu","08/31/2022","$177,258.00","","david.soloveichik@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7931","1045","$0.00","Computer science and electrical engineering have mastered electronic computation, yet there is an important domain of computation that remains poorly understood: chemical information processing. Computation due to chemical reactions is prevalent in biology - for example, every cell in our body must perform sophisticated information processing on internal and external chemical signals. Analogously, it is important to learn how to rationally engineer biochemical pathways that are capable of decision-making. Unlike silicon chips, molecular computers could operate inside cells and control their activity. Programmable chemical reactions could be useful for a range of applications in manufacturing, chemical sensing, and medicine. For example, ""smart drugs"" that target drug activity to disease cells and activate in response to specific molecular clues would have minimal side effects and improve therapeutic outcomes. This proposal addresses a key algorithmic challenge in chemical information processing: how to compute robustly despite the disordered and error-prone nature of the chemical environment. Mathematical models (chemical reaction networks) provide the clarity of thought to explicate universal principles of proofreading chemical algorithms, while a laboratory realization (using engineered DNA molecules) provides the necessary grounding. The success of this proposal will lead not only to new theoretical understanding but to a new generation of functional molecular devices. Specifically, this proposal will address a long-standing challenge in medical diagnostics: enzyme-free sequence-specific detection of DNA. Enzyme-free systems have the potential to be adapted to ""in-the-field"" operation, where sophisticated laboratory equipment is out of reach. The research program proposed is tightly coupled to educational and outreach activities. This proposal will fund the development of college courses and instructional material, which will train students in applying the proven principles of computer science and electrical engineering to the new domain of molecular computation. The primary educational goal is to encourage the next generation of scientists to break through traditional disciplinary barriers and create the scientific and engineering fields of tomorrow. Further, the project will contribute to early STEM education through gamification.<br/><br/>The proposed work uses the formal model of chemical reaction networks to rigorously explore the principles of robust chemical computation. Chemical reaction networks formalize the computation that results from molecules interacting in a well-mixed solution obeying chemical kinetics. The work described could yield chemical algorithms for decreasing error exponentially as more molecules are involved in the computation, as well as for fast and entirely error-free computation of simple functions. Additionally, the impossibility results obtained, such as the fundamental tradeoffs between computational power and robustness, will help avoid pursuing untenable goals. Complementary to deriving general laws for the computational power of chemical kinetics, another thrust of this proposal addresses the problem of leak (spurious reactions) for a commonly used molecular primitive from DNA nanotechnology (strand displacement reactions). Leak makes it difficult to distinguish positive signal and background, which results in reduced sensitivity, as well as significantly decreased computation speed. This proposal outlines the first principled approach to proofreading in strand displacement systems that could achieve arbitrarily low levels of leak."
"1526466","SHF: Small: Secure Power Management and Delivery Exploiting Intelligent Power Networks On-Chip","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/27/2015","Eby Friedman","NY","University of Rochester","Standard Grant","Yuanyuan Yang","07/31/2019","$460,000.00","","friedman@ece.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7798","7923, 7941","$0.00","The continued advance of society and emerging market segments require functionally diverse semiconductors. In these modern heterogeneous systems, functionally diverse circuits are integrated on-chip, requiring a wide range of high quality power supply voltages. In addition, as the need for portable, secure, and high-performance Integrated Circuits (ICs) increases, intelligent management of the energy budget becomes a primary concern. To satisfy evolving power delivery requirements, the classical approach for power generation and distribution has changed over the last decade. Power generation circuits are physically closer to the loads to provide enhanced control over the quality of delivered power. Heterogeneous power delivery systems with different types of off-chip, in-package, and distributed on-chip power converters must be effectively integrated into complex integrated circuit design methodologies. Software and firmware solutions are necessary to address the high design complexity of these hierarchical nonlinear power delivery systems with thousands of power delivery components and billions of loads. An effective power delivery solution must be developed which will provide a systematic and intelligent methodology, scalable distributed architectures, algorithms for distributed power management, and specialized circuits to support the development of next generation power efficient integrated systems.<br/><br/>The primary research results of this project are to provide a systematic, scalable, intelligent, and secure solution for efficient on-chip power delivery and management. A fine grain power management framework comprising a variety of circuits, algorithms, and architectures will be developed to control power routing and switching, while optimally allocating power among a variety of different power domains at run time. To achieve efficient real-time multi-voltage power delivery, the following components will be developed: (a) a distributed, intelligent, and secure architecture for heterogeneous scalable on-chip power delivery, (b) a systematic methodology that exploits the distributed nature of the proposed architecture to provide real-time adaptive and efficient control over the quality of power, and (c) specialized circuits such as power routers, microcontrollers, and ultra-small voltage converters. These novel capabilities will fundamentally change the manner in which power is delivered and managed in complex, high performance heterogeneous systems. The development of these innovative power delivery methodologies for heterogeneous ICs will also provide new directions for educational initiatives targeting both university teaching and research activities within the broader academic community. The PI will promote the advancement and diversity of the science and engineering workforce to enhance underrepresented minority enrollment in science, technology, and engineering (STEM) programs."
"1815316","AF: Small: Locality and Energy in Distributed Computing","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","05/22/2018","Seth Pettie","MI","University of Michigan Ann Arbor","Standard Grant","Rahul Shah","09/30/2021","$449,968.00","","pettie@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7796","7923, 7934","$0.00","Distributed computing is the area of computer science that reasons about the ability of networks of independent computers to solve computational problems.  This project focuses on locality sensitive models of distributed computing, which model many types of networks arising in the real world, for example, wired computer networks, wireless sensor networks, and networks of biological agents (cells, ants, etc.).  The concept of local interaction is a compelling one that has been studied across the sciences. Theoretical advances in locality-sensitive distributed models will shed new light on similar models studied by biologists, physicists, and neuroscientists, and thereby have a broad impact across scientific disciplines. A key goal of this project is to develop and actively promote practical and theoretically attractive models of energy-efficiency for distributed computing. This project will support the development of a new course in theoretical distributed computing at the University of Michigan.<br/><br/>The project will focus mainly on the LOCAL model and derivatives that incorporate congestion, energy, radio communication, and randomization. One goal of this project is to develop a complexity theory for these models, and specifically to develop ""time hierarchy"" type theorems, characterize the value of random bits, search for complete problems within various complexity classes, and prove unconditional separations between easy and hard problems.  Another goal is to understand the exact complexity of critical algorithmic primitives in these distributed models, including symmetry-breaking primitives and information-dissemination primitives like broadcast and gossiping.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1422004","Collaborative RUI: Quadrilateral Surface Meshes with Provable Quality Guarantees","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","08/15/2014","Suneeta Ramaswami","NJ","Rutgers University Camden","Standard Grant","Rahul Shah","08/31/2019","$198,936.00","","rsuneeta@camden.rutgers.edu","311 N. 5th Street","Camden","NJ","081021400","8562252949","CSE","7796","7923, 7929, 9229","$0.00","The representation and reconstruction of complex three-dimensional objects is critical in a wide range of applications in computing today. Polygonal meshes have become the industry standard for the representation of surfaces with highly complex geometry and arbitrary genus in computer graphics and geometry processing applications. While triangle meshes are the most popular type of mesh representation for surfaces, quadrilateral meshes are better suited than triangle meshes in several applications such as character animation, texture mapping, spline-based surface modeling, mesh compression, and some specific finite element analysis applications. Provably good algorithms for generating triangle meshes from surfaces given by parametric or implicit functions or as point point clouds are widely available. However, algorithms to generate quadrilateral meshes with provable quality guarantees for such surfaces are not as prevalent, in part because the problem of generating a quadrilateral mesh from a given surface is intrinsically harder than its triangular counterpart. The goal of this project is to develop algorithms for quadrilateral meshes for various surface representations with provable guarantees on element quality as measured by commonly used metrics such as angle bounds or aspect ratio, and mesh quality as measured by mesh size or anisotropy. Direct and indirect methods (which generate a quad mesh from a triangle mesh), as well as parameterization guided methods will be utilized. Techniques from computational geometry and graph theory will play a central role in the design and development of algorithms. <br/><br/>The automated generation of provably good quadrilateral meshes for surfaces is a fundamental problem that is of interest both in theory, as it raises several geometric, combinatorial, and graph-theoretic questions, as well as practice, where the computational pipeline from designing a model for the surface to the end stage of simulation or animation is frequently dominated by the meshing process. A formal understanding of these questions is critical not only for the theoretical underpinnings of automated mesh generation, but also for the sound practice of utilizing the meshes in a wide range of applications. <br/><br/>As a collaborative effort between PIs at three undergraduate institutions, involvement of undergraduate students in research projects is an integral part of this project. An important and particular goal for this project is the creation of a larger peer group for female and minority undergraduate Computer Science majors by providing opportunities for collaboration and joint research projects between students across all three institutions. Through early and active involvement of undergraduates in the project, the PIs also seek to create a pipeline of female and minority students bound for graduate school in Computer Science."
"1420750","AF:Small:Algorithmic Number Theory","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","07/30/2014","Eric Bach","WI","University of Wisconsin-Madison","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2019","$450,000.00","","bach@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7796","7923, 7933","$0.00","This project addresses the design and analysis of algorithms in number theory.  It focuses on three subareas in detail: problems related to the prime factorization of integers, the behavior of iterated maps on finite algebraic objects such as fields and rings, and the exploitation of ideas from combinatorics and analytic number theory to further our knowledge about polynomial factoring.<br/><br/>The problems studied in this project are among the most important in computational number theory, and provide key examples for computational complexity theory and the emerging theory of quantum computing.  Algorithms to solve these problems are also extremely useful for secure and reliable electronic communication, for computer algebra, and for pseudo-random number generation. Results obtained in the project will have the potential to impact all of the above areas.  They will also enhance the connections between theoretical computer science and pure mathematics. This is because the behavior of number-theoretic algorithms is intimately connected to significant questions in analytic number theory, algebraic geometry, probability theory, and dynamical systems.<br/><br/>A key component of the project is graduate student support.  Such aid, in the form of research assistantships supervised by the PI, will help maintain the pipeline of algorithmically trained researchers in the mathematical sciences.  The project will also help integrate research and teaching, by helping him to develop and publicize interdisciplinary courses and seminars on topics such as applied number theory, communication technology, and quantum computation."
"1844455","Student Travel Support for ACM BCB 2018: Conference on Bioinformatics, Computational Biology, and Health Informatics","CCF","COMPUTATIONAL BIOLOGY","09/01/2018","08/23/2018","Dongmei Wang","GA","Georgia Tech Research Corporation","Standard Grant","Mitra Basu","08/31/2019","$10,000.00","","maywang@bme.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7931","7556, 7931","$0.00","This project provides travel support for students (graduate, undergraduate, and high school) to attend ACM SIGBio (Associate Computing Machinery Special Interest Group in Bioinformatics, Computational Biology, and Health Informatics) annual International Conference on Bioinformatics, Computational Biology, and Health Informatics (ACMBCB) in Washington DC during Aug. 29-Sept. 01, 2018 (http://acm-bcb.org/). ACMBCB2018 aims to provide an interactive forum that bridges computer science, mathematics, statistics, with biology and health, and to support advanced research, training, and outreach. It brings together top researchers, educators, practitioners, and students around the world to share innovative biological and medical discoveries, new informatics algorithms, new computational tools that will advance biological and health science, and improve national health. It consists of systematically organized activities such as plenary keynotes, workshops, tutorials, research oral sessions, poster sessions, and panels for researchers and practitioners from different disciplines to interact, to discuss, and to collaborate. It will encourage diverse groups of students, especially the underrepresented minority (women, minority, and disabled) to actively participate in conference activities, to grow ACM SIGBio community, and to advance the fast-growing interdisciplinary education and research. <br/><br/>As part of this award activity, ACMBCB2018 will organize a Student Forum and ""Women in Bioinformatics"" panel with senior academic and industrial experts serving as the mentors. These mentors will provide feedback to attending students' research, and will share their personal experience and wisdom in career development. These students will write a short summary to report their ACMBCB2018 experience.  Overall, the project will assist students in gaining knowledge, to improve their visibility and to advance novel advancements in bioinformatics, computational biology, and health informatics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814906","SHF: Small: Models and Design Tools for Tethered Molecular Circuits","CCF","COMPUTATIONAL BIOLOGY","06/15/2018","06/04/2018","Matthew Lakin","NM","University of New Mexico","Standard Grant","Mitra Basu","05/31/2021","$450,000.00","","mlakin@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","7931","7923, 7946, 9150","$0.00","Molecular computing devices detect input signals, including molecules indicative of disease states, and carry out autonomous information processing tasks on these inputs at the nanoscale. Recent work has explored the use of molecular computing devices in which the computing components are tethered to a DNA origami tile, with the layout of the components influencing their computational functionality. Such devices have a number of advantages, including faster computation times and the ability to reuse devices to scale up a circuit. This project will develop computational models, algorithms, and software tools to automate the design, implementation, and analysis of this new class of molecular computing devices. The award will enable more reliable design of these systems, providing enhanced guarantees that they will function as intended when constructed in the laboratory. The software produced will be released to the broader research community. The project will support training and education of undergraduate and graduate students, and members of underrepresented groups will take part via tracked science involvement programs. <br/><br/>When modeling and simulating molecular circuits involving tethered components, the geometry of molecular interactions is of vital importance. This project will use a novel representation of molecular structures via sets of geometric constraints on variables that represent the physical positions of specific parts of the structure. Finding a solution to these constraints corresponds to proving that the specified structure can adopt the corresponding physical conformation. This will be used as a filter to eliminate any tethered reactions for which the resulting product structure could not actually form, for example, if the reactants are too far apart to actually interact. These models will be tested and parameterized by experimentally validating their predictions with wet lab experiments, and novel designs for modular circuits will be developed as a proof of concept. The developed algorithms will be implemented and distributed as usable software tools that enable computer-aided design of tethered molecular systems. Thus, the project will adapt established techniques from theoretical computer science to handle the unique challenges of tethered molecular computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1331352","BSF:2012362:Parallel GPU Algorithms for Proximity Analysis of Freeforms","CCF","SPECIAL PROJECTS - CCF","10/01/2013","09/11/2013","Sara McMains","CA","University of California-Berkeley","Standard Grant","Nina Amla","09/30/2019","$40,784.00","","mcmains@me.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","2878","7798","$0.00","This project is funded as part of the United States-Israel Collaboration in Computer Science (USICCS) program. Through this program, NSF and the United States - Israel Binational Science Foundation (BSF) jointly support collaborations among US-based researchers and Israel-based researchers. The objective of this research project is to collaboratively design efficient algorithms to compute distance structures for analyzing 3D solid models. These structures are essential building blocks for algorithms in numerous application areas, including shape analysis, segmentation, proximity queries, meshing for finite element analysis, pattern recognition, and motion planning, with wide-ranging societal and economic benefits ranging from security to drug design. <br/><br/>The proposed approach builds on the collaborating PIs preliminary results that introduced an algorithmic approach that combines lower-envelope methods and massively parallel Graphics Processing Unit (GPU) computations. New algorithms will be developed to compute distance structures directly from the curved surfaces defining CAD models, including the defacto industry standard of Non-Uniform Rational B-Spline (NURBS) surfaces. This problem was previously considered intractable because of the resulting high order surfaces if solved analytically. The proposed GPU-based approach will address issues of speed and robustness by analyzing the curved surfaces directly, instead of using piecewise-linear approximations of the input. The research will encompass both theoretical error analysis of worst-case error and measurement of the approximation error in practice so as to optimally address the speed versus accuracy tradeoff."
"1844818","NSF Student Travel Grant for DNA24: The 24th International Conference on DNA Computing and Molecular Programming","CCF","COMPUTATIONAL BIOLOGY","10/01/2018","08/16/2018","Erik Winfree","CA","California Institute of Technology","Standard Grant","Mitra Basu","09/30/2019","$15,000.00","","winfree@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7931","7556, 7946","$0.00","With a vision to establish novel molecular programming rules for engineering for developing synthetic systems inside or outside of living cells, the annual International Conference on DNA Computing and Molecular Programming has been one of the premier interdisciplinary forums where scientists with diverse backgrounds (e.g., in computer science, physics, chemistry, biology and mathematics) come together to present their highest quality research and discuss new ideas.  This proposal aims to provide student travel support for the 24th International Conference on DNA Computing and Molecular Programming (DNA24), to be held at Jinan, China during October 8-12, 2018.  By funding travel for U.S.-based students, the organizers are actively encouraging and incentivizing a new generation of researchers to benefit from participating in the conference, fostering the development of the next generation of molecular programmers, by encouraging students to attend, present their work, and interact with other important players in the field. Up to 15 successful student applicants will be supported, by providing assistance to women and underrepresented minorities who are delivering oral or poster presentations at the conference, and to graduate students who are otherwise unable to afford attending this conference.  The availability of the awards to US citizens and students at US institutions will be included in conference announcements, soliciting applications.  <br/><br/>This highly interdisciplinary conference emphasizes topics that bridge computation, biology, and nanotechnology and attracts researchers in the fields of computer science, mathematics, chemistry, molecular biology, and nanotechnology. The scope includes control of molecular folding and self-assembly to construct nanostructures; demonstration of switches, gates, devices, and circuits with biomolecules; molecular motors and molecular robotics; computational processes in vitro and in vivo; studies of fault-tolerance and error correction; synthetic biology and in vitro evolution; software tools for analysis, simulation, and design; a range of applications in engineering, physics, chemistry, biology, and medicine.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1527767","CIF:Small:Next-Generation Compressive Phase-Retrieval Using Sparse-Graph Codes: Theory, Design and Applications","CCF","COMM & INFORMATION FOUNDATIONS","08/01/2015","07/06/2016","Kannan Ramchandran","CA","University of California-Berkeley","Standard Grant","Phillip Regalia","07/31/2019","$516,000.00","Laura Waller","kannanr@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7797","7797, 7923, 7936, 9251","$0.00","This research addresses the fundamental science of measurement system design and recovery of large-scale structured signals of interest when (a) critical phase information cannot be measured, and (b) very few measurements are available.   Also known in the technical literature as compressed phase-retrieval, this research has applications to a plethora of important scientific fields like optics, quantum physics, bio-medical imaging, astronomy, and material science.  The inability to measure phase information renders the signal recovery problem particularly challenging.  Most prior approaches are either computationally prohibitive and therefore hard to scale, or based on unproven heuristics that come with no performance guarantees. In contrast, this research fundamentally addresses the challenge of scale together with provable performance guarantees in the theory, design, experimental evaluation, and applications of compressed phase-retrieval systems.  <br/><br/>This research addresses the theoretical and algorithmic foundations for next-generation compressed phase-retrieval systems.  These are derived from a novel interdisciplinary mix of tools from coding theory, graph theory, statistical signal processing, as well as their integration into applications involving  optical imaging and quantum information systems.  The research revolves around a novel paradigm built on a family of sparse-graph codes that represent a radical departure from existing approaches based on either computationally intensive convex relaxation methods or greedy heuristics without performance guarantees.  This research has the potential to revolutionize the design and applications of next-generation compressed phase retrieval systems, similar to how Low-Density-Parity-Check (LDPC) codes have revolutionized modern communication systems, with respect to (i) measurement cost (approaching the fundamental limits); (ii)  computational and memory cost (enabling real-time or near-real-time processing); and (iii)  provable performance guarantees (with respect to the problem scale and robustness to noise)."
"1813050","CIF: Small: Collaborative Research: On the Fundamental Nature of the Age of Updates","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2018","05/30/2018","Yin Sun","AL","Auburn University","Standard Grant","Phillip Regalia","05/31/2021","$250,000.00","","yzs0078@auburn.edu","310 Samford Hall","Auburn University","AL","368490001","3348444438","CSE","7797","7923, 7935, 9150","$0.00","The Age of Information (AoI) has emerged as a novel concept, metric, and tool in the broad area of Information Sciences. AoI is clearly connected to many aspects of Information Theory, Signal Processing, and multiple applications. However, the fundamental nature of this concept has been elusive so far. This project will investigate all three aspects of the AoI, namely its significance as a performance metric, its usefulness as a tool, and, more importantly, its potential to explore profound aspects of, and fundamental interconnections among, the theories that underlie our understanding of information structure, causal information processing, and the context (or semanteme) of information. Especially in the latter, the context of the communication process is emerging as a well-defined and useful extension of the Shannon doctrine of communication. Namely, communication needs not be just reproducing at point B a message selected at point A. Rather, it is important to consider whether communication occurs for a purpose, such as for computation, prediction, or monitoring. Educational efforts will include course development at both undergraduate and graduate levels. The investigators will also strengthen the strong programs of their Institutions in recruiting women and under-represented minorities, as well as the involvement of undergraduate students in research.<br/><br/>The proposed work aims at using the new concept of Age of Information to discover the relationships between Information Theory and Signal Processing, which are two of the main pillars of Information Science. Its foundational core is the context of communication, namely on the purposes and goals of signal transmission. This research will be carried out around three thrust areas: (i) the project will explore how the transmission and the age of received updates relate to the information structure of a signal, and understand how information ages over time; (ii) the project will use an innovative approach to the traditional problems of signal processing by relating Nyquist's theory to causal signal reconstruction; (iii) the project will use AoI as a tool in handling the problem of caching and network control in volatile environments (e.g., internet of things and sensor networks).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1253370","CAREER: Reliable and Fault Tolerant Super Energy Efficient Nanomagnetic Computing in the Presence of Thermal Noise","CCF","ELECT, PHOTONICS, & MAG DEVICE, SOFTWARE & HARDWARE FOUNDATION, DES AUTO FOR MICRO & NANO SYST, IntgStrat Undst Neurl&Cogn Sys","07/01/2013","07/29/2018","Jayasimha Atulasimha","VA","Virginia Commonwealth University","Continuing grant","Sankar Basu","06/30/2019","$519,632.00","","jatulasimha@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","1517, 7798, 7945, 8624","1045, 7945, 8089, 8091, 8551","$0.00","Nanomagnetic logic and memory have emerged as powerful alternatives to conventional transistor based circuits because a multiferroic nanomagnet, switched with electrically generated strain (""straintronics""), is a far more energy-efficient switch than a transistor. However, nanomagnetic logic is extremely error-prone at room temperature since thermal fluctuations can seriously disrupt magnetization dynamics during switching. In order to make nanomagnetic logic viable, the issue of high error rate must be addressed and a solution found to mitigate its deleterious effects. This project pursues an approach for reducing error rates without calling for impractical on-chip error-correction resources.<br/> <br/>This research could enable processors to be built with superbly energy-efficient technology that can lead to implantable smart medical devices, button-sized face recognition systems, structural health monitoring systems, and consumer applications such as wearable electronics. A graduate course on nanoscale magnetism with an emphasis on nanomagnetic computing will be created. With the support of the Richmond Area Program for Minorities in Engineering, minority high school students will be hosted in the Principle Investigator's lab every summer. The Principle Investigator will develop innovative hands-on workshops to explain nanomagnetic memory and logic to K-12 students from selected schools that have low representations in Virginia science fairs and will also transfer this knowledge and expertise to teachers from these schools. Science fair participation from such schools will be analyzed to study the impact of this 5-year innovative outreach effort in K-12 teaching."
"1703765","AF:Medium:Collaborative:RUI:Structure in Motion:Algorithms for Kinematic Design","CCF","ALGORITHMIC FOUNDATIONS","06/01/2017","06/20/2018","Ileana Streinu","MA","Smith College","Continuing grant","Rahul Shah","05/31/2021","$445,526.00","","istreinu@smith.edu","10 ELM STREET","Northampton","MA","010636304","4135842700","CSE","7796","7924, 7929, 9229","$0.00","New technologies and techniques in robotics (miniature fabrication), computational biology (DNA assembly), material science (nano-technology), and even origami give rise to new questions in kinematic design ? the design of moving assemblies.  This project addresses the theory and corresponding algorithms for a number of fundamental problems in kinematic design. The goal is to design the shape of a (deformable) geometric object, along with one or a family of its deformation trajectories. In robotics, for instance, one may want to build a robot arm capable of moving its end-effector from any point in a source domain to any point in a destination domain (geometric structure design), in a way that would avoid singularities and collisions (trajectory design). In manufacturing and CAD, one is interested in designing structures that can be assembled from standard smaller parts; for example, a polyhedral surface might be assembled from rigid triangular pieces whose sides are joined together. Another example is folding a 3D origami shape from a creased piece of (flat) paper: both the 3D shape and the folding trajectories have to be designed in advance. The results are anticipated to have an impact on several scientific fields where geometric modeling and geometric simulations are being used, including mechanics, robotics, computational biology and materials science (nano-technology).  The project will engage a diverse population of students, at all levels (post-doc, graduate and undergraduate). The PIs will develop new educational materials related to emerging multidisciplinary connections.<br/><br/>New kinematic design principles rely on the mutual dependence of mobility and structure. Recent results of the PIs on robot-arm singularities and precise positional workspace determination provide insights and techniques for an integrated approach, which coordinates structural parameters and path-planning. For innovative patterns of metamaterials with auxetic capabilities, a rigorous geometric theory of periodic frameworks will be combined with efficient algorithms  for framework generation and visualization. Advancing these methods will throw new light on conformational changes in crystals and biological self-assembly problems."
"1722710","SHF: Small: Program Analysis-based Makeover for HPC Application Resilience","CCF","SOFTWARE & HARDWARE FOUNDATION","01/15/2016","12/20/2016","Chao Wang","CA","University of Southern California","Standard Grant","Almadena Y. Chtchelkanova","08/31/2019","$420,000.00","","wang626@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","7923, 7942","$0.00","HPC resilience in the presence of increased system failures is a major technical hurdle for realizing the vision of the US National Research Council for conducting exascale science. Existing techniques, based primarily on checkpoint and replay, are no longer effective for emerging systems with orders-of-magnitude more hardware and software components. This project aims to overcome the main limitation of existing techniques: the detection and mitigation of silent errors by developing and leveraging automated software analysis and synthesis techniques.<br/>The new methods under development can compile a tunable degree of resilience into the application software code, and have potential to transform the development of future generations of HPC applications. By treating the software code as white-boxes, as opposed to black-boxes, these new methods can provide significantly more economical solutions to the HPC resilience problem compared to existing techniques. The project will help realize the US NRC's vision of conducting exascale science, which is crucial for addressing the nation?s urgent needs in frontiers such as new energy, health care, and national security.<br/>This project develops automated program analysis techniques for identifying invariants from software code, and leveraging these invariants to detect and mitigate silent errors at run time. By treating the application software code as white-boxes, it seeks to generate invariants that capture the expected program behavior. By leveraging the invariants as correctness conditions, it overcomes the major hurdle in detecting silent errors, which is the lack of visible symptoms. In addition to detecting errors, the invariants are also used by runtime monitors to intelligently perturb the execution order or memory state to proactively avoid failures at run time. When the rollback recovery becomes inevitable, the invariants are used as guidance to minimize the re-execution overhead.<br/>The proposed methods and software tools are evaluated on real applications from the research community as well as sources such as SciDAC."
"1644368","EAGER: An Analog Hardware System for Solving  Boolean Satisfiability","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2016","05/05/2017","Xiaobo Hu","IN","University of Notre Dame","Standard Grant","Sankar Basu","06/30/2019","$314,612.00","Zoltan Toroczkai","shu@cse.nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7798","7916, 7945, 9251","$0.00","This proposal explores the design of a low-power and high-performance analog hardware system capable of solving Boolean Satisfiability (SAT) problem which is at the heart of many decision, scheduling, error-correction and cyber security applications. Since SAT belongs to a well-known family of hard decision problems in computer science, an efficient solution would have a profound impact in all of computational sciences, engineering, and societal applications. The project builds on close collaborations among theoreticians and hardware designers to create opportunities to cross-pollinate research areas that traditionally have had little intersection in this context. The project will allow the PIs to incorporate new research discoveries into relevant coursework, and offer research opportunities for undergraduate and graduate students, including those from underrepresented groups. <br/><br/>This proposed effort will study the potential of analog hardware based on some related deterministic Continuous Time Dynamical System (CTDS) in the form of coupled ordinary differential equations, which have been recently introduced for the study of the SAT problem. The CTDS performs gradient descent on an energy function, which itself changes in time, coupled to the performance of the dynamics through exponentially driven auxiliary variables. The project will study systematically the question of whether and by how much a CTDS based analog hardware SAT solver can outperform digital SAT solvers in terms of performance and energy efficiency. It will also advance the understanding of the impact of hardware induced noises on analog SAT solvers. In summary, the project attempts to provide insights into the relationship between the nonlinear dynamical system properties of the analog solver and the computational hardness of constraint satisfaction problems, and thus lay the foundation for analog hardware designs for CTDS solvers, as well as that for SAT solvers."
"1514434","AF: Medium: Collaborative Research: Information Compression in Algorithm Design and Statistical Physics","CCF","ALGORITHMIC FOUNDATIONS","06/15/2015","06/08/2015","Alistair Sinclair","CA","University of California-Berkeley","Standard Grant","Tracy J. Kimbrel","05/31/2019","$286,298.00","","sinclair@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7796","7924, 7926","$0.00","The existence of connections between probabilistic algorithms, statistical physics and information theory has been known for decades and has yielded a number of unexpected breakthroughs. Recent discoveries of the PIs and other researchers give clear indications that these connections go much deeper than previously thought. A key new idea is the realization that stochastic local search algorithms can be judged by their capacity to compress the randomness they consume, with convergence following as a consequence of compressibility. Further exploration of this idea is expected to have significant impact, both conceptual and technical, in multiple scientific fields. This includes algorithm design by information theoretic methods, the study of phase transitions in statistical mechanical systems based on information bottleneck arguments, and non-constructive proofs of existence of combinatorial objects. The project will offer a wide range of research opportunities at various levels of sophistication for graduate and undergraduate students in three state universities.<br/><br/>Information compression arguments have recently found striking applications in computer science and combinatorics. A glowing example is Moser's proof of the algorithmic Lovasz Local Lemma, which suggested an entirely new way of reasoning about randomized algorithms. Inspired by the work of Moser, one of the PIs with a collaborator has very recently created a general framework for analyzing stochastic local search algorithms using information compression. The framework is purely algorithmic, completely bypassing the Probabilistic Method. Besides helping to analyze the running times of existing algorithms it can also be used as a powerful new tool for designing novel, non-obvious randomized algorithms. The proposed research further develops this framework with the aim of unearthing completely new applications in computer science and combinatorics, while establishing mathematically rigorous connections to statistical physics. Concrete examples of such applications to be investigated include new tools for bounding the mixing time of Markov chains and algebraic connections between randomized algorithms and the classical theory of phase transitions in statistical physics."
"1840681","NSF Student Travel Grant for 2018 International Symposium on Distributed Computing (DISC)","CCF","INFORMATION TECHNOLOGY RESEARC, ALGORITHMIC FOUNDATIONS","07/01/2018","06/27/2018","Konstantin Busch","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Tracy J. Kimbrel","06/30/2019","$5,000.00","","busch@csc.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1640, 7796","7926, 9150","$0.00","This award will support student attendance at the 2018 International Symposium on Distributed Computing (DISC), in New Orleans, LA, October 15 to 19, 2018.  Distributed computing is ubiquitous, with many applications in the Internet and in small-scale devices.  DISC gives an opportunity to expose students to the fundamental concepts behind the algorithms that drive distributed computing applications.<br/><br/>This award will provide partial support to approximately 10 students to attend this top conference in the area of distributed computing. Efforts will be made to support students from under-represented groups and geographic diversity among students will be taken into account. The award will encourage and facilitate the student participation in this top-tier computer science conference.  It will contribute towards attracting more under-represented groups to computer science research giving students an opportunity to engage in the important technical, professional, and social exchanges that DISC fosters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840547","NSF Student Travel Grant for 2019 Annual Conference on Quantum Information Processing (QIP)","CCF","QUANTUM COMPUTING","10/01/2018","06/27/2018","Xiaodi Wu","MD","University of Maryland College Park","Standard Grant","Almadena Y. Chtchelkanova","09/30/2019","$10,000.00","","xwu@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7928","057Z, 7556","$0.00","The 22nd Quantum Information Processing Workshop (QIP 2019) will be co-hosted by the University of Colorado, Boulder (UC Boulder) and the Joint Institute for Laboratory Astrophysics (JILA) on January 12-18, 2019. This workshop is the central event in the area of Quantum Computing and, more generally, Quantum Information Science, bringing together over 500 of the leading researchers in theoretical aspects of quantum information and computation, and play a role as one of the most important venues for the groundbreaking research in theoretical computer science. Students attending this event receive a broad exposure to the most novel and exciting ideas and results in the field by contributed talks, poster sessions, rump sessions, tutorial sessions, and industry sessions.  In addition, participation in this top conference is an excellent opportunity for young researchers to develop necessary research skills in presentation and collaborations. By enabling the attendance of students with financial difficulty and encouraging the diversity of the participants, this travel grant potentially has long-term benefits after QIP 2019 has concluded.<br/> <br/>This project funds the participation of US-based students in this event. Participation at this meeting will enhance the research experiences for students and provide increased opportunities for new collaborations. NSF support will help broaden participation at the QIP Workshop by subsidizing the travel for up to 16 student participants. Organizers aim at increasing the number of student and postdoctoral participants by targeting women, undergraduates, and under-represented groups for participation. The aim is to support young researchers who might not otherwise be able to attend QIP 2019 due to financial difficulty as well as to encourage the diversity of the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814254","AF: Small: Geometry and Complexity Theory","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/21/2018","Joseph Landsberg","TX","Texas A&M University Main Campus","Standard Grant","Tracy J. Kimbrel","05/31/2021","$482,511.00","","jml@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","7796","7923, 7926, 7927","$0.00","Linear algebra, which includes computing the solutions to a system of linear equations, is at the heart of all scientific computation. The core computation of linear algebra is matrix multiplication. In 1968 V. Strassen discovered that the widely used and assumed best algorithm for matrix multiplication  is not optimal. Since then there has been intense research in both developing better algorithms and determining the limits of how much the current algorithms can be improved. There are three parts to the project. The first two are:  practical construction of algorithms for matrix multiplication,  and determining the above-mentioned limits. The third addresses a fundamental question of L. Valiant which is an algebraic analog of the famous P versus NP problem. Valiant asked if a polynomial that can be written down efficiently also must admit an efficient algorithm to compute it. All three parts will be approached using theoretical mathematics not traditionally utilized in the study of these questions (representation theory and algebraic geometry). The practical construction of algorithms in this project could potentially have impact across all scientific computation. The novel use of modern mathematical techniques previously not used in theoretical computer science will enrich both fields, opening new questions in mathematics and providing new techniques to computer science.<br/><br/>The exponent of matrix multiplication, denoted omega, is the fundamental constant that governs the complexity of basic operations in linear algebra. It is currently known that omega is somewhere between 2 and 2.38. Independent of the exponent, practical matrix multiplication (of matrices of size that actually arise in practice) is only around 2.79. For example, matrices of size 1000x1000 may be effectively multiplied by performing  (1000)^{2.79} arithmetic operations. If algorithms to achieve an omega of 2.38 were known, the same matrix operation can be performed using 220 million fewer arithmetic operations. By exploiting representation theory, this project will develop practical algorithms with the goal of lowering this practical exponent. It will also address the exponent by analyzing the feasibility of Strassen's asymptotic rank conjecture and its variants, which are proposed paths towards proving upper bounds on the exponent. The project will also address two aspects of Valiant's conjecture on permanent versus determinant. First, commutative algebra will be used to improve the current lower bound for the conjecture, which has not advanced since 2005. The investigator and a co-author have proven that Valiant's conjecture is true under the restricted model of equivariance. The second aspect will investigate loosening this restriction to weaker hypotheses under which the conjecture is still provable.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"7501793","Summer Conference Concerning National Curriculum Programs InScience, Social Science & Interdisciplinary Programs For    Elementary School Administrators","CCF","ELEM SCHL RES PERS WKSHOP","01/01/1900","01/10/1975","May Beth Givan","IA","University of Iowa","Standard Grant","  name not available","01/01/1900","$17,801.00","","","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7320","","$0.00",""
"1718902","AF: Small: Symmetry and regularity in the theory of computing","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","06/26/2017","Laszlo Babai","IL","University of Chicago","Standard Grant","Tracy J. Kimbrel","08/31/2020","$450,000.00","","lbabai@gmail.com","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7796","7923, 7926, 7927","$0.00","The acronym ""NP"" describes the class of mathematical puzzles that may or may not have a solution but if they do, the solution is easy to verify. (Think of Sudoku.) Such puzzles are at the heart of much of the progress behind the algorithms that power computation and communication, the twin pillars of societal activity in the information age, including science, commerce, banking, security, etc.  While verifying a solution to such puzzles is easy, just how difficult it is to find a solution (or decide that none exists) is an open question, one of the great open problems of mathematics today, known as the ""P vs. NP problem. Many puzzles in the class NP are known to be as hard as they can get; these problems are called ""NP-complete."" At the other end of the spectrum is the class ""P"" of ""tractable"" problems, i.e., problems solvable in ""polynomial time"" -- a theoretical benchmark of efficiency. NP-problems that do not belong to either extreme (neither in P nor NP-complete) are called ""NP-intermediate.""  In contrast to the vast collection of important problems known to be NP-complete and the rich set of problems in P, there are very few natural candidates for NP-intermediateness. One of the most prominent among these is the Graph Isomorphism (GI) problem that asks the simple question, given two networks of nodes and links, are they in effect the same (after relabeling the nodes)?<br/><br/>The PI has recently achieved a result on the complexity of GI that has been hailed as a breakthrough. For three decades, the best bound was ""moderately exponential"" (Luks, 1983), still far out in the intractable domain, but not as bad as ""exponential,"" the suspected complexity of NP-complete problems. Recently the PI dramatically reduced the complexity estimate, down to ""quasipolynomial,"" acomplexity status ""in the suburbs"" of the tractable domain, borrowing Scott Aaronson's phrase.  The result required a deeper understanding of the interplay between the notionsof symmetry and regularity of mathematical objects. The gap between these tworelated notions is the essence of the GI problem. ""Group theory"" is the nameof the algebraic theory of symmetry; this classical branch of mathematics has provided the key tools for the PI's work.<br/><br/>The PI's result generated a measure of interest across the globe, unprecedented in the PI's career and seldom seen in connection with any single result; it extended beyond the core research communities and has fascinated scientists, engineers, educators, and amateurs. The PI's first seminarlecture about the result drew an overflow audience to the largest lecture theater at the University of Chicago and was live tweeted to a global audience. Subsequently the PI gave marathon seminars and lecture series at eminent researchinstitutions coast to coast and overseas and lectured at conferences in CS, combinatorics, group theory, quantum computing.  The result was widely reported in thepopular science press.  This interest attests to the foundational nature of the problem, the unexpectedstrength of the result, and the connection of the work to multiple areas of mathematics and computation theory.  While its immediate impact outside CS is in mathematics, notably in algebraic combinatorics and asymptotic group theory, italso addresses as well as raises questions pertinent to the underlying philosophy of computational complexity theory. Invitation by the Mathematical Association of America (MAA) to deliver an address at the Joint MathematicsMeetings (January 2018) underlines the interest of mathematics educators in the work.<br/><br/>Groups of undergraduates have shown interest. The PI gave presentations at a Research Experiences for Undergraduates (REU) Site at the University of Chicago, at Budapest Semesters in Mathematics (BSM) (ahighly-rated study-abroad program for American undergraduates the PI helped found decades ago), and a student seminar in St. Andrews, U.K., arranged by a studentwho previously attended the PI's lecture at Chicago. This year, a student from the Univ. of Michigan who was among the PI's BSM audience will studyunder the PI's mentorship in Chicago over the summer. (Note: both students mentioned in this paragraph are female, a reflection of the fact that both BSM andthe Chicago REU Site make a special effort to recruit qualified female applicants.) Building on the popularity of his work, the PI continues his outreach with the larger goal of popularizing mathematics, the theory of computing, and the close interaction of these fields. A professor at St. Andrews reported increasedinterest in group theory since the PI's presentation there.<br/><br/>Researchers in the field of quantum computing have also shown interest in the PI's work. The central problem of the theory of quantum computation is to identify computational tasks that can be solved more efficiently in the quantum model than in the classical model. Following Shor's celebrated paper that demonstrated that two of the most important candidate NP-intermediate problems, factoring integers and discrete logarithm, can be solved efficiently in the quantum model using ""quantum Fourier transform"" (QFT), attention turned to GI as a problem to be attacked by a nonabelian version of QFT. While these attempts have asyet not been successful, the PI's work now provides a more fine-grained approach, some components of which may motivate a search for a quantum approach. ThePI has been invited to address quantum computing audiences, even though the PI's work is limited to the classical model.<br/><br/>Building on the momentum of the GI accomplishment, the PI will explore newfrontiers in the area of NP-intermediate problems, as well as broaden his agenda to include models of computation where structural symmetry is either part ofthe problem definition or is expected to give a new direction to the investigations. Problems of the first type include canonical forms, hypergraph isomorphism, and the group isomorphism problem, as well as the equivalence problem for certain classes of non-explicit structures such as linear codes and permutation groups. Problems of the second type include the sensitivity problem for Booleanfunctions and problems in the ""property testing"" model, including local list-decoding of homomorphism codes, an area initiated by the classic paper of Goldreich and Levin on Hadamard codes and more recently championed by Madhu Sudan andhis collaborators.<br/><br/>Problems at the interface of algorithms and group theory are expected to play akey role in the entire project. Beyond the combinatorial problems that have arisen from the study of GI, the project leads to analogous questions of the symmetry vs. regularity gap in linear algebra.<br/><br/>An important extension of the GI problem is the question, can one find canonical forms as efficiently as testing isomorphism under the new result. The PI expects that isomorphism of hypergraphs can be tested in time quasipolynomial inthe number of vertices and polynomial in the number of edges, providing an interpolation between the GI result and a 1999 result of Luks on hypergraph isomorphism. The PI's results on canonical structures (""Split-or-Johnson routine"") invite a study of connections to mathematical logic, suggested by audience members at a recent workshop at the Simons Institute for the Theory of Computing in Berkeley.<br/><br/>The isomorphism problem for explicitly given groups represents a barrier to further progress on GI; its quasipolynomial complexity has not been significantlyreduced over the past several decades, in spite of the availability of powerful algebraic machinery. The PI aims to gain a better understanding of this barrier in the critical case of nilpotent groups of class 2. The related equivalence problem for linear codes may also have connections to cryptography"
"1733860","AitF: Mechanism Design and Machine Learning for Peer Grading","CCF","Algorithms in the Field, ALGORITHMIC FOUNDATIONS","09/01/2017","04/06/2018","Jason Hartline","IL","Northwestern University","Standard Grant","Tracy Kimbrel","08/31/2021","$716,000.00","Douglas Downey, Eleanor O'Rourke","hartline@eecs.northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7239, 7796","7932, 9102, 9251","$0.00","This project explores the design and analysis of peer grading technology.  A peer grading system is an online tool that collects student submissions, assigns review tasks to the students and graders, and aggregates reviews to produce assessments of both the submissions and the peer reviews.  The PIs have developed a prototype system and have collected preliminary evidence that suggests that peer review has important potential benefits:<br/><br/>1.  Learning by reviewing: Students learn from critical assessment of other students' work.  In the PIs' prototype at Northwestern, 60% of the students reported that peer review helped them learn course material and 55% of the students reported that peer review helped them to prepare better homework solutions themselves.<br/><br/>2.  Reduced grading staff: Peer grading reduces the grading load on course staff and allows for effective teaching with larger classes.  This is especially important currently, as interest in computer science classes increases at a faster pace than teaching resources.  In the PIs' prototype at Northwestern, the course staff graded 1/5 of the student submissions.<br/><br/>3.  Promptness of feedback: Reduced teacher grading enables prompt feedback to students.  In the PIs' prototype at Northwestern, peer reviews were available within three days and final assessment of both the submission and peer reviews were available within five days.  Prior to introducing peer review, assessments took one to two weeks.<br/><br/>A peer grading system is comprised of three main components:<br/><br/>1.  The review matching algorithm determines which peers should review which submissions and which submissions should be reviewed by the teacher.<br/><br/>2.  The submission grading algorithm aggregates the reviews of the peers and the submissions and assigns grades to the submissions.<br/><br/>3.  The review grading algorithm compares the peer reviews with the teacher reviews and assigns grades to the peer reviews.  Without this algorithm, peers may not put effort into providing quality reviews, and the reviews will be neither accurate for grading nor beneficial for the peer.<br/><br/>The details of these algorithms are crucial for the proper working of the peer review system.  A main research effort of this project is to identify the algorithms to use for each of these components.  The review matching algorithm affects the accuracy of the subsequent grading algorithms and the grading load of the teacher.  The submission grading algorithm determines which peer reviews are accurate and which are inaccurate and uses this understanding to assign grades to the submissions that are representative of the submission quality.  The review grading algorithm incentivizes the peers to put in sufficient effort to determine whether a submission is good or bad and it is calibrated so that good reviews and bad reviews get the appropriate review grades.  <br/><br/>The PIs have implemented prototypes of these algorithms as part of a peer grading system that has been prototyped in Northwestern computer science classes.  However, the space of possible algorithms is large and the PIs' work on the prototype has yet to determine the algorithms that combine to give the best education outcomes.  A main focus of this project will be improving the understanding of which algorithms lead to the best education outcomes.<br/><br/>Theoretical work in algorithms and machine learning provides a starting point for the project's study of good algorithms for peer grading systems.  A key endeavor of the project is translating and applying these theoretical algorithms to the peer grading domain.  As one example, proper scoring rules are a natural approach for grading the peer reviews.  However, test runs of the PIs' prototype implementation suggest that these rules might not be so good in practice.  Both new models and algorithms are needed in theory, and these new algorithms need to work in practice."
"1921795","AF: Medium: Collaborative Research: Circuit Lower Bounds via Projections","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","02/22/2019","Li-Yang Tan","CA","Stanford University","Continuing grant","Tracy Kimbrel","03/31/2020","$266,775.00","","liyang@ttic.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","7924, 7927","$0.00","Computers play a central role in how we work, play, and communicate with each other in today's world.  It is a truism that  computers have grown steadily more powerful over the years, but equally important (if not more so) than the amount of sheer computing power available is how efficiently we are able to harness that power.  Finding an efficient strategy to solve a given problem (in the language of computer science, an efficient algorithm) can often spell the difference between success and failure.  (As an illustrative analogy, consider the task of assembling a large jigsaw puzzle.  A poor choice of strategy such as a brute-force approach of trying each pair of pieces against each other may be infeasibly slow, while a cleverer approach such as grouping pieces by their color can be radically more efficient and lead to a feasible solution.)  But in order to fully understand the abilities of efficient  algorithms, it is crucial to also understand their limits: what is it that efficient algorithms *cannot* do? The field of ""computational  complexity"", which is the subject of the PIs' project, seeks to mathematically prove that certain computational problems do not admit *any* efficient algorithm no matter how long and hard we try to develop one.  Such results can have both practical value (by guiding algorithm development away from ""dead ends"") and deep theoretical significance, as they play a profound role in shaping our fundamental understanding of the phenomenon of computation.<br/><br/>The 1980s witnessed exciting progress on a range of Boolean circuit models (a mathematical abstraction of the digital circuits that modern computers are built from) in computational complexity; researchers succeeded in proving many lower bounds establishing that various computational problems have no efficient algorithms in these models.  However, further progress slowed significantly after the 1980s. Many of the landmark results obtained in this era were based on the ""method of random restrictions"", which roughly speaking uses probabilistic arguments to show that Boolean circuits can be dramatically simplified by making certain random substitutions of constant values for input variables.  In this project the PIs will intensively investigate an extension of the method of random restrictions which they call the ""method of random projections."" Rather than simply substituting constant values for input variables, the random projection method additionally identifies groups of variables, ""projecting"" them all to the same new variable so that they must all take the same value.  While the underlying idea is simple, it turns out that this identification of variables helps ""maintain useful structure"" which is extremely useful for proving lower bounds.  In recent work the PIs have successfully used this new ""method of random projections"" to solve several decades-old problems in Boolean circuit lower bounds and related areas (which in some cases had notoriously resisted progress since the 1980s or 1990s).  As the main intellectual goals of the project, the PIs will continue to develop and apply the method of random projections to attack important open problems in Boolean circuit complexity.<br/><br/>In addition to the technical goals described above, other central goals of the project are to educate, communicate, and inspire.  The PIs will train graduate students through research collaboration, disseminate research results through seminar talks, survey articles and other publications, and continue ongoing outreach activities aimed at increasing interest in and awareness of theoretical computer science topics in a broader population, including presentations at elementary schools."
"1816372","AF: Small: Parallels in Approximability of Discrete and Continuous Optimization Problems","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","05/22/2018","Madhur Tulsiani","IL","Toyota Technological Institute at Chicago","Standard Grant","Rahul Shah","09/30/2021","$497,239.00","","madhurt@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7926","$0.00","Optimization problems arise naturally in many areas such as scheduling, artificial intelligence, software engineering, control of robotic systems, statistics and machine learning. Many of these problems require too long to solve exactly - a common approach for dealing with this has been to design techniques which can efficiently find approximate solutions that are 'good enough' for the task at hand. The study of what approximations are best possible, as well as methods for achieving them, has also led to many new ideas in theoretical computer science, leading to a rich mathematical theory. This project considers several such problems (arising in different areas) which represent challenges to our current understanding. The goal of the project is to develop unified techniques for solving and analyzing them. The project includes several opportunities for training and mentoring of graduate and undergraduate students. Another aim of the project is to develop a collaborative forum for theoretical computer science students in the Chicago area, which can be used to discuss technical ideas and develop expository material.<br/><br/>This project considers various problems in discrete and continuous optimization, which represent bottlenecks for algorithmic techniques for designing approximation algorithms, as well as for techniques proving hardness of approximation. The difficulty of understanding many of these problems arises from the fact that many of them only impose a relatively weak global constraint on the solutions, which is hard to exploit algorithmically and also not amenable to techniques for proving inapproximability. The project considers several continuous optimization problems which offer an ideal testbed for the development of new algorithmic techniques, while still capturing the bottlenecks in proving inapproximability of related discrete problems. The aim of this project is to examine such problems from the following perspectives: (1) average-case hardness and lower bounds for the Sum-of-Squares hierarchy of convex relaxations; (2) techniques and barriers for proving inapproximability; and (3) conditions under which good approximations are achievable.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617790","CCF-BSF: AF: Small: Metric Embeddings and Partitioning for Minor-Closed Graph Families","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","05/31/2016","Anupam Gupta","PA","Carnegie-Mellon University","Standard Grant","Tracy Kimbrel","06/30/2019","$450,000.00","","anupamg@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","022Z, 7923, 7926","$0.00","The interaction between algorithm design and metric embeddings has been a very fruitful one. Over the past two decades, the toolbox of metric embeddings has become an indispensable one for the algorithm designer. The reason is simple: embeddings give a set of techniques to simplify graphs and metric spaces. This has led to approximation algorithms for many graph partitioning and network design problems. In turn, better graph decompositions have led to better embeddings.  Nevertheless, the interplay between graph topology and the metric geometry is not fully understood. This proposal focuses on developing new techniques for interesting families of graphs.  Broader impacts include the engagement of undergraduate students and women in research, and the training of graduate students and postdoctoral associates. Moreover, the proposed research should help develop deeper connections between computer science and mathematics. Being part of a joint initiative between NSF and the US-Israel Binational Science Foundation, this project will increase collaboration between researchers working on similar topics in the United States and Israel.<br/><br/>The technical aspects of the research focus on questions in metric embeddings and graph partitioning for planar, bounded tree-width and path-width graphs, and general minor-closed families of graphs.  These include getting better low-diameter partitions for these families via the bounded-threatener program, getting better L1 and tree embeddings, and improved metric and graph compression techniques, such as improved vertex-sparsifiers and spanners."
"1254044","CAREER: Understanding Polynomial Structure Analytically and Algorithmically","CCF","ALGORITHMIC FOUNDATIONS, ALGORITHMS","07/01/2013","07/04/2017","Madhur Tulsiani","IL","Toyota Technological Institute at Chicago","Continuing grant","Tracy Kimbrel","06/30/2019","$511,860.00","","madhurt@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796, 7926","1045, 7927, 9251","$0.00","Developing new analytic tools and proof techniques will advance computer science in a fundamental way.  The aim of this project is to explore techniques used in several different areas of theoretical computer science and mathematics in a unified way. One such development involves the study of low-degree polynomials. This research will address problems arising from two main considerations:<br/><br/>- Understanding the analytical structure of functions which have a small but significant correlation with a low-degree polynomial over a finite field.<br/><br/>- Understanding the expressive power of low-degree polynomials over reals for capturing proofs of several combinatorial facts.<br/><br/>The former leads to many questions spanning the areas of arithmetic combinatorics, coding theory, learning theory, hardness of approximation and Fourier analysis. Some of these questions extend the notions of list-decoding in coding theory and are also key to developing an algorithmic version of the theory of higher-order Fourier analysis, developed recently to study questions in arithmetic combinatorics.  The latter consideration will strengthen our understanding of  techniques from combinatorial optimizations and proof complexity.  Progress on these questions will shed light on the design of new approximation algorithms using linear and semidefinite programs.<br/><br/>Part of this project also includes designing two new courses, to be offered at Toyota Technical Institute and the University of Chicago.  The PI also plans to collect short ""research summaries"" to outline some of the exciting  questions in different areas of theory, and make them accessible to advanced high-school and undergraduate students."
"1749609","CAREER: The Interplay between Combinatorial Optimization and Algorithmic Convex Geometry","CCF","ALGORITHMIC FOUNDATIONS","03/15/2018","02/07/2019","Yin Tat Lee","WA","University of Washington","Continuing grant","Rahul Shah","02/28/2023","$206,571.00","","yintat@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","1045, 7926","$0.00","Optimization problems, which look for the best solution satisfying given constraints, may arise in discrete settings, where there is a choice of a distinct set of possibilities like nodes in a graph, or continuous settings, where the choices are numbers that can be tuned with fine adjustments. Convex optimization problems are a special class in which the constraints are simple enough that continuous problems gain geometric and/or discrete structures. These structures have been studied extensively over the past century. Some of these techniques like linear programming have become fundamental tools in all fields of science.  More recently, there have been interesting cases in which continuous methods have yielded advances in discrete optimization. In this project, the PI seeks to develop significantly more efficient algorithms to solve convex problems, especially problems come from graph and combinatorial problems. A major obstacle to developing nearly linear-time algorithms for both continuous and discrete problems is the lack of understanding of convex geometry and its connection to algorithms. <br/><br/>This project focuses on three key topics and related goals: 1) Algorithmic Convex Geometry: understand how well convex sets can be approximated by ellipsoids (KLS conjecture), explore ways to deform an explicit given polytope as a Riemannian manifold and use the deformations to develop faster polytope sampling algorithms; 2) Combinatorial Optimization: break the square root iterations barrier for solving linear programming by understanding the relation between convex geometry and interior point methods to develop a fine-grained complexity for interior point methods; 3) Convex Optimization: investigate cutting plane methods via the geometry of localization sets, develop efficient cutting plane methods via algorithmic convex geometry and explore non-convex optimizations via sampling algorithms. Improved algorithms for optimization developed via this project through an improved understanding of these topics can have a broad impact across various sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618203","SHF:SMALL:Certified cost recurrences for higher-order functional programs","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2016","05/31/2016","Norman Danner","CT","Wesleyan University","Standard Grant","Anindya Banerjee","05/31/2020","$461,830.00","Daniel Licata","ndanner@wesleyan.edu","237 HIGH ST","Middletown","CT","064593208","8606853683","CSE","7798","7923, 7943, 9251","$0.00","One of the main jobs of a computer scientist is inventing new solutions to computational problems.  There are often many different ways of solving the same problem, and running these different solutions on a computer can cost drastically different amounts of time, memory, power, money, or other resources.  Given limited resources, it is often the case that one solution will accomplish a desired goal, while another solution will fail to do so -- not because the solution is wrong in principle, but because it would cost too much. Therefore, programmers need to be able to predict how much a solution will cost before actually running it, in order to predict whether a proposed solution will successfully accomplish the desired goals.  In this project, the principal investigators, along with a postdoctoral fellow and students, are investigating new techniques for predicting the resources used by programs.  The intellectual merits of the project are advancing the state of the art in interactive reasoning about program cost, building on several different areas of computer science research.  The project is investigating methods that can be implemented in interactive tools, so that a computer can help with making these cost predictions and checking that programmers' predictions are correct.  The project's broader significance and importance are improving the quality of software, by making it easier for programmers to both code in a high-level language and reason precisely and formally about the cost of their programs, leading to faster and more maintainable code.  The project is training undergraduate, graduate, and postdoctoral researchers for scientific careers.  Finally, the techniques developed by this project may inform the development of pedagogical tools for computer science students.<br/><br/>More technically, this project is developing the foundations of a tool that programmers can use to semi-automatically analyze the execution cost of programs, in the style of an interactive theorem prover or proof assistant. The main topic of investigation is formally certified methods for the extraction and solution of cost recurrences from source code -- a method of cost analysis that allows a smooth transition between automated and manual verification methods, and is applicable to a wide class of programs.  The investigators' prior work in this area shows that the process of extracting recurrences from higher-order functional programs on programmer-defined inductive datatypes can be viewed as a translation from the source language to a complexity language, followed by a semantic interpretation of the complexity language. This method is certified by a bounding theorem, proved by logical relations, which implies that the cost predicted by the recurrences bounds the execution cost of the program on all inputs. This project is extending these techniques to more fully-featured source languages (e.g., supporting general recursion and coinductive datatypes); to other forms of cost analyses (e.g., parallel cost and amortized analysis); and to deeper analysis of extracted recurrences (e.g., methods for solving higher-order recurrences; a syntactic logic for manipulating recurrences). The project is developing formalizations and implementations of the proposed techniques, and applying the techniques to verifying compiler optimizations."
"1704285","AF:Medium:Collaborative:RUI:Structure in Motion:Algorithms for Kinematic Design","CCF","ALGORITHMIC FOUNDATIONS","06/01/2017","06/20/2018","Ciprian Borcea","NJ","Rider University","Continuing grant","Rahul Shah","05/31/2021","$63,851.00","","borcea@rider.edu","2083 Lawrenceville Road","Lawrenceville","NJ","086480400","6098965000","CSE","7796","7924, 7929, 9229","$0.00","New technologies and techniques in robotics (miniature fabrication), computational biology (DNA assembly), material science (nano-technology), and even origami give rise to new questions in kinematic design ? the design of moving assemblies.  This project addresses the theory and corresponding algorithms for a number of fundamental problems in kinematic design. The goal is to design the shape of a (deformable) geometric object, along with one or a family of its deformation trajectories. In robotics, for instance, one may want to build a robot arm capable of moving its end-effector from any point in a source domain to any point in a destination domain (geometric structure design), in a way that would avoid singularities and collisions (trajectory design). In manufacturing and CAD, one is interested in designing structures that can be assembled from standard smaller parts; for example, a polyhedral surface might be assembled from rigid triangular pieces whose sides are joined together. Another example is folding a 3D origami shape from a creased piece of (flat) paper: both the 3D shape and the folding trajectories have to be designed in advance. The results are anticipated to have an impact on several scientific fields where geometric modeling and geometric simulations are being used, including mechanics, robotics, computational biology and materials science (nano-technology).  The project will engage a diverse population of students, at all levels (post-doc, graduate and undergraduate). The PIs will develop new educational materials related to emerging multidisciplinary connections.<br/><br/>New kinematic design principles rely on the mutual dependence of mobility and structure. Recent results of the PIs on robot-arm singularities and precise positional workspace determination provide insights and techniques for an integrated approach, which coordinates structural parameters and path-planning. For innovative patterns of metamaterials with auxetic capabilities, a rigorous geometric theory of periodic frameworks will be combined with efficient algorithms  for framework generation and visualization. Advancing these methods will throw new light on conformational changes in crystals and biological self-assembly problems."
"1553166","CAREER:  Bridging the gap between theoretical and experimental self-assembly through computational modeling","CCF","ALGORITHMIC FOUNDATIONS, SOFTWARE & HARDWARE FOUNDATION, COMPUTATIONAL BIOLOGY","07/01/2016","05/03/2018","Matthew Patitz","AR","University of Arkansas","Continuing grant","Mitra Basu","06/30/2021","$515,954.00","","patitz@uark.edu","210 Administration Building","FAYETTEVILLE","AR","727011201","4795753845","CSE","7796, 7798, 7931","1045, 7931, 7946, 9150, 9251","$0.00","The goal of this project is to develop software which enables techniques from theoretical modeling of self-assembling systems based on DNA nanotechnology to be developed and evaluated using rigorous molecular dynamics simulations, and then incorporated into physical molecular designs and implementations.  The software to be developed will consist of a fully integrated suite of open source software which will create a seamless pathway for the design of self-assembling systems based on components called tiles, via a high-level programming language interface to an abstract tile assembly simulator, all the way through the final output of the fully specified DNA strands which have been verified via highly accurate and DNA specific molecular simulations.  The second main component of the proposed work is the integration of theoretical techniques and designs for algorithmic self-assembling systems into DNA-based motifs, and the performance of extensive simulation-based experiments to develop molecular designs for components which yield systems that are more robust to error and varying environmental conditions than current systems, allowing them to be more scalable and widely utilized.<br/><br/>Developing robust and scalable molecular self-assembling systems has the promise to greatly impact many aspects of science and technology, potentially enabling atomically precise manufacturing of materials with carefully specified properties, implementation of molecular ""robots"", and targeted drug delivery mechanisms which are able to diagnose and treat diseases in vivo. To realize the great promise of this field, it is important to recognize that it is fundamentally interdisciplinary and incorporates fields such as physics, chemistry, mathematics, computer science, and biochemical engineering, among others, and to foster the development of researchers experienced in these areas and also skilled at this interdisciplinary work. This project focuses on interdisciplinary collaboration and student education, includes the development of an interdisciplinary course (""Introduction to DNA Nanotechnology''), the hosting of interdisciplinary workshops for high school students and also for experienced researchers, conducting interdepartmental seminars, and the development of software which can be easily used by scientists from many disciplines to quickly become proficient in this area.  This will help train extremely valuable researchers capable of synthesizing knowledge and techniques spanning many disciplines.<br/><br/>This project will create a freely released software suite capable of automating the design of DNA-based self-assembling systems.  The work will involve extending current simulation software to be massively parallelizable, allowing simulations to be run across large supercomputing clusters, and the creation of new software modules able to perform automated, quantitative analyses of simulation results.   Additional modules will be created to enable the suite to provide an end-to-end solution from theoretical design to simulation-based validation, along with an easily used web-based front end and database storage of results. The theoretical aspects of the project will involve incorporation of various theoretical construction techniques into molecular designs, and validation of those designs.  Some specific theoretical techniques which will be exploited and evaluated include the incorporation of geometric hindrance to enforce correct algorithmic behavior, and hierarchical self-assembly."
"1617306","AF: Small: Approximate Counting, Markov Chains and Phase Transitions","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","05/25/2016","Eric Vigoda","GA","Georgia Tech Research Corporation","Standard Grant","Rahul Shah","08/31/2019","$250,000.00","","ericvigoda@gmail.com","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","7923, 7926","$0.00","This project studies algorithms for counting and sampling problems.  For an exponentially large set of discrete, combinatorial objects, the goal is to estimate the size of this set or randomly sample from it in polynomial-time. Algorithms for these problems are used in a variety of scientific fields, often with little rigorous guarantees on their performance, and the results of this project will enhance the reliability of such studies.  The results of this project will enhance connections between statistical physics and theoretical computer science by formalizing connections between phase transitions in statistical physics with the efficiency of algorithms for this type of counting/sampling problems. In addition, the PI will organize inter-disciplinary workshops tying together researchers from statistical physics, discrete mathematics, and theoretical computer science.<br/><br/>Loopy Belief Propagation (BP) and Markov Chain Monte Carlo (MCMC) algorithms are two popular algorithms for the counting/sampling problems of approximating partition functions and sampling from Gibbs distributions. In this project the PI intends to present new techniques for proving convergence results for loopy BP and MCMC algorithms. This will result in new, efficient counting/sampling algorithms for problems of combinatorial interest including weighted independent sets and colorings of a graph. These results will enhance recent results establishing beautiful connections between the approximability of counting problems for graphs of maximum degree D with statistical physics phase transitions for infinite D-regular trees."
"1813078","CIF: Small: Collaborative Research: On the Fundamental Nature of the Age of Updates","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2018","05/30/2018","Anthony Ephremides","MD","University of Maryland College Park","Standard Grant","Phillip Regalia","05/31/2021","$241,283.00","","etony@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7797","7923, 7935, 9150","$0.00","The Age of Information (AoI) has emerged as a novel concept, metric, and tool in the broad area of Information Sciences. AoI is clearly connected to many aspects of Information Theory, Signal Processing, and multiple applications. However, the fundamental nature of this concept has been elusive so far. This project will investigate all three aspects of the AoI, namely its significance as a performance metric, its usefulness as a tool, and, more importantly, its potential to explore profound aspects of, and fundamental interconnections among, the theories that underlie our understanding of information structure, causal information processing, and the context (or semanteme) of information. Especially in the latter, the context of the communication process is emerging as a well-defined and useful extension of the Shannon doctrine of communication. Namely, communication needs not be just reproducing at point B a message selected at point A. Rather, it is important to consider whether communication occurs for a purpose, such as for computation, prediction, or monitoring. Educational efforts will include course development at both undergraduate and graduate levels. The investigators will also strengthen the strong programs of their Institutions in recruiting women and under-represented minorities, as well as the involvement of undergraduate students in research.<br/><br/>The proposed work aims at using the new concept of Age of Information to discover the relationships between Information Theory and Signal Processing, which are two of the main pillars of Information Science. Its foundational core is the context of communication, namely on the purposes and goals of signal transmission. This research will be carried out around three thrust areas: (i) the project will explore how the transmission and the age of received updates relate to the information structure of a signal, and understand how information ages over time; (ii) the project will use an innovative approach to the traditional problems of signal processing by relating Nyquist's theory to causal signal reconstruction; (iii) the project will use AoI as a tool in handling the problem of caching and network control in volatile environments (e.g., internet of things and sensor networks).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1718007","CIF: Small: Signal Processing for Quanta Image Sensors: Reconstruction, Sampling, and Applications","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2017","06/28/2017","Stanley Chan","IN","Purdue University","Standard Grant","Phillip Regalia","06/30/2020","$480,947.00","","stanchan@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7923, 7935, 7936","$0.00","The proliferation of digital cameras over the past decades has revolutionized almost every aspect of science, technology and society. However, as we continue to shrink image sensors to increase spatial resolution and reduce camera size, the size of the photon-collecting site of a sensor will soon reach a fundamental limit below which no meaningful signal can be generated. The goal of this research is to investigate a new type of image sensor called the Quanta Image Sensors (QIS). Compared to conventional image sensors, QIS offer substantially higher spatial resolution, temporal resolution, and dynamic range. Because of these important features, the sensor has the potential to impact a wide range of applications in digital cameras, consumer electronics, wearable devices, medical imaging, and physical science. <br/><br/>The focus of the research is to develop signal processing theories and algorithms to support QIS. Building upon previous studies in QIS, this research puts extra emphasis on bridging the theories to practice. Specific questions addressed include: (1) Image reconstruction. How to reconstruct images from the massive binary bit-stream collected by the QIS without using iterative algorithms? (2) Sampling. How to control the sensor?s sampling mechanism in spatial-temporal resolution, color space, and threshold to maximize the signal to noise ratio? (3) Application. How to track fast moving objects using QIS without reconstructing images? Results from this research will foster new signal processing techniques for imaging in photon limited environments."
"1813123","SHF:Small:A Domain-Specific Language for Designing Cognitive-Science Experiments","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","06/04/2018","Matthew Flatt","UT","University of Utah","Standard Grant","Anindya Banerjee","09/30/2021","$423,575.00","","mflatt@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798","7923, 7943","$0.00","When a cognitive scientist designs an experiment, the design typically specifies factors of interest, the way that multiple factors are combined, and constraints that avoid combinations that are not interesting or that balance the frequency of combinations. Computers have enabled more sophisticated experimental designs, since a program can generate the steps for an experiment, but writing a program that correctly handles an experiment's constraints can be difficult. Worse, subtle mistakes in the program can bias the generated stimuli and invalidate the experiment's results. The project's novelties are a programming language that lets a scientist describe an experiment's design in higher level terms --- that is, higher than a general-purpose language's loops and conditionals --- and an execution engine to generate an experiment's stimuli from its description. The project's impacts are to make experiment design faster, to provide a better way for scientists to communicate their experiment designs, and to improve the reproducibility of experiments by ensuring statistical properties of the experiment's stimuli.<br/><br/>The key to good statistical properties for generated stimuli is to uniformly sample the space of solutions to the experiment's constraints. In a simple experiment where all factors are crossed and combinations are weighted equally, random selection is unbiased and easy to implement. The programming problem is considerably more difficult when constraints are imposed on the combinations, such as having twice as many of one kind of stimuli in a row, versus having some other kind of stimuli in a row. State-of-the-art tools, such as UNIGEN, can sample from a constrained space, but the space must be expressed as a SAT formula, which is far from the way that cognitive scientists think about their designs. The project will combine this sampling technology with tools for hosting and compiling domain-specific languages, allowing an experimenter to work in domain terms while bridging the gap between the experimenter's view and the implementation mechanics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1905145","Collaborative Research: Verification Mentoring Workshop at Computer Aided Verification 2019-2021","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2019","03/22/2019","Loris D'Antoni","WI","University of Wisconsin-Madison","Standard Grant","Nina Amla","04/30/2022","$33,200.00","","loris@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7798","7556, 8206","$0.00","This grant will enable students to attend the Verification Mentoring Workshop (VMW 2019) which will be co-located with the International Conference on Computer Aided Verification (CAV), to be held in New York City, July 23-26, 2019. CAV is one of the premier conferences in computer science, dedicated to the advancement of the theory and practice of computer-aided formal analysis methods for hardware and software systems. The purpose of Verification Mentoring Workshop is to provide mentoring and career advice to early-stage graduate students, to attract them to pursue research careers in the area of computer-aided verification. Funded students will benefit greatly from the opportunity to engage in the critical technical, professional, and social exchanges that both conferences foster.<br/><br/>Computer-aided verification covers a broad range of applications, from ensuring the correctness and safety of computer systems, to improving design and development productivity. The funded students will attend CAV, a top conference where they will learn about current research problems and interact with leaders in the field, and the VMW workshop which will focus on mentoring and career advice specific to early-stage graduate students who aim to pursue research careers in the area of computer-aided verification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1559855","REU Site: DIMACS REU in Computing Theory and Applications Impacting Society","CCF","RSCH EXPER FOR UNDERGRAD SITES","04/01/2016","06/07/2018","Rebecca Wright","NJ","Rutgers University New Brunswick","Standard Grant","Rahul Shah","03/31/2019","$413,771.00","Margaret Cozzens, Lazaros Gallos","rwright@barnard.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1139","9102, 9250","$0.00","This project is the continuation of an ongoing REU site at DIMACS (Rutgers University). Undergraduate students recruited from across the nation participate in summer research projects relating to foundations and applications of theoretical computer science. Each year, the program recruits 10 outstanding undergraduate students from diverse groups including women, underrepresented groups, and non-research institutions. A strong point of this site is the experience and expertise of the mentoring team, which has been able to produce valuable research experiences and excellent research publications in the past. Many past students have gone on to pursue successful research careers.<br/><br/>The site offers a wide variety of projects ranging from theoretical topics such as circuit complexity, graph matching, and error-correcting codes to more applied topics such as robot planning, active learning, security and privacy. It also offers socially relevant projects such as food webs, analyzing immune-tumor interactions, and genome folding. Many of these are leading-edge research projects that not only develop students' research skills but also give them exposure to respective research areas. The students also benefit from DIMACS industry partners in the region. The project will lead to publishable research and contribute to building the future work force."
"1905108","Collaborative Research: Verification Mentoring Workshop at Computer Aided Verification 2019-2021","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2019","03/22/2019","Roopsha Samanta","IN","Purdue University","Standard Grant","Nina Amla","04/30/2022","$66,800.00","","roopsha@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","7556, 8206, 9102","$0.00","This grant will enable students to attend the Verification Mentoring Workshop (VMW 2019) which will be co-located with the International Conference on Computer Aided Verification (CAV), to be held in New York City, July 23-26, 2019. CAV is one of the premier conferences in computer science, dedicated to the advancement of the theory and practice of computer-aided formal analysis methods for hardware and software systems. The purpose of Verification Mentoring Workshop is to provide mentoring and career advice to early-stage graduate students, to attract them to pursue research careers in the area of computer-aided verification. Funded students will benefit greatly from the opportunity to engage in the critical technical, professional, and social exchanges that both conferences foster.<br/><br/>Computer-aided verification covers a broad range of applications, from ensuring the correctness and safety of computer systems, to improving design and development productivity. The funded students will attend CAV, a top conference where they will learn about current research problems and interact with leaders in the field, and the VMW workshop which will focus on mentoring and career advice specific to early-stage graduate students who aim to pursue research careers in the area of computer-aided verification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832377","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","EXPERIMENTAL EXPEDITIONS","03/01/2018","08/01/2018","Kenneth Brown","NC","Duke University","Continuing grant","Almadena Chtchelkanova","02/28/2023","$400,000.00","","kenneth.r.brown@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1725657","SPX: Enabling Scalable Synchronizations for General Purpose GPUs","CCF","SPX: Scalable Parallelism in t","10/01/2017","09/08/2017","Jun Yang","PA","University of Pittsburgh","Standard Grant","Marilyn McClure","09/30/2021","$850,000.00","Rami Melhem","juy9@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","042Y","026Z","$0.00","Today?s GPUs have been successfully used for general purpose processing including high-performance computing, machine learning, graph analytics, to name a few applications. However, general purpose computing has different complexity and characteristics from those of graphics processing. A crucial characteristic is the need for coordination and synchronization among parallel threads, which occurs more often in general purpose applications than in graphics rendering. However, unlike common CPU designs, current GPU designs are missing full-fledged support for synchronization primitives that are convenient for implementing transparent scalability and achieving high performance for many parallel applications.<br/><br/>This project develops truly effective, efficient, easy-to-use and easy-to-implement synchronization mechanisms for GPGPU while overcoming major obstacles pertaining to the GPU architecture. The first thrust will identify GPGPU synchronization primitives that are deemed sufficient to handle a large set of applications with static and dynamic dependencies. The second thrust will provide solutions and methodologies on how to use the new primitives in the most efficient way to suit the characteristics of applications. The third thrust addresses the main challenges in implementing those primitives in efficiency, possible context switching overhead, and memory capacity limitations. The success in this project will help fuel the spread of accelerator architectures for high-performance computing, cloud and mobile systems. It will increase efficiency for improved performance and reduced energy/power consumption, leading to a greener IT industry. Trained students will become future taskforce to continue the revolution of extending computing into every realm of science, life, commerce and culture."
"1351115","CAREER: Design Automation for Microfluidic Large Scale Integration Laboratories-on-a-Chip","CCF","COLLABORATIVE RESEARCH, SOFTWARE & HARDWARE FOUNDATION","03/01/2014","09/20/2017","Philip Brisk","CA","University of California-Riverside","Continuing grant","Sankar Basu","02/29/2020","$506,816.00","","philip@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7298, 7798","1045, 5952, 7556, 7945","$0.00","Emerging laboratory-on-a-chip technology enables automation and miniaturization of biochemical reactions for applications such as DNA sequencing, drug discovery, detection of influenza and other pathogens, biothreat detection, and water purification. Scientists presently must design new devices for each application by hand. This may take weeks or months for complex applications. This project will design and implement computer software that automatically converts a scientist's biological application into a physical device layout that can be fabricated. Successful completion of this project will enhance scientific productivity and lower the barrier-to-entry for new users.<br/> <br/>In terms of broader impact, laboratories-on-a-chip are expected to improve and reduce the cost of global healthcare, particularly through point-of-care testing. This project will reduce the design cost for laboratories-on-a-chip through productivity enhancements, and these savings can be passed directly to the consumer. At the University of California, Riverside, the PI will introduce undergraduate and graduate courses on laboratory-on-a-chip technology, which will be assessed by a qualified professional development officer. The PI will include women and underrepresented minority students in this project, and has a planned an outreach program with a collaborator at Crafton Hills College to promote computer science education to underrepresented minority high school students in the economically-disadvantaged Inland Empire region of Southern California."
"1750333","CAREER: New Algorithms for Submodular Optimization","CCF","ALGORITHMIC FOUNDATIONS","02/01/2018","01/10/2018","Alina Ene","MA","Trustees of Boston University","Continuing grant","Rahul Shah","01/31/2023","$99,138.00","","aene@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7796","1045, 7926","$0.00","Submodular optimization provides general solutions to a wide range of applications from monitoring water distribution networks to summarizing large corpora of documents and speech recognition. Most of the existing submodular optimization algorithms are not suitable for modern datasets, since they are designed for worst-case instances and they suffer from prohibitive running times and poor empirical performance. This project aims to develop scalable algorithmic approaches with improved empirical performance for submodular optimization and to transfer theoretical insights to applications. The proposed research brings together insights from computer science, mathematics and optimization, and strengthens connections among these fields. The project will involve training the next wave of students and equipping them with technical tools to work in all these fields.<br/><br/>The project focuses on three inter-related research directions in submodular function optimization: (a) Design faster algorithms for minimizing submodular functions with a decomposable or sum structure. The approach is to build on a rich set of tools from both discrete and continuous optimization. (b) Design algorithms for constrained submodular maximization problems with improved approximation guarantees and faster running times. The focus is on settling the approximability of constrained submodular maximization problems with a non-monotone objective and on designing faster algorithms for central families of constraints. (c) Design algorithms and frameworks for allocation or labeling problems with submodular costs. The main goal is to obtain more expressive algorithmic frameworks and efficient algorithms."
"1619283","SHF: Small: High-Performance, Energy-Efficient Mobile Web Computing","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/10/2016","Vijay Janapa Reddi","TX","University of Texas at Austin","Standard Grant","Yuanyuan Yang","05/31/2019","$400,000.00","","vj@eecs.harvard.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7923, 7941","$0.00","The Web is continuously transforming society - shaping communications, catalyzing innovations, even shaping thought processes. Over the past decade, the role of the Web has shifted from information retrieval (Web 1.0) to providing a platform for interactive and engaging user experiences (Web 2.0). The Web is once again entering a new age, transcending user engagement to provide intelligent services that integrate multiple devices together. The driving force behind the Web's evolution is the ubiquity of mobile devices-undoubtedly today's most pervasive personal computing platform. The PI will study the crucial conjunction between mobile devices and Web technologies because mobile devices struggle to deliver high computational capability on a battery-power budget. The PI's integrated education plan, involving workshops, new curriculum and cutting-edge Web technologies, will stimulate students from a young age, who come from a diverse range of backgrounds, ethnicities, age groups and skill sets, to pursue academic interests in relevant scientific and technological fields. The exposure will nurture students to embrace careers in science, technology, engineering, and mathematics. <br/><br/>The research will lay the groundwork and establish the foundations needed to develop future high-performance and energy-efficient mobile computing systems that can usher in the next generation of Web applications. The PI will research and investigate optimizations across the layers of the execution stack: at the (1) hardware layer the research will yield insights into new mobile processor architectures that achieve high efficiency by exploiting domain-specific and runtime knowledge; at the (2) runtime software layer, the project will lay the foundations for feedback-driven mechanisms that can dynamically learn to operate the architecture at energy-efficient points without compromising end-user application performance and user experience; at the (3) application layer, the research will discover language hints to empower application writers with awareness of their applications' energy consumption."
"1917325","NSF Student Travel Grant for 2019 Great Lakes Bioinformatics Conference (GLBIO)","CCF","COMPUTATIONAL BIOLOGY","04/15/2019","03/07/2019","Tijana Milenkovic","IN","University of Notre Dame","Standard Grant","Mitra Basu","03/31/2020","$10,000.00","","tmilenko@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7931","7556","$0.00","The Great Lakes Bioinformatics Conference (GLBIO) strives to enhance educational opportunities and research infrastructure throughout the North American Great Lakes region, to make the region a world leader in bioinformatics and computational biology. In 2019, GLBIO will be held at the University of Wisconsin at Madison, during May 19-22. An important goal of the 14th edition of this annual conference is to foster long-term, collaborative relationships among computational and life science researchers and educators from academia, government, and industry. GLBIO provides an interdisciplinary forum for sharing scientific information, including discussing methods, research findings, and educational experiences regarding computational investigations of biological problems. The annual meeting includes invited keynote presentations, oral presentations selected from peer-reviewed full paper or abstract submissions, tutorials, workshops, special sessions, and poster presentations. This travel award supports student participation from US universities at GLBIO 2019. Students will be educated on cutting-edge scientific developments that will drive future discoveries in the fields of bioinformatics and computational biology. Also, they will have opportunities to present their work as oral talks or posters, or to participate in recruiting events with universities or industry partners. By helping further students' careers and train interdisciplinary scientists, this NSF student travel grant will benefit global scientific competitiveness of the US.<br/><br/>Historically, more than half of the GLBIO attendees have been students, over 90% of whom have been US-based. Through this NSF student travel grant, GLBIO 2019 will create a valuable training opportunity for 20 student attendees, by providing them with a partial funding support to attend the conference (as a combination of conference registration, lodging, and transportation costs). Special focus will be on first-time conference attendees who will not necessarily be presenting their work at the conference, and of the students who will be presenting their work, on female, undergraduate or high school, underrepresented minority, or disabled students. Applications for student travel grants will be solicited as follows. All students presenting their work at the conference will be sent emails making them aware of the student travel grant opportunity. The grant opportunity will be advertised on the GLBIO 2019 web site, via conference announcements, and outreach to under-represented student groups and organizations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618046","SHF: Small: From Declarative Specifications of Search Problems to Efficient Solutions","CCF","SOFTWARE & HARDWARE FOUNDATION","05/15/2016","05/13/2016","Neng-Fa Zhou","NY","CUNY Brooklyn College","Standard Grant","Nina Amla","04/30/2020","$385,855.00","","zhou@sci.brooklyn.cuny.edu","Office of Research & Sponsored P","Brooklyn","NY","112102889","7189515622","CSE","7798","7923, 8206","$0.00","Search is one of the fundamental techniques used by many intelligent software systems. There is a big chasm between the languages that programmers use to describe problems and the encodings that are suitable for solvers to conduct search. This research aims to narrow the gap by designing and implementing algorithms for translating declarative specifications of combinatorial problems into efficient encodings. The intellectual merits are novel frameworks and algorithms for translating planning specifications into sophisticated and efficient tabled logic programs, and cutting-edge algorithms for compiling high-level constraints into SAT (satisfiability) encodings. The project's broader significance and importance are its potential to produce long-lasting, significant economic, educational, and social impact because of the ubiquity of combinatorial problems, and the resulting theory and prototypes that will form the technology which enables future systems to obtain high-quality solutions to a variety of combinatorial problems. <br/><br/>This research will focus on two types of combinatorial problems: AI planning and constraint satisfaction problems solved using SAT. Tabled logic programming has been shown to be a powerful and flexible modeling and solving language for planning problems. Nevertheless, it is an art, not a science, to develop efficient planning models in tabled logic programming. The algorithms designed by this research for translating planning specifications into efficient tabled logic programs will have the following capabilities: (1) convert factored representations of states into structural representations that exploit symmetries; (2) extract domain-dependent control knowledge, such as deterministic actions and partial orders; and (3) learn representation-specific heuristics. Global constraints are an important part of constraint programming. They not only allow easy modeling of many problems, but also enable use of powerful propagation algorithms. SAT encoding algorithms have been proposed for some of the global constraints. Nevertheless, there are no well-established algorithms for encoding global constraints into SAT; many other global constraints, such as graph constraints, have not received much attention despite their usefulness in practical applications. This research will produce algorithms for a comprehensive set of global constraints, and will develop a cutting-edge constraint-solving system based on these algorithms."
"1730082","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","EXPERIMENTAL EXPEDITIONS","03/01/2018","08/01/2018","Margaret Martonosi","NJ","Princeton University","Continuing grant","Almadena Chtchelkanova","02/28/2023","$400,000.00","","martonosi@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1442840","CyberSEES:Type 2: Collaborative Research: Combining Experts and Crowds to Address Challenging Societal Problems","CCF","CyberSEES","01/15/2015","08/13/2014","Jeffrey Nickerson","NJ","Stevens Institute of Technology","Standard Grant","Rahul Shah","12/31/2019","$399,926.00","","jnickerson@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","8211","8208","$0.00","From writing an encyclopedia, to developing software, to folding proteins, more and more problems that used to be solved by small groups of experts are now being solved by much larger ""crowds"" of people using the Internet. For many complex problems, however, the creativity, energy, and diversity that crowds bring are not enough to solve the problems. Various kinds of specialized knowledge that only experts have are also needed. This project focuses on creating the cyber-infrastructure to combine these two kinds of resources--experts and crowds--in solving complex societal problems. The project will address this question in the context of what many people believe is one of humanity?s most important sustainability challenges today: how to deal with trending environmental dynamics. Much of the proposed work includes developing and testing ideas on the team's CoLab, a software platform and online community focused on this issue, which was developed with prior NSF and other support.  More specifically, the project will investigate how experts and crowds can work together to perform four key problem-solving activities: decomposing the overall problem into smaller pieces, generating potential solutions for the pieces, integrating the pieces into overall solutions, and evaluating the solutions. For example, the project will study (a) whether crowds generate better ideas when they start with ""seed"" ideas from experts, (b) how certain kinds of expert knowledge can be encoded in computer simulation models that crowds can use on their own, and (c) to what degree expert evaluations can be approximated by combinations of semi-experts, novices, and software tools.<br/>     <br/>The results are expected to include (a) open source software applicable to many sustainability and other challenges, (b) a set of processes and methodologies with which communities can effectively use this software, and (c) a diverse community of tens of thousands of people using this approach to address issues associated with trending environmental changes.  The primary intellectual contribution of the proposed work will be better processes and computer tools for creating on-line communities that combine the best features of both experts and crowds to solve complex societal problems. Many aspects of the technical and organizational approaches to be studied should be of interest to researchers in a variety of fields, including collective intelligence, computer-supported cooperative work, human computer interaction, computer science, engineering, organizational design, psychology, and public policy.  By engaging students and the general public to come up with credible ideas for what people can do about complex problems, this project will help educate a much broader community about the actual issues involved. If successful, the work will likely lead to the development of better approaches for complex societal problems than any that would otherwise have been developed. Many aspects of the results will be applicable to a wide range of domains such as education, healthcare, and business problems like strategic planning and budgeting."
"1757680","REU Site: Cultivating Next Generation Software Engineering Researchers","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/01/2018","02/07/2018","Mehdi Mirakhorli","NY","Rochester Institute of Tech","Standard Grant","Rahul Shah","02/28/2021","$360,000.00","Yi Wang","mehdi@se.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","1139","9250","$0.00","This project is for a new Research Experiences for Undergraduates (REU) Site at Rochester Institute of Technology (RIT). It will provide 10 undergraduate students with the opportunity to conduct research for 10 weeks with faculty members in an increasingly growing field of software engineering. A nationwide recruitment process will be used to select cohorts of undergraduate students. The key goal is to target underrepresented groups (women, minorities, and persons with disabilities), first generation college students, and students from undergraduate institutions with limited research opportunities. In addition to conducting research, the students will participate in other professional development activities such as industry field trips, professional seminars, invited speaker series, career guidance, and graduate school preparation. Thus, the project will inspire the students to pursue advanced degree programs and research career in computer science. Finally, software systems have become the critical infrastructure of the society and are increasingly important to national interests including health, energy, cybersecurity, and so on. The project will prepare a large pool of individuals trained in the field of software engineering to address the increasing national needs.<br/><br/>The 10 talented undergraduate students will work in small teams on carefully planned research experiences. The project focuses on three main themes, Developers' Productivity, Communication and Collaboration, and Design and Creativity, and spans the entire software development life cycle. The planned research projects will focus on (1) the development of novel software engineering tools and techniques for assisting developers in daily activities, enhancing productivity; (2) human and social aspects of software development, particularly, the interaction among software development team members and how those interactions improve teamwork; and (3) innovation in every aspect of software engineering, including envisioning products, eliciting requirements, analyzing requirements, and designing software systems that satisfy specified requirements and quality attributes. The project will encourage the students to disseminate their research findings through presenting and publishing in peer-reviewed venues.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617577","AF: Small: New Perspectives on Mathematical Programming Relaxations","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","06/03/2016","Moses Charikar","CA","Stanford University","Standard Grant","Rahul Shah","06/30/2019","$450,000.00","","moses@cs.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7796","7923, 7926","$0.00","The project will study methods for designing efficient heuristics with provable guarantees for hard optimization problems. Optimization problems arise in many different contexts (e.g. finding the best routes for delivery trucks, finding clusters in a social network, etc). We now know that for many such problems, we do not expect to design algorithms that are efficient (i.e. run in polynomial time) that produce optimal solutions. The field of approximation algorithms develops principled methods to design algorithms for such hard optimization problems. This project will advance our knowledge of mathematical programming relaxations ? a widely used tool in the design of such approximation algorithms. The idea here is as follows: First, express the problem in terms of integer variables with constraints on them. Next relax the problem by allowing variables to take on fractional values. The relaxed version can be solved exactly in polynomial time. Finally, map solutions to the fractional version back to integers, while attempting to maintain the quality of the solution as far as possible. The project will attempt to advance our understanding of the power of this technique and its limitations for fundamental problems in the field. In particular, the PI will investigate structure in problem instances where exact solutions to the original problem can be obtained from solving the relaxation, study new relaxations for basic problems such as k-means clustering. The PI will also study relaxations for problems where the goal is find a dense structure inside a graph and problems involving laying out the vertices of a graph on a line.<br/><br/>Course materials for graduate and undergraduate courses will be developed distilling research results from this project, as well as new developments in the field. The exact recovery questions investigated in this project for mathematical programming relaxations are of great interest in communities other than theoretical computer science; new insights here have the potential to influence and impact these other communities. Undergraduates will be involved in appropriately chosen pieces of the project so as to expose them to research at an early stage. The PI will encourage the participation of women and minorities in this research. He will organize outreach and community building activities at Stanford, leveraging his experience with organizing such activities in the past.<br/><br/><br/>This project will study and shed new light on mathematical programming relaxations for hard combinatorial optimization problems in various settings. The broad goals of this project are:<br/><br/>1. Exact recovery via mathematical programming: Investigate and understand the phenomenon of mathematical programming relaxations returning integer solutions. The PI will apply this new lens to linear programming (LP) and semidefinite programming (SDP) relaxations for several problems. Specifically, the PI will study SDP relaxations for computing transformation invariant metrics on sets of points and solving approximate graph isomorphism for correlated random graph models (introduced recently in the study of de-anonymization and privacy in social networks).<br/><br/>2. Clustering problems: Investigate a recently introduced SDP relaxation for the k-means objective to obtain a better worst case approximation factor. The PI will also investigate the performance of this relaxation on random models of clustered points that better represent instances in practice.<br/><br/>3. Densest subgraph: Investigate hypergraph generalizations of the k-densest-subgraph problem and a close variant, the smallest-m-edge-subgraph, to understand the best approximation factor achievable and whether natural planted instances of random hypergraphs suggest a natural limit for efficient approximation algorithms. The PI will also study lower bounds for densest subgraph, to understand the Lasserre hierarchy for planted instances that are predicted to be the hardest for the problem.<br/><br/>4. Layout problems: The PI will study two different classical optimization problems involving mapping the vertices of a (un)directed graph to a line: minimizing (a) Cutwidth, and (b) Feedback Arc Set, to improve the worst case approximation factors. For cutwidth, the PI will investigate a new mathematical programming relaxation motivated by lift-and-project hierarchies. For Feedback Arc Set, the PI will study a new semidefinite programming relaxation.<br/><br/>5. Constructive Discrepancy: Recent advances in designing efficient algorithms for minimizing discrepancy suggest that new breakthroughs are possible. A recent result builds on a breakthrough on the Kadison-Singer problem to show that the integrality gap of a natural relaxation for Asymmetric Traveling Salesman is small. The PI will investigate whether this existential result can be made algorithmic. The PI will also study the Beck-Fiala and Komlos conjectures, where efficient algorithms can lead to new progress.<br/>"
"1916817","CAREER:Cross-Core Learning in Future Manycore Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","01/01/2019","02/11/2019","Abhishek Bhattacharjee","CT","Yale University","Continuing grant","Anindya Banerjee","06/30/2020","$126,608.00","","abhib@cs.rutgers.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7798","1045, 7941, 9251","$0.00","As computing devices solve increasingly complex and diverse problems, engineers seek to design processors that provide higher performance, while remaining energy-efficient for environmental reasons. To achieve this, processor vendors have embraced manycore devices, where thousands of cores cooperate on a single chip to solve large-scale problems in a parallel manner. They have further incorporated heterogeneity, combining cores with different architectures on a single chip in a bid to provide ever-increasing performance per watt. This project boosts the search for higher energy-efficient performance by inventing novel cross-core learning techniques. Cores in current chips individually learn about the behavior of parallel programs in order to run programs more efficiently in the future, devoting complex and power-hungry hardware structures to do this. However, this research observes that parallel programs tend to exercise the hardware structures of different cores in correlated ways, meaning that the behavior of the program run on one core can be communicated to other cores for various performance and power benefits. As such, this form of intelligent cross-core information exchange is effective in achieving high performance per watt across computing domains from datacenters to embedded systems<br/><br/>In this light, this research provides techniques to deduce how similarly a parallel program's various threads exercise their cores' hardware structures (looking at a range of different programmer, compiler, and architectural mechanisms to do so). When this is detected, cross-core learning hardware gleans the information that is most useful to exchange to improve performance or power, and then transmits this information among heterogeneous cores using low-overhead hardware/software techniques. This project develops a lightweight runtime software layer to orchestrate this information exchange, relying on dedicated hardware support when necessary. Through developing this framework, cross-core learning is applied to a number of specific cases, ranging from higher-performance manycore cache prefetching and branch prediction, to performance and power-management techniques for interrupts and exceptions in scale-out systems, as well as thread and instruction scheduling.  Furthermore, this project heavily disseminates knowledge on how to design and program large-scale manycore systems (or scale-out systems) by involving students at the graduate, undergraduate, and high-school levels through active research and coursework. Overall, this work impacts the engineering community and broader society by: (1) helping to achieve high-performance, but also energy-efficient and environmentally-friendly computing systems; (2) providing academics and chip designers a design methodology and infrastructure to study manycore design; (3) broadening the participation of underrepresented groups in computer science; (4) educating graduate, undergraduate, and high-school students on parallel programming for manycore systems."
"1617192","AF: Small: Collaborative Research: Cell Signaling Hypergraphs: Algorithms and Applications","CCF","ALGORITHMIC FOUNDATIONS","08/15/2016","05/25/2016","John Kececioglu","AZ","University of Arizona","Standard Grant","Mitra Basu","07/31/2020","$212,007.00","","kece@cs.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","7796","7923, 7931","$0.00","Proteins in the living cell interact with each other in complex ways. Graphs have emerged as a natural way to represent these interactions. In a conventional graph representation, a node represents a protein and an edge represents an interaction between two proteins. Although such graphs have been in widespread use for many years, they do not accurately capture important features of protein interactions, such as proteins that operate in groups called complexes, reactions involving such complexes that may have more than two reactants and products, as well as the influence of other proteins whose presence can regulate reactions. This project will develop a new representation called signaling hypergraphs that naturally describes the relationships between multiple groups of proteins as complexes, reactants, products, and regulators. Furthermore, the project will develop novel algorithms for fundamental computational challenges in the analysis of signaling hypergraphs, and apply this new representation and these algorithms to widely-used databases of cellular reactions. <br/><br/>The project will actively involve undergraduate students in research by recruiting them through the Virginia Tech Initiative to Maximize Student Diversity, and the Virginia Tech Undergraduate Research in Computer Science program. Students will engage in multiple semesters of research with the goal of ultimately leading their own individual projects, and obtaining co-authorship in publications. In this way the project will expose students to how computational thinking plays a major role in modern molecular biology, thereby meeting an important goal of STEM education. <br/><br/>This project focuses on developing algorithms for the analysis of cell signaling hypergraphs. Aim 1 focuses on methods for generating products efficiently by finding short paths through signaling hypergraphs, while accounting for feedback loops and reaction regulators. Aim 2 develops algorithms for discovering missing proteins, complexes, and reactions in a signaling pathway. Finally, Aim 3 will release open-source software implementing the algorithms for signaling hypergraphs developed in this project."
"1850227","CRII: CIF: Secret Sharing Under Communication Constraints in Wireless Networks","CCF","COMM & INFORMATION FOUNDATIONS, EPSCoR Co-Funding","07/01/2019","02/13/2019","Remi Chou","KS","Wichita State University","Standard Grant","Phillip Regalia","06/30/2021","$156,344.00","","remi.chou@wichita.edu","1845 Fairmount","Wichita","KS","672600007","3169783285","CSE","7797, 9150","7797, 7935, 8228, 9150","$0.00","As wireless networks become ubiquitous, the ability to maintain trust and securely exchange confidential information among interconnected wireless devices and users is crucial. Secret sharing is a cryptographic primitive that can help to achieve secure communication with applications that include access control, key management, digital signatures, and others. Although this primitive is widely used, it is still not fully understood how wireless communication constraints such as noise, limited bandwidth, and scalability can be optimally accounted for in the design of secret sharing schemes. This project aims for new guidelines for the implementation of secret sharing directly adapted to wireless communication constraints, to improve security for users, minimize the communication cost associated with the implementation, and facilitate deployment in dense wireless networks through improved scalability.<br/><br/>Most existing secret sharing schemes hinge on the assumption that the dealer can securely communicate the shares of the secret to the participants over noiseless channels with unlimited bandwidth. Under such an assumption, the implementation of secret sharing schemes in wireless networks requires additional layers of protocols able to deal with unsecured, noisy, and bandwidth limited communication channels between the dealer and the participants. The objective of this project is to understand whether such a layered approach is optimal and whether a scheme that simultaneously addresses the constraints of wireless communication and the requirements of secret sharing could provide gains in terms of communication efficiency and security guarantees. The project investigates two interdependent research tasks: (i) understanding the fundamental security limits for secret sharing in wireless networks under bandwidth constraints and noise, and (ii) the design of explicit low-complexity coding schemes using tools from cryptography and coding theory.<br/>This award is jointly funded by the Division of Computing and Communication Foundations in the Directorate for Computer & Information Science & Engineering and the Established Program to Stimulate Competitive Research in the Office of Integrative Activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1317560","Collaborative Research: Visual Cortex on Silicon","CCF","INFORMATION TECHNOLOGY RESEARC, Cyber-Human Systems (CHS), EXPERIMENTAL EXPEDITIONS, SOFTWARE & HARDWARE FOUNDATION","10/01/2013","06/20/2016","Vijaykrishnan Narayanan","PA","Pennsylvania State Univ University Park","Continuing grant","Ephraim Glinert","09/30/2019","$4,913,456.00","John Carroll, Chitaranjan Das, Mary Beth Rosson, C. Lee Giles","vijay@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1640, 7367, 7723, 7798","7367, 7723, 7798, 7941, 9251","$0.00","The human vision system understands and interprets complex scenes for a wide range of visual tasks in real-time while consuming less than 20 Watts of power. This Expeditions-in-Computing project explores holistic design of machine vision systems that have the potential to approach and eventually exceed the capabilities of human vision systems. This will enable the next generation of machine vision systems to not only record images but also understand visual content. Such smart machine vision systems will have a multi-faceted impact on society, including visual aids for visually impaired persons, driver assistance for reducing automotive accidents, and augmented reality for enhanced shopping, travel, and safety. The transformative nature of the research will inspire and train a new generation of students in inter-disciplinary work that spans neuroscience, computing and engineering discipline.<br/><br/>While several machine vision systems today can each successfully perform one or a few human tasks - such as detecting human faces in point-and-shoot cameras - they are still limited in their ability to perform a wide range of visual tasks, to operate in complex, cluttered environments, and to provide reasoning for their decisions.  In contrast, the mammalian visual cortex excels in a broad variety of goal-oriented cognitive tasks, and is at least three orders of magnitude more energy efficient than customized state-of-the-art machine vision systems. The proposed research envisions a holistic design of a machine vision system that will approach the cognitive abilities of the human cortex, by developing a comprehensive solution consisting of vision algorithms, hardware design, human-machine interfaces, and information storage. The project aims to understand the fundamental mechanisms used in the visual cortex to enable the design of new vision algorithms and hardware fabrics that can improve power, speed, flexibility, and recognition accuracies relative to existing machine vision systems. Towards this goal, the project proposes an ambitious inter-disciplinary research agenda that will (i) understand goal-directed visual attention mechanisms in the brain to design task-driven vision algorithms; (ii) develop vision theory and algorithms that scale in performance with increasing complexity of a scene; (iii) integrate complementary approaches in biological and machine vision techniques; (iv) develop a new-genre of computing architectures inspired by advances in both the understanding of the visual cortex and the emergence of electronic devices; and (v) design human-computer interfaces that will effectively assist end-users while preserving privacy and maximizing utility. These advances will allow us to replace current-day cameras with cognitive visual systems that more intelligently analyze and understand complex scenes, and dynamically interact with users.<br/><br/>Machine vision systems that understand and interact with their environment in ways similar to humans will enable new transformative applications. The project will develop experimental platforms to: (1) assist visually impaired people; (2) enhance driver attention; and (3) augment reality to provide enhanced experience for retail shopping or a vacation visit, and enhanced safety for critical public infrastructure. This project will result in education and research artifacts that will be disseminated widely through a web portal and via online lecture delivery. The resulting artifacts and prototypes will enhance successful ongoing outreach programs to under-represented minorities and the general public, such as museum exhibits, science fairs, and a summer camp aimed at K-12 students. It will also spur similar new outreach efforts at other partner locations. The project will help identify and develop course material and projects directed at instilling interest in computing fields for students in four-year colleges. Partnerships with two Hispanic serving institutes, industry, national labs and international projects are also planned."
"1421508","AF: Small: Algorithmic Energy Management in New Information Technologies","CCF","ALGORITHMIC FOUNDATIONS","06/01/2014","06/03/2014","Kirk Pruhs","PA","University of Pittsburgh","Standard Grant","Tracy Kimbrel","05/31/2019","$399,585.00","","kirk@cs.pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","7796","7923, 7926","$0.00","Due to the convergence of several trends (most notably Moore's law) about a decade ago, the related quantities of power, energy, and temperature rapidly emerged as the most important computational resources and constraints in many information technologies. We are thus about a decade into a green computing revolution in which many information technologies are being redesigned with energy consumption as a first class design criterion. One of the many technological challenges that arise as part of this green computing revolution are new algorithmic resource management problems, in which the resource that must be managed is energy related. Over the last several decades, when computer instructions (""time"") and computer memory (""space"") were the dominant computational resources, computer science researchers developed many techniques for designing time and space efficient algorithms, and for analyzing the time and space required by particular algorithms on simple models of a computer. The ability to reason abstractly about time and space in simple computational models is undoubtedly a valuable skill for computer scientists and software engineers. However, the physics of energy is sufficiently different than the physics of time and space that the standard models for studying time and space as computational resources do not seem to be the appropriate models for studying energy as a computational resource.<br/><br/>Thus the PI will develop new models for studying energy as a computational resource. Necessitated by the nonlinear nature of the relationship between energy and performance, the PI will develop new nonlinear algorithmic design and analysis techniques. The long term payoff of the project is expected to be an algorithmic theory of energy as a computational resource in simple models. This theory will eventually be taught to future software engineers, and will serve these software engineers, when faced with problems in which power/energy/temperature is the key scarce resource, just as the current algorithmic theory of time as a computational resource now serves them.<br/><br/>The project will support training several undergraduate and Ph.D. students at the University of Pittsburgh in algorithmic energy management. The project will support visits by outside Ph.D. students and post-docs interested in beginning or continuing a research program in algorithmic energy management. The PI will participate regularly in green computing academic conferences and will facilitate communication between the algorithmic and computer systems research communities.<br/><br/>The PI will specifically investigate algorithmic energy management problems within four information technology settings:<br/><br/>1. Online scheduling of power heterogeneous processors: Simulation studies indicate that heterogeneous multiprocessor architectures potentially offer higher performance than current homogeneous multiprocessor architectures. It is also known that traditional priority scheduling algorithms that work well for single processor and uniform processor architectures do not work well for heterogeneous multiprocessor architectures. The main goal is to find algorithms that provably work well for heterogeneous multiprocessor architectures.<br/><br/>2. Computing optimal energy tradeoff schedules for speed scalable processors: Energy and time performance are generally conflicting goals. The goal is to understand and efficiently compute schedules that optimally balance these conflicting demands.<br/><br/>3. Computing near energy optimal routings in networks of speed scalable routers with shutdown: The two most common energy management mechanisms are power heterogeneous components and components that may be shut down. When these are both included in a computer network, the standard routing problem becomes more complex as the energy used per router is neither concave nor convex. The goal is to understand and efficiently compute good routings in this setting.<br/><br/>4. Energy efficient circuits: Near threshold computing is a potential approach to more energy efficient computing on the circuit level. However, while lowering the supply voltage decreases power, it increases the probability of functional failures, thus necessitating more fault-tolerant (and thus larger) circuits. The goal is to understand and efficiently compute the most energy efficient way to balance the competing demands of the energy used per gate and the number of gates."
"1629444","XPS: FULL: Bridging Parallel and Queueing-Theoretic Scheduling","CCF","ALGORITHMIC FOUNDATIONS, Exploiting Parallel&Scalabilty","07/01/2016","04/20/2018","Guy Blelloch","PA","Carnegie-Mellon University","Standard Grant","Tracy Kimbrel","06/30/2019","$841,000.00","Mor Harchol-Balter, Umut Acar","guyb@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796, 8283","7934, 9251","$0.00","Scheduling computational tasks on computational resources has played a fundamental role in computer science since the 1950s.  One branch of scheduling (queueing theoretic) has focused on analyzing multiple sequential jobs competing for a shared resource with the goal of minimizing the response time (latency) over all jobs.  Most operating system and server schedulers use ideas developed as part of this research.  Another branch of scheduling (parallel) has focused on analyzing a single parallel job running on a dedicated parallel machine, with the goal of maximizing efficiency (throughput) of the job.  Most schedulers for dynamically parallel programs use ideas developed as part of this research.  Until recently these branches were adequate on their own, and the communities studying them have had very little interaction.  However, the mainstream availability of parallel hardware, and the need to handle many parallel jobs sharing a single resource, has recently changed this.  The goal of this project is to bridge the two branches by developing new theory and practical scheduling algorithms, that can handle multiple dynamically parallel jobs competing for shared resources.  The project brings together PIs with expertise from each area, and will apply and combine techniques from each area.  The project has the potential to have significant broad impact on the theory and practice of widely used shared parallel systems.  The project will include an educational outreach component in which the PIs will include ideas from the project in courses they teach.<br/><br/>This project is the first to tackle the union of these two domains. The PIs will develop scheduling algorithms for a stream of arriving jobs, where the jobs are complex multi-threaded fine-grained parallel jobs with dynamic parallelism and dependencies among tasks.  The research will address three specific challenges, as follows. Challenge 1, Statistical Characterization/Modeling: Scheduling multiple jobs requires knowing something about when each job will complete and what it will do in the future, such as the number of tasks it will create or the granularity of tasks.  A significant part of the project is devoted to measuring parallel jobs and statistically characterizing them to create simple models of their behavior.  Challenge 2, Algorithmic Development and Analysis: There are currently no scheduling algorithms for the problem that is being considered.  While queueing theory touts the optimality of always running the job with the ""shortest expected remaining time,"" this has little meaning when jobs are parallel and there are many resources.  New theorems and analytical techniques will be developed. Challenge 3, Implementation and Benchmarking: An important component of the project will be the implementation and benchmarking of our algorithms on prototype systems.  The PIs will investigate multiple metrics including latency, throughput, fairness, and robustness."
"1527032","AF: SMALL: Approximation Algorithms Matching Integrality Gaps for Network Design","CCF","ALGORITHMIC FOUNDATIONS","09/01/2015","08/27/2015","Ramamoorthi Ravi","PA","Carnegie-Mellon University","Standard Grant","Tracy Kimbrel","08/31/2019","$399,998.00","","ravi@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7923, 7926","$0.00","Optimization problems that often arise in practice can be cast as those of designing appropriate networks that connect specified locations. Many such network design problems such as the traveling salesperson problem are computationally hard to solve exactly; nevertheless, approximation algorithms that run quickly yet deliver near-optimal solutions have been devised for solving them, often using these problems as exemplars to develop new techniques. A large set of these techniques for designing approximation algorithms are based on applying the mathematical modeling formalism of linear programming to obtain a starting point, and using different ways to convert them to actual solutions that are cheap. This project will study some prototypical network design problems under the linear programming formalism to establish the limits of its accuracy. In the process, the project will attempt to discover new techniques for designing such approximation algorithms, adding to the existing toolkit. Methods from the project will be useful in designing new stand-alone solutions for many network design problems as well as enhancing current methods that employ the linear programming framework. The project will synthesize diverse ideas from algorithm design, optimization theory and combinatorial discrete mathematics, and will broaden participation by supporting two female doctoral students. The educational plan will disseminate the results to a broad audience in Computer Science, Mathematics, Operations Research and Business where the PI is a professor.<br/> <br/>The set of problems examined in this project contains some of the long standing open problems in the theory of approximation algorithms, such as the symmetric traveling salesperson problem and its variants: the traveling salesperson path and two-edge-connected subgraph problems; it also includes other classical problems such as tree augmentation and prize-collecting Steiner forest that arise in a variety of network design applications. All these problems share the property that for their natural linear programming relaxations, the limits of these relaxations, also known as their integrality gaps, have not yet been established. The goal of the project is to design new approximation algorithms with performance ratios that match the exact integrality gap for these problems. These algorithms will be based on advancing current techniques such as primal-dual methods and iterated rounding, and developing new methods based on structural decompositions."
"1618287","AF: Small: Quantum Algorithms and Complexity","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","06/24/2016","Sean Hallgren","PA","Pennsylvania State Univ University Park","Standard Grant","Dmitri Maslov","06/30/2019","$450,000.00","","hallgren@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7796","7203, 7923, 7928","$0.00","This project studies the power of quantum computers.  The main goal is to determine which problems have efficient quantum algorithms and<br/>which ones remain hard in the presence of quantum computers.  This is a fundamental question in computer science.  One objective is to find new applications for quantum computers.  A second objective is to understand which cryptosystems are secure against quantum computers. These systems must be based on problems that cannot be solved efficiently by quantum computers.  The PI and his graduate students are considering these types of problems.<br/><br/>The project considers new approaches for the dihedral hidden subgroup problem.  A quantum algorithm for this problem, if found, would break some candidates for quantum-secure cryptosystems.  If more evidence is found that the problem is difficult to solve, then it increases the confidence in the security of the proposed systems. Another type of potential speedup is for optimization problems.  The work addresses whether certain settings for the approximate adiabatic algorithm exist for which the quantum algorithm achieves a better<br/>approximate solution than the best classical algorithm.  The last problem deals with understanding Hamiltonian complexity."
"1535967","AitF: FULL: From Worst-Case to Realistic-Case Analysis for Large Scale Machine Learning Algorithms","CCF","Algorithms in the Field","09/01/2015","10/05/2017","Maria-Florina Balcan","PA","Carnegie-Mellon University","Standard Grant","Tracy Kimbrel","08/31/2019","$719,986.00","Tom Mitchell, Avrim Blum","ninamf@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7239","012Z","$0.00","The aim of this project is to develop mathematical models, analysis, and algorithms that will advance both the design and understanding of large-scale machine learning systems.  In recent years, machine learning has come into widespread use across a range of applications, and we have also seen significant advances in the theoretical understanding of learning processes. Yet despite these successes, there remains a gulf between theory and application.  For example, applications often demonstrate success on problems that theory tells us are intractable in the worst case.  Furthermore, as modern machine learning applications scale up from learning of single tasks to learning many tasks simultaneously, new theory is needed to analyze these larger scale multi-task learning settings.  This project aims to bridge this gap by developing and applying theory targeted toward realistic-case analysis of learning problems, which capture the structures that enable applications to succeed even when theoretical analyses show the impossibility of doing so in the worst case.  This work will be guided by problems at the core of NELL and InMind, two current learning systems that address large-scale multi-task machine learning problems, for reading the web and providing highly personalized electronic assistants to hundreds of interconnected mobile phone users.<br/><br/>More specifically, this project has three main components:<br/><br/>(1) To develop computationally efficient algorithms for clustering, constrained optimization, and related optimization tasks crucial to large-scale machine learning, with provable guarantees under natural, realistic non-worst-case analysis models.<br/><br/>(2) To develop foundations and practical algorithms for multi-task and life-long learning that exploit explicit and implicit structure to minimize key resources including computation time and human labeling effort, as well as address key constraints such as privacy.<br/><br/>(3) To apply the algorithms developed to solve key challenges in two current large-scale learning systems, NELL and InMind.<br/><br/>The proposed work will aid the development of large-scale machine learning applications, as well as create important connections between multiple areas of significant importance in modern machine learning and theoretical computer science. In addition to advising students on topics connected to this project, research progress (on multi-task learning, life-long learning, and clustering) will be integrated in the curricula of several courses at CMU and course materials will be made available on the world wide web. Course projects based on this research will be available to students in the introductory machine learning course at CMU, which enrolls over 600 students each year. In addition, students seeking topics for undergraduate thesis or independent study may also pursue research affiliated with this project."
"1733556","AitF: Algorithms and Mechanisms for Kidney Exchange","CCF","Algorithms in the Field","10/01/2017","10/05/2017","Ariel Procaccia","PA","Carnegie-Mellon University","Standard Grant","Rahul Shah","09/30/2021","$799,621.00","Avrim Blum, Tuomas Sandholm","arielpro@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7239","","$0.00","Severe cases of renal failure require kidney transplantation. But the demand for kidneys is huge while the supply is quite limited. Even when a willing donor is found, several hurdles must be cleared before transplantation can take place. Enter kidney exchange, the idea that patients can exchange willing but incompatible donors. In its most basic form ? 2-way exchanges ? two patient-donor pairs swap kidneys, that is, the first donor donates to the second patient and the second donor to the first patient. However, exchanges along longer cycles and even chains are also taking place. In recent years several kidney exchange programs have become operational, building on the work of economists and computer scientists. And while significant progress has already been made, computer science has an even bigger role to play in kidney exchange research. Indeed, the theme of this proposal is that challenges in kidney exchange give rise to a wealth of exciting theoretical questions. Moreover, solving these problems matters: These problems are important to the design and optimization of real-world kidney exchange programs.<br/><br/>An overarching goal of this proposal is to narrow the gap between the theory and practice of kidney exchange. The proposed research will incorporate elements such as 3-way exchanges, weighted edges, and chains initiated by altruistic donors into existing work and develop new models that ? while still abstractions of reality ? are able to distill the essence of practical kidney exchange challenges. The project can therefore impact the evolution of kidney exchange programs, which are still in their infancy. <br/>In more detail, the project focuses on two main research directions: <br/><br/>1. Dealing with incentives: Transplant centers care foremost about their own patients. Thus if the individual transplant centers cannot each be confident that their own patients will fare at least as well if they participate in the exchange than if they do not, then they may not join. Even more subtly, the transplant centers may join but ""hide"" their easier-to-match patients. The proposed research aims to tackle both of these challenges. The goal is to develop algorithms and analysis for exchanges that produce optimal or near-optimal solutions, while providing strong incentive guarantees for transplant centers to join.<br/><br/>2. Dealing with crossmatches: Crossmatch tests require mixing samples of the blood of potential donors and patients, and hence are only done after a matching is computed. Unfortunately, crossmatch tests are quite likely to fail, leading to the collapse of large portions of supposedly optimal exchanges. Optimization that takes crossmatches into account offers significant gains compared to the common practice today. The proposal contains plans to more broadly develop a full theoretical and algorithmic understanding of the integration of crossmatch tests into the optimization, and the fundamental tradeoffs involved."
"1617041","DNA Computation in Cells","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2016","05/31/2016","Alexander Deiters","PA","University of Pittsburgh","Standard Grant","Mitra Basu","05/31/2019","$450,000.00","","deiters@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","7798","7923, 7946","$0.00","DNA logic gates are powerful computation devices because the outputs are chemically equivalent to the inputs, and the output of one gate can act as the input for a subsequent gate - similar to electronic gates. This enables serial connections of gates, generating signaling cascades that can be assembled into complex molecular circuits based on nucleic acid hybridization where both the encoded program and the machinery that runs the program are composed of DNA. The use of synthetic DNA as a material for biocomputing in human cells has several advantages: DNA circuits rely on fully programmable Watson-Crick base-pairing interactions and thus can be rationally designed at the molecular level, delivering a high degree of control. New components, such as gates, amplifiers, and sub-networks can be readily encoded in DNA sequences, which provides modularity and adaptability. <br/>        <br/>The completion of the research objectives will lead to broadly applicable approaches to programmable biological computation devices that are functional in human cells. The developed methodologies will be of interest to scientists who are developing or applying DNA-programmable algorithms or assemblies to biological systems; furthermore, they will facilitate interactions with the field of microRNA biology. The outputs of the DNA computation process can be readily interfaced with cellular and organismal systems, thus extending the impact of this methodology into broader applications. In addition, the proposed biocomputing circuitry is inexpensive to manufacture and easy to assemble - in contrast to traditional techniques for the analysis of microRNA patterns. Due to its multidisciplinary nature, this project will train the next generation of students in the programming in oligonucleotides, oligonucleotide chemistry, and cell biology. Based on developed expertise, outreach activities with museums and schools will be conducted in order to excite children at a young age (and their parents) about STEM, specifically at the interface of computer science, biology, and chemistry.<br/><br/>This research will test if cellular microRNA pattern detection, analysis, and response functions of devices can be combined in DNA-based computation circuitry. This will represent an innovative approach to the analysis of microRNA patterns directly inside of live cells. Output signals of DNA-based circuits are not restricted to electrical or optical outputs (as in case of traditional detection and analysis methods), but can be designed as oligonucleotides, fluorophores, dyes, and other molecules. An innovative DNA-analog based biocomputing system will be developed in order to protect the encoded oligonucleotide programs from biological environments and highly specific self-assembly capabilities will provide flexible designs for next-generation molecular computation devices in living systems. The following research objectives will be pursued: (1) demonstrate DNA computing of increasing complexity in live cells, (2) interface cellular DNA computing with signal amplification, (3) engineer cellular DNA computing to provide molecular outputs, and (4) use new materials to encode oligonucleotide programs in order to protect the DNA devices from cellular environments."
"1540541","BSF: 2014414: New Challenges and Perspectives in Online Algorithms","CCF","SPECIAL PROJECTS - CCF","09/01/2015","08/06/2015","Anupam Gupta","PA","Carnegie-Mellon University","Standard Grant","Nina Amla","08/31/2019","$40,000.00","","anupamg@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878","2878","$0.00","While the traditional design and analysis of algorithms assumes that complete knowledge of the entire input is available to the algorithm, the area of online algorithms deals with the case where the input is revealed in parts, and the online algorithm is required to respond to each new part immediately upon arrival, without knowledge of the future. Previous decisions of the online algorithm cannot be revoked. Thus, the main issue in online computation is obtaining good performance in the face of uncertainty, since the future is unknown to the algorithm. The problems in this setting arise in all of computer science, as well in much of sequential decision-making, machine learning, and many other areas.<br/><br/>The proposed research is focused on a deeper investigation of the primal-dual approach to online algorithm design. The topics investigated in this project are (a) extending the success of linear optimization to the convex case, (b) relaxing monotonicity of the variables and developing principled approaches for algorithms with preemption, and (c) understanding the connection of online primal-dual approaches and online machine learning algorithms. As part of the broader impact, the research is likely to lead to better algorithms for a variety of problems both in traditional algorithm design and in other areas like machine learning and algorithmic game theory."
"1525932","AF: Small: Fair Division at Scale","CCF","ALGORITHMIC FOUNDATIONS","06/15/2015","06/08/2015","Ariel Procaccia","PA","Carnegie-Mellon University","Standard Grant","Tracy Kimbrel","05/31/2019","$450,000.00","","arielpro@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7923, 7932","$0.00","The mathematical study of fair division dates back to the 1940s. Nowadays the literature encompasses provably fair solutions for a wide variety of problems, many of them relevant to society at large. But, to date, very few fair division methods have been implemented. Building on its rich history, the field of fair division is poised to impact society on a grander scale, for example, via Spliddit (www.spliddit.org), a fair division website created by the PI. The overarching theme of this proposal is that the advent of fairness at scale gives rise to novel algorithmic challenges, revolving around the design of methods that are practical, widely applicable, and provably fair. Solutions to these challenges call for a synthesis of ideas from theoretical computer science and microeconomic theory. <br/><br/>The evolution of Spliddit plays a key role in the project: Insights arising from the research will be implemented to improve Spliddit's publicly accessible methods. Moreover, new applications will be added in order to further widen the website's reach. The project also includes plans to enhance Spliddit's educational content through the creation of instructive videos, thereby making the website a valuable learning and teaching resource. Finally, the project contains plans to make a societal impact that extends beyond Spliddit. In particular, the PI will work with California school districts to develop a fair method for allocating unused space.<br/><br/>On a technical level, the thrust of the project is twofold. (i) Dividing indivisible goods: One of five applications currently available on Spliddit, the division of indivisible goods is a notoriously difficult problem from a fair division perspective. To obtain a provably fair method, Spliddit relies on the notion of maximin share (MMS) guarantee. While exact MMS allocations may be infeasible, the project explores several notions of approximation. The project also explores the feasibility boundary of MMS allocations. Foundational algorithmic challenges lie at the heart of these research directions. (ii) Sharing credit: Spliddit's credit calculator fairly determines the contribution of each individual to a group project, using a method developed by de Clippel et al. Perhaps the method's most compelling guarantee is impartiality: a player's report does not affect her own share of the credit. Dividing credit for a scientific paper, with the goal of fairly ordering the authors by contribution, is an especially attractive potential application; but there is no guarantee that players will not be able to affect their position in the resultant ranking. The project aims to circumvent this obstacle via randomization and approximation."
"1553477","CAREER: Interactions with Untrusted Quantum Devices","CCF","ALGORITHMIC FOUNDATIONS, QUANTUM COMPUTING, Secure &Trustworthy Cyberspace","02/01/2016","02/11/2019","Thomas Vidick","CA","California Institute of Technology","Continuing grant","Dmitri Maslov","01/31/2021","$427,356.00","","vidick@cms.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7796, 7928, 8060","1045, 7434, 7926, 7927, 7928, 9251","$0.00","Fundamental research in quantum information performed since the 1980s shows that the principles of quantum mechanics can lead to dramatic computational and cryptographic consequences, from exponentially faster algorithms to unbreakable cryptosystems. The first practical quantum devices, from single-photon receptors used in quantum cryptography to large-scale quantum optimizers, are now being actively developed. These stunning experimental advances raise a very practical challenge: how can classical users establish and maintain a trusted interaction with a priori unknown and untrusted quantum devices?<br/><br/>This project aims to address the many aspects of this question, from the development of novel secure cryptographic protocols to the study of the fundamental consequences of quantum mechanical devices for the theory of computation. Research in this direction is essential for the establishment of a secure quantum network of trusted interactions. Making progress will require a combination of insights from computer science, physics, and mathematics. By nature the project is highly interdisciplinary, and the PI will put substantial effort into disseminating the ideas that support, and arise from, the research. This includes making best use of the possibilities for communication through the internet (including online seminars and courses) and encouraging, via outreach and teaching, the emergence of a generation of researchers equipped to address the upcoming challenges posed by the interaction of physics and information technologies.<br/><br/>Research by the PI on this project will build upon recent striking developments in cryptography and complexity theory, including the framework of device-independence and the theory of quantum multi-prover interactive proof systems. Recent works in these areas have identified a key property of quantum entanglement, the monogamy of entanglement, which places very strong constraints on quantum mechanical systems. The PI will develop techniques that leverage entanglement and its monogamy in order to enable testing and secure interactions between classical users and quantum devices. The insights gained in the process will find applications beyond the framework of the project to the many areas of physics in which entanglement plays a role, from the theory of superconductors to that of black holes."
"1750539","CAREER: Leveraging Sparsity in Massively Distributed Optimization","CCF","SOFTWARE & HARDWARE FOUNDATION","02/01/2018","02/11/2019","Stratis Ioannidis","MA","Northeastern University","Continuing grant","Almadena Chtchelkanova","01/31/2023","$190,507.00","","IOANNIDIS@ECE.NEU.EDU","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7798","1045, 7942","$0.00","This project develops novel parallel optimization techniques based on the Frank-Wolfe algorithm, enabling the massive parallelization, at an unprecedented scale, of several problems of key significance to computer science, engineering, and operations research. Massively parallelizing such problems can have a significant practical impact on both academia and industry. Using Apache Spark as a development platform, algorithms developed by the project can be implemented, deployed and evaluated over hundreds of machines and thousands of CPUs. The Massachusetts Green High Performance Computing Center (MGHPCC) as well as cloud services, such as Amazon Web Services and the Google Cloud Platform, are leveraged for this deployment, demonstrating both the scalability of developed algorithms as well as their applicability to commercial cluster environments. Educational activities are closely integrated with this research agenda, including a course developed by the principal investigator using MGHPCC as a computing platform, and outreach activities developed jointly with Northeastern University's Center for STEM Education.<br/><br/>This research advances our knowledge and understanding of the formal conditions under which problems can be massively parallelized via map-reduce implementations of the Frank-Wolfe algorithm. The project leverages sparsity properties that optimization problems exhibit under Frank-Wolfe, thereby enabling their parallelization via map-reduce operations. Beyond tailored, problem-specific implementations, the project identifies formal, structural properties of problems (or, classes of problems) under which such massive parallelization via map-reduce is possible. The use of Frank-Wolfe as a building block for parallelization, both in convex optimization but also in submodular maximization settings, is transformative. In the latter case, it amounts to a non-combinatorial approach for parallelization, attaining the same approximation guarantee as serial algorithms. <br/>"
"1514089","SHF: Medium: Collaborative Research: Atomic scale to circuit modeling of emerging nanoelectronic devices and adapting them to SPICE simulation package","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2015","06/15/2015","Michael Leuenberger","FL","University of Central Florida","Standard Grant","Sankar Basu","05/31/2019","$290,003.00","","michael.leuenberger@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7798","7924, 7945","$0.00","Aggressive scaling of CMOS technology and concomitant inventions of nanoscale nascent technologies have fueled the growth of computer, information, communication and consumer electronics industries of the 21st Century by leveraging the ground-breaking discoveries in nanoscience and nanotechnology. The workhorse of multibillion-dollar semiconductor industry, the CMOS technology is approaching its scaling limit due to the strong quantum-mechanical effects present at the nanoscale. To sustain the accelerated pace of economic growth during the post-CMOS era, this multi-university collaborative research proposal envisages building the roadmap of VLSI technology in two significant ways. First, the research is mooted to extend quantum transport principles to simulate emerging nano-devices based on novel semiconductor and 2-D layered materials by exploiting non-charge based degrees of freedom, electron spin controlled magnetization, interaction between electromagnetic waves and semiconductors in metamaterial structures, and topological states in topological insulators. Second, the research will systematically scale these properties from their fundamental atomistic limits to circuit level integration by developing industry-graded SPICE-compatible compact models for heterogeneous circuits that will define the landscape of beyond Moore?s Law VLSI systems. Integrative education, training, and outreach activities envisioned in this collaborative proposal will encompass K-12, undergraduate, graduate, female, minority, and postdoctoral fellows by leveraging the existing outreach activities of participating universities in order to advance science and engineering education in broader segments of the society.<br/><br/>Using density-functional theory (DFT), time-dependent density functional theory (TD-DFT), time-dependent density-matrix functional theory (TD-DMFT), to phenomenological Extended Huckel to effective mass, in conjunction with non-equilibrium Green?s function (NEGF) methods, quantum field theory, and finite-difference time domain (FDTD) methods, a wide variety of computational methods are going to be developed to tackle the modeling of multiscale circuits in future VLSI systems. The software packages and multiscale modeling tools resulting from the proposed research activity are going to provide computer chip designers and manufacturers the ability to model complex hybrid substrates comprising nanoscale electronic, spintronic, opto-electronic, and plasmonic devices. The resulting software is going to be written with a view to enabling researchers from universities and practicing engineers in industries to develop their own modules that will engender improved system functionality, integration density, and operational speed."
"1849899","CRII: AF: Pseudorandomness: New Frontiers and Techniques","CCF","ALGORITHMIC FOUNDATIONS","03/01/2019","01/30/2019","Eshan Chattopadhyay","NY","Cornell University","Standard Grant","Tracy Kimbrel","02/28/2021","$175,000.00","","ec583@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7796","7796, 7927, 8228, 9102","$0.00","The role of randomness in computation is extremely important with crucial applications to various branches of computer science including design of fast algorithms for practical problems, distributed computing, cryptography, and security. Scientists and economists use randomness extensively in simulations of physical and biological systems and other complex environments. However, a major problem in practice is that physical sources of randomness are defective and only produce correlated bits that contain some entropy.  This leads to the following  fundamental question: To what extent is the use of randomness inherent in applications? Can we reduce the amount or quality of randomness required to perform these tasks? Most applications of randomness in cryptography are inherent. Indeed, we require the secret keys for various cryptographic applications, such as credit card transactions, to be uniformly random.  On the other hand, it is not known if the use of randomness is fundamental in designing efficient algorithms.  This project intends to study these fundamental questions, and in particular, provide theoretical guarantees on the quality of randomness used in cryptography and security, and reduce the amount of randomness used in design of efficient algorithms. <br/><br/>The area of Pseudorandomness provides a unified approach to the above problems and a major goal in this area is to efficiently construct deterministic objects that are random-like. The idea is to replace a random object used in an application, with an explicit construction of a random-like object, thus removing the need for random bits.  Important notions in this area are randomness extractors and pseudorandom generators. A randomness extractor is an algorithm that produces purely random bits from defective sources of randomness. Such extractors can be used in purifying defective sources before carrying out important cryptographic protocols. A pseudorandom generator is a deterministic algorithm that takes a short uniformly random string and stretches it into a much longer string that still ""looks random"" to the appropriate application (e.g., algorithms that use limited memory). This project intends to study several concrete directions to extend the state-of-art on efficient constructions of randomness extractors and pseudorandom generators that will lead to resolving important open problems in computational complexity theory and derandomization.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1762521","Collaborative Research: Visual Cortex on Silicon","CCF","EXPERIMENTAL EXPEDITIONS","08/01/2017","11/07/2017","Alan Yuille","MD","Johns Hopkins University","Continuing grant","Ephraim Glinert","09/30/2019","$177,277.00","","ayuille1@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7723","7723","$0.00","The human vision system understands and interprets complex scenes for a wide range of visual tasks in real-time while consuming less than 20 Watts of power. This Expeditions-in-Computing project explores holistic design of machine vision systems that have the potential to approach and eventually exceed the capabilities of human vision systems. This will enable the next generation of machine vision systems to not only record images but also understand visual content. Such smart machine vision systems will have a multi-faceted impact on society, including visual aids for visually impaired persons, driver assistance for reducing automotive accidents, and augmented reality for enhanced shopping, travel, and safety. The transformative nature of the research will inspire and train a new generation of students in inter-disciplinary work that spans neuroscience, computing and engineering discipline.<br/><br/>While several machine vision systems today can each successfully perform one or a few human tasks ? such as detecting human faces in point-and-shoot cameras ? they are still limited in their ability to perform a wide range of visual tasks, to operate in complex, cluttered environments, and to provide reasoning for their decisions.  In contrast, the mammalian visual cortex excels in a broad variety of goal-oriented cognitive tasks, and is at least three orders of magnitude more energy efficient than customized state-of-the-art machine vision systems. The proposed research envisions a holistic design of a machine vision system that will approach the cognitive abilities of the human cortex, by developing a comprehensive solution consisting of vision algorithms, hardware design, human-machine interfaces, and information storage. The project aims to understand the fundamental mechanisms used in the visual cortex to enable the design of new vision algorithms and hardware fabrics that can improve power, speed, flexibility, and recognition accuracies relative to existing machine vision systems. Towards this goal, the project proposes an ambitious inter-disciplinary research agenda that will (i) understand goal-directed visual attention mechanisms in the brain to design task-driven vision algorithms; (ii) develop vision theory and algorithms that scale in performance with increasing complexity of a scene; (iii) integrate complementary approaches in biological and machine vision techniques; (iv) develop a new-genre of computing architectures inspired by advances in both the understanding of the visual cortex and the emergence of electronic devices; and (v) design human-computer interfaces that will effectively assist end-users while preserving privacy and maximizing utility. These advances will allow us to replace current-day cameras with cognitive visual systems that more intelligently analyze and understand complex scenes, and dynamically interact with users.<br/><br/>Machine vision systems that understand and interact with their environment in ways similar to humans will enable new transformative applications. The project will develop experimental platforms to: (1) assist visually impaired people; (2) enhance driver attention; and (3) augment reality to provide enhanced experience for retail shopping or a vacation visit, and enhanced safety for critical public infrastructure. This project will result in education and research artifacts that will be disseminated widely through a web portal and via online lecture delivery. The resulting artifacts and prototypes will enhance successful ongoing outreach programs to under-represented minorities and the general public, such as museum exhibits, science fairs, and a summer camp aimed at K-12 students. It will also spur similar new outreach efforts at other partner locations. The project will help identify and develop course material and projects directed at instilling interest in computing fields for students in four-year colleges. Partnerships with two Hispanic serving institutes, industry, national labs and international projects are also planned."
"1537085","CAREER: New Foundations for Next-Generation  Reliable Throughput Architecture Design","CCF","SOFTWARE & HARDWARE FOUNDATION","12/01/2014","09/20/2017","Xin Fu","TX","University of Houston","Continuing grant","Yuanyuan Yang","01/31/2020","$411,484.00","","xfu8@central.uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","7798","1045, 7941, 9150","$0.00","With the demand on improving performance and energy-efficiency, novel technologies including non-volatile memory (e.g., spin-transfer torque RAM (STT-RAM)), 3D integration technology (3D), and near-threshold voltage computing (NTC) have been increasingly deployed in the state-of-the-art throughput processors. Since the novel technologies are not designed for dependable computing, the reliability challenges, which have been a crucial issue in conventional throughput architecture design, become the major obstacle for integrating them into next-generation throughput processors. There is a pressing need for the investigation of innovative techniques that are able to take advantage of throughput processors' unique features for characterizing and improving the reliability of the next-generation new-technology based throughput architecture design. <br/><br/>The paramount reliability challenges in throughput processors include particle strikes induced soft errors, hard errors driven by aging effects, and manufacturing process variations. The principle investigator is building new foundations for vulnerability characterization and prediction, error detection, and fault tolerance against those dominant reliability challenges in throughput processors integrated with novel technologies. The project objectives include: (1) modeling and analyzing the vulnerability of novel-technology (e.g., STT-RAM, NTC, and 3D) enabled throughput processors in the presence of soft error, aging effects, and process variations; (2) fast and accurate predictive model to forecast the vulnerability phase behavior of throughput processors under new technologies; (3) developing the light-weight error detection mechanisms; and (4) exploring the opportunities and challenges introduced by the novel technologies to cost-effectively tolerate various types of errors in next-generation throughput architecture design. The proposed research will significantly promote the capability of architecting reliable throughput processors in future technologies beyond CMOS, making it possible to fulfill the Moore's Law without suffering the negative effects caused by various fault mechanisms. Moreover, this project will realize the desire of applying throughput processors into a wide range of computing scale from mobile computing to cloud computing, and increasing the deployment of throughput processors in support of supercomputing in science and engineering (e.g., finance, medical, biology, petroleum, aerospace, and geology). This project will also contribute to society through engaging high-school and undergraduate students from minority-serving institutions into research, expanding the computer engineering curriculum with reliability modeling and optimization techniques on throughput processors, attracting women and under-represented groups into graduate education, and disseminating research infrastructure for education and training of US IT workforce."
"1812927","AF: Small: Collaborative Research: Effective Numerical Algorithms and Software for Nonlinear Eigenvalue Problems","CCF","ALGORITHMIC FOUNDATIONS, EPSCoR Co-Funding","10/01/2018","07/30/2018","Agnieszka Miedlar","KS","University of Kansas Center for Research Inc","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$140,851.00","","amiedlar@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457568","7858643441","CSE","7796, 9150","7923, 7933, 9102, 9150","$0.00","The eigenvalue problem is a central topic in science and engineering arising from a wide range of applications and posing major numerical challenges. For decades, it has been the focus of numerous theoretical research activities for developing various efficient numerical algorithms. These efforts have led to the development of new software that is essential to assist the everyday work of many engineers and scientists. In spite of progress made on solving the eigenvalue problem, methods available for handling these problems remain limited in their scope and they have not resulted in effective general-purpose software so far. The primary goal of this project is to fill this gap by advancing the state of the art in solution methods for nonlinear eigenvalue problems which are both mathematically and practically far more challenging than the traditional linear eigenvalue problems. The combined expertise of the investigating team is well suited for exploring new algorithms in this arena, analyzing them, and developing new effective software that can universally impact a wide range of disciplines (engineering, physics, chemistry, and biology). The outcome of the project are expected to open new and efficient ways to solve nonlinear eigenvalue problems. A new suite of state of the art numerical routines will be developed, fully tested, and publicly released.<br/><br/>The goal of this project is to advance the state-of-the-art in solution methods for nonlinear eigenvalue problems. The new approaches that are envisioned are expected to be particularly effective for solving large-scale problems using parallelism. The main thrust of the project is the development of novel eigenvalue algorithms based on generalizations of Cauchy integral type methods for the nonlinear case, combined with projection methods such as Krylov and subspace iteration. A starting point in this investigation is the FEAST approach which will be adapted to the nonlinear context. Because the problems under consideration are expected to be large and sparse, the team will investigate methods that rely on domain decomposition where the original physical domain is partitioned into a number of subdomains in order to exploit parallelism. Among other goals, the team will carefully study the extension of the tools that are exploited in the linear case, such as spectrum slicing (computing eigenvalues by parts), block methods, and iterative linear solves, to the nonlinear case.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1350344","CAREER: Static-Analysis-Driven Engineering of Modern Software Systems","CCF","SOFTWARE & HARDWARE FOUNDATION, Secure &Trustworthy Cyberspace","02/01/2014","05/29/2018","Matthew Might","UT","University of Utah","Continuing grant","Anindya Banerjee","01/31/2020","$449,994.00","","might@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798, 8060","1045, 7943, 9150","$0.00","Users of software are all too familiar with its shortcomings: software<br/>is slow, software is buggy and software is insecure.  When a complex<br/>software system fails, it is unhelpfully simplistic to blame the<br/>implementors of the system as incompetent.  The truth is that software<br/>engineers are uniquely disadvantaged among the traditional engineering<br/>disciplines because they lack a viable predictive model for the<br/>systems they design and build. That is, a software engineer cannot<br/>predict the behavior of software in practice in the same way that a<br/>civil engineer can predict the behavior of a bridge under load. The<br/>primary intellectual merit of this research is that it lays the<br/>critical, systematic foundations for the science of prediction for<br/>software. The broader impacts are to enable engineers to build better<br/>software with the aid of predictivity. Moreover, this research also<br/>seeks to develop courses and educational material to train the next<br/>generation of software engineers in the art of constructing fast,<br/>safe, reliable and secure software in this fashion.  As this research<br/>transfers into practice and engineers adopt this methodology, it will<br/>significantly strengthen the foundation of national<br/>cyberinfrastructure.<br/><br/>The core technical thrust of this research is the development of a<br/>systematic method for the synthesis of static analyzers for complex,<br/>modern programming languages.  It also explores whether or not this<br/>methodology can be automated in whole or in part. To motivate the<br/>development of this method, this research applies the method to the<br/>synthesis of intensional static analyzers for popular scripting<br/>languages such as JavaScript, Perl, PHP, Ruby and Python?many of which<br/>happen to be the languages powering modern, web-based software.  The<br/>foundational technical concept of this research is the systematic<br/>transformation of small-step interpreters into static analyzers.<br/>Small-step analyzers promise unique advantages over traditional<br/>techniques, including more opportunities for optimizing speed and<br/>precision, and clearer, easier reasoning about the soundness of the<br/>results of the analysis."
"1442909","Cybersees Type 2:  Cyber-Enabled Water and Energy Systems Sustainability Utilizing Climate Information","CCF","GRANT OPP FOR ACAD LIA W/INDUS, CyberSEES","09/01/2014","12/11/2018","Sankarasubraman Arumugam","NC","North Carolina State University","Standard Grant","Bruce Hamilton","02/29/2020","$1,241,845.00","Gnanamanikam Mahinthakumar, Joe DeCarolis, Ning Lu, Sreerama(Sarat) Sreepathi","sankar_arumugam@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","1504, 8211","019Z, 8208","$0.00","Continually increasing water demand (due to population growth) and fuel costs threaten the reliability of water and energy systems and also increase operational costs. In addition, climatic variability and long-term change increase the vulnerability of these two systems. For instance, reservoir systems primarily depend on monthly to seasonal precipitation; whereas power systems demand depend on variations in diurnal temperature over the season. Currently, both systems consider climatological averages for their short-term (0-3 months) management, which ignores uncertainty in the climate resulting in reduced hydropower (i.e., increased spill) from reservoirs and increased operational costs for power systems from excessive fuel stockpiling. While seasonal climate forecasts contain appreciable levels of skill over parts of the US in both winter and summer, the uptake of these forecasts for co-optimization of water and power systems has been limited due to lack of a framework to assimilate, visualize and communicate probabilistic forecasts into management models. The project will develop a prototype visual analytic simulation-optimization framework for better communication and visualization of probabilistic information on the decision space for improving water and power systems sustainability utilizing monthly to seasonal multimodel climate forecasts. <br/> <br/>The primary goal of this poject is to develop an integrated cyber-enabled approach for improved water and energy sustainability utilizing the monthly to seasonal climate forecasts by: A. Developing a cyberinfrastructure framework using Optimus-PRIME that co-optimizes water and power system allocations by incorporating precipitation, power demand and wind power ensembles. B. Developing a visual analytic framework for interactive exploration of multi-objective decision space and interaction with the optimization engine by leveraging recent developments in visual analytics. C. Developing fine-grained and coarse-grained resource utilization strategies for targeting different parts of the computation on heterogeneous many-core/accelerator based architectures. D. Analyzing risk management strategies that include improved fuel stockpiling, scheduled plant maintenance and reservoir operational schedules to minimize operational costs utilizing the multimodel electricity demand, wind power, and streamflow forecasts available for the pilot basin. F. Performing scenario analyses that consider increased renewable energy availability -- hydropower and wind -- in the power generation mix, reduced CO2 emission scenarios under current and increased water demand potentials contingent on multimodel climate forecasts. F. Synthesizing the findings from objectives (A-F) by considering various virtual water and power system configurations that are typical across the country for generalization and broader application. The fundamental contribution of the proposal lies in developing a prototype cyberinfrastructure for improving water and power systems management utilizing climate forecasts. Using the framework, the study proposes to minimize the operational costs of these two interdependent systems by considering various scenarios for maximizing the renewable energy potential utilizing multimodel climate (precipitation, temperature and wind) forecasts. By utilizing the streamflow and power demand forecasts, a paradigm shift in water and power systems management is targeted that promotes various proactive strategies such as forward purchasing of fuels, reducing emissions and meeting target reservoir storage to ensure both systems reliability. To overcome the computational challenges arising from probabilistic forecasts, an HPC-enabled cyberinfrastructure will parallelize the computations across ensembles and provide a seamless interaction between water and power stochastic optimization models. For interactive exploration of multi-objective decision spaces in the water and power sectors, new developments in visual analytics will be employed that can lead to improved stakeholder facilitated solutions for the pilot basin and promote learning and understanding through virtual systems.<br/><br/>Developed research modules and findings will be incorporated into various advanced graduate courses at NCSU in the water resources and environmental engineering, computer-aided engineering and power system engineering curricula. The developed climate information based risk-management tools will also be used in the NCSU's Climate Change and Society Professional Science Master's program, which exclusively draws professionals with interdisciplinary background. The investigators will also work with the State Climate Office of NC and federal agencies, PNNL and ORNL, to implement the algorithms developed in the project for broader applications. The investigators will also develop customized podcasts and modules on climate-water-energy nexus and their significance to sustainability for high school students by collaborating with the engineering place at NCSU."
"1547205","INSPIRE: Optimization Algorithms for Regional Thermoelectric Power Generation with Nonlinear Interference","CCF","INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS, INSPIRE","09/01/2015","08/06/2015","Matthew Johnson","NY","CUNY Herbert H. Lehman College","Standard Grant","Tracy Kimbrel","08/31/2019","$316,000.00","Charles Vorosmarty","mpjohnson@gmail.com","250 Bedford Park Blvd West","Bronx","NY","104681589","7189608107","CSE","1640, 2878, 7796, 8078","7926, 8020, 8653, 9251","$0.00","This INSPIRE project is jointly funded by the Algorithmic Foundations program in the Computing and Communications Foundations Division in the Directorate for Computer and Information Science and Engineering, the Environmental Sustainability program in the Chemical, Bioengineering, Environmental, and Transport Systems Division in the Directorate for Engineering, and the Office of Integrative Activities (OIA) INSPIRE program.<br/><br/>A thermoelectric power plant's operations can affect its surrounding environment, for example by raising water temperatures, which can be harmful to aquatic life, and so must comply with government regulation such as the Clean Water Act (CWA). It has recently been observed that the effects of power plants' operations can be much longer-reaching and subtler than this, however: the use of this warmer water by a second power plant located downstream can degrade that plant's efficiency, causing it in turn to heat the river water more than it otherwise would have, or even forcing it to shut down in order to comply with the CWA. Such complex dynamics characterizing the joint effects of a region's power plants suggest possible gains from managing plants jointly rather than individually. This analytical perspective prompts consideration of a huge variety of potential benefits to seek and costs to avoid in optimizing regional plant operations, and the interference phenomenon prompts (re)consideration of a number of classical algorithmic problems in the field of combinatorial optimization. This research offers many potential societal benefits in terms of environmental protection, economic savings, energy security, protection from blackouts, public health, and so on. Insights provided by the algorithmic solutions this project develops will be conveyed to decision makers and, if successful, will ultimately lead to improvements in management practices in existing plants and in long-range strategic planning. The project will provide research training for graduate students and will expose undergraduates at Lehman College and CCNY (both Minority-Serving Institutions) to interdisciplinary scientific research.<br/><br/>This project will inaugurate the study of a novel class of combinatorial optimization problems. More specifically, it will investigate new variations on classical problems such as knapsack and job scheduling, modified to incorporate a distinctive feature of the motivating application setting, i.e. the *nonlinear interference* that can occur between active power plants. The PI and his team will design efficient (near-)optimal algorithms for these problems in the sense of guaranteed approximation, and, leveraging existing analytical models, they will perform algorithmic engineering studies assessing their algorithms' real-world viability. Finally, using tools from algorithmic game theory, they will quantify and provide a rigorous foundation for the perceived benefits of solving the motivating plant management problems jointly rather than plant-by-plant."
"1839204","RAISE/TAQS: Quantum simulation of materials and molecules using quantum computation","CCF","OFFICE OF MULTIDISCIPLINARY AC","10/01/2018","09/11/2018","Fernando Brandao","CA","California Institute of Technology","Standard Grant","Dmitry Maslov","09/30/2021","$1,000,000.00","Garnet Chan, Austin Minnich, Edgar Solomonik","fbrandao@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1253","049Z, 057Z, 7928","$0.00","The objective of this award is to explore how transformative advances in the simulation of quantum dynamics can be realized on quantum computers that will be available in the next five years. Such near-term quantum computers containing 49 to 100 qubits, though modest in size, can represent and carry out operations on quantum states that are intractable on classical computers. However, it is not yet clear which physically relevant problems are infeasible on classical computers yet solvable with near-term quantum computers. The goal of this project is to delineate the boundary between efficient classical and quantum algorithms for quantum dynamics, for which there is a strong theoretical basis to believe that quantum computers can outperform classical computers for certain kinds of physical simulations and observables.  The outcome of this project will be a significant advance in computational tools needed to describe quantum dynamics in physically relevant problems in chemistry, physics, and materials science, and will particularly address how quantum computers may serve as a transformational new tool to study these fundamental processes.<br/><br/>The project will advance state-of-the-art tensor network classical algorithms as well as with quantum algorithms that can be implemented on near-term quantum computers with limited gate depth and noisy qubits. The award addresses several long-standing scientific questions.  First, the researchers will investigate whether accurate simplifications exist for classical algorithms if only simple observables are desired and how the presence of noise affects these classical approximations. Second, the team will work on determining the size of a physically relevant boundary problem between quantum and classical simulation methods using the best available classical algorithms and high performance implementations. Third, the researchers will identify robust quantum algorithms for noisy real-time and imaginary-time quantum dynamics on near-term architectures. Finally, the project will apply these results to assess what are the best near-term chemical and materials systems in which to study quantum dynamics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740248","E2CDA: Type I: Collaborative Research: Energy-efficient analog computing with emerging memory devices","CCF","Energy Efficient Computing: fr, SOFTWARE & HARDWARE FOUNDATION","09/15/2017","06/27/2018","Qiangfei Xia","MA","University of Massachusetts Amherst","Continuing grant","Sankar Basu","08/31/2020","$230,025.00","","qxia@umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","015Y, 7798","7798, 7945, 8089, 9251","$0.00","The main goal of this project is to develop analog computing circuits that will greatly exceed their digital counterparts in energy-efficiency, speed, and density by employing emerging nonvolatile memory devices. Though analog circuits have been around for a long time, their applications in computing have been rather limited, largely due to the lack of efficient implementations of analog weights. This impediment could be overcome now due to the rapid progress in the emerging nonvolatile memory devices, such as metal-oxide memristors, which are the focus in this project. The analog memory functionality of memristors, combined with high retention and sub-10-nm scaling prospects, might for the first time enable extremely fast and energy-efficient analog implementations of many core operations, such as vector-by-matrix multiplication, which are central to many existing and emerging future applications such as internet-of-the-things and sensor networks, robotics, and energy efficient neuromorphic systems.  The results of the proposed research will be integrated into educational curriculum and will help to train material science and electrical engineering students of all levels in this exciting field.<br/><br/>The main caveat of the considered analog circuits is their limited operation accuracy, primarily due to the noise and variability in memory devices. The mitigation of this challenge by several means will be one of the main focuses of the project, and will be addressed with highly-interconnected research effort across device, circuit, and architectural layers. At the device level, detailed electrical characterization of analog operation and ways to improve it via material engineering, optimization of electrical stress, and development of efficient tuning algorithms to cope with device variations will be explored. Guided by experimentally-verified device models, the design of several representative analog computing circuits will be optimized. Circuit modeling tools will be developed to capture rich design trade offs in area, speed, energy efficiency, and precision, calibrated on experimental results from wafer-scale integrated memristor circuits, and used for detailed comparison with state-of-the-art digital counterparts. Finally, accurate circuit models will guide exploration of circuit architectures that mitigate limitations of analog computing and assist with detailed system level simulations."
"1725649","SPX: Collaborative Research: Cross-layer Application-Aware Resilience at Extreme Scale (CAARES)","CCF","SPX: Scalable Parallelism in t","08/15/2017","01/29/2018","Manish Parashar","NJ","Rutgers University New Brunswick","Standard Grant","Vipin Chaudhary","07/31/2020","$267,247.00","","parashar@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","042Y","026Z","$0.00","The increasing demands of science and engineering applications push the limits of current large-scale systems, and is expected to achieve exascale (10^18 FLOPS) performance early in the next decade. One of the lesser studied challenge at extreme scales is the reliability of the computing system itself, primarily due to the very large number of cores and components utilized and to the sharp decrease of the Mean Time Between Failures on such systems (in the order of tens of minutes). This project departs from the traditional single component fault management model, and explores how multiple software libraries (and application components) used in the context of a single parallel application can interact to provide the holistic fault management support necessary for parallel applications targeting capability computing. This exploration will not be limited to software developed using a single parallel programming paradigm, but will be extended to encompass the more challenging case where multiple programming paradigms can be combined to achieve a common goal, to simulate a set of large scale scientific applications in use today.<br/> <br/>The goal of this project is to depart from the current siloed resilience mechanisms, and propose cross-layer composition solutions that can fundamentally address these resilience challenges at extreme scales.This exploration will not be limited to software developed using a single parallel programming paradigm, but will be extended to encompass the more challenging case where multiple programming paradigms can be combined to achieve a common goal, to simulate a set of large scale scientific applications in use today. More specifically, this proposal will address the following research challenges: (1) development of a theoretical foundation for a deeper understanding of the challenges and opportunities arising from combining different resilience models and methodologies; (2) design of a flexible programming abstraction to allow different resilience models and mechanisms to be combined to cooperate and address resilience in a more holistic manner; and (3) development of basic, programming paradigm independent, constructs necessary to implement cross-layer and domain-specific approaches to support resilience and to understand related performance / quality trade-offs. The proposed approach will be validated by exposing these generic abstractions in two different programming paradigms (MPI and OpenSHMEM), by creating and developing specialized concepts for each of these paradigms. This will enable the assessment of the validity of the concepts and the corresponding overheads imposed by the different software layers, using few software frameworks and applications."
"1725499","SPX: Collaborative Research: Cross-layer Application-Aware Resilience at Extreme Scale (CAARES)","CCF","SPX: Scalable Parallelism in t","08/15/2017","08/09/2017","Barbara Chapman","NY","SUNY at Stony Brook","Standard Grant","Vipin Chaudhary","07/31/2020","$266,674.00","","barbara.chapman@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","042Y","026Z","$0.00","The increasing demands of science and engineering applications push the limits of current large-scale systems, and is expected to achieve exascale (10^18 FLOPS) performance early in the next decade. One of the lesser studied challenge at extreme scales is the reliability of the computing system itself, primarily due to the very large number of cores and components utilized and to the sharp decrease of the Mean Time Between Failures on such systems (in the order of tens of minutes). This project departs from the traditional single component fault management model, and explores how multiple software libraries (and application components) used in the context of a single parallel application can interact to provide the holistic fault management support necessary for parallel applications targeting capability computing. This exploration will not be limited to software developed using a single parallel programming paradigm, but will be extended to encompass the more challenging case where multiple programming paradigms can be combined to achieve a common goal, to simulate a set of large scale scientific applications in use today.<br/> <br/>The goal of this project is to depart from the current siloed resilience mechanisms, and propose cross-layer composition solutions that can fundamentally address these resilience challenges at extreme scales.This exploration will not be limited to software developed using a single parallel programming paradigm, but will be extended to encompass the more challenging case where multiple programming paradigms can be combined to achieve a common goal, to simulate a set of large scale scientific applications in use today. More specifically, this proposal will address the following research challenges: (1) development of a theoretical foundation for a deeper understanding of the challenges and opportunities arising from combining different resilience models and methodologies; (2) design of a flexible programming abstraction to allow different resilience models and mechanisms to be combined to cooperate and address resilience in a more holistic manner; and (3) development of basic, programming paradigm independent, constructs necessary to implement cross-layer and domain-specific approaches to support resilience and to understand related performance / quality trade-offs. The proposed approach will be validated by exposing these generic abstractions in two different programming paradigms (MPI and OpenSHMEM), by creating and developing specialized concepts for each of these paradigms. This will enable the assessment of the validity of the concepts and the corresponding overheads imposed by the different software layers, using few software frameworks and applications."
"1849588","SemiSynBio: Collaborative Research: Very Large-Scale Genetic Circuit Design Automation","CCF","SemiSynBio Semiconductor Synth, Genetic Mechanisms","10/01/2018","08/22/2018","Eduardo Sontag","MA","Northeastern University","Continuing grant","Mitra Basu","09/30/2021","$118,125.00","","eduardo.sontag@gmail.com","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","061Y, 1112","","$0.00","The computing power of biology is incredible, evident in the natural world in the intricate patterns underlying materials and the body plan of animals. Cells build these structures by using networks of interacting bio-molecules, encoded in their DNA, that function as microscopic computers, the power of which grows as many cells communicate to work together on a problem. The goal of this project is to significantly scale-up the ability to build these systems by design such that cells can be programmed to perform complex computational tasks. This will be done by creating software that allows a user to write code, exactly as one would program a computer, which is then compiled to a DNA sequence. New theoretical tools will be applied to determine the power required by the cell to run these programs and how best to distribute tasks between circuits encoded in cells and conventional electronic systems. This research will broadly impact biotechnology, which is increasingly being used to commercially produce a wide range of products, from consumer goods to high-end advanced materials. Current products do not harness the computational potential of cells; in other words, all the genes are turned on all the time. This research will enable cells to be programmed to build chemicals and materials in multiple steps, both by performing the computations inside of the cells and also communicating across cells. This work is interdisciplinary and requires backgrounds in Biology, Chemistry, Mathematics, Biological Engineering, Electrical Engineering, and Computer Science. As such, the project includes the development of new educational platforms in anticipation of a need in industry for students trained at the interface between traditionally separated fields. This includes a new undergraduate-level Synthetic Biology Design course, an industrial co-op, and curriculum material ""How to Grow Almost Anything,"" which will be made public at an international level. <br/><br/>To build the complexity of the natural world, cells use regulatory networks made up of interacting bio-molecules to control the timing and conditions for gene regulation. For the last 20 years, researchers have been able to build synthetic genetic circuits by artfully combining regulatory interactions. The problem is that the largest of such circuits only consist of ~10 regulators, far smaller than natural networks, which drastically limits the computation that can be performed. The proposed research will develop technologies that collectively enable a massive scale-up in computational complexity to ~10^5 regulators. The first objective seeks to increase the size of circuits within cells. Logic gates based on Cas9 have enormous scale-up potential, but are limited by dCas9 toxicity and sequence repeats. A set of gates will be designed to fix these problems, guided by mathematical modeling. A framework for design automation will be developed that enables a Verilog specification to be converted into a logic diagram, that is then divided up amongst many interacting cells. The second objective seeks to distribute a genetic circuit design across multiple communicating cells. The number and reliability of cell-cell communication signals will be improved by directed evolution to increase the number of channels from 2 to 8. These will be implemented in living cells and non-living systems, thus enabling a broad range of applications inside and outside the bioreactor. Combined with 50 gates/cell, this platform offers the possibility of multicellular circuits containing 10^5+ gates. Some applications require deployment as a non-living system, for example when the application is outside of the lab, thus requiring containment. The third objective seeks to translate the parts developed in Objectives 1 and 2 to operate in multiple communicating lipid vesicles encapsulating cell-free protein extract. Cas9 gates and additional communication channels will be characterized to expand the computational potential. These will be characterized as gates and implemented using Electronic Design Automation tools to automate the design of large systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807520","SemiSynBio: Collaborative Research: Very Large-Scale Genetic Circuit Design Automation","CCF","SemiSynBio Semiconductor Synth, Genetic Mechanisms","10/01/2018","08/01/2018","Eduardo Sontag","MA","Northeastern University","Continuing grant","Mitra Basu","09/30/2021","$0.00","","eduardo.sontag@gmail.com","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","061Y, 1112","7465","$0.00","The computing power of biology is incredible, evident in the natural world in the intricate patterns underlying materials and the body plan of animals.  Cells build these structures by using networks of interacting bio-molecules, encoded in their DNA, that function as microscopic computers, the power of which grows as many cells communicate to work together on a problem. The goal of this project is to significantly scale-up the ability to build these systems by design such that cells can be programmed to perform complex computational tasks.  This will be done by creating software that allows a user to write code, exactly as one would program a computer, which is then compiled to a DNA sequence. New theoretical tools will be applied to determine the power required by the cell to run these programs and how best to distribute tasks between circuits encoded in cells and conventional electronic systems. This research will broadly impact biotechnology, which is increasingly being used to commercially produce a wide range of products, from consumer goods to high-end advanced materials.  Current products do not harness the computational potential of cells; in other words, all the genes are turned on all the time.  This research will enable cells to be programmed to build chemicals and materials in multiple steps, both by performing the computations inside of the cells and also communicating across cells. This work is interdisciplinary and requires backgrounds in Biology, Chemistry, Mathematics, Biological Engineering, Electrical Engineering, and Computer Science. As such, the project includes the development of new educational platforms in anticipation of a need in industry for students trained at the interface between traditionally separated fields.  This includes a new undergraduate-level Synthetic Biology Design course, an industrial co-op, and curriculum material ""How to Grow Almost Anything,"" which will be made public at an international level.<br/><br/> To build the complexity of the natural world, cells use regulatory networks made up of interacting bio-molecules to control the timing and conditions for gene regulation. For the last 20 years, researchers have been able to build synthetic genetic circuits by artfully combining regulatory interactions. The problem is that the largest of such circuits only consist of ~10 regulators, far smaller than natural networks, which drastically limits the computation that can be performed. The proposed research will develop technologies that collectively enable a massive scale-up in computational complexity to ~10^5 regulators. The first objective seeks to increase the size of circuits within cells.  Logic gates based on Cas9 have enormous scale-up potential, but are limited by dCas9 toxicity and sequence repeats. A set of gates will be designed to fix these problems, guided by mathematical modeling.  A framework for design automation will be developed that enables a Verilog specification to be converted into a logic diagram, that is then divided up amongst many interacting cells. The second objective seeks to distribute a genetic circuit design across multiple communicating cells. The number and reliability of cell-cell communication signals will be improved by directed evolution to increase the number of channels from 2 to 8.  These will be implemented in living cells and non-living systems, thus enabling a broad range of applications inside and outside the bioreactor. Combined with 50 gates/cell, this platform offers the possibility of multicellular circuits containing 10^5+ gates. Some applications require deployment as a non-living system, for example when the application is outside of the lab, thus requiring containment. The third objective seeks to translate the parts developed in Objectives 1 and 2 to operate in multiple communicating lipid vesicles encapsulating cell-free protein extract. Cas9 gates and additional communication channels will be characterized to expand the computational potential. These will be characterized as gates and implemented using Electronic Design Automation tools to automate the design of large systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1725692","SPX: Collaborative Research: Cross-layer Application-Aware Resilience at Extreme Scale (CAARES)","CCF","SPX: Scalable Parallelism in t","08/15/2017","08/09/2017","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Vipin Chaudhary","07/31/2020","$266,079.00","","bosilca@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","042Y","026Z","$0.00","The increasing demands of science and engineering applications push the limits of current large-scale systems, and is expected to achieve exascale (10^18 FLOPS) performance early in the next decade. One of the lesser studied challenge at extreme scales is the reliability of the computing system itself, primarily due to the very large number of cores and components utilized and to the sharp decrease of the Mean Time Between Failures on such systems (in the order of tens of minutes). This project departs from the traditional single component fault management model, and explores how multiple software libraries (and application components) used in the context of a single parallel application can interact to provide the holistic fault management support necessary for parallel applications targeting capability computing. This exploration will not be limited to software developed using a single parallel programming paradigm, but will be extended to encompass the more challenging case where multiple programming paradigms can be combined to achieve a common goal, to simulate a set of large scale scientific applications in use today.<br/> <br/>The goal of this project is to depart from the current siloed resilience mechanisms, and propose cross-layer composition solutions that can fundamentally address these resilience challenges at extreme scales.This exploration will not be limited to software developed using a single parallel programming paradigm, but will be extended to encompass the more challenging case where multiple programming paradigms can be combined to achieve a common goal, to simulate a set of large scale scientific applications in use today. More specifically, this proposal will address the following research challenges: (1) development of a theoretical foundation for a deeper understanding of the challenges and opportunities arising from combining different resilience models and methodologies; (2) design of a flexible programming abstraction to allow different resilience models and mechanisms to be combined to cooperate and address resilience in a more holistic manner; and (3) development of basic, programming paradigm independent, constructs necessary to implement cross-layer and domain-specific approaches to support resilience and to understand related performance / quality trade-offs. The proposed approach will be validated by exposing these generic abstractions in two different programming paradigms (MPI and OpenSHMEM), by creating and developing specialized concepts for each of these paradigms. This will enable the assessment of the validity of the concepts and the corresponding overheads imposed by the different software layers, using few software frameworks and applications."
"1817602","AF: Small: RUI: Unifying Self-Assembly Through Tile Automata","CCF","COMPUTATIONAL BIOLOGY","07/01/2018","06/27/2018","Robert Schweller","TX","The University of Texas Rio Grande Valley","Standard Grant","Mitra Basu","06/30/2021","$495,042.00","Tim Wylie","robert.schweller@utrgv.edu","1201 West University Dr","Edinburg","TX","785392909","9566652889","CSE","7931","7923, 7946, 9229","$0.00","Self-assembly is the bottom-up process by which simple unorganized components autonomously combine to form large complex structures. This process is abundant in nature as a key underlying mechanism for the construction of biological organisms. The local interactions of simple self-assembling systems are often capable of simulating general purpose computation as well, leading researchers to begin using self-assembly technology as a tool for the precise algorithmic manipulation of matter at nano-scales. This project introduces and explores the Tile Automata abstract model of self-assembly to serve as a framework for unifying the diverse set of established and experimentally motivated models of self-assembly. The work develops fundamental theoretical results within this framework, and connects various established experimental models using Tile Automata as a central hub - yielding surprising connections between previously disparate models. Beyond providing a foundation for a theory of self-assembly, this framework also provides an important tool to influence and simplify experimental work. Some aspects of the DNA-based model are implausible, but by using the connections with Tile Automata, the work will show that the power of the unrealistic features may be attained within a more plausible limited version of the model, thus providing a guide for experimental implementation. A major goal of this work is to provide undergraduate research opportunities and increase Computer Science research participation among underrepresented groups. Another outcome of this work will be the continued development of software simulators for various models as well as the implementation of new models. <br/><br/>The project introduces a mathematical model termed Tile Automata that combines the local interaction rules from Cellular Automata systems with self-assembly properties from tile-based self-assembly models. Although the majority of self-assembly models are motivated by experimental techniques, the abstractions may unintentionally limit or unrealistically extend the power of the system. Part of the research is to identify and remove these properties. The first focus of this work is on the DNA based signal tile model where unrealistic aspects of when signals fire may be exploited to achieve results within the model that are impossible experimentally. The investigators address these issues by proposing multiple small variations to signal passing, which each remove the issue with the original model. Each variation is then tied to the unifying framework and equality is achieved within a scale factor for each variation and for the original signal tile model, thereby providing an experimentally feasible path for achieving the full power of the model. The second focus is on repulsive forces within self-assembly models, which provide an enormous amount of power and allow for unrealistic communication across arbitrary distance within the model. The project seeks to show that repulsive forces may be simulated within the Tile Automata framework if assemblies are allowed to temporarily connect to check compatibility and then disconnect if not, which shows equivalent capabilities can be achieved without the improbable communication. The third focus is on other common self-assembly models and active or movement-based models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1317470","Collaborative Research: Visual Cortex on Silicon","CCF","INFORMATION TECHNOLOGY RESEARC, EXPERIMENTAL EXPEDITIONS","10/01/2013","08/24/2018","Subhasish Mitra","CA","Stanford University","Continuing grant","Ephraim P. Glinert","09/30/2019","$680,000.00","","subh@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","1640, 7723","7723","$0.00","The human vision system understands and interprets complex scenes for a wide range of visual tasks in real-time while consuming less than 20 Watts of power. This Expeditions-in-Computing project explores holistic design of machine vision systems that have the potential to approach and eventually exceed the capabilities of human vision systems. This will enable the next generation of machine vision systems to not only record images but also understand visual content. Such smart machine vision systems will have a multi-faceted impact on society, including visual aids for visually impaired persons, driver assistance for reducing automotive accidents, and augmented reality for enhanced shopping, travel, and safety. The transformative nature of the research will inspire and train a new generation of students in inter-disciplinary work that spans neuroscience, computing and engineering discipline.<br/><br/>While several machine vision systems today can each successfully perform one or a few human tasks ? such as detecting human faces in point-and-shoot cameras ? they are still limited in their ability to perform a wide range of visual tasks, to operate in complex, cluttered environments, and to provide reasoning for their decisions.  In contrast, the mammalian visual cortex excels in a broad variety of goal-oriented cognitive tasks, and is at least three orders of magnitude more energy efficient than customized state-of-the-art machine vision systems. The proposed research envisions a holistic design of a machine vision system that will approach the cognitive abilities of the human cortex, by developing a comprehensive solution consisting of vision algorithms, hardware design, human-machine interfaces, and information storage. The project aims to understand the fundamental mechanisms used in the visual cortex to enable the design of new vision algorithms and hardware fabrics that can improve power, speed, flexibility, and recognition accuracies relative to existing machine vision systems. Towards this goal, the project proposes an ambitious inter-disciplinary research agenda that will (i) understand goal-directed visual attention mechanisms in the brain to design task-driven vision algorithms; (ii) develop vision theory and algorithms that scale in performance with increasing complexity of a scene; (iii) integrate complementary approaches in biological and machine vision techniques; (iv) develop a new-genre of computing architectures inspired by advances in both the understanding of the visual cortex and the emergence of electronic devices; and (v) design human-computer interfaces that will effectively assist end-users while preserving privacy and maximizing utility. These advances will allow us to replace current-day cameras with cognitive visual systems that more intelligently analyze and understand complex scenes, and dynamically interact with users.<br/><br/>Machine vision systems that understand and interact with their environment in ways similar to humans will enable new transformative applications. The project will develop experimental platforms to: (1) assist visually impaired people; (2) enhance driver attention; and (3) augment reality to provide enhanced experience for retail shopping or a vacation visit, and enhanced safety for critical public infrastructure. This project will result in education and research artifacts that will be disseminated widely through a web portal and via online lecture delivery. The resulting artifacts and prototypes will enhance successful ongoing outreach programs to under-represented minorities and the general public, such as museum exhibits, science fairs, and a summer camp aimed at K-12 students. It will also spur similar new outreach efforts at other partner locations. The project will help identify and develop course material and projects directed at instilling interest in computing fields for students in four-year colleges. Partnerships with two Hispanic serving institutes, industry, national labs and international projects are also planned."
"1855919","CAREER: Overcoming limitations to approximating combinatorial optimization problems","CCF","ALGORITHMIC FOUNDATIONS","04/04/2018","10/23/2018","Alexandra Kolla","CO","University of Colorado at Boulder","Continuing grant","Rahul Shah","04/30/2020","$190,205.00","","alexkolla@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7796","1045, 7927","$0.00","This proposal seeks to develop a comprehensive understanding of the limitations of approximation algorithms for combinatorial optimization problems. Combinatorial optimization problems are of great importance to various areas such as operations research, machine learning, VLSI design, computational biology and statistical physics. The task of finding algorithms for combinatorial optimization problems arise in countless applications, from billion-dollar operations to everyday computing tasks. Many optimization problems are NP-hard and thus cannot be solved exactly in polynomial time unless P=NP. However, the majority of such problems are key elements to practical applications where often, if the optimal solution is hard to  find, producing an approximate solution su;ffices.<br/><br/>The research proposed will study the limitations of approximation algorithms posed by the Unique Games Conjecture and the limitations posed by the presence of unlikely worst-case instances. The PI aims to design a methodology to partially or fully overcome such limitations by addressing both of those factors. The proposal outlines a challenging plan focusing on research in a broad cross-section of spectral graph theory, convex optimization, multi-commodity flows, and harmonic analysis.   The contributions of the work described in this proposal will have great impact on the theory of approximability as well as real world problems for which efficient, exact algorithms will be provided for semi-random instances and will naturally result in collaborations between researchers across many different  fields such as mathematics, theory of computer science, industrial engineering, operations research and networking."
"1729369","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","EXPERIMENTAL EXPEDITIONS","03/01/2018","09/19/2018","Peter Shor","MA","Massachusetts Institute of Technology","Continuing grant","Almadena Y. Chtchelkanova","02/28/2023","$990,741.00","Edward Farhi, Isaac Chuang, Aram Harrow","shor@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1533753","XPS: FULL: DSD: Scalable High Performance with Halide and Simit Domain Specific Languages","CCF","Exploiting Parallel&Scalabilty","08/01/2015","07/20/2015","Saman Amarasinghe","MA","Massachusetts Institute of Technology","Standard Grant","Anindya Banerjee","07/31/2019","$845,000.00","Fredo Durand, Armando Solar-Lezama, Wojciech Matusik","saman@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8283","","$0.00","Title: XPS: FULL: DSD: Scalable High Performance with Halide and Simit Domain Specific Languages<br/><br/>Today, getting scalable parallel performance requires heroic programming by experts. In this proposal we are developing a methodology based on Domain Specific Languages (DSLs) to simplify the programmer effort required to harness the power of scalable parallelism. The intellectual merits of this proposal are to show that DSLs can provide a way for programmers to take advantage of scalable performance without a heroic effort. Having a simple path for scalable parallel performance will have a broader significance and importance on areas such as climate modeling and other simulations of large-scale science, by enabling them to efficiently utilize large-scale machines and the cloud.<br/><br/>This proposal aims to radically simplify high performance DSL construction. First, it will introduce a unified transformation framework where complex program transformations are described by example. Using synthesis technology, combinations of localized rewriting rules will be extracted and applied, simplifying the implementation while providing correctness guarantees. Second, it will build a unified parallel low-level intermediate representation by extending LLVM. With the new parallel backend, DSLs only have to expose algorithmic parallelism and the backend will do all architecture-specific mapping of parallelism to vector, non-uniform memory access (NUMA), graphics processing unit (GPU) and distributed parallel components. Third, it will develop a unified auto-tuning framework. Effectiveness of frontend transformations depends on the ability of backends to exploit them. The unified auto-tuning framework will completely eliminate this complexity by offloading transformation selection to the auto-tuner which will use sophisticated machine learning techniques to empirically select transformations that yield the best scalable parallel performance.  The ideas introduced will be demonstrated through two important domain-specific languages ? the Halide DSL for image processing pipelines and the Simit DSL for physical simulations."
"1565235","AF: Large: Collaborative Research: Algebraic Proof Systems, Convexity, and Algorithms","CCF","ALGORITHMIC FOUNDATIONS","05/01/2016","07/25/2017","Pablo Parrilo","MA","Massachusetts Institute of Technology","Continuing grant","Tracy J. Kimbrel","04/30/2021","$1,165,654.00","Jonathan Kelner, Ankur Moitra","Parrilo@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","7925, 7926, 7927","$0.00","This project tackles some of the central questions in algorithms, optimization, and the theory of machine learning, through the lens of the ""Sum of Squares"" algorithmic framework. In particular, it will allow us to understand what classes of functions can be efficiently minimized, and what computational resources are needed to do so. If successful, this will significantly advance our understanding in all these key areas, produce new practical algorithmic methodologies, as well as build new connections with other fields, including quantum information theory, statistical physics, extremal graph theory and more.<br/><br/>This collaborative grant will foster new interactions between intellectual communities that have had relatively little interaction so far, and the PIs will organize workshops, courses, and other events that bring these communities together. The students and postdocs trained will gain a uniquely broad view of the landscape of these areas.<br/><br/>The PIs propose a unified approach to the development and analysis of convex proof systems that include and generalize the ""Sum of Squares"" (SoS) method. Despite considerable recent progress, understanding SoS?s performance seems to be out-of-reach for most current techniques.  Significant progress in this area requires the synthesis of ideas and techniques from different domains, including theoretical computer science, optimization, algebraic geometry, quantum information theory and machine learning. The research plans include both theory-building and problem-solving aspects, with the ultimate goal of obtaining a complete understanding of the SoS method and related proof systems, as well as their algorithmic implications.<br/><br/>Research efforts will be directed along several thrusts: Unique Games and related problems (Small Set Expansion, Max Cut, Sparsest Cut), analysis of average-case problems (e.g., Planted Clique), applications to Machine Learning (sparse PCA, dictionary learning), algorithmic speedups of SoS, and connections to math and physics (e.g., quantum entanglement, p-spin glasses, extremal graph theory and algebraic geometry). While the main focus is on theoretical aspects, this project is also concerned with effective computational methods, and the outcomes may yield novel practical techniques for machine learning and optimization.<br/><br/>Other key features of this proposal include its strong integration with curriculum development, undergraduate research projects, and training the next wave of graduate students and postdocs and equipping them with the necessary tools to work across these areas."
"1452616","CAREER: Applications of Quantum Information Theory","CCF","QIS - Quantum Information Scie, ALGORITHMIC FOUNDATIONS, QUANTUM COMPUTING","06/01/2015","07/09/2018","Aram Harrow","MA","Massachusetts Institute of Technology","Continuing grant","Dmitry Maslov","05/31/2020","$472,000.00","","aram@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7281, 7796, 7928","1045, 7203, 7928","$0.00","Quantum information is the study of what quantum mechanics means as a theory of information.  Striking, but still incompletely understood, features of the theory include the uncertainty principle, entanglement, quantum cryptography and quantum computing.  <br/><br/>The first part of the proposed research will tackle core questions in quantum information theory that have been barriers to its wider applicability.  For example: to what extent can the ""monogamy"" (or non-shareability) of quantum entanglement be used to distinguish it from classical correlation?  What does it mean for information in a system to be contained in its short-range or long-range degrees of freedom?<br/><br/>The second part of this research will extend the reach of quantum information theory by developing applications in condensed-matter physics, optimization algorithms and other areas of math and physics.  Through progress on the core problems above, this research hopes to develop better simulations of quantum systems, better understanding of how entanglement functions in many-body physics and new algorithms for hard optimization problems.  These tools are expected to be of use not only to researchers in quantum information, but to a wide range of physicists and computer scientists.  <br/><br/>This research will be done together with a graduate student, who will learn techniques from physics, computer science and mathematics.  Additionally, related topics will be integrated into undergraduate teaching."
"1619085","CIF: Small: Massive Uncoordinated and Sporadic Multiple Access -- Strengthening Connections between Coding and Random Access","CCF","COMM & INFORMATION FOUNDATIONS","06/15/2016","06/17/2016","Krishna Narayanan","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","05/31/2019","$495,010.00","Panganamala Kumar, Jean-Francois Chamberland","krn@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7797","7923, 7935","$0.00","The wireless landscape is poised to change, once again, within the next few years due to the emergence of machine-driven communications. This creates new challenges for wireless traffic, with packets originating from sporadic transmissions rather than sustained connections. Currently deployed scheduling policies are ill-equipped to deal with such traffic because they rely on gathering information about channel quality and queue length for every active device. The goal of this research initiative is to address this deficiency and devise novel access schemes tailored to massive uncoordinated and sporadic multiple access, thereby readying wireless infrastructures for the traffic of tomorrow. The broader impacts of this research program include providing pragmatic solutions to some of the challenges posed by an evolving wireless landscape, strengthening wireless infrastructures, and contributing to the training of a globally competitive Science, Technology, Engineering and Math  workforce.  The research tasks are attuned to societal needs in information technologies, an important economic driver for our nation.  The wide dissemination of the findings will enhance the scientific understanding of wireless systems, access strategies, and iterative methods.<br/><br/>The intellectual merit of this research initiative lies in exploiting the close connections between message-passing decoding and serial interference cancellation to create new access strategies. Linking advances in iterative methods to uncoordinated random access embodies the type of crosscutting research that can lead to disruptive technologies and paradigm shifts. This project embraces the evolving perspective of harnessing interference in wireless networks rather than fighting it or avoiding it. This viewpoint underlies many recent successes in network coding and distributed storage. This project brings forth such a perspective in the design of large-scale wireless networks."
"1714425","AF: Small: Computational Algebraic Methods for Systems of Partial Difference-Differential Equations","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/31/2017","Alexander Levin","DC","Catholic University of America","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2020","$181,746.00","","levin@cua.edu","620 Michigan Ave.N.E.","Washington","DC","200640001","2026355000","CSE","7796","7923, 7933","$0.00","Natural processes, like the formation of a snowflake, can reveal deep connections between physical laws, which are often modeled as systems of mathematical equations, and symmetries, which can be captured mathematically as group actions.   In computer algebra software, which can help solve systems of equations, it would be good to be able to ensure that operations respect group actions and preserve symmetries.   Despite the over sixty-year history of algorithmic approaches in differential and difference algebra, there are currently no efficient computational techniques to analyze systems of algebraic partial difference-differential equations (PDDEs) and more general systems of partial differential equations (PDEs) with group action. This project aims to develop the theory and algorithms to determine the structure of solutions of a system of PDDEs or PDEs with additional conditions imposed by the action of transformation groups (e. g., PDEs with symmetries), for describing physical, chemical, or biological processes with symmetries.  The educational goal of the project is to involve into an interdepartmental program on applications of symbolic computation the PI will develop, not only undergraduate majors in computer science, mathematics, and physics at the Catholic University of America (CUA), but also engineering majors in the field of automatic control and biology majors who work with continuous and discrete mathematical models of biology systems. <br/><br/>The key research directions of this project are: (1) Development of computational methods and algorithms for difference-differential elimination and for decomposition of solution sets of systems of algebraic PDDEs and PDEs with group action into unions of characterizable (?simple??) components. Extension of these techniques to systems with weighted basic operators. (2) Elaboration of methods and algorithms for the evaluation of dimension characteristics of the solution sets of the above-mentioned systems. In particular, the PI will obtain algorithms for computing dimension polynomials and quasi-polynomials that express the Einstein?s strength of a system of PDDEs.  (3) Application of the developed techniques to systems of PDDEs and PDEs with group action that arise in physics, automatic control, chemistry and biology.  <br/>The main methods and approaches of the project include the characteristic set technique for difference-differential polynomials and its generalizations to the cases of several term orderings and weighted basic operators, the relative Groebner basis method, the techniques of dimension polynomials and quasi-polynomials, and decomposition methods for algebraic PDDEs and PDEs with group action.  The results will be demonstrated in interdisciplinary research projects at CUA."
"1715187","AF: Small: Communication Amid Uncertainty","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","08/30/2017","Madhu Sudan","MA","Harvard University","Standard Grant","Tracy J. Kimbrel","08/31/2020","$450,000.00","","madhu@cs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7923, 7926, 7927","$0.00","Modern communication devices possess enormous ability to compute and to store information. These abilities enable a rich collection of potential ways in which the device can aid its user and adapt to their preferences. Unfortunately, this ability to adapt to the user also introduces challenges when communicating with other similar devices. Each device is now uncertain about the exact knowledge and behavior of the other devices. This project explores the theoretical foundations for communicating with such uncertainty. On the one hand, it focuses on qualitative issues such as ""misunderstanding"" and explores how misunderstanding can be detected and corrected before influential actions are taken.  On the other hand, it also explores the quantitative issues behind how large shared context can lead to efficient (short) communications even in the presence of uncertainty. The broader intellectual impact of the project will come from expanded connections between the mathematical fields of communication and computer science to fields such as linguistics, philosophy, neuroscience, and communication studies. Broader impact among the scientific community will also be achieved by the mentoring and education of junior researchers (Ph.D. candidates) who intend to pursue their own careers in research. Educational courses and materials will be developed based on this interdisciplinary research project. Finally, the project will actively seek broad dissemination of the progress in research by presentation of the research and its outcomes in seminars at leading conferences, workshops, and academic and industrial research institutions, and by posting publications on publicly available websites.  <br/><br/>The scientific foundations for a theory of uncertain communication lead to questions on a model of communication that is a blend of the Shannon model from the 1940s, and the Yao model from the 1970s. On the one hand, the Shannon model leads to a rich collection of problems that can be solved adequately when there is no uncertainty. The Yao model, on the other hand, presents a natural model for capturing uncertainty via the setting of correlated inputs. Blending the two leads to rich questions including:<br/><br/>1) Can information be compressed down to its entropy when sender and receiver are uncertain about the priors used by each other?<br/><br/>2) Can the ubiquitous use of randomness be replaced by mildly correlated random variables while conserving the complexity of communication?<br/><br/>3) Can communication remain efficient even if there is uncertainty about the exact goal of the communication?<br/><br/>This project explores questions such as the above by ascribing precise mathematical measures that capture the questions and then analyzing the resulting measures."
"1714091","SHF: Small: Making Strassen's Algorithm Practical","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2017","07/19/2017","Robert van de Geijn","TX","University of Texas at Austin","Standard Grant","Almadena Y. Chtchelkanova","07/31/2020","$465,884.00","Field Van Zee, Margaret Myers","rvdg@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7923, 7942, 9251","$0.00","High-performance linear algebra software libraries are at the core of scientific computing and machine learning applications.  At the core of many high-performance linear algebra libraries lies the matrix multiplication operation because many other matrix operations can be cast in terms of matrix multiplication and matrix multiplication itself can attain high performance.  Strassen?s algorithm, first proposed in 1969, is a clever scheme for reducing the number of arithmetic calculations that must be performed when computing a matrix multiplication.  It has mostly been a theoretical curiosity that has led to a sequence of improvements over the years. Some practical applications of Strassen?s algorithm for very large problem sizes have been encountered in, for example, the aerospace industry.  Very recently, it was shown that Strassen?s algorithm, and some of its variations, can be made practical for small problem sizes, opening up a range of new academic and practical directions of research.  The project will pursue these directions and will incorporate the advances in high-performance software libraries.  In essence, it will give the user a performance boost of up to around 30%, for free. <br/><br/>The proposed work will create a practical framework and analysis for the implementation of a broad family of Strassen-like algorithms, building on a model of computation that captures the interaction between software and hardware. This will yield the most thorough understanding to date of the practical implementation of such algorithms.  The proposed project will also deliver a software library for practical use in computational science and machine learning applications that cast computation in terms of matrix-matrix multiplication and/or tensor contractions, with a mechanism for choosing the best algorithm from that family. It builds on recent advances regarding the high-performance implementation of linear algebra software libraries.  What was shown was that such libraries can be composed from small kernels that can be highly optimized for a specific architecture.  These kernels have become the building blocks for traditional algorithms for matrix operations.  In this research, they also become the building blocks for high-performance algorithms that incorporate Strassen?s algorithm and closely related so-called fast matrix multiplication algorithms.  The resulting software will be released under open source license to facilitate its use and study. Pedagogical outreach will include the development of a Massive Open Online Course on ""Programming for Performance"" in which Strassen-like algorithms and their practical implementation will be a prominent enrichment. The project involves several members from traditionally underrepresented groups and will continue a long tradition of involvement by undergraduates."
"1317433","Collaborative Research:  Visual Cortex on Silicon","CCF","INFORMATION TECHNOLOGY RESEARC, EXPERIMENTAL EXPEDITIONS","10/01/2013","09/03/2015","Laurent Itti","CA","University of Southern California","Continuing grant","Ephraim P. Glinert","09/30/2019","$1,450,000.00","","itti@pollux.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1640, 7723","7723","$0.00","The human vision system understands and interprets complex scenes for a wide range of visual tasks in real-time while consuming less than 20 Watts of power. This Expeditions-in-Computing project explores holistic design of machine vision systems that have the potential to approach and eventually exceed the capabilities of human vision systems. This will enable the next generation of machine vision systems to not only record images but also understand visual content. Such smart machine vision systems will have a multi-faceted impact on society, including visual aids for visually impaired persons, driver assistance for reducing automotive accidents, and augmented reality for enhanced shopping, travel, and safety. The transformative nature of the research will inspire and train a new generation of students in inter-disciplinary work that spans neuroscience, computing and engineering discipline.<br/><br/>While several machine vision systems today can each successfully perform one or a few human tasks ? such as detecting human faces in point-and-shoot cameras ? they are still limited in their ability to perform a wide range of visual tasks, to operate in complex, cluttered environments, and to provide reasoning for their decisions.  In contrast, the mammalian visual cortex excels in a broad variety of goal-oriented cognitive tasks, and is at least three orders of magnitude more energy efficient than customized state-of-the-art machine vision systems. The proposed research envisions a holistic design of a machine vision system that will approach the cognitive abilities of the human cortex, by developing a comprehensive solution consisting of vision algorithms, hardware design, human-machine interfaces, and information storage. The project aims to understand the fundamental mechanisms used in the visual cortex to enable the design of new vision algorithms and hardware fabrics that can improve power, speed, flexibility, and recognition accuracies relative to existing machine vision systems. Towards this goal, the project proposes an ambitious inter-disciplinary research agenda that will (i) understand goal-directed visual attention mechanisms in the brain to design task-driven vision algorithms; (ii) develop vision theory and algorithms that scale in performance with increasing complexity of a scene; (iii) integrate complementary approaches in biological and machine vision techniques; (iv) develop a new-genre of computing architectures inspired by advances in both the understanding of the visual cortex and the emergence of electronic devices; and (v) design human-computer interfaces that will effectively assist end-users while preserving privacy and maximizing utility. These advances will allow us to replace current-day cameras with cognitive visual systems that more intelligently analyze and understand complex scenes, and dynamically interact with users.<br/><br/>Machine vision systems that understand and interact with their environment in ways similar to humans will enable new transformative applications. The project will develop experimental platforms to: (1) assist visually impaired people; (2) enhance driver attention; and (3) augment reality to provide enhanced experience for retail shopping or a vacation visit, and enhanced safety for critical public infrastructure. This project will result in education and research artifacts that will be disseminated widely through a web portal and via online lecture delivery. The resulting artifacts and prototypes will enhance successful ongoing outreach programs to under-represented minorities and the general public, such as museum exhibits, science fairs, and a summer camp aimed at K-12 students. It will also spur similar new outreach efforts at other partner locations. The project will help identify and develop course material and projects directed at instilling interest in computing fields for students in four-year colleges. Partnerships with two Hispanic serving institutes, industry, national labs and international projects are also planned."
"1652560","CAREER:  Quantifying Noisy Quantum Resources","CCF","OFFICE OF MULTIDISCIPLINARY AC, SPECIAL PROJECTS - CCF, QIS - Quantum Information Scie, QUANTUM COMPUTING","06/15/2017","09/18/2018","Graeme Smith","CO","University of Colorado at Boulder","Continuing grant","Dmitry Maslov","05/31/2022","$280,346.00","","gsbsmith@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1253, 2878, 7281, 7928","1045, 7203, 7928","$0.00","While quantum mechanics was originally developed to describe the realm of the very small, it has become increasingly clear over the past decades that quantum theory is at bottom a theory of information. This realization opens the door for a more complete understanding of both quantum theory and information theory.  Asking how to best store, transmit, and process information allows the identification of key features of quantum theory and enables the identification of the fundamental limits applying to all information technologies.<br/><br/>This project aims to identify, quantify, and ultimately understand the fundamental resources in quantum information theory.  It studies optimal interconversion rates of noisy resources, such as quantum communication links and noisy quantum states. Understanding how such noisy resources can be combined to generate quantum synergies is a key challenge to be tackled, as is identifying potential application of such synergies.  Just as crucially, determining when such synergies (aka non-additivities) are ruled out is a key challenge to be tackled: This project seeks to determine which resources are additive, and how they can be quantified.  Finally, the project seeks to use techniques developed to address additivity and non-additivity to build bridges between quantum information theory and computational complexity theory.<br/><br/>The research will be carried out by the PI together with a graduate student, who will learn methods and ideas from physics, information theory, theoretical computer science, and mathematics."
"1832312","NSF Expeditions in Computing Principal Investigator's Meeting - 2018","CCF","EXPERIMENTAL EXPEDITIONS","05/01/2018","04/18/2018","Frankie King","TN","Vanderbilt University","Standard Grant","Mitra Basu","04/30/2019","$367,139.00","","frankie.king@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","7723","7556, 7723","$0.00","This project supports the organization of the second NSF Expeditions in Computing (Expeditions) Principal Investigators' (PI) Meeting at the Newseum's Knight Conference Center in Washington, DC, between Sunday, December 9 and Tuesday, December 11, 2018. The 2018 Expeditions PI Meeting builds on the previous Expeditions PI Meeting and Expeditions research by fostering collaboration and promoting close interaction between academia and industry, governments, research labs, etc. The meeting is a unique opportunity to interact with a broad cross-section of the scientific community, CISE leadership, as well as with leaders in both industry and academia. It also provides an important venue for scanning horizons activities, establishing new collaborations between different collaborating teams, and exploring possible technology transfer and partnerships.<br/><br/>These meetings are designed to advance knowledge by exposing the progress of compelling and transformative collaborative research conducted by multi-institutional and multi-disciplinary research teams that promise disruptive innovations in computer and information science and engineering, to allow PIs to showcase their research, and to provide an opportunity to network. The 2018 Expeditions PI Meeting will bring together approximately 150 stakeholders comprising academia, industry, and government. The Expeditions PI Meeting 2018 will take place on the 10th year anniversary of the Expeditions program launch.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849048","EAGER: AF: Collaborative Research: Weak Derandomizations in Time and Space Complexity","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","09/11/2018","Vinodchandran Variyam","NE","University of Nebraska-Lincoln","Standard Grant","Tracy J. Kimbrel","09/30/2019","$50,000.00","","vinod@cse.unl.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","CSE","7796","7916, 7927","$0.00","Computational complexity theory classifies natural computational problems into various complexity classes based on the amount of resources needed to solve them. Typical resources are time, memory, amount of randomness, and circuit size. This project aims to advance the state of the art in understanding the power and limitations of randomness and circuit size, using two new, previously untested, techniques. Intuition gained from this project will enhance our understanding of practical computational problems arising from fields beyond computer science. The project's exploration has the potential to solve central, longstanding, open questions in complexity theory. <br/><br/>The first part of the project will investigate unconditional de-randomization of probabilistic time via de-randomization of multi-pass, probabilistic space-bounded computations. In particular, this project will explore a new approach to obtain an asymptotically better deterministic simulation of probabilistic linear time than what is currently known.  The second part of this project aims to prove new fixed-polynomial size circuit lower bounds via designing pseudo-deterministic approximation algorithms for certain counting problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849053","EAGER: AF: Collaborative Research: Weak Derandomizations in Time and Space Complexity","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","09/11/2018","Pavan Aduri","IA","Iowa State University","Standard Grant","Tracy J. Kimbrel","09/30/2019","$49,806.00","","pavan@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7796","7916, 7927","$0.00","Computational complexity theory classifies natural computational problems into various complexity classes based on the amount of resources needed to solve them. Typical resources are time, memory, amount of randomness, and circuit size. This project aims to advance the state of the art in understanding the power and limitations of randomness and circuit size, using two new, previously untested, techniques. Intuition gained from this project will enhance our understanding of practical computational problems arising from fields beyond computer science. The project's exploration has the potential to solve central, longstanding, open questions in complexity theory. <br/><br/>The first part of the project will investigate unconditional de-randomization of probabilistic time via de-randomization of multi-pass, probabilistic space-bounded computations. In particular, this project will explore a new approach to obtain an asymptotically better deterministic simulation of probabilistic linear time than what is currently known.  The second part of this project aims to prove new fixed-polynomial size circuit lower bounds via designing pseudo-deterministic approximation algorithms for certain counting problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1514371","SHF: Medium: Collaborative Research: Atomic scale to circuit modeling of emerging nanoelectronic devices and adapting them to SPICE simulation package","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2015","06/15/2015","Pinaki Mazumder","MI","University of Michigan Ann Arbor","Standard Grant","Sankar Basu","05/31/2019","$400,622.00","","mazum@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","7924, 7945","$0.00","Aggressive scaling of CMOS technology and concomitant inventions of nanoscale nascent technologies have fueled the growth of computer, information, communication and consumer electronics industries of the 21st Century by leveraging the ground-breaking discoveries in nanoscience and nanotechnology. The workhorse of multibillion-dollar semiconductor industry, the CMOS technology is approaching its scaling limit due to the strong quantum-mechanical effects present at the nanoscale. To sustain the accelerated pace of economic growth during the post-CMOS era, this multi-university collaborative research proposal envisages building the roadmap of VLSI technology in two significant ways. First, the research is mooted to extend quantum transport principles to simulate emerging nano-devices based on novel semiconductor and 2-D layered materials by exploiting non-charge based degrees of freedom, electron spin controlled magnetization, interaction between electromagnetic waves and semiconductors in metamaterial structures, and topological states in topological insulators. Second, the research will systematically scale these properties from their fundamental atomistic limits to circuit level integration by developing industry-graded SPICE-compatible compact models for heterogeneous circuits that will define the landscape of beyond Moore?s Law VLSI systems. Integrative education, training, and outreach activities envisioned in this collaborative proposal will encompass K-12, undergraduate, graduate, female, minority, and postdoctoral fellows by leveraging the existing outreach activities of participating universities in order to advance science and engineering education in broader segments of the society.<br/><br/>Using density-functional theory (DFT), time-dependent density functional theory (TD-DFT), time-dependent density-matrix functional theory (TD-DMFT), to phenomenological Extended Huckel to effective mass, in conjunction with non-equilibrium Green?s function (NEGF) methods, quantum field theory, and finite-difference time domain (FDTD) methods, a wide variety of computational methods are going to be developed to tackle the modeling of multiscale circuits in future VLSI systems. The software packages and multiscale modeling tools resulting from the proposed research activity are going to provide computer chip designers and manufacturers the ability to model complex hybrid substrates comprising nanoscale electronic, spintronic, opto-electronic, and plasmonic devices. The resulting software is going to be written with a view to enabling researchers from universities and practicing engineers in industries to develop their own modules that will engender improved system functionality, integration density, and operational speed."
"1563838","AF: Medium: Collaborative Research: The Power of Randomness for Approximate Counting","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","07/25/2017","Santosh Vempala","GA","Georgia Tech Research Corporation","Continuing grant","Rahul Shah","08/31/2020","$671,300.00","Eric Vigoda","vempala@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","7924, 7926","$0.00","The study of the complexity of counting problems has a long and rich history in theoretical computer science. Counting problems (and closely related sampling problems) arise naturally in many different fields, for example in statistical physics they correspond to partition functions and for studies of the equilibrium states of idealized models of physical systems, and in Bayesian inference they arise for the study of posterior distributions or maximum likelihood distributions. The specific questions addressed here are long-standing open problems, progress on which will be of wide interest. The project will develop new tools for approximate counting and is likely to make new and useful connections between statistical physics, probability and computational complexity. The research results will be disseminated via course notes, a summer school and workshops. Any practical algorithms that result will be made publicly available.<br/><br/>The overall goal of the project is to extend the known boundary of polynomial-time tractability for counting problems, to understand whether randomness is essential and how it could be eliminated, and to push the limits of the current fastest randomized algorithms towards practicality. Specific aims include: (1) Polynomial-time randomized approximation schemes for some fundamental problems that have thus far eluded efficient solutions, (2) Deterministic polynomial-time approximation schemes for some central problems that have celebrated randomized algorithms and (3) Faster randomized algorithms for classical counting problems."
"1755829","CRII: CIF: Fundamental Limits of Conditional Stochastic Optimization","CCF","COMM & INFORMATION FOUNDATIONS","06/01/2018","12/18/2017","Niao He","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","05/31/2020","$175,000.00","","niaohe@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7935, 8228","$0.00","Decision-making in the presence of randomness has been a fundamental and longstanding challenge in many fields of science and engineering.  In the wake of recent breakthroughs in artificial intelligence, there has been a prominent transition of interests and demands from classical (single-stage) stochastic optimization to multi-stage stochastic programming. In contrast to classical stochastic optimization, multi-stage stochastic problems are known to suffer from the curse of dimensionality, for which efficient universal oracle-based algorithms are not readily available. The goal of this research is to build bridges from classical stochastic optimization to multi-stage stochastic problems by developing an understanding of the fundamental limits of an intermediate class of optimization problems - conditional stochastic optimization - in the hopes of closing the algorithmic and theoretical gaps. Because of its specificity (i.e., it involves nonlinear functions of conditional expectations and lacks unbiased stochastic oracles), this class of optimization problems falls beyond the theoretical and practical grasp of the vast majority of state-of-the-art optimization algorithms. <br/><br/>The investigator will undertake a systematic study of this subject by (i) establishing new techniques for the design of algorithms adapted to different observation schemes and exploitable structures and (ii) developing sample complexities and non-asymptotic convergence analysis for the proposed algorithms. This research will significantly extend the current scope of stochastic optimization in both theory and applicability.  It will also lay the foundation for achieving the long-term goal of bridging to multi-stage decision-making problems and enriching the computational toolbox and theoretical developments for optimization under uncertainty.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813805","SHF: Small: Hot DNA Computation: Speeding up DNA-based Computation, CRNs, and Robotics using Strand-Displacing Polymerase","CCF","COMPUTATIONAL BIOLOGY","10/01/2018","09/04/2018","John Reif","NC","Duke University","Standard Grant","Mitra Basu","09/30/2021","$224,999.00","","reif@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7931","7923, 7946","$0.00","To date, there has been considerable success in experimental demonstrations of DNA-based molecular devices to implement molecular-scale Boolean circuit computations, chemical reaction systems, and robotics. However, these experiments take hours to perform due to relatively slow reaction rates. Some progress has been made (by the project's investigators and others) in speeding up DNA-based computations, but the reactions still take tens of minutes. The objective of this project is to substantially speed-up DNA-based computations. The work is highly interdisciplinary and will provide interdisciplinary education at undergraduate and graduate levels. The project will engage students (with stress on women and under-represented minorities) from different academic levels across multiple disciplines in mentoring and teaching. Hands-on demonstrations of DNA computing and robotic devices will be designed for outreach programs at Duke and North Carolina Science & Math High School. Workshops and lectures will help disseminate knowledge of advanced DNA-based nanoscience concepts to undergraduate and graduate student audiences.<br/><br/>The project work will include design, simulation, and experimental demonstration of protocols which make use of only DNA hybridization and strand-displacing polymerization reactions. In particular, the project will not make use of the much slower either strand-displacement hybridization reactions or restriction enzyme reactions). The designs for Boolean circuit computations will be simulated and optimized. Experimental demonstrations will be made for each design, first in solution, and then experimentally demonstrated with the components attached to DNA nanostructures to allow for further speed-up via localized reactions. The project tasks include as Task 1, the design, simulation and demonstration of fast DNA logic circuits using strand-displacing polymerase reactions; this will include experimental demonstrations of multiple large-scale Boolean circuit computations executed in solution. Initial work by the project's investigators has already experimentally demonstrated a Boolean circuit computation (in solution) of a square root computation with 4 Boolean inputs that ran in approximately 15 minutes, and it is expected that considerable speed-ups when these reactions are localized. Task 2 is the experimental demonstration of localized reactions using strand-displacing polymerase; here DNA logical circuits will be attached to self-assembled DNA nanotracks and DNA origami, and the work will include experimental demonstrations of Boolean circuit computations executed in a localized fashion. Task 2 will also demonstrate a high-speed localized chain reaction on a self-assembled DNA track using strand-displacing polymerase reactions using a novel design where the gates form a self-assembled nanotrack and the reaction gradually de-assembles the track; this may provide a swift medical diagnostic system for targeted nucleic acid detection of infections.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750983","CAREER: Understanding and Combating Numerical Bugs for Reliable and Efficient Software Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2018","09/05/2018","Cindy Rubio Gonzalez","CA","University of California-Davis","Continuing grant","Sol J. Greenspan","06/30/2023","$218,977.00","","crubio@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7798","1045, 7798, 7944, 9102, 9251","$0.00","The use of numerical software has grown rapidly over the past few years. From machine learning to safety-critical systems, a large variety of applications today make heavy use of floating point. Unfortunately, floating point introduces imprecision in numerical<br/>calculations. Analyzing, testing, and optimizing floating-point programs are difficult tasks. There is a large variety of numerical errors that can occur in such programs, including extreme sensitivity to roundoff, incorrectly handled exceptions, and nonreproducibility. This has led to numerous software bugs that have caused catastrophic failures. The goal of this research is to understand and combat numerical bugs. The intellectual merits are to advance the state of the art in analysis, testing and optimization of numerical software, and in extending these techniques to new domains beyond scientific applications. The importance of the research lies in the impact of the developed techniques and tools on improving the reliability and performance of real-world numerical programs, on which many other applications depend.<br/><br/>This research develops program analysis techniques and tools to (1) find frequent and impactful numerical bugs in programs, (2) propose<br/>fixes for these bugs, and (3) optimize numerical programs in different application domains to improve their performance. The research is driven by empirical studies that encompass several aspects of numerical software. First, a large-scale empirical study of numerical software is conducted to categorize real-world numerical bugs and their fixes. Second, an empirical study of test suites for numerical software is conducted to determine the effectiveness of testing in real-world numerical software. Based on the observations made through these empirical studies, a series of dynamic and static analyses are designed to detect and fix a variety of numerical bugs. These analyses are made available as part of an analysis and testing framework for numerical software. Novel precision tuning techniques are developed to enable scalable optimizations that lead to higher speedups, and to extend the scope of precision tuning to new application domains such as machine learning. The research has strong broader impacts in education and outreach. These include the development of new courses on software engineering and testing with a focus on numerical software, a Computer Science summer boot camp, and a mentoring program for underrepresented minorities especially focused on Latino students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1505970","AF: Medium: Collaborative research:  Advanced algorithms and high-performance software for large scale eigenvalue problems","CCF","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS, ALGORITHMIC FOUNDATIONS","07/15/2015","05/30/2017","Yousef Saad","MN","University of Minnesota-Twin Cities","Continuing grant","Balasubramanian Kalyanasundaram","06/30/2019","$360,690.00","","saad@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1253, 1271, 7796","7924, 7933, 9263","$0.00","Scientists and engineers in areas ranging from physics, chemistry, computer science, to economics, and statistics focus considerable attention on computing ""eigenvalues"" and ""eigenvectors"" of matrices.  They are central to the study of vibrations when building earthquake-resistant structures, to energy computation in solid-state physics, and to ranking web search results.  In spite of the enormous progress that has been made in the last few decades in solution methods for large eigenvalue problems, the current state-of-the-art methods remains unsatisfactory when dealing with the new generation of problems that need tens of thousands of eigenvectors for matrices that can have sizes in the tens of millions.<br/><br/>In recent years a new class of techniques has emerged that can compute wanted eigenpairs of large matrices by parts.  In these methods, 'windows' or 'slices' of the spectrum can be computed independently of one another and orthogonalization between eigenvectors in different slices is no longer necessary. When the number of eigenpairs to be computed is very large this divide-and-conquer approach becomes mandatory because orthogonalizing very large bases is prohibitive.  The resulting interior eigenvalue problems arise in a number of other situations and are now considered by the linear algebra community to be among the most challenging numerical problems to solve, and solution methods for handling them are still lagging.<br/><br/>The goal of this project is to advance the state of the art in solution methods for interior eigenvalue problems. The main thrust of the project is the development of novel algorithms based on a combination of Krylov or block-Krylov projection techniques and complex rational filters. A starting point in this investigation is the FEAST approach.  This project addresses many interesting questions in several areas, starting with methodologies for solving eigenvalue problems, to approximation theory questions for designing rational filter functions, and ending with effective parallel implementations.  Methods based on a domain decomposition framework will also be considered to deal with the common situation where the matrix (or pair of matrices in the generalized case) is (are) distributed.<br/><br/><br/>The broader impacts of this project highlight the impact on training, the dissemination of new efficient software, and the use of the software by-products in specific applications. All general-purpose codes that are developed under this project will be freely distributed into the public domain.  This project will have an impact on the training of graduate and undergraduate students in a field that is vital to the needs of academia, industry, and government laboratories."
"1510010","AF: Medium: Collaborative research: Advanced algorithms and high-performance software for large scale eigenvalue problems","CCF","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS, ALGORITHMIC FOUNDATIONS","07/15/2015","05/30/2017","Eric Polizzi","MA","University of Massachusetts Amherst","Continuing grant","Balasubramanian Kalyanasundaram","06/30/2019","$470,309.00","","polizzi@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","1253, 1271, 7796","7924, 7933, 9251, 9263","$0.00","Scientists and engineers in areas ranging from physics, chemistry, computer science, to economics, and statistics focus considerable attention on computing ""eigenvalues"" and ""eigenvectors"" of matrices.  They are central to the study of vibrations when building earthquake-resistant structures, to energy computation in solid-state physics, and to ranking web search results.  In spite of the enormous progress that has been made in the last few decades in solution methods for large eigenvalue problems, the current state-of-the-art methods remains unsatisfactory when dealing with the new generation of problems that need tens of thousands of eigenvectors for matrices that can have sizes in the tens of millions.<br/><br/>In recent years a new class of techniques has emerged that can compute wanted eigenpairs of large matrices by parts.  In these methods, 'windows' or 'slices' of the spectrum can be computed independently of one another and orthogonalization between eigenvectors in different slices is no longer necessary. When the number of eigenpairs to be computed is very large this divide-and-conquer approach becomes mandatory because orthogonalizing very large bases is prohibitive.  The resulting interior eigenvalue problems arise in a number of other situations and are now considered by the linear algebra community to be among the most challenging numerical problems to solve, and solution methods for handling them are still lagging.<br/><br/>The goal of this project is to advance the state of the art in solution methods for interior eigenvalue problems. The main thrust of the project is the development of novel algorithms based on a combination of Krylov or block-Krylov projection techniques and complex rational filters. A starting point in this investigation is the FEAST approach.  This project addresses many interesting questions in several areas, starting with methodologies for solving eigenvalue problems, to approximation theory questions for designing rational filter functions, and ending with effective parallel implementations.  Methods based on a domain decomposition framework will also be considered to deal with the common situation where the matrix (or pair of matrices in the generalized case) is (are) distributed.<br/><br/><br/>The broader impacts of this project highlight the impact on training, the dissemination of new efficient software, and the use of the software by-products in specific applications. All general-purpose codes that are developed under this project will be freely distributed into the public domain.  This project will have an impact on the training of graduate and undergraduate students in a field that is vital to the needs of academia, industry, and government laboratories."
"1812695","AF: Small: Collaborative Research: Effective Numerical Algorithms  and Software  for Nonlinear Eigenvalue Problems","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","07/30/2018","Yousef Saad","MN","University of Minnesota-Twin Cities","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$138,993.00","","saad@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7796","7923, 7933","$0.00","The eigenvalue problem is a central topic in science and engineering arising from a wide range of applications and posing major numerical challenges. For decades, it has been the focus of numerous theoretical research activities for developing various efficient numerical algorithms. These efforts have led to the development of new software that is essential to assist the everyday work of many engineers and scientists. In spite of progress made on solving the eigenvalue problem, methods available for handling these problems remain limited in their scope and they have not resulted in effective general-purpose software so far. The primary goal of this project is to fill this gap by advancing the state of the art in solution methods for nonlinear eigenvalue problems which are both mathematically and practically far more challenging than the traditional linear eigenvalue problems. The combined expertise of the investigating team is well suited for exploring new algorithms in this arena, analyzing them, and developing new effective software that can universally impact a wide range of disciplines (engineering, physics, chemistry, and biology). The outcome of the project are expected to open new and efficient ways to solve nonlinear eigenvalue problems. A new suite of state of the art numerical routines will be developed, fully tested, and publicly released.<br/><br/>The goal of this project is to advance the state-of-the-art in solution methods for nonlinear eigenvalue problems. The new approaches that are envisioned are expected to be particularly effective for solving large-scale problems using parallelism. The main thrust of the project is the development of novel eigenvalue algorithms based on generalizations of Cauchy integral type methods for the nonlinear case, combined with projection methods such as Krylov and subspace iteration. A starting point in this investigation is the FEAST approach which will be adapted to the nonlinear context. Because the problems under consideration are expected to be large and sparse, the team will investigate methods that rely on domain decomposition where the original physical domain is partitioned into a number of subdomains in order to exploit parallelism. Among other goals, the team will carefully study the extension of the tools that are exploited in the linear case, such as spectrum slicing (computing eigenvalues by parts), block methods, and iterative linear solves, to the nonlinear case.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618981","AF: Small: Quantitative and Algorithmic Aspects of Semi-algebraic Sets and Partitions","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","06/28/2016","Saugata Basu","IN","Purdue University","Standard Grant","Balasubramanian Kalyanasundaram","06/30/2019","$399,640.00","","sbasu@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7796","7923, 7933","$0.00","""Divide and conquer,"" a time-honored technique in computer science and<br/>mathematics, partitions a problem into smaller pieces to make it<br/>easier to solve. This project deepens the study of partitioning in two<br/>directions, both in the context of semi-algebraic geometry -- which is<br/>the study of geometric and topological properties of sets defined by<br/>real polynomial equalities and inequalities.<br/><br/>The first direction is to systematize polynomial partitioning, which<br/>has recently become a powerful tool capable of tackling many<br/>long-standing open questions in the area of incidence geometry. In<br/>computational geometry, the space or sets being partitioned are often<br/>complex but partitioning is by simple objects: splitting by planes or<br/>cutting into trapezoids.  Polynomial partitioning allows high degree<br/>cutting polynomials, necessitating more precise degree-based upper<br/>bounds on the topology of semi-algebraic sets and real varieties than<br/>the ones known before, and leading to a very fruitful interaction<br/>between the fields of real algebraic geometry and discrete geometry.<br/>The PI will develop new techniques in semi-algebraic geometry to meet<br/>the new demands imposed on the field by the developments in discrete<br/>geometry, and develop a new theory of polynomial partitioning that can<br/>systematize their study and lead to new applications as well.<br/><br/>The second is to generalize quantitative and algorithmic<br/>semi-algebraic geometry to a much more general and geometric setting<br/>-- the category of constructible sheaves.  Constructible sheaves come<br/>with an underlying semi-algebraic partition (on whose elements the<br/>stalks of the sheaf are locally constant).  Kashiwara and Schapira<br/>proved a fundamental theorem on the stability of this category under<br/>the six standard sheaf operations (analogous to Tarski-Seidenberg<br/>principle for semi-algebraic sets). The PI will study the quantitative<br/>and algorithmic questions related to constructible sheaves. The<br/>complexity upper bounds and algorithmic results would have wide<br/>applications.<br/><br/>The project will have impact in several areas of mathematics and<br/>computation -- including quantitative and algorithmic semi-algebraic<br/>geometry, discrete and computational geometry, computational<br/>complexity theory and quantitative study of the solutions to linear<br/>systems of partial differential equations.  The project will train a<br/>graduate student and a postdoctoral researcher in quantitative and<br/>algorithmic real algebraic geometry and its modern applications."
"1813480","AF: Small: Collaborative Research: Effective Numerical Algorithms  and Software for Nonlinear Eigenvalue Problems","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","07/30/2018","Eric Polizzi","MA","University of Massachusetts Amherst","Standard Grant","Balasubramanian Kalyanasundaram","09/30/2021","$218,268.00","","polizzi@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7796","7923, 7933","$0.00","The eigenvalue problem is a central topic in science and engineering arising from a wide range of applications and posing major numerical challenges. For decades, it has been the focus of numerous theoretical research activities for developing various efficient numerical algorithms. These efforts have led to the development of new software that is essential to assist the everyday work of many engineers and scientists. In spite of progress made on solving the eigenvalue problem, methods available for handling these problems remain limited in their scope and they have not resulted in effective general-purpose software so far. The primary goal of this project is to fill this gap by advancing the state of the art in solution methods for nonlinear eigenvalue problems which are both mathematically and practically far more challenging than the traditional linear eigenvalue problems. The combined expertise of the investigating team is well suited for exploring new algorithms in this arena, analyzing them, and developing new effective software that can universally impact a wide range of disciplines (engineering, physics, chemistry, and biology). The outcome of the project are expected to open new and efficient ways to solve nonlinear eigenvalue problems. A new suite of state of the art numerical routines will be developed, fully tested, and publicly released.<br/><br/>The goal of this project is to advance the state-of-the-art in solution methods for nonlinear eigenvalue problems. The new approaches that are envisioned are expected to be particularly effective for solving large-scale problems using parallelism. The main thrust of the project is the development of novel eigenvalue algorithms based on generalizations of Cauchy integral type methods for the nonlinear case, combined with projection methods such as Krylov and subspace iteration. A starting point in this investigation is the FEAST approach which will be adapted to the nonlinear context. Because the problems under consideration are expected to be large and sparse, the team will investigate methods that rely on domain decomposition where the original physical domain is partitioned into a number of subdomains in order to exploit parallelism. Among other goals, the team will carefully study the extension of the tools that are exploited in the linear case, such as spectrum slicing (computing eigenvalues by parts), block methods, and iterative linear solves, to the nonlinear case.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1564132","AF: Medium: Collaborative Research:Numerical Algebraic Differential Equations","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","07/25/2017","Chee Yap","NY","New York University","Continuing grant","Balasubramanian Kalyanasundaram","06/30/2020","$369,135.00","","yap@cs.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7796","7924, 7933","$0.00","Many basic physical principles, like conservation of mass or momentum for a fluid, are captured mathematically as systems algebraic differential equations.  Simplifying and solving these systems (which means reducing the number or complexity of the equations, and finding inputs that satisfy all equations) are fundamental to applications in many areas, including cellular biology, approximation for chemical reaction systems, combinatorics, and analysis.  The theoretical and algorithmic study of such systems spans more than a century, using three methods: purely symbolic, numerical, and hybrid symbolic-numeric.  <br/><br/>Symbolic methods (the quadratic formula being the simplest example) give the strongest guarantees of reliability, at a high (even exorbitant) cost in computational time and memory, since the same algorithm solves both mathematically hard and easy instances.  Numerical methods (the basis for computational simulation) allow small errors or approximations for speed; small intermediate errors produce corrupted outputs on singular and ill-conditioned (that is, nearly singular) input instances. In this project, a hybrid symbolic-numeric approach will be developed. Hybrid algorithms are more adaptive and have lower complexity than symbolic algorithms, and can avoid the errors of numerical algorithms.<br/><br/>In more technical detail, the three investigators apply existing and develop new methods of symbolic-numeric computation and differential algebra, producing algorithms that run on all inputs. They bring together existing methods of numerical algebraic geometry and software packages, such as Bertini, with recent theoretical results in differential algebra that provide upper bounds needed for guaranteed results. New near-optimal root isolation techniques are developed, implemented, and applied to solve systems of differential equations with finitely many solutions.  The work spans from theory to producing practical tools.<br/><br/>As part of this project the three investigators mentor and train students in symbolic and numeric computation at CUNY (noted for serving minority and low-income students) and NYU, and more broadly in New York City and Long Island, by activities ranging from  developing a Symbolic-Numeric Computing course for graduate students at the Computer Science program of the CUNY Graduate Center and NYU, to advising high school students in projects."
"1563942","AF: Medium:  Collaborative Research: Numerical Algebraic Differential Equations","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","09/19/2017","Alexey Ovchinnikov","NY","CUNY Queens College","Continuing grant","Balasubramanian Kalyanasundaram","06/30/2020","$534,398.00","Victor Pan","aovchinnikov@qc.cuny.edu","65 30 Kissena Blvd","Flushing","NY","113671575","7189975400","CSE","7796","7924, 7933","$0.00","Many basic physical principles, like conservation of mass or momentum for a fluid, are captured mathematically as systems of algebraic differential equations.  Simplifying and solving these systems (which means reducing the number or complexity of the equations, and finding inputs that satisfy all equations) are fundamental to applications in many areas, including cellular biology, approximation for chemical reaction systems, combinatorics, and analysis.  The theoretical and algorithmic study of such systems spans more than a century, using three methods: purely symbolic, numerical, and hybrid symbolic-numeric.  <br/><br/>Symbolic methods (the quadratic formula being the simplest example) give the strongest guarantees of reliability, at a high (even exorbitant) cost in computational time and memory, since the same algorithm solves both mathematically hard and easy instances.  Numerical methods (the basis for computational simulation) allow small errors or approximations for speed; small intermediate errors produce corrupted outputs on singular and ill-conditioned (that is, nearly singular) input instances. In this project, a hybrid symbolic-numeric approach will be developed. Hybrid algorithms are more adaptive and have lower complexity than symbolic algorithms, and can avoid the errors of numerical algorithms.<br/><br/>In more technical detail, the three investigators apply existing and develop new methods of symbolic-numeric computation and differential algebra, producing algorithms that run on all inputs. They bring together existing methods of numerical algebraic geometry and software packages, such as Bertini, with recent theoretical results in differential algebra that provide upper bounds needed for guaranteed results. New near-optimal root isolation techniques are developed, implemented, and applied to solve systems of differential equations with finitely many solutions.  The work spans from theory to producing practical tools.<br/><br/>As part of this project the three investigators mentor and train students in symbolic and numeric computation at CUNY (noted for serving minority and low-income students) and NYU, and more broadly in New York City and Long Island, by activities ranging from  developing a Symbolic-Numeric Computing course for graduate students at the Computer Science program of the CUNY Graduate Center and NYU, to advising high school students in projects."
"1763718","SHF: Collaborative Research: Biocompatible I/O Interfaces for Robust Bioorthogonal Molecular Computing","CCF","COMPUTATIONAL BIOLOGY","10/01/2018","06/21/2018","Darko Stefanovic","NM","University of New Mexico","Standard Grant","Mitra Basu","09/30/2021","$200,000.00","Matthew Lakin","darko@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","7931","7924, 7931, 9150","$0.00","Sugar molecules can exist in two forms that are the mirror image of each other but otherwise indistinguishable, like the left and right hand. All DNA molecules in nature contain right-hand sugars that cause them to assume the well-known double-helix shape with a right-handed twist. Proteins in cells in nature recognize the right-handed DNA in order to perform useful cellular functions, one of which is destroying single-stranded DNA. Over the last decade, DNA has come to be used in synthetic devices for sensing, computing, and diagnosing pathogens and disease. However, when these devices are introduced into a cell, they are prone to being destroyed by these proteins. This project will develop devices using left-handed DNA, which is not found in nature and is resistant to this form of degradation. In particular, the project will develop (1) input interfaces, for signaling from targets of interest, such as small molecules or natural DNA (D-DNA), to left-handed DNA (L-DNA), and (2) output interfaces, for signaling from left-handed DNA back into a relevant natural molecular pathway (such as DNA translation and transcription). This development will enable future devices that can sense multiple markers of the state of a cell (healthy or diseased), then integrate the sensors using DNA computing, and finally act on the cell, for example to destroy it if diseased, but the device itself will consist mainly of left-handed DNA and will therefore be robust in the cell. The project will involve both graduate and undergraduate students. It will be interdisciplinary, involving computer science and biomedical engineering, and will be carried out at the University of New Mexico and Columbia University.<br/><br/>The first aim of the project is to study the binding interactions between L-DNA (left-handed DNA) molecular logic devices and naturally occurring molecules. This study will produce a toolbox of basic techniques for implementing input interfaces that can detect naturally occurring target molecules, which in turn, can translate those binding events for information processing within a bio-orthogonal L-DNA logic circuit. This will be done by characterizing the actuation of hybrid L-DNA/D-DNA molecular computing components by pure D-DNA input strands. The second aim of the project is developing output interfaces that will enable L-DNA systems to produce some effect on the environment (i.e., carry out some form of actuation) as a result of their programmed molecular computations. Specifically, the project will focus on one particular mechanism for generating a useful output signal from an L-DNA molecular logic circuit, namely, gene knockdown by the allosteric ""activation"" of sequestered antisense D-nucleic acids by an L-DNA molecular circuit. Together, these will provide a mechanism for L-DNA molecular logic circuits to actuate via control of gene expression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813041","SHF: Small: Retrospective and Prospective Studies of the Effects of Gender Bias in Software Engineering","CCF","SOFTWARE & HARDWARE FOUNDATION","10/01/2018","08/24/2018","Emerson Murphy-Hill","NC","North Carolina State University","Standard Grant","Sol J. Greenspan","09/30/2021","$0.00","","emerson@csc.ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798","7923, 7944","$0.00","Gender bias has been researched extensively in the social science community, but relatively little research has been done in software engineering specifically, despite such issues coming to prominence in the technology sphere. In 2017, the investigator published a study on gender differences and bias in open source on a large scale, looking at contribution success rates in open-source software projects. The results showed that the likelihood of a contribution being accepted was higher for women than for men, but the trend is reversed when the gender of the pull requester is visible. By increasing understanding of the biases that decrease exclusivity of women, the largest underrepresented group in software engineering, this project aims to broaden participation and have significant benefits to society.<br/><br/>The project will conduct four families of foundational studies about gender bias in software engineering, leveraging a unique and extensive dataset. The first family of studies will re-examine existing studies using several new methods. The second family will triangulate prior results using other platforms and other outcome measures. The third family will investigate four aspects of bias uncovered in prior literature. The fourth family of studies will be experiments that isolate the effect of biases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841954","EAGER: Redistricting Design via Clustering in Euclidean and Planar-Graph Metrics","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","10/01/2018","08/17/2018","Philip Klein","RI","Brown University","Standard Grant","Rahul Shah","09/30/2019","$100,000.00","","klein@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","2878, 7796","7916, 7926, 9150","$0.00","The award deals with the design and analysis of algorithms for geographical problems, and algorithms for computational problems involving forming clusters of input datapoints. As an example, the project will involve investigation of a particular approach to electoral redistricting by computer, an approach that builds on clustering techniques and concepts.  The goals of the project are to design and assess redistricting methods that will yield compact, contiguous districts that are population-balanced to within a difference of one person, to quantitatively evaluate the compactness of the districts, to seek faster algorithms to compute the districts, and to study the degree to which the method is resistant to gerrymandering. One potential impact of this activity is to help inform the ongoing societal discussion of redistricting by demonstrating the availability of a transparent redistricting methods that achieve high-quality fair districting plans.  Another potential impact is in the training of new computer scientists.  Because research on redistricting addresses a perceived problem in society, it attracts and engages many people, especially those motivated by a desire for positive societal impact.  The project will support training both directly and indirectly.  First, students, particularly women and members of historically underrepresented groups, will be recruited to assist in performing this research and in disseminating the results. Second, the disseminated results will inspire and encourage people to study computer science.  This project will serve as an example of how algorithms can be used for social good. <br/><br/>The research draws on the study of the following optimization problem: given an integer k and a set of points in a metric space, find a partitioning of the given points into 'k' clusters so as to minimize the sum of squared intra-cluster distances subject to each cluster being assigned a '1/k' fraction of the locations.  Although this problem is computationally intractable, even a locally-optimal solution has desirable characteristics; for example, if the metric space is the Euclidean plane, the clusters can be represented by convex polygons with few sides on average. There are several obstacles to using this approach in practical redistricting, such as: (1) exact locations of people are not available; (2) finding a locally optimal solution can be time-consuming; (3) the Euclidean metric does not take into account geographical  barriers. The project will explore ways of overcoming these obstacles.  In addition, the project will investigate other questions, such as how well does local search approximate the minimum sum of squared distances, and how easy or difficult is it to find local solutions that give one group a significant political advantage.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1410022","AF: Medium: Quantum Hamiltonian Complexity: Through the Computational Lens","CCF","ALGORITHMIC FOUNDATIONS","09/01/2014","09/20/2016","Umesh Vazirani","CA","University of California-Berkeley","Continuing grant","Dmitry Maslov","08/31/2019","$1,200,000.00","","vazirani@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7796","7924, 7927, 7928","$0.00","Quantum computation has taught us that, in general, quantum systems are exponentially powerful.  This is a double-edged sword: while making quantum computers possible, it is also an enormous obstacle to analyzing and understanding physical systems.  Indeed, this is not just an abstract concern; with the major push in condensed matter physics towards studying and creating highly entangled states of matter, the computational complexity of these systems has moved to the fore as a major issue, and this is the main focus of the emerging area of Quantum Hamiltonian Complexity.  Here is a list of three basic questions in this area:<br/>1.  Can ""typical"" states of ?naturally occurring? quantum systems be described succinctly (i.e. polynomial rather than exponential in the number of particles)?<br/>2.  Does the exponential complexity of general quantum systems persist at high temperature?<br/>3.  Is the scientific method sufficiently powerful to be applicable to general quantum systems?<br/><br/>Each of these can be formulated as a precise computational question -- the first is a natural generalization of a question about the complexity class NP, the second about the celebrated PCP theorem and the third about interactive proof systems.  Over the last few years the outline of remarkable positive answers to some of these questions has emerged, and there are in place some of the basic techniques necessary to tackle these questions.  This project aims to pursue this line of inquiry.<br/><br/>This project is interdisciplinary at the deepest level. It brings a computational lens to bear on some of the most basic issues in condensed matter physics and in the philosophy of science.  Conversely, it extends the core concepts of computational complexity theory in important new directions, at the same time drawing on deep insights from physics to make progress on fundamental questions."
"1439042","XPS: FULL: SDA: Collaborative Research: RUI:  SCORE: Scalability-Oriented Optimization","CCF","Exploiting Parallel&Scalabilty","09/01/2014","07/17/2014","Stephen Freund","MA","Williams College","Standard Grant","Anindya Banerjee","08/31/2019","$252,000.00","","freund@cs.williams.edu","880 Main St.","Williamstown","MA","012672600","4135974352","CSE","8283","","$0.00","Title: XPS: FULL: SDA: Collaborative Research: SCORE: Scalability-Oriented Optimization<br/><br/>Modern CPUs, which contain an increasingly large number of processing units or ""cores"", offer the promise of continued increases in performance as the number of cores increases. Unfortunately, it is notoriously difficult for programmers to fully take advantage of this processing power. Computations can be viewed as cars on a network of highways: we want traffic to flow as fast as possible without any crashes. Programming languages offer ""synchronization operations""---the programmer equivalent of traffic lights---which improve safety but reduce speed. For large programs, managing the tension between the twin goals of safety (more lights) and performance (fewer lights) can be out of reach for all but expert programmers. This project, SCORE (scalability-oriented optimization), lifts this burden by automatically maximizing program performance while maintaining correctness. The intellectual merits of this work are the development of a suite of techniques to identify bottlenecks in programs, and transform their code or execution environment to eliminate those bottlenecks. The project's broader significance and importance are to enable non-expert programmers to achieve high performance on modern, multicore platforms, and thus dramatically increase the performance and efficiency of existing and new software; contributing to the national software research infrastructure; and increasing access to science research opportunities and training for students.<br/><br/>As with optimizing compilation for sequential code, SCORE lifts the burden of concurrency optimization from programmers, letting them focus exclusively on getting the logic of their program right. By handling architectural and synchronization optimizations without programmer involvement, SCORE lets programmers deliver applications that portably and effectively harness a wide range of multicore architectures. SCORE comprises a suite of new dynamic analyses, static analyses, and runtime systems to enable scalability-oriented optimization. It uncovers bottlenecks and ranks them by the performance impact of removing them. This information guides a bottleneck-remediation dynamic analysis to identify a range of opportunities for concurrency optimizations. Finally, a code robustification phase augments the optimized code with lightweight checking and recovery code to ensure correct execution."
"1565264","AF: Large: Collaborative Research: Algebraic Proof Systems, Convexity, and Algorithms","CCF","ALGORITHMIC FOUNDATIONS","05/01/2016","07/25/2017","Boaz Barak","MA","Harvard University","Continuing grant","Tracy J. Kimbrel","04/30/2021","$651,472.00","","b@boazbarak.org","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7925, 7926, 7927","$0.00","This project tackles some of the central questions in algorithms, optimization, and the theory of machine learning, through the lens of the ""Sum of Squares"" algorithmic framework. In particular, it will allow us to understand what classes of functions can be efficiently minimized, and what computational resources are needed to do so. If successful, this will significantly advance our understanding in all these key areas, produce new practical algorithmic methodologies, as well as build new connections with other fields, including quantum information theory, statistical physics, extremal graph theory and more.<br/><br/>This collaborative grant will foster new interactions between intellectual communities that have had relatively little interaction so far, and the PIs will organize workshops, courses, and other events that bring these communities together. The students and postdocs trained will gain a uniquely broad view of the landscape of these areas.<br/><br/>The PIs propose a unified approach to the development and analysis of convex proof systems that include and generalize the ""Sum of Squares"" (SoS) method. Despite considerable recent progress, understanding SoS?s performance seems to be out-of-reach for most current techniques. Significant progress in this area requires the synthesis of ideas and techniques from different domains, including theoretical computer science, optimization, algebraic geometry, quantum information theory and machine learning. The research plans include both theory-building and problem-solving aspects, with the ultimate goal of obtaining a complete understanding of the SoS method and related proof systems, as well as their algorithmic implications.<br/><br/>Research efforts will be directed along several thrusts: Unique Games and related problems (Small Set Expansion, Max Cut, Sparsest Cut), analysis of average-case problems (e.g., Planted Clique), applications to Machine Learning (sparse PCA, dictionary learning), algorithmic speedups of SoS, and connections to math and physics (e.g., quantum entanglement, p-spin glasses, extremal graph theory and algebraic geometry). While the main focus is on theoretical aspects, this project is also concerned with effective computational methods, and the outcomes may yield novel practical techniques for machine learning and optimization.<br/><br/>Other key features of this proposal include its strong integration with curriculum development, undergraduate research projects, and training the next wave of graduate students and postdocs and equipping them with the necessary tools to work across these areas."
"1834515","Travel Support for Workshop: Rocky Mountain Summit on Quantum Information","CCF","INFORMATION TECHNOLOGY RESEARC, QUANTUM COMPUTING","08/01/2018","08/07/2018","Felix Leditzky","CO","University of Colorado at Boulder","Standard Grant","Dmitry Maslov","07/31/2019","$10,000.00","Graeme Smith","Felix.Leditzky@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1640, 7928","7556, 7928","$0.00","The purpose of this award is to support student travel to the Rocky Mountain Summit on Quantum Information, which aims to bring together leading scientists in the broad field of quantum information science. The focus of the workshop is on quantum information theory, in particular quantum Shannon theory, as well as quantum information methods employed in high energy physics and condensed matter theory. This is an important emerging research area, with a growing strength in US.  The topic of focus is most closely related to Information Theory.  A major venue in Information Theory, IEEE Symposium on Information Theory (ISIT), also supported by NSF, and this workshop are collocated, providing a unique opportunity to the workshop attendees to benefit from the participation of Information Theory community at large.  <br/><br/>The award will support travel for up to 15 students and postdoctoral scholars to this workshop, focusing on those needing the support most and giving the priority to underrepresented groups. The workshop is to be held at the University of Colorado Boulder, from June 25 to June 29, 2018.  Workshop website, http://jila.colorado.edu/rmsqi, provides general information about the workshop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817577","CIF: Small: Combinatorial Inverse Problems in Distance Geometry","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","07/27/2018","Ivan Dokmanic","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","09/30/2019","$157,079.00","","dokmanic@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7935","$0.00","From navigating the seas to reconstructing the shape of proteins, reasoning from distances is at the heart of civilization. In many applications one has a list of distances but no labeling information to assign them to objects. For example, when navigating buildings by multipath, echoes coming from different walls all look the same; when solving the geometry of nanostructured materials by powder diffraction, distances between different pairs of atoms are all summarized in a single pair distribution function. The goal of this project is to advance algorithms and theory for the underlying mathematical problem ""the unlabeled distance geometry problem"" with potential to impact indoor mapping and positioning, nanoscience, and genomics.<br/><br/>The unlabeled distance geometry problem lies at the intersection of signal processing, computer science, acoustics, and experimental nanoscience. In this project the investigator proposes to develop algorithms for the unlabeled distance geometry problem with provable guarantees, as well as practical recipes for using those algorithms in major applications. The team will place particular emphasis on the connections with the state-of-the-art developments in signal processing, especially phase retrieval, matrix completion, and semidefinite relaxations for non-convex problems, by developing computationally efficient relaxations that work with high probability over typical instances.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1717883","SHF: Small: Collaborative Research: Automated Numerical Solver EnviRonment (ANSER)","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/04/2017","Boyana Norris","OR","University of Oregon Eugene","Standard Grant","Almadena Y. Chtchelkanova","07/31/2020","$224,999.00","","norris@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","7798","7923, 7942, 9102","$0.00","The computational science community is tackling ever larger and more complex applications. The solution of the underlying mathematics problems requires using high-end parallel computing resources effectively, and delivering performance without degrading productivity is critical for the success of scientific computing. Converting mathematics from algorithms to high-quality implementations, however, is a difficult process, whether an application is developed from scratch or by leveraging existing software libraries. Modern numerical packages provide numerous solutions with widely varying performance. Selecting among these possibilities requires expertise in numerical computation, mathematical software, compilers, and computer architecture, but even such broad knowledge does not guarantee the selection of the best-performing method for a particular problem. In response to these challenges, ANSER (Automated Numerical Solver EnviRonment) automates the selection and configuration of algorithms such as sparse linear solvers, eigensolvers, and graph methods in the context of large-scale scientific and engineering applications. The overall approach is generalizable to any situation involving multiple solutions whose performance varies with input problem properties.  ANSER increases developer productivity and promotes effective use of modern parallel architectures to solve large-scale scientific and engineering problems. This work also impacts the training of the next-generation scientific workforce by involving graduate and undergraduate students in this model-guided development of high-performance software. <br/> <br/>ANSER, the Automated Numerical Solver EnviRonment, is an open-source web-based platform that supports the development of both scientific applications and high-performance libraries.  It selects, configures and, in some cases, generates implementations of high-performance numerical algorithms.  ANSER defines a methodology for automating the process of identifying problem features, creating performance models (based on combining analytical and machine learning approaches), and employing them in creating and configuring numerical software. ANSER initially targets widely used numerical packages for nonlinear partial differential equations and solution of eigenvalue problems, but it is designed to be extensible to other types of numerical methods, such as graph computations and n-body simulations. In addition to traditional dissemination methods (open-source software releases and publications), ANSER integrates semantic analysis of scientific computing literature to discover numerical methods similar to those provided by the target libraries and to identify and connect with our users. ANSER provides multiple interfaces to support different types of users, including students, computational scientists, and numerical library developers."
"1717947","CCF-BSF: AF: Small: New Randomized Approaches in Approximation Algorithms","CCF","ALGORITHMIC FOUNDATIONS","08/01/2017","07/17/2017","Mohit Singh","GA","Georgia Tech Research Corporation","Standard Grant","Rahul Shah","07/31/2020","$450,000.00","","mohitsinghr@gmail.com","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","7923, 7926","$0.00","In the modern world, many industries make every day crucial decisions by solving large scale discrete optimization problems including routing of delivery trucks, designing schedule of airplanes, and deciding on the logistics of inventory supplies. Unfortunately, many of these problems are computationally hard to solve exactly. The area of approximation algorithms has grown as a general theory to develop and analyze solutions that can be efficiently computed as well as give near-optimal solutions. One of the most successful approaches in this area has been to: (1) formulate the problem at hand using an easily solvable continuous mathematical characterization; and (2) applying a rounding method that converts the solution to the continuous characterization into a discrete feasible solution to the original problem. This project aims to develop a new generic and broadly applicable rounding method based on probabilistic tools. The success of the project will help obtain new and improved solution methods to many fundamental discrete optimization problems.<br/> <br/>The PIs of this collaborative NSF-BSF project compose a harmonious team as both PIs have complementary proficiencies in algorithmic research. This will contribute to achieving one of the primary goals of this project, which is to extend and maintain the US-Israeli scientific collaboration between the PIs. Another paramount goal of this research project is to include students, both undergraduate and graduate, as full active participants. A special emphasis will be given to students coming from under-represented groups. The PIs plan to assist the dissemination of knowledge by integrating results from this research into inter-disciplinary curriculum activities at all levels, spanning computer science, applied mathematics and operations research.<br/> <br/>The project aims to enrich the algorithmic toolkit in the area of approximation algorithms and show that a new approach based on discretized Brownian motion is applicable to a wide range of NP-hard problems. A special emphasis is given on optimization problems that are considered fundamental including traveling salesman problem, maximum cut and maximum satisfiability. Any new insight into the solution of fundamental problems, such as the ones mentioned above, is likely to have much wider implications as these problems form basic building blocks for numerous discrete optimization problems occurring in both theory and practice. Thus, this project also aims to study the applicability of the newly developed technique to such problems."
"1717854","SHF: Small: Collaborative Research: Automated Numerical Solver EnviRonment (ANSER)","CCF","SOFTWARE & HARDWARE FOUNDATION","08/15/2017","08/04/2017","Elizabeth Jessup","CO","University of Colorado at Boulder","Standard Grant","Almadena Y. Chtchelkanova","07/31/2020","$225,000.00","","jessup@cs.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7798","7923, 7942, 9102","$0.00","The computational science community is tackling ever larger and more complex applications. The solution of the underlying mathematics problems requires using high-end parallel computing resources effectively, and delivering performance without degrading productivity is critical for the success of scientific computing. Converting mathematics from algorithms to high-quality implementations, however, is a difficult process, whether an application is developed from scratch or by leveraging existing software libraries. Modern numerical packages provide numerous solutions with widely varying performance. Selecting among these possibilities requires expertise in numerical computation, mathematical software, compilers, and computer architecture, but even such broad knowledge does not guarantee the selection of the best-performing method for a particular problem. In response to these challenges, ANSER (Automated Numerical Solver EnviRonment) automates the selection and configuration of algorithms such as sparse linear solvers, eigensolvers, and graph methods in the context of large-scale scientific and engineering applications. The overall approach is generalizable to any situation involving multiple solutions whose performance varies with input problem properties.  ANSER increases developer productivity and promotes effective use of modern parallel architectures to solve large-scale scientific and engineering problems. This work also impacts the training of the next-generation scientific workforce by involving graduate and undergraduate students in this model-guided development of high-performance software. <br/> <br/>ANSER, the Automated Numerical Solver EnviRonment, is an open-source web-based platform that supports the development of both scientific applications and high-performance libraries.  It selects, configures and, in some cases, generates implementations of high-performance numerical algorithms.  ANSER defines a methodology for automating the process of identifying problem features, creating performance models (based on combining analytical and machine learning approaches), and employing them in creating and configuring numerical software. ANSER initially targets widely used numerical packages for nonlinear partial differential equations and solution of eigenvalue problems, but it is designed to be extensible to other types of numerical methods, such as graph computations and n-body simulations. In addition to traditional dissemination methods (open-source software releases and publications), ANSER integrates semantic analysis of scientific computing literature to discover numerical methods similar to those provided by the target libraries and to identify and connect with our users. ANSER provides multiple interfaces to support different types of users, including students, computational scientists, and numerical library developers."
"1617678","AF: Small: Collaborative Research: Cell Signaling Hypergraphs: Algorithms and Applications","CCF","ALGORITHMIC FOUNDATIONS","08/15/2016","05/02/2017","T. Murali","VA","Virginia Polytechnic Institute and State University","Standard Grant","Mitra Basu","07/31/2019","$303,993.00","","murali@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7796","7923, 7931, 9251","$0.00","Proteins in the living cell interact with each other in complex ways. Graphs have emerged as a natural way to represent these interactions. In a conventional graph representation, a node represents a protein and an edge represents an interaction between two proteins. Although such graphs have been in widespread use for many years, they do not accurately capture important features of protein interactions, such as proteins that operate in groups called complexes, reactions involving such complexes that may have more than two reactants and products, as well as the influence of other proteins whose presence can regulate reactions. This project will develop a new representation called signaling hypergraphs that naturally describes the relationships between multiple groups of proteins as complexes, reactants, products, and regulators. Furthermore, the project will develop novel algorithms for fundamental computational challenges in the analysis of signaling hypergraphs, and apply this new representation and these algorithms to widely-used databases of cellular reactions. <br/><br/>The project will actively involve undergraduate students in research by recruiting them through the Virginia Tech Initiative to Maximize Student Diversity, and the Virginia Tech Undergraduate Research in Computer Science program. Students will engage in multiple semesters of research with the goal of ultimately leading their own individual projects, and obtaining co-authorship in publications. In this way the project will expose students to how computational thinking plays a major role in modern molecular biology, thereby meeting an important goal of STEM education. <br/><br/>This project focuses on developing algorithms for the analysis of cell signaling hypergraphs. Aim 1 focuses on methods for generating products efficiently by finding short paths through signaling hypergraphs, while accounting for feedback loops and reaction regulators. Aim 2 develops algorithms for discovering missing proteins, complexes, and reactions in a signaling pathway. Finally, Aim 3 will release open-source software implementing the algorithms for signaling hypergraphs developed in this project."
"1525194","CIF: Small: Feasible Point Pursuit for Non-convex QCQPs: Algorithms and Signal Processing Applications","CCF","COMM & INFORMATION FOUNDATIONS","09/01/2015","08/06/2015","Nikolaos Sidiropoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Phillip Regalia","08/31/2019","$449,468.00","Konstantinos Slavakis","nikos@virginia.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7797","7923, 7936","$0.00","Over the last 20 years, convex optimization has become an indispensable design and analysis tool in science and engineering. Still, many important design and analysis problems are non-convex and NP-hard. Among them, non-convex quadratically constrained quadratic programs (QCQPs) are nowadays often encountered in wireless communications, networking, and signal processing. Examples include phased-array unicast and multicast beamforming, and phase retrieval and its applications in X-ray crystallography, optics, diffraction imaging, astronomical imaging, and microscopy. Few methods are currently effective for non-convex QCQPs, and only in special cases. This research investigates several promising new approaches to obtaining and refining feasible points for general non-convex QCQPs, and their applications in phase retrieval, beamforming, radar, and wireless networking. The project offers rich opportunities for graduate and undergraduate student engagement in cross-disciplinary research, as well as freshman and K12 outreach. <br/><br/>In preliminary work, a Feasible Point Pursuit - Successive Convex Approximation (FPP-SCA) method was proposed, and initial tests revealed exciting results ? motivating further research that comprises two intertwined thrusts: one on design and analysis of new FPP methods and algorithms; and another on new signal processing applications of non-convex QCQP methods. In addition to analyzing and improving the original FPP-SCA, two new approaches are being considered. One is derived using bilinearization, leading to alternating optimization; the other using proportional fairness as a surrogate for max-min fairness, leading to low-complexity FPP using an adaptively weighted cyclically projected gradient approach. The optimality gap for fixed points of this iteration is analyzed, pertinent simplifications are pursued for special problem instances, and the link between max-min and proportional fairness is further explored ? all with an eye towards improving our fundamental understanding of non-convex QCQPs."
"1705028","AF:Medium:Fine-Grained Derandomization","CCF","ALGORITHMIC FOUNDATIONS","09/01/2017","07/27/2018","David Zuckerman","TX","University of Texas at Austin","Continuing grant","Tracy J. Kimbrel","08/31/2021","$679,726.00","Hadar Dana Moshkovitz","diz@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7796","7924, 7926, 7927","$0.00","For many problems, randomized algorithms offer significant speedups over known algorithms that don't use randomness. However, randomization introduces uncertainty in the outcome of the algorithm, as there will be a small probability of error. The purpose of this project is to explore when uncertainty can be eliminated without giving up on speed. The project integrates education and outreach, including mentoring of students, public lectures and popular science writing. <br/><br/>The PIs identify several families of problems where randomized algorithms outperform the best known deterministic ones, including: problems on dense graphs like finding an approximate clique in a dense graph; algebraic problems like polynomial identity testing and factorization; and problems that can be solved using Markov chains, like approximately counting the number of perfect matchings in a bipartite graph. The PIs wish to develop new techniques for derandomization that do not increase the run-time substantially. The PIs also want to prove that, under widely believed complexity assumptions, some randomized algorithms are impossible to derandomize without a significant loss in speed."
"1833617","NSF Student Travel Grant for 2018 AGT Mentoring Workshop Co-Located with Economics and Computation (EC)","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","05/01/2018","05/08/2018","Ruta Mehta","IL","University of Illinois at Urbana-Champaign","Standard Grant","Tracy J. Kimbrel","04/30/2019","$10,000.00","Nicole Immorlica, Seth Weinberg","rutameht@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","2878, 7796","7932","$0.00","This award will support students to attend the Algorithmic Game Theory (AGT) Mentoring Workshop and the 19th Association for Computing Machinery (ACM) Conference on Economics and Computation (EC), June 18 - 21, 2018, in Ithaca, NY. The field of Algorithmic Game Theory is at the intersection of theoretical computer science and economics. This makes it most suitable to handle foundational issues posed by tremendous growth in economic and social systems on the Internet, and therefore the field has been thriving in the past few decades. Economics and Computation is the leading conference in this area, with the main aim of strengthening the field by bringing together both computer scientists and economists. The investigators are organizing a one-day AGT mentoring workshop to be co-located with EC.<br/><br/>The workshop will help to mentor and train students pursuing research careers in the field of AGT. Travel funds from NSF will facilitate participants' attendance at the mentoring workshop and EC, where they will learn about current research and interact with leaders in the field. The award will cover travel, lodging and registration fees for approximately 8 to 10 carefully selected students who otherwise would find it difficult to attend. The selection criteria for the travel award and the workshop program will aim to encourage participation by underrepresented minorities and women.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1524433","Small: Collaborative Research: Transform-to-Perform: Languages, Algorithms, and Code Transformations for High-Performance FEM","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2015","07/08/2015","Andreas Kloeckner","IL","University of Illinois at Urbana-Champaign","Standard Grant","Almadena Y. Chtchelkanova","06/30/2019","$219,408.00","","andreask@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7923, 7942","$0.00","Simulation of natural and engineering phenomena is a multi-layered technical task with high demands on mathematical sophistication and computational power.  Producing a computer simulation code requires work at many different levels of detail.  A computer program should represent a scientific problem in a language close to that used by domain specialists, but this differs greatly from low-level, hardware-specific details of computers.  Bridging between the two requires several different links.  <br/>This project sets up many intermediate software stages, called ?representations? modeling the domain knowledge of engineers, numerical analysts, and computer scientists by describing partial differential equations, the so-called weak forms needed for numerical methods, loop nests required to build discrete operations, and finally low-level code that can be executed by computers.  ?Transformations? are then programs connecting these representations, injecting knowledge about algorithms and hardware. The key advance in this research is that, through this chain of transformations, domain knowledge about each level of detail, be it application-related, numerical, or computational, can be supplied at the appropriate level of detail. The tools developed in this project promote the advancement of science by both shortening the development time and increasing the resulting power of high-performance simulation codes used by scientists and engineers, enabling them to impact the world."
"1527460","SHF: Small: On-Die Learning: A Pathway to Post-Deployment  Robustness and Trustworthiness of Analog/RF ICs","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2015","07/08/2015","Yiorgos Makris","TX","University of Texas at Dallas","Standard Grant","Sankar Basu","06/30/2019","$350,000.00","","yiorgos.makris@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7798","7923, 7945","$0.00","Towards enabling reliable computing and promoting technology trustworthiness, this project seeks to facilitate cost-effective realization of robust and trusted integrated circuits (ICs), with particular emphasis in the analog/RF domain. While manufactured ICs are subjected to extensive scrutiny in order to weed out defective or suspicious parts prior to their deployment, a variety of reasons such as silicon aging and adverse operational or environmental conditions might cause the performances of a previously healthy analog/RF IC to fail its design specifications. Similarly, field-activated triggers of hidden capabilities might cause a previously trusted analog/RF IC to exhibit malicious functionality. With analog/RF ICs now prevalent in most electronic systems, due to the rapid growth of wireless communications, sensor applications, and the Internet of Things (IoT), equipping them with robustness and trustworthiness evaluation mechanisms becomes paramount to the applications wherein they are deployed. The proposed research is complemented by educational and outreach activities, including development of a new educational module, by providing opportunities for graduate and undergraduate research in interdisciplinary projects spanning Electrical Engineering, Computer Science and Applied Mathematics, as well as exposure of local community members to the topics of self-test, self-calibration, and post-deployment trust evaluation of analog/RF ICs through tutorials and seminars organized by the Texas Analog Center of Excellence (TxACE) at the University of Texas at Dallas. <br/><br/>More specifically, this project seeks to enhance post-deployment robustness and trustworthiness of analog/RF ICs by integrating machine learning-based on-die monitoring and calibration capabilities. The key focus of this project is the cost-effective integration of on-die learning capabilities, which can be trained to (i) determine whether an analog/RF IC complies with its specifications, (ii) calibrate its performances, and/or (iii) detect the activation of potentially malicious circuitry, based on simple measurements from on-chip sensors. Through design, fabrication and characterization of two different analog/RF ICs, this project seeks to demonstrate that on-die intelligence can be integrated to provide post-deployment self-test, calibration and trust evaluation capabilities."
"1514219","SHF: Medium: Collaborative Research: Atomic scale to circuit modeling of emerging nanoelectronic devices and adapting them to SPICE simulation package","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2015","06/15/2015","Avik Ghosh","VA","University of Virginia Main Campus","Standard Grant","Sankar Basu","05/31/2019","$209,232.00","","ag7rq@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7798","7924, 7945","$0.00","Aggressive scaling of CMOS technology and concomitant inventions of nanoscale nascent technologies have fueled the growth of computer, information, communication and consumer electronics industries of the 21st Century by leveraging the ground-breaking discoveries in nanoscience and nanotechnology. The workhorse of multibillion-dollar semiconductor industry, the CMOS technology is approaching its scaling limit due to the strong quantum-mechanical effects present at the nanoscale. To sustain the accelerated pace of economic growth during the post-CMOS era, this multi-university collaborative research proposal envisages building the roadmap of VLSI technology in two significant ways. First, the research is mooted to extend quantum transport principles to simulate emerging nano-devices based on novel semiconductor and 2-D layered materials by exploiting non-charge based degrees of freedom, electron spin controlled magnetization, interaction between electromagnetic waves and semiconductors in metamaterial structures, and topological states in topological insulators. Second, the research will systematically scale these properties from their fundamental atomistic limits to circuit level integration by developing industry-graded SPICE-compatible compact models for heterogeneous circuits that will define the landscape of beyond Moore?s Law VLSI systems. Integrative education, training, and outreach activities envisioned in this collaborative proposal will encompass K-12, undergraduate, graduate, female, minority, and postdoctoral fellows by leveraging the existing outreach activities of participating universities in order to advance science and engineering education in broader segments of the society.<br/><br/>Using density-functional theory (DFT), time-dependent density functional theory (TD-DFT), time-dependent density-matrix functional theory (TD-DMFT), to phenomenological Extended Huckel to effective mass, in conjunction with non-equilibrium Green?s function (NEGF) methods, quantum field theory, and finite-difference time domain (FDTD) methods, a wide variety of computational methods are going to be developed to tackle the modeling of multiscale circuits in future VLSI systems. The software packages and multiscale modeling tools resulting from the proposed research activity are going to provide computer chip designers and manufacturers the ability to model complex hybrid substrates comprising nanoscale electronic, spintronic, opto-electronic, and plasmonic devices. The resulting software is going to be written with a view to enabling researchers from universities and practicing engineers in industries to develop their own modules that will engender improved system functionality, integration density, and operational speed."
"1525697","Small: Collaborative Research: Transform-to-Perform: Languages, Algorithms, and Code Transformations for High-Performance FEM","CCF","ALGORITHMIC FOUNDATIONS, SOFTWARE & HARDWARE FOUNDATION","07/15/2015","07/08/2015","Robert Kirby","TX","Baylor University","Standard Grant","Almadena Y. Chtchelkanova","06/30/2019","$230,591.00","","Robert_Kirby@baylor.edu","One Bear Place #97360","Waco","TX","767987360","2547103817","CSE","7796, 7798","7923, 7933, 7942","$0.00","Simulation of natural and engineering phenomena is a multi-layered technical task with high demands on mathematical sophistication and computational power.  Producing a computer simulation code requires work at many different levels of detail.  A computer program should represent a scientific problem in a language close to that used by domain specialists, but this differs greatly from low-level, hardware-specific details of computers.  Bridging between the two requires several different links.  <br/>This project sets up many intermediate software stages, called ?representations? modeling the domain knowledge of engineers, numerical analysts, and computer scientists by describing partial differential equations, the so-called weak forms needed for numerical methods, loop nests required to build discrete operations, and finally low-level code that can be executed by computers.  ?Transformations? are then programs connecting these representations, injecting knowledge about algorithms and hardware. The key advance in this research is that, through this chain of transformations, domain knowledge about each level of detail, be it application-related, numerical, or computational, can be supplied at the appropriate level of detail. The tools developed in this project promote the advancement of science by both shortening the development time and increasing the resulting power of high-performance simulation codes used by scientists and engineers, enabling them to impact the world."
"1533844","XPS: FULL: DSD: Parallel Motion Planning for Cloud-connected Robots","CCF","Exploiting Parallel&Scalabilty","09/01/2015","08/17/2015","Ron Alterovitz","NC","University of North Carolina at Chapel Hill","Standard Grant","M. Mimi McClure","08/31/2019","$670,536.00","Jan Prins","ron@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8283","","$0.00","Robots are entering new domains, from self-driving vehicles on real-world streets, to autonomous aerial vehicles for package delivery, to assistive robots helping people with disabilities in their homes with daily activities. Autonomous robots in these domains often need extensive computational resources for motion planning, which involves computing a safe motion for a robot through an environment that avoids obstacles and accomplishes the task. To enable low-power mobile robots to achieve their full potential, new algorithms and software frameworks are needed that fully leverage parallel computation and the warehouse-scale computing available via the cloud.   This project aims to develop a new software framework for motion planning for cloud-connected robots that effectively parallelizes motion planning and distributes the computation across the robot's embedded multicore processor and multiple cloud-based compute servers. <br/><br/>This research combines ideas from multiple areas of computer science and engineering, including robotics, parallel algorithms, high-performance computing, motion planning, and cloud computing. Locally on the robot, the project parallelizes traditional motion planners to fully leverage low-power, embedded multicore processors. Simultaneously, the project enables the robot to request computation time from cloud-based resources to significantly increase the computing power available for the motion planning task, and thus increase the responsiveness and quality of motion plans. The new algorithms and software aim to enable robots to autonomously complete tasks in new domains where the challenge of motion planning is currently prohibitive, broadening the applicability of robots to new societally-relevant domains. <br/><br/>The concepts and software developed in this project are being integrated into undergraduate and graduate courses taught across topics ranging from robotics to high-performance computing. Another goal of the project is to create fun, hands-on, interactive demonstrations using cloud-connected robots to inspire children to consider STEM fields."
"1524909","SHF: Small: Uncertainty Modeling and Design Methods for Heterogeneous Embedded Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2015","07/24/2015","Cristinel Ababei","WI","Marquette University","Standard Grant","Yuanyuan Yang","07/31/2019","$251,068.00","","cristinel.ababei@marquette.edu","P.O. Box 1881","Milwaukee","WI","532011881","4142887200","CSE","7798","7923, 7941","$0.00","Future embedded systems will contain tens and projected hundreds of heterogeneous cores. The increased complexity and heterogeneity, however, come with new design and optimization challenges including increased design uncertainties due to process, voltage, and temperature variations and poor reliability due to elevated rates of faults. Therefore, it is imperative to rethink existing embedded system design approaches in order to directly consider these issues during the design and optimization processes. To address these challenges, this research develops a new design method to address the increased uncertainties and to improve reliability and performance of future complex embedded systems. <br/><br/>This new design method is developed with both general probabilistic and non-probabilistic uncertainty models. These uncertainty models are used to develop a multi-objective computer-aided design automation framework, which incorporates several algorithmic innovations based on Monte Carlo techniques and evolutionary algorithms. At the heart of the proposed design flow lies the hardware/software co-synthesis or mapping problem, which is solved with enhanced evolutionary algorithms. By bridging several areas of research including uncertainty modeling, hardware/software co-synthesis of embedded systems, robust multi-objective optimization, and design automation tools development, the proposed design method enables the development of better embedded systems, which have an increasingly dramatic impact on society via applications ranging from automotive and consumer electronics to military and space. The proposed research is also closely integrated with a broad and diverse education and outreach plan aimed at inspiring female students to pursue careers in science, technology, engineering, and mathematics. More broadly, the results of this project impact significantly the design of future integrated systems by building the foundation for subsequent studies aimed at establishing robust multi-objective optimization for next-generation embedded systems."
"1812070","CIF: Small: Secure Quantum Communication with Limited Resources","CCF","CYBERINFRASTRUCTURE, QUANTUM COMPUTING","10/01/2018","07/13/2018","Walter Krawec","CT","University of Connecticut","Standard Grant","Dmitry Maslov","09/30/2021","$309,582.00","","walter.krawec@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7231, 7928","057Z, 7923, 7928","$0.00","Developing a communication infrastructure, secure against powerful adversaries (with potential access to advanced quantum computers and capabilities), is of vital importance to our society.  Using only classical communication, two parties cannot establish a shared secret key (needed to securely communicate with one another) unless some computational assumption is made on the power of the adversary.  However, by carefully using the power of quantum communication, one can achieve unconditional security for this task.  This project seeks to understand the capabilities of quantum communication protocols where users have only limited access to quantum functionality while an adversary has unlimited access to such resources.  These limitations on the users may be intentional (for instance, to reduce the cost of a quantum communication device), or unintentional (for instance, if a device malfunctions or breaks down during operation). The work in this project will provide new theoretical results of broad application to quantum information science and also pave the way for a future communication infrastructure that is highly secure, practical, and robust to device failure and cost limitations.  Besides the theoretical and practical benefits of this work, there are also impacts in this project in educational activities and outreach efforts.  The work on this project will enable undergraduate researchers an opportunity to not only contribute to quantum cryptographic research, but to also learn and gain insight into this important field.  The future will see a vast increase in the use of quantum technologies, making the education of students in this field all the more important.<br/><br/>This project will develop a deeper understanding of the theoretical foundations of secure quantum communication by discovering and analyzing novel quantum cryptographic protocols that utilize participants with limited quantum functionality.  The research efforts involve analyzing these limited-resource protocols in a variety of security models (including various forms of device independence) and devise innovative methods for users, despite having limited quantum capabilities, to counter an adversarial agent allowing for highly secure, and efficient, communication systems, even over high-noise channels.  To achieve this, the PI will also be developing new quantum information theoretic techniques needed to analyze these protocols in such highly restricted setting.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1422152","AF: Small: Using Notions of Simulation to Explore the Power of Self-Assembling Systems","CCF","ALGORITHMIC FOUNDATIONS, SOFTWARE & HARDWARE FOUNDATION","07/01/2014","04/28/2017","Matthew Patitz","AR","University of Arkansas","Standard Grant","Mitra Basu","06/30/2019","$487,474.00","","patitz@uark.edu","210 Administration Building","FAYETTEVILLE","AR","727011201","4795753845","CSE","7796, 7798","7923, 7931, 7946, 9251","$0.00","A wide variety of self-assembling systems exist in nature.  These are systems in which complex structures are formed from individual molecules that combine with each other autonomously, obeying only local rules of behavior and without any external guidance or placement.  This process is responsible for the formation of a huge diversity of inorganic structures (such as crystals like snowflakes), as well as numerous biological structures (including cellular membranes and viruses).  Hoping to harness the power of self-assembly to create advanced materials, complex nanoscale structures, and perform molecular computing, researchers have begun developing artificial self-assembling systems.  This work has consisted of laboratory implementations as well as mathematical and computational modeling.<br/>Theoretical computer science has provided numerous insights into the creation and understanding of theoretical models of self-assembly, and these models have provided valuable understanding which has productively guided laboratory experiments.  In this project, the body of work in defining, developing, and comparing the relative powers of theoretical models of self-assembling systems will be extended by the PI.  In particular, the relative powers of various models will be studied by comparing their abilities to simulate each other.  Additionally, the abilities of systems within existing and new models to mimic biological processes such as evolution and immune system behavior will be investigated.<br/><br/>This project is divided into two main components.  The first consists of a series of studies of the abilities of various theoretical models to simulate each other when certain parameters are fixed, especially the so-called ""temperature"" parameter fixed at 1, making cooperative behaviors difficult or impossible.  Whether or not certain models, again with fixed parameters, are intrinsically universal will also be explored.  Additionally, while most current theoretical work comparing the relative powers of models is concerned with a strong notion of simulation which includes both the productions and dynamics of the simulating systems, relaxed notions which focus solely on the structures produced (i.e. the productions) of simulating systems will be investigated.  While the previous method has produced great theoretical understanding, this new approach is geared toward a more practical understanding which can guide experimentalists desiring to build predefined structures.<br/><br/>The second main component of this project involves work related to studying the abilities of various self-assembling systems to simulate complex and important biological activities such as protein folding, the formation of prion-like structures, self-replication, evolution, and immune system behaviors.  For much of this, an existing model utilizing dynamically changing basic components, the Signal Tile Assembly Model, will be employed.   Further, a new model based on square 2D components which can fold along their boundaries, allowing for the formation of 3D structures by ""foldable"" 2D building blocks, will be developed and studied, with special focus on their abilities to abstractly mimic protein-like behaviors.<br/><br/>The work will consist of mathematical modeling as well as computational modeling in the form of simulation software.  Results and software, will be made freely available with already existing software and content on www.self-assembly.net."
"1421211","SHF: Small: Generation of Scientific Software Libraries","CCF","CI REUSE, SOFTWARE & HARDWARE FOUNDATION","07/15/2014","07/10/2014","Don Batory","TX","University of Texas at Austin","Standard Grant","Sol J. Greenspan","06/30/2019","$515,517.00","Robert van de Geijn","batory@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","6892, 7798","7433, 7923, 7944, 9251","$0.00","Making experts more productive is critically important to software development.  This is particularly so in Scientific Computing (SC) where experts are rare and whose task is to develop software libraries on which SC research in academia, government labs, and industry depends. This project strives to create programming practices and tools that will revolutionize library development in SC.  By encoding expert knowledge about the science, mathematics, and software of a domain, a transformative approach called Design by Transformation (DxT) can automatically generate software that experts would have to write manually. DxT-generation is faster, cheaper, and produces better-trusted code.  In dense linear algebra libraries, DxT has exposed errors in manually-written code and found faster algorithms than those produced by experts. <br/><br/>This research applies DxT to three new software domains: fault-tolerant dense linear algebra, graph analysis, and tensor computation.  While improving the performance of their software libraries will itself have an impact in national labs and industry, the lessons learned about automating software development will have broad effects.  SC software is growing more complicated as hardware complexity evolves (e.g., multi-core, many-core, co-processors, and GPUs). Automatic generation will aid experts in being more productive in this increasingly complicated engineering endeavor. It will enable American scientists and engineers to maintain their advantage in scientific library engineering.  Further, there is evidence that the DxT approach to engineering has a pedagogical value in teaching students and new engineers about software.  Encoding new domains will test these benefits."
"1619456","SHF: Small: Collaborative Research: Design of Many-core NoCs for the Dark Silicon Era","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","05/31/2016","Lizhong Chen","OR","Oregon State University","Standard Grant","Yuanyuan Yang","06/30/2019","$349,978.00","","chenliz@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7798","7923, 7941","$0.00","The proliferation of computing systems of various forms and scales have significantly advanced science, technology, discovery and society at large for the benefit of human kind. As the key building blocks of current and future computing systems, including the Internet of Things, many-core chip multiprocessors (CMPs) are facing unprecedented power challenges brought on by limits in Dennard scaling. This necessitates many-core chips to be designed with the ability to power down on-chip resources to effectively provide scalable performance while keeping power and energy consumption proportional to computing load. This necessity leads to considerable portions of many-core chips having to go dark, thus ushering in the era of dark silicon. To facilitate dark silicon computing, not only computational resources (i.e., processor cores) but also communication resources (i.e., networks on chips, or NoCs) used to connect the computational resources must be developed that can be powered up or down proportionally with performance scalability in response to prevailing load.<br/><br/>This research investigates new opportunities, significant challenges, and innovative solutions for harnessing dark silicon in NoC architectures that meet performance, power and energy requirements in the dark silicon era. The objective is to enable non-essential NoC routers to be powered down when needed as well as to enable a corresponding maximum number of routers and router components to be powered down for a given reduction in the number of powered-up processor cores in order to provide energy-proportional, low-power, on-chip communication. Among some of the specific lines of research that are explored are alternative topologies and coordinated routing algorithms to enable more efficient power-gating of NoC routers, holistic approaches for exploiting coordination between the NoC and other on-chip system components as well as factoring in key application characteristics, and novel packet-oriented dynamic power control schemes that explore energy-saving opportunities beyond the conventionally targeted low-load traffic region. Beyond its technical contributions that can impact fundamental advancement in dark silicon computing, this research also has impact more broadly on research education and outreach. Findings from this research are incorporated into graduate curriculum, courses, and undergraduate research experiences. Outreach activities to broaden participation in computing of persons from diverse backgrounds and development levels are also featured."
"1619472","SHF: Small: Collaborative Research: Design of Many-core NoCs for the Dark Silicon Era","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","05/31/2016","Timothy Pinkston","CA","University of Southern California","Standard Grant","Yuanyuan Yang","06/30/2019","$100,022.00","","tpink@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7798","7923, 7941","$0.00","The proliferation of computing systems of various forms and scales have significantly advanced science, technology, discovery and society at large for the benefit of human kind. As the key building blocks of current and future computing systems?including the Internet of Things, many-core chip multiprocessors (CMPs) are facing unprecedented power challenges brought on by limits in Dennard scaling. This necessitates many-core chips to be designed with the ability to power down on-chip resources to effectively provide scalable performance while keeping power and energy consumption proportional to computing load. This necessity leads to considerable portions of many-core chips having to go dark, thus ushering in the era of dark silicon. To facilitate dark silicon computing, not only computational resources (i.e., processor cores) but also communication resources (i.e., networks on chips, or NoCs) used to connect the computational resources must be developed that can be powered up or down proportionally with performance scalability in response to prevailing load.<br/><br/>This research investigates new opportunities, significant challenges, and innovative solutions for harnessing dark silicon in NoC architectures that meet performance, power and energy requirements in the dark silicon era. The objective is to enable non-essential NoC routers to be powered down when needed as well as to enable a corresponding maximum number of routers and router components to be powered down for a given reduction in the number of powered-up processor cores in order to provide energy-proportional, low-power, on-chip communication. Among some of the specific lines of research that are explored are alternative topologies and coordinated routing algorithms to enable more efficient power-gating of NoC routers, holistic approaches for exploiting coordination between the NoC and other on-chip system components as well as factoring in key application characteristics, and novel packet-oriented dynamic power control schemes that explore energy-saving opportunities beyond the conventionally targeted low-load traffic region. Beyond its technical contributions that can impact fundamental advancement in dark silicon computing, this research also has impact more broadly on research education and outreach. Findings from this research are incorporated into graduate curriculum, courses, and undergraduate research experiences. Outreach activities to broaden participation in computing of persons from diverse backgrounds and development levels are also featured."
"1838434","AF: EAGER: The Power of Isolation in Computing","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","06/25/2018","Dieter van Melkebeek","WI","University of Wisconsin-Madison","Standard Grant","Tracy J. Kimbrel","09/30/2020","$125,000.00","","dieter@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7796","7916, 7927","$0.00","Computer science and engineering have made great strides in building high-performing software and hardware systems.  Further progress on the hardware front requires multiple processors to work in parallel on the same task.  To make adequate use of the parallel hardware, the software needs to be parallelized as well - it needs to specify how to break up the task among the various processors.  The fact that a given problem may have several solutions often complicates software parallelization.  This is because the processors do not have much time to coordinate among themselves.  Without coordination, they may be working towards different, incompatible solutions.  Isolation is a strategy to ensure all processors work towards the same solution.  It also has a wide range of other algorithmic uses. This project focuses on the power of isolation, which is the process of singling out a solution to a computational problem that may have many solutions. Though fundamental in nature and aimed at developing the underlying theory, the project may lead to practical improvements, e.g., for computational problems that involve detecting similarities between certain types of structures.  Graduate training and education are core to the project. <br/><br/>The project consists of several thrusts that center around the notion of isolation: <br/>(1) Derandomizing known isolation procedures for problems that capture various models of computation.  Known procedures are based on the Isolation Lemma, which assigns small random weights to the components of a solution so as to make the solution of minimum total weight unique.  The project aims to reduce the number of random bits needed and ultimately remove the need for randomness completely while maintaining efficiency. <br/><br/>(2) Developing deterministic or randomized isolation procedures for well-studied intermediate problems, namely, isomorphism problems on graphs and more expressive structures.  This relates to a number of known open questions regarding these problems, including the connection with testing rigidity of structures and with finding a canonical form for the structures. <br/><br/>(3) Refuting the Unique Games Conjecture, a central conjecture in the area of hardness of approximation with ties to several other mathematical fields.  The conjecture states that approximating the optimal yield of strategies for so-called label cover games is as hard for cases that satisfy a certain isolation-like property as it is in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763632","SHF: Collaborative Research: Biocompatible I/O Interfaces for Robust Bioorthogonal Molecular Computing","CCF","COMPUTATIONAL BIOLOGY","10/01/2018","06/21/2018","Milan Stojanovic","NY","Columbia University","Standard Grant","Mitra Basu","09/30/2021","$100,000.00","","mns18@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7931","7931, 9150","$0.00","Sugar molecules can exist in two forms that are the mirror image of each other but otherwise indistinguishable, like the left and right hand. All DNA molecules in nature contain right-hand sugars that cause them to assume the well-known double-helix shape with a right-handed twist. Proteins in cells in nature recognize the right-handed DNA in order to perform useful cellular functions, one of which is destroying single-stranded DNA. Over the last decade, DNA has come to be used in synthetic devices for sensing, computing, and diagnosing pathogens and disease. However, when these devices are introduced into a cell, they are prone to being destroyed by these proteins. This project will develop devices using left-handed DNA, which is not found in nature and is resistant to this form of degradation. In particular, the project will develop (1) input interfaces, for signaling from targets of interest, such as small molecules or natural DNA (D-DNA), to left-handed DNA (L-DNA), and (2) output interfaces, for signaling from left-handed DNA back into a relevant natural molecular pathway (such as DNA translation and transcription). This development will enable future devices that can sense multiple markers of the state of a cell (healthy or diseased), then integrate the sensors using DNA computing, and finally act on the cell, for example to destroy it if diseased, but the device itself will consist mainly of left-handed DNA and will therefore be robust in the cell. The project will involve both graduate and undergraduate students. It will be interdisciplinary, involving computer science and biomedical engineering, and will be carried out at the University of New Mexico and Columbia University.<br/><br/>The first aim of the project is to study the binding interactions between L-DNA (left-handed DNA) molecular logic devices and naturally occurring molecules. This study will produce a toolbox of basic techniques for implementing input interfaces that can detect naturally occurring target molecules, which in turn, can translate those binding events for information processing within a bio-orthogonal L-DNA logic circuit. This will be done by characterizing the actuation of hybrid L-DNA/D-DNA molecular computing components by pure D-DNA input strands. The second aim of the project is developing output interfaces that will enable L-DNA systems to produce some effect on the environment (i.e., carry out some form of actuation) as a result of their programmed molecular computations. Specifically, the project will focus on one particular mechanism for generating a useful output signal from an L-DNA molecular logic circuit, namely, gene knockdown by the allosteric ""activation"" of sequestered antisense D-nucleic acids by an L-DNA molecular circuit. Together, these will provide a mechanism for L-DNA molecular logic circuits to actuate via control of gene expression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1350939","CAREER: Separations in Cryptography","CCF","ALGORITHMIC FOUNDATIONS, Secure &Trustworthy Cyberspace","06/01/2014","06/19/2018","Mohammad Mahmoody Ghidary","VA","University of Virginia Main Campus","Continuing grant","Tracy J. Kimbrel","05/31/2019","$423,413.00","","mohammad@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7796, 8060","1045, 7434, 7927","$0.00","Since the seminal work of Shannon in 1949 cryptography has been founded on unproven computational complexity. The security of cryptographic systems could fall apart if the assumptions behind their design turn out to be false. Thus, it is crucial to base the security of crypto-systems on weakest possible assumptions. A main component of finding minimal assumptions is to ``separate'' cryptographic tasks from assumptions that are weaker than those used in constructions. In light of recent developments in cryptography, the following two directions will be pursued:<br/><br/>New Techniques: A formal model to abstract the properties of cryptographic proofs and techniques is necessary to prove separations. Previously studied models, however, do not consider some of the most useful recent techniques in cryptography and mainly focus on uniform, black-box, and classical (non-quantum) proof techniques. A major goal of this project is to develop new foundations to model these new cryptographic techniques.<br/><br/>New Tasks and Assumptions: Recently cryptography has gone through a revolution of exploring feasibility of highly structured crypto tasks, even if this comes at the cost of using newly introduced assumptions. The second major goal of this project is to deepen our understanding of the assumptions necessary for achieving these tasks. A closely related goal is to identify the relative power of these new tasks among themselves.<br/><br/>To disseminate the above ideas and achieve broader impact, the project will include educational activities such as: course development, outreach to high school students to motivate them pursue degrees in theoretical computer science and cryptography, and mentoring graduate as well as undergraduate students."
"1618574","SHF: Small: Mechanical Verification of QBF Results","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/19/2016","Marienus Heule","TX","University of Texas at Austin","Standard Grant","Nina Amla","07/31/2019","$500,000.00","","marijn@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7923, 8206","$0.00","Many important industrial applications, such as verification and synthesis problems, can be efficiently solved by satisfiability (SAT) solvers.  However, this approach involves translating the original problem into SAT that typically results in generating dozens to thousands of nearly identical copies of subproblems.  The quantified Boolean formula (QBF) formalism provides a convenient framework to compactly translate many of these interesting problems. For example, software verification and hardware synthesis problems can be translated into QBF, while avoiding generating these nearly identical copies.  Hence, QBF facilities a compact representation of crucial problems in computer science.<br/><br/>The expressiveness of QBF comes at a price: it is hard validate the results produced by these solvers.  The existing approaches for addressing this problem all have disadvantages.  Prevalent approaches involve costly validation algorithms and limit the used techniques.  A recent technological advancement, known as clausal proofs, takes care of most problems.  However, efficiently checking clausal proofs is complicated, thus trusting the results of one complex program (a QBF solver) depends on the correctness of another complex program (the checker).  To boost confidence in the results of QBF solvers, a mechanically-verified checker is required.  This research develops a uniform, complete, and trustworthy framework for QBF solving which is urgently needed for the scientific and industrial application of QBF solvers."
"1815893","CIF: Small: Learning Quantum Information Measures","CCF","QUANTUM COMPUTING","10/01/2018","05/23/2018","Jayadev Acharya","NY","Cornell University","Standard Grant","Dmitry Maslov","09/30/2021","$487,967.00","Aaron Wagner","acharya@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7928","7923, 7928","$0.00","Quantum information processing systems are becoming practical with the expectation that these systems will soon be commercialized within a decade, and will be a part of our everyday life. However, our understanding of the amount of information that can be gleaned from a quantum system is still very rudimentary. For example, the complexity of the basic question of learning a quantum state (called tomography) was only resolved in the past few years, whereas its classical analogue of distribution estimation is now textbook material in any introductory statistics course.  Research under this award will be dedicated to the study of the problem of estimating fundamental information measures, as well as introduce new information measures for quantum systems. The project will establish the fundamental limits of the information processing capabilities of quantum information systems, as well as design novel algorithms that can match these limits. The project outcomes can enable the development of efficient quantum computer and communication systems.<br/><br/>The award will develop a deeper understanding of a quantum system as a statistical mechanism, and integrate ideas from various disciplines such as representation theory, information theory, statistics, and computer science. A novel variation of the statistical principle of maximum likelihood estimation as a general methodology for quantum property estimation will be proposed and developed. The project will also study the extension of representation theory results of Schur-Weyl duality to higher dimensions to obtain optimal quantum measurement schemes for properties of multiple quantum systems. The work includes proposing measurement schemes that are more realistic than the theoretically-optimal ones considered in the literature that in practice are too complex to be implemented with existing technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1524455","AF: Small: Collaborative Research: Making Computational Geometry Polynomial in Derivation Length and in Dimension","CCF","ALGORITHMIC FOUNDATIONS","07/01/2015","06/26/2015","Elisha Sacks","IN","Purdue University","Standard Grant","Rahul Shah","06/30/2019","$250,000.00","","sacks@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7796","7923, 7929","$0.00","Computational geometry is the discipline that creates algorithms to<br/>design and manipulate shapes.  It has wide application in science,<br/>engineering, and industry -- CAD/CAM systems are a notable example.<br/>The problem is, where we see shapes and their spatial relations, the<br/>computer sees only numbers -- and usually, for speed, only<br/>approximate, ""floating-point"" numbers.  E.g., points on a line that<br/>are represented as numerical coordinates, when rounded to the<br/>number of digits the computer stores, may no longer lie on any common<br/>line.  Subsequent calculations that rely on a line being straight or<br/>two lines intersecting at most once can then go badly astray.  Error<br/>in numerical computation can sometimes be limited by rounding, but<br/>there is no practical technique for rounding three-dimensional shapes.<br/><br/>This project investigates shape rounding by encasement:<br/>High-complexity shapes are encased in approximating polyhedra with<br/>floating-point vertex coordinates.  Given an encasement, the PIs have<br/>described a rounding algorithm that projects input features to nearby<br/>encasement features. For dealing with intersections of surfaces, the<br/>project creates output-sensitive algorithms for another type of<br/>encasement, an isolating encasement that can have distant boundary<br/>vertices but cannot encase other features.<br/><br/>Encasement is only part of the solution, so the project also explores<br/>ways to compute topological structure using bounded-complexity<br/>arithmetic by avoiding numerical computation for the degenerate (zero)<br/>expressions.  It investigates using graph theory to analyze explicit<br/>expressions and algebraic techniques to analyze expressions involving<br/>roots of polynomials.<br/><br/>The outcome is to include a software library for implementing<br/>computational geometry algorithms with automated shape rounding.  The<br/>project integrates education and research through an introductory<br/>computational geometry course in which standard algorithms are taught<br/>and implemented using the library.  Previously, the computational cost<br/>of multi-step algorithms forced students to consider each algorithm in<br/>isolation."
"1816361","SHF:Small:  EM-Aware Physical Design and Run-Time Optimization for sub-10nm 2D and 3D Integrated Circuits","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2018","05/21/2018","Sheldon Tan","CA","University of California-Riverside","Standard Grant","Sankar Basu","07/31/2021","$450,000.00","","stan@ece.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7923, 7945","$0.00","Electro-Migration (EM) has emerged as a major design constraint and reliability issue for nano-meter-scale integrated circuits (ICs) and emerging three-dimensional (3D) stacked ICs.  Due to its importance, many advances have been made recently in EM modeling and fast numerical assessment techniques.  However, those advanced EM models have not been fully exploited by existing EM-aware physical design and optimization methods to reduce and mitigate the overly conservative VLSI design practices. The new EM models can naturally consider wire topology and structure impacts on the EM failures of interconnect wires and recovery effects of EM aging process for the first time, thus opening new opportunities for EM optimization at physical design stages. Novel EM optimization techniques to be explored in this award will improve IC reliability amid continued aggressive transistor scaling and increasing power density.  The research in this project will contribute significantly to the core knowledge and technologies of EM-aware physical design and optimization for nano-meter VLSI designs. This investigator will seek to recruit underrepresented minority students to further contribute to the diversity in U.S. science and technology workforce.<br/><br/>This project will develop advanced EM-aware physical optimization techniques and run-time EM mitigation techniques for traditional two-dimensional (2D) and emerging 3D stacked ICs in the nano-meter regime. First, the research will develop new EM-aware optimization techniques for power delivery networks of mainstream 2D and emerging 3D ICs based on the newly proposed EM immortality-check rules for general interconnect trees. The new optimization algorithms will also consider the EM-induced aging effects for targeted lifetime optimization using more accurate EM lifetime estimation methods. Second, the research will explore the run-time recovery effects of the EM aging process to extend the EM lifetime of the signal and power/ground (P/G) networks in 3D stacked ICs. The new optimization methods will, thus, help extend the lifetime of the 3D stacked ICs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1813438","AF: Small: Analysis, Geometry, and Hardness of Approximation","CCF","ALGORITHMIC FOUNDATIONS","10/01/2018","05/15/2018","Subhash Khot","NY","New York University","Standard Grant","Tracy J. Kimbrel","09/30/2021","$500,000.00","","khot@cs.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7796","7923, 7926, 7927","$0.00","Algorithms are systematic procedures to solve computational problems. There are a class of problems called ""NP-hard"" problems which are believed to be computationally infeasible or ""hard"", and without any efficient algorithms. A well-known representative problem in this class is the Travelling Salesperson (TSP) problem: given a large number of cities and pairwise distances among them, what is the minimum length tour that visits all the cities?  NP-hard problems, though computationally hard, do arise in practice and do need to be ""solved"" somehow.  A natural approach is to design efficient algorithms that compute approximate solutions along with a guarantee on the quality of approximation. In the case of TSP, there is an efficient algorithm that computes a tour that is guaranteed to be at most 1% longer than the minimum length tour (which might be good enough in practice).  A huge amount of research has been devoted to designing good approximation algorithms for numerous NP-hard problems. However, it is also of interest to investigate whether there are limitations on the quality of approximation that can be achieved by an efficient algorithm. Indeed, it turns out that for several NP-hard problems, computing an approximation better than a specific threshold is as hard as computing the exact solution (and hence infeasible). These latter kind of results, called ""hardness of approximation"" results, are the focus of this project. The project aims at studying analytic and geometric questions that are motivated by their applications to theoretical computer science (TCS) and primarily to hardness of approximation. The research goals of the proposal will be integrated with teaching, mentoring, and dissemination activities.<br/><br/>Hardness of approximation has been a highly influential topic of research starting with the Probabilistically Checkable Proofs (PCP) Theorem in early 1990s. While there have been major successes towards characterizing precise approximation thresholds for basic NP-hard problems, this quest remains largely open. The investigator for this project proposed the Unique Games Conjecture (UGC) in 2002 to make further progress, which turned out to be quite successful, and led to many novel research directions. This project focuses on (1) the analytic and geometric questions that arise from the investigator's recent work towards proving the UGC; (2) understanding the phenomenon of approximation resistance of predicates (which would certainly lead to challenging analytic questions and would likely be related to the recently resolved Dichotomy Conjecture); (3) analytic questions that are not necessarily related to hardness of approximation, but are among long-term goals in this area at the interface of analysis, geometry, and hardness of approximation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1527324","SHF:  Small: Physics-Based Electromigration Assessment and Validation For Reliability-Aware Design and Management","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2015","05/12/2017","Sheldon Tan","CA","University of California-Riverside","Standard Grant","Sankar Basu","05/31/2019","$482,000.00","","stan@ece.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7798","7923, 7945, 9251","$0.00","Long-term reliability has become a significant challenge for design of integrated circuit (IC) chips used in today's computers and smart phones. Among many effects, electromigration-induced reliability has become the dominant limiting factor due to aggressive transistor scaling and increasing power density. In this project, the PI proposes to address IC reliability problems by providing more accurate electromigration models and assessment techniques at both circuit level and system level so that electromigration induced reliability and aging effects can be fully accounted, leveraged and optimized at both the design stage and run time stage to improve chip reliability and their lifetime at minimum design costs. Furthermore, through the collaborations with the industry partners, the proposed techniques are expected to impact the design and tool development community, thus increasing design productivity. This project also enables the institution to hire students from underrepresented groups, thus enhancing the diversity of science and technology workforce.<br/><br/>The project seeks to develop new physics-based electromigration models and full-chip assessment techniques, to perform model validation for accurate yet efficient electromigration verification at the design stage, and to enforce electromigration-aware reliability management at run time for nanometer IC chips. First, the research will develop new physics-accurate electromigration models to better predict mean time to failure for multi-branch interconnect  trees, which are commonly seen in IC chips, for both void nucleation and void growth phases. Second, the project will develop electromigration models that can accommodate time-varying temperature and current densities, which reflect  a more realistic chip working condition due to time-varying loads for  both single wire and multi-branch interconnect trees. On top of this, the project team will develop resource-based electromigration models, which are more amenable to the system-level run-time reliability optimization and management."
"1617791","SHF: Small: DNA Circuits for Analog Computations","CCF","SOFTWARE & HARDWARE FOUNDATION","07/01/2016","06/30/2016","John Reif","NC","Duke University","Standard Grant","Mitra Basu","06/30/2019","$308,001.00","","reif@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7798","7923, 7946, 9251","$0.00","Analog devices have potential advantages over Boolean circuits, particularly for performing numerical computations, and analog circuits are often much more compact and require less resources. These advances are enhanced at molecular scales, where resources are scarce and compact designs are crucial. PI proposes extension of DNA computation from Boolean to analog computation. The analog DNA circuits can be used to control a wide variety of molecular devices. The central goals of this project are (i) to develop (design, simulate, and experimentally test) two architectures for analog DNA circuits, (ii) develop DNA-based methods for digital-to-analog and analog-to-digital conversions to allow hybrid analog-digital DNA circuits, and (iii) to provide demonstrations of applications of analog DNA circuits.<br/><br/>The work will involve students at all levels: the graduates students, undergraduates, and high school students. Females and minority students will especially be recruited. Students working on this project will receive training at Duke Univ. in computer science, chemistry, and DNA-based nanoscience. In addition, there are also opportunities for summer internships for undergraduates and high school students. Analog DNA circuits have many important potential applications such as analog control devices, where real values are sensed and analog computations provide controlling output. Prior devices for control of chemical reactions systems that provide for molecular species sensing and response have been limited to finite-state control; analog DNA circuits will allow much more sophisticated analog processing and control. DNA-based molecular robotics have allowed devices to operate autonomously (e.g., to walk on a nanostructure) but have been limited to finite-state control, and analog DNA circuits will allow molecular robotics to include real-time analog control circuits to provide much more sophisticated control, e.g. for control articulated joints of a molecular robot?s limb. Many systems that dynamically learn (e.g., neural networks and probabilistic inference) require analog computation, and analog DNA circuits can be used for back-propagation computation of neural nets and Bayesian inference computation of probabilistic inference systems.<br/><br/>The project introduces two architectures for molecular-scale analog computation. In both, the input and outputs of analog gates are directly encoded by relative concentrations of input and output strands respectively, without requiring thresholds for converting to Boolean signals. The 1st architecture has 3 gates: addition, subtraction, and multiplication. Analog circuits constructed from these gates can compute polynomials as well as approximate inverse, and division. The 2nd proposed architecture provides a novel DNA-based method to compute analytic functions such as sqrt(x), ln(x), and exp(x) using multiple DNA-based autocatalytic reaction systems working together. The project also introduces DNA analog-to-digital (A/D) and digital-to-analog (D/A) converters that enable the communication between analog and digital DNA circuits. The project includes full-scale designs, simulations, and experimental demonstrations of the two architectures, demonstrations of hybrid analog-digital DNA circuits, and a small-scale demonstration of an application of analog DNA circuits for control of a chemical reaction system: sensing input concentrations of molecules and controlling output of concentrations of molecules."
"1830899","Women in Theory Workshop 2018","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","04/01/2018","03/29/2018","Madhu Sudan","MA","Harvard University","Standard Grant","Tracy J. Kimbrel","03/31/2019","$50,000.00","Lisa Zhang, Shubhangi Saraf, Tal Rabin","madhu@cs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","2878, 7796","7556","$0.00","This award supports the 6th Biennial Women-in-Theory Workshop at Harvard University in June 2018. The intended audience of the workshop is graduate students in the field of theoretical computer science (TCS). Motivated by the low overall number of female faculty members, researchers, and students in TCS world wide, the workshop has two goals. The first is to deliver an invigorating educational program to the student participants by inviting leading female researchers to present tutorials of their research topics. The second is to provide an outstanding opportunity to bring together women students from different institutions across the country and internationally, so as to foster a sense of kinship and camaraderie, and to provide access to role models in this area by having senior and junior faculty members and industrial researchers present.  <br/><br/>The format of this workshop consists of technical tutorials, a non-technical talk, a student rump session, and a panel discussion, following the model of the first five workshops in this series. The first five Women-in-Theory workshops were greeted with great enthusiasm both from the participants and from the TCS research community at large. Since the first workshop in 2008, the organizers have received overwhelmingly positive feedback and many participants have matured to be successful researchers. These serve as testimonials that the workshop is accomplishing its primary goal of helping women become more successful in TCS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757828","REU Site: Software Safety and Reliability: Research and Application","CCF","RSCH EXPER FOR UNDERGRAD SITES","02/01/2018","01/26/2018","Weichen Wong","TX","University of Texas at Dallas","Standard Grant","Rahul Shah","01/31/2021","$359,994.00","Gopal Gupta","ewong@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1139","9250","$0.00","As software safety and reliability are vital for safe and reliable operation of many systems and in reduction of catastrophic accidents, this Research Experiences for Undergraduates (REU) site at the University of Texas at Dallas provides cutting-edge research opportunities on these important topics to students, especially those from<br/>underrepresented groups or universities with limited resources. After gaining skills in critical thinking, oral and written communication, problem solving, and research methods, students will be encouraged and well prepared for graduate education and research careers in science and technology. Their interactions with mentors and other participants will provide avenues to share ideas and results. Research connections will be established with students' home institutions to continue collaboration among multiple institutions in different geographical regions. REU website will publicize research articles, videos and slides of lectures and project presentations, and announcements for effective community outreach.<br/><br/>The subject of this research is software safety and reliability, with a focus on evaluating the strengths and weaknesses of existing methodologies and current practices for safety assurance and reliability prediction.  Issues to be studied include how the introduction of software safety and reliability requirements may affect the software lifecycle, and studying how software processes, methods and tool support should be adjusted. Further exploration will emphasize how safe software systems can be unreliable; how reliable software systems can be unsafe; and how to make software systems both safe and reliable. Our findings will be compiled to provide a comprehensive picture of how software safety and reliability can be achieved effectively and efficiently. An important aspect of this site is a close collaboration with industry. REU students will take field trips to industry sponsors, allowing them to speak directly with practitioners to better understand how software safety and reliability is applied to real-life applications. Workshops on technical writing and oral presentation will be held to improve students' proficiency in preparing and delivering technical reports. Seminars on ethics and professional responsibility will also be given to ensure REU students behave properly and respectfully."
"1755808","CRII: CIF:  Next-Generation Group Testing for Neighbor Discovery in the IoT via Sparse-Graph Codes","CCF","COMM & INFORMATION FOUNDATIONS","04/01/2018","12/18/2017","Ramtin Pedarsani","CA","University of California-Santa Barbara","Standard Grant","Phillip Regalia","03/31/2020","$174,938.00","","ramtin@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7797","7935, 8228","$0.00","Group testing is a fundamental inference problem that aims to detect a set of defective items from a larger set of items via group tests. Group testing has a variety of applications in different fields including communications, computer science, machine learning, biology, and signal processing. The main challenges in group testing are to design a small number of tests such that the defective items can be reliably recovered, and to efficiently recover the defective items using a low-complexity decoding algorithm. The goal of this project is to address both challenges by developing fast and near-optimal group testing schemes. The application area that the project focuses on is active neighbor discovery in the Internet of Things (IoT). In an IoT setting, there is an abundance of low-energy devices that collect and transmit information. The main challenge in such systems is to enable a massive number of devices to communicate via a scalable and low-complexity random access scheme. This project addresses this challenge by designing large-scale active neighbor discovery protocols based on group testing.<br/><br/>The key idea of this research is to view the group testing problem from a coding-theoretic lens to develop recovery algorithms with near-optimal sample complexity (number of tests) and optimal decoding complexity. The main ingredients of this coding-theoretic approach are to: (i) design the tests based on a sparse-graph code; (ii) develop a fast peeling-based decoder with sublinear computational complexity for detecting the defective items; and (ii) leverage powerful tools from modern coding theory such as density evolution to minimize the sample complexity of the algorithm. As a concrete application, by addressing the fundamental challenge of scale in the theory of group testing, the proposed work aims to develop active user detection schemes for large-scale communication systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1149637","CAREER: Extractors, Pseudorandom Generators, and Other Explicit Constructions","CCF","ALGORITHMIC FOUNDATIONS","03/01/2012","11/16/2018","Anup Rao","WA","University of Washington","Continuing grant","Tracy Kimbrel","09/30/2019","$499,290.00","","anuprao@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","1045, 7927","$0.00","It is widely acknowledged that the physical universe is unpredictable.  In computer science this unpredictability, modeled as access to randomness, turns out to be an enabling feature in algorithm design, cryptography and distributed computing.  For example, known methods of encrypting sensitive information for transmission over the internet rely crucially on the ability of computers to generate random sequences, and many fast algorithms are randomized algorithms.  So it is worthwhile to investigate exactly what can be done with and without randomness, and to identify the minimal assumptions on the randomness under which we can still get the benefits of randomized methods.  This is the central question of the area of derandomization: To what extent can we do without the use of randomness in computation?<br/><br/>In this project, PI will investigate basic questions related to the theme of derandomization towards the goals of showing that  (1) every randomized polynomial time algorithm can be simulated by a deterministic algorithm, that (2) every randomized algorithm with small memory can be simulated by a deterministic algorithm using small memory, and that (3) cryptography can be achieved even with defective sources of randomness. Besides being involved in graduate and undergraduate teaching around the subject of the proposal, the PI will organize reading groups, participate in efforts to recruit from underrepresented groups, and be involved in projects to increase graduate recruiting at education at the high school level."
"1540547","BSF:2014163:Approximability of network design problems","CCF","SPECIAL PROJECTS - CCF","09/01/2015","08/06/2015","Guy Kortsarz","NJ","Rutgers University Camden","Standard Grant","Tracy Kimbrel","08/31/2019","$50,000.00","","guyk@crab.rutgers.edu","311 N. 5th Street","Camden","NJ","081021400","8562252949","CSE","2878","2878","$0.00","This project aims to derive fundamental results in the field of network design.  In network design, one wishes to derive a network which simultaneously has low cost and desirable properties such as high connectivity for fault tolerance.  The goal of this project is to find improved approximation algorithms - which find a network with cost and quality provably near the best possible for a given problem instance - or to prove new inapproximability results.  Problems in this area have a large practical significance in many areas including transportation planning, road planning, and power grids.  The project will provide undergraduate research opportunities.  The PI plans to test some of the algorithms developed in practical settings with the assistance of students from the recently launched Computer Science Undergraduate Research Academy at Rutgers-Camden.<br/>The project will focus in particular on approximability of NP-hard network design problems including some of the most significant connectivity problems: Directed Steiner Tree, Directed Steiner Forest, Group Steiner Tree, Multicommodity Buy-at-Bulk, Minimum Poise Tree, Tree Augmentation, Directed Rooted 2-Survivable Network, and the Minimum-cost Vertex k-Connected Sub-graph problem."
"1730449","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","EXPERIMENTAL EXPEDITIONS, QUANTUM COMPUTING","03/01/2018","08/16/2018","Frederic Chong","IL","University of Chicago","Continuing grant","Almadena Chtchelkanova","02/28/2023","$2,014,578.00","John Reppy, Diana Franklin, David Schuster","chong@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7723, 7928","7723, 7928, 9251","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763761","SHF: Medium: Time Based Deep Neural Networks: An Integrated Hardware-Software Approach","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2018","04/04/2018","Chris Kim","MN","University of Minnesota-Twin Cities","Continuing grant","Sankar Basu","04/30/2022","$443,392.00","Sachin Sapatnekar, Qi Zhao","chriskim@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7798","7924, 7945","$0.00","Recent advancements in deep learning hardware and algorithms are providing computers with unprecedented levels of human-like intelligence for applications such as self-driving cars, patient diagnosis and treatment, speech processing, strategy games, and education. Traditional deep learning algorithms rely on powerful computers tethered to the cloud which incurs a large communication overhead, requires extensive computing resources, and compromises privacy and security. There is a strong consensus among experts that the next frontier in deep learning will be highly-efficient neural network processors running on mobile platforms. This project aims at developing a compact and low power alternative to conventional deep learning computing hardware, specifically targeted for edge devices. The proposed approach is based on a novel computing concept called time-based circuits, which can deliver a similar level of inference performance at only a fraction of the power consumption compared to traditional methods. Throughout the project, the investigators will consider transferring the new neural network computing methods to industry. The new time-based deep learning computation methods will be incorporated into the graduate and undergraduate curricula, as well as K-12 outreach activities, of the electrical engineering and computer science departments at the University of Minnesota.<br/><br/>This project will focus on both hardware and software techniques for enabling deep learning applications on resource-constrained mobile platforms. On the hardware side, the team will demonstrate a prototype low-power deep neural network processor where internal operations such as convolution, pooling, and activation functions are performed entirely in the time domain. On the software side, the team will develop pruning, approximation, and hybrid approaches that can effectively reduce the complexity of deep neural networks with minimal impact on the overall inference accuracy. A unique aspect of this project is the continual interaction between the hardware and software groups to deliver the first fully time-based deep neural network engine targeted for edge devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1907711","SHF: Medium: Collaborative Research: Toward Extreme Scale Fault-Tolerance: Exploration Methods, Comparative Studies and Decision Processes","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2017","12/14/2018","Dorian Arnold","GA","Emory University","Standard Grant","Almadena Chtchelkanova","07/31/2020","$342,000.00","","darnold@cs.unm.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","7798","7924, 7942, 9150","$0.00","Current high-performance computing (HPC) research target computer systems with exaflop (1018 or a quintillion floating point operations per second) capabilities. Such computational power will enable new, important discoveries across all basic science domains. Application resilience to computer faults and failures is a major challenge to the realization of extreme scale computing systems. This project, Simulation and Modeling for Understanding Resilience and Faults at Scale (SMURFS), addresses this challenge by developing methods to improve our predictive understanding of the complex interactions amongst a given application, a given real or hypothetical hardware and software system environment, and a given fault-tolerance strategy at extreme scale. Specifically, SMURFS develops:<br/>1. New simulation and modeling capabilities for studying application resilience at scale;<br/>2. Capabilities to execute a comprehensive set of comparative fault-tolerance studies; and<br/>3. Effective prescriptions to guide application developers, hardware architects and system designers to realize efficient, resilient extreme scale capabilities.<br/><br/>SMURFS explores the impact of faults and failures, fault mitigation strategies and emerging technologies by providing new analytical and component models for predicting fault-tolerant application behavior at scale. The Iron simulation framework integrates these models for validation and comprehensive performance studies over a wide range of representative applications, application proxies, fault-tolerance protocols and hardware configurations. These studies inform a rule-based system for prescribing best fault-tolerance practices and configurations for new candidate applications and scenarios.<br/><br/>SMURFS renders (1) new simulation and analytical models that predict application performance at scale; (2) detailed understandings of how application features interplay with different fault-tolerance strategies and hardware technologies; (3) new knowledge about application behavior at scale; and (4) valuable insight and prescriptions for designing, developing and deploying future extreme scale HPC systems.<br/><br/>More broadly, artifacts like the Iron framework and the public suite of application traces will be valuable to the HPC research, engineering, development, procurement and administrative communities. Researchers can use these artifacts for their own research that can impact the HPC exploration and design space.  For example, this framework can be instrumental in the co-design of cohesive extreme scale applications, software environments and hardware platforms. Additionally, Iron-based research can inform and improve scientific computing practices, accelerating the rate of scientific discovery.  Finally, Iron will be useful as an instructional device to teach about HPC issues both in classroom and tutorial contexts and other programs that engage diverse populations of middle, high school and college students in New Mexico and Tennessee."
"1734706","Computing Community Consortium III","CCF","INFORMATION TECHNOLOGY RESEARC","04/01/2018","02/15/2019","Elizabeth Mynatt","DC","Computing Research Association","Cooperative Agreement","Nina Amla","03/31/2022","$1,762,736.00","Mark Hill, Ann Drobnis","mynatt@cc.gatech.edu","1828 L St., NW","Washington","DC","200360000","2022662949","CSE","1640","","$0.00","The Computing Community Consortium (CCC) is established through a Cooperative Agreement between the National Science Foundation (NSF) and the Computing Research Association (CRA). Since its inception, the purpose of the CCC has been to catalyze the development of bold, far-reaching visions for computing research, and to facilitate the communication of those research visions to stakeholders both within and beyond the computing research ecosystem.  The CCC fosters proactive engagement with both computing and non-computing research stakeholders -- agencies, industry, professional societies, consortia, and philanthropic stakeholders -- to ensure that it continues to expand its network and impact.<br/><br/>The CCC provides the continuity and capacity to bring together stakeholders that depend on computing innovation as part of increasingly diverse and multi-disciplinary national and societal needs. The CCC plays a vital and effective role in catalyzing the computing research community to articulate far reaching and strategic research visions that will continue to shape the field for decades to come. The majority of CCC visioning activities and white papers bring together multidisciplinary expertise and needs that combine to create important disciplinary and interdisciplinary challenges for the computing field. The results of these activities are disseminated back to the community to ensure continued revitalization and advancement of new areas and topics for future computing research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763681","SHF: Medium: Embracing Architectural Heterogeneity through Hardware-Software Co-design","CCF","SOFTWARE & HARDWARE FOUNDATION","06/01/2018","05/21/2018","Chitaranjan Das","PA","Pennsylvania State Univ University Park","Continuing grant","Almadena Chtchelkanova","05/31/2021","$658,354.00","Anand Sivasubramaniam, Mahmut Kandemir","das@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7798","7924, 7941","$0.00","The last decade has witnessed a proliferation of heterogeneity across diverse application domains spanning from high-end datacenters to low-cost embedded systems, because they are capable of better performance and energy efficiency compared to homogeneous multicore architectures. These systems typically include a subset of CPUs, GPUs, FPGAs and ASICs as compute engines and hence, present unique programming/resource management challenges. However, the lack of required compiler and runtime support, present a barrier to the widespread adoption of heterogeneous systems. Furthermore, the design of the underlying heterogeneous architecture in terms of number and placement of various compute engines, memory subsystems and interconnects for a given area/power budget to satiate various application demands, is not fully explored. Therefore, it is imperative to investigate the entire system stack in a cohesive manner spanning applications, system software and underlying hardware for providing the required support for efficient application executions.  Thus, the main goal of this research project is to enable dynamic mapping of an application to different computing engines for improving performance/power efficiency and system utilization. The outcomes of this project are poised to change the way the programmers and users perceive heterogeneity and interact with it. The research on heterogeneous computing will be integrated with the educational activities and student training at Penn State for nurturing the future workforce in science and engineering, with active participation of female graduate students and undergraduates (Honors) students. <br/> <br/>The project consists four tasks. Task-I aims at conducting a profile-based workload characterization for various application domains including deep learning, cloud computing and high-performance computing on diverse hardware platforms to understand their performance/power utility. This will be used to develop a machine-learning (ML) based model for initial assignment of tasks to different compute engines.  Task-II is aimed at exploring compiler/programming support to transform application code into suitable device-agnostic 'codelets', that serve as the granularity for seamless scheduling and execution across different hardware units. Task-III investigates runtime support to optimally schedule and seamlessly move the codelets across the hardware units for improving system performance. Finally, Task-IV explores design of heterogeneous platforms by analyzing issues such as degree of heterogeneity, placement and integration of various computing engines on a chip and across chips, the underlying communication support.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1651794","CAREER: Direct Manipulation Programming Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","03/01/2017","02/11/2019","Ravi Chugh","IL","University of Chicago","Continuing grant","Anindya Banerjee","02/28/2022","$316,546.00","","rchugh@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7798","1045, 7943","$0.00","Computer programming is an increasingly vital and powerful tool, but two realities inhibit even greater impact. First, programming consists of an ""Edit-Run-View"" workflow that slows the pace of the creative process; only in latter stages can the results be visualized, at which point the programmer must return to text-editing to make subsequent changes. Furthermore, programming is limited to experts. Ideally, users would be aided in the programming process by interactive graphical user interfaces (GUIs), akin to those available for domains such as word processing, spreadsheets, and graphic design. The goal of this research is to develop theoretical and practical foundations to integrate the expressive power of programming with the ease-of-use of direct manipulation GUIs. The intellectual merits of this research are to answer several open questions about computer programming: (1) How can interactions with the output of a program be used to determine the user's intended modifications to the program? (2) How can interactions with the source code of a program be used to determine the user's intended, higher-level modifications to the program? (3) How can user interfaces integrate the expressiveness of general-purpose programming languages with the intuitive workflow of direct manipulation systems? The project's broader significance and importance are to incorporate answers to these questions into innovative software technology that will (a) be released freely to the public for use by expert and non-expert users, (b) support university-level curricular activities that help bridge computer science with other disciplines such as visual arts, and (c) support middle- and high-school curricular activities that help promote interest in programming and computational thinking.<br/><br/>The project blends and advances techniques in program synthesis, semi-automated refactoring, and human-computer interaction. Three primary activities are pursued: (1) Develop program synthesis algorithms that monitor the previous execution of a program and allow users to specify program modifications by directly manipulating program output. (2) Develop program synthesis algorithms that allow users to specify program modifications by visually and structurally manipulating program text. (3) Design semi-automated programming systems that enable expert and non-expert users to create a variety of digital objects with less text-based editing than in existing programming languages and less mouse-based editing than in existing direct manipulation systems."
"1253700","CAREER:Cross-Core Learning in Future Manycore Systems","CCF","SOFTWARE & HARDWARE FOUNDATION, COMPUTER ARCHITECTURE","07/01/2013","05/30/2017","Abhishek Bhattacharjee","NJ","Rutgers University New Brunswick","Continuing grant","Anindya Banerjee","03/31/2019","$520,000.00","","abhib@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7798, 7941","1045, 7941, 9251","$0.00","As computing devices solve increasingly complex and diverse problems, engineers seek to design processors that provide higher performance, while remaining energy-efficient for environmental reasons. To achieve this, processor vendors have embraced manycore devices, where thousands of cores cooperate on a single chip to solve large-scale problems in a parallel manner. They have further incorporated heterogeneity, combining cores with different architectures on a single chip in a bid to provide ever-increasing performance per watt. This project boosts the search for higher energy-efficient performance by inventing novel cross-core learning techniques. Cores in current chips individually learn about the behavior of parallel programs in order to run programs more efficiently in the future, devoting complex and power-hungry hardware structures to do this. However, this research observes that parallel programs tend to exercise the hardware structures of different cores in correlated ways, meaning that the behavior of the program run on one core can be communicated to other cores for various performance and power benefits. As such, this form of intelligent cross-core information exchange is effective in achieving high performance per watt across computing domains from datacenters to embedded systems<br/><br/>In this light, this research provides techniques to deduce how similarly a parallel program's various threads exercise their cores' hardware structures (looking at a range of different programmer, compiler, and architectural mechanisms to do so). When this is detected, cross-core learning hardware gleans the information that is most useful to exchange to improve performance or power, and then transmits this information among heterogeneous cores using low-overhead hardware/software techniques. This project develops a lightweight runtime software layer to orchestrate this information exchange, relying on dedicated hardware support when necessary. Through developing this framework, cross-core learning is applied to a number of specific cases, ranging from higher-performance manycore cache prefetching and branch prediction, to performance and power-management techniques for interrupts and exceptions in scale-out systems, as well as thread and instruction scheduling.  Furthermore, this project heavily disseminates knowledge on how to design and program large-scale manycore systems (or scale-out systems) by involving students at the graduate, undergraduate, and high-school levels through active research and coursework. Overall, this work impacts the engineering community and broader society by: (1) helping to achieve high-performance, but also energy-efficient and environmentally-friendly computing systems; (2) providing academics and chip designers a design methodology and infrastructure to study manycore design; (3) broadening the participation of underrepresented groups in computer science; (4) educating graduate, undergraduate, and high-school students on parallel programming for manycore systems."
"1815487","CIF: Small: RUI: Low Correlation and Highly Nonlinear Structures for Communications and Sensing","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2018","06/28/2018","Daniel Katz","CA","The University Corporation, Northridge","Standard Grant","Phillip Regalia","09/30/2021","$330,887.00","","daniel.katz@csun.edu","18111 Nordhoff Street","Northridge","CA","913308309","8186771403","CSE","7797","7923, 7935","$0.00","Many communications and remote sensing systems require modulation protocols that are described by digital sequences, which may be regarded as words composed of symbols from a prescribed alphabet, such as the binary alphabet with symbols 0 and 1. The efficiency of the system will often depend on producing sequences that are as uncorrelated as possible: they should not resemble shifted (time-delayed) versions of each other, nor even of themselves. Lack of resemblance between a sequence and shifted versions of itself aids in synchronization and timing, which is useful in radar and sonar. Lack of resemblance between two different sequences (no matter how they are shifted) prevents confusion between different users in communications networks. Random sequences are not ideal for these applications, as even random sequences are expected to have occasional repetitions. It is more advantageous to use pseudorandom sequences that avoid repetition to a greater degree than random sequences do. These pseudorandom sequences and related mathematical structures, such as Boolean functions, are also significant in other information-theoretic problems, such as in cryptography, where one seeks to design permutations that have a simple underlying mathematical form (to ease encryption and decryption) but avoid resembling easily detectable patterns (to resist cryptanalysis). Pseudorandom sequences find further applications in error-correcting codes, antenna arrays, scientific instrumentation, and acoustic design, and thus science and technology benefit both from the analysis of known digital sequences and the discovery of new ones.<br/><br/>The goal of this project is to create and investigate digital sequences and related mathematical structures with good correlation properties. This project considers both periodic and aperiodic forms of correlation, as both are important in applications. In periodic correlation, the shifting of the sequences is cyclic, and the maximum length linear feedback shift register sequences (m-sequences) are a common building block in the design of digital sequences with low periodic correlation. Finding pairs of m-sequences with low mutual correlation is equivalent to finding highly nonlinear permutations of finite fields, which can be used to make cryptosystems resilient to linear cryptanalysis. This project will investigate m-sequence pairs with exceptional correlation properties, which translate into exceptional nonlinearity properties of the corresponding permutations. Extremal properties, such as exceptionally high nonlinearity or exceptionally few correlation values, are sought out, and this project will investigate bounds and limitations on these extremes using tools from abstract algebra, combinatorics, and number theory, as well as empirical computational explorations. In aperiodic correlation, the shifting of sequences is a non-cyclic translation, and various families of sequences whose correlation properties make them superior to random sequences are known, but their analysis has been difficult and many open questions remain. This project will analyze the performance of these sequences both empirically and theoretically, and will seek new families of sequences with good correlation properties<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1846327","CAREER: Robustness of Inductive Reasoning Engines","CCF","SOFTWARE & HARDWARE FOUNDATION","03/01/2019","02/01/2019","Roopsha Samanta","IN","Purdue University","Continuing grant","Nina Amla","02/29/2024","$94,990.00","","roopsha@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","1045, 8206, 9102","$0.00","The past decade has seen a renaissance in the field of machine learning and simultaneously witnessed an explosion of interest in the related area of Programming by Example (PBE). While both fields have enjoyed spectacular successes, their algorithms can be brittle and can drive applications to unexpected failures. The cause of many failures in both machine-learning and PBE systems can be traced back to their shared task of inductive reasoning: learning some artifact in a hypothesis space from a set of examples. Since examples are inherently incomplete specifications, there can be a large number of artifacts that fit a set of examples but fail to generalize to an unseen example. This project advocates for a more principled approach to constructing such inductive reasoning engines based on a formal characterization of their reliability.<br/> <br/>The project casts the problem of reliability of these systems as one of robustness: is the change in the artifact learnt acceptable, or, at least predictable, in the presence of small changes to the set of examples? The project integrates concepts from formal methods, logic, relational reasoning, and computational learning theory to develop new foundations, algorithms and tools for the design and analysis of robust inductive reasoning engines. The multi-faceted project will impact formal methods and programming languages (through contributions to inductive synthesis and relational reasoning), machine learning (through automated techniques for addressing the dataset shift problem), and society (through users of inductive reasoning engines, and education activities targeting expansion of scientific literacy and computer science pathways). The investigator plans broad dissemination of results (through a workshop on robustness co-founded by the investigator, talks at outreach platforms and a graduate course).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1617710","AF: Small: Approximation algorithms for quantum mechanical problems","CCF","ALGORITHMIC FOUNDATIONS","07/01/2016","01/29/2019","Sevag Gharibian","VA","Virginia Commonwealth University","Standard Grant","Dmitri Maslov","06/30/2019","$380,754.00","","sgharibian@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","7796","7923, 7928","$0.00","A fundamental problem in computer science is the constraint satisfaction problem MAX-SAT, which asks: Given a set of Boolean constraints on n bits, what is the maximum number of constraints which can be simultaneously satisfied by an assignment to all the bits? Despite its many applications, MAX-SAT is unfortunately believed to be impossible to solve efficiently. Fortunately, such problems can often be solved approximately via so-called approximation algorithms.<br/>In the quantum setting, a physically motivated generalization of MAX-SAT exists, known as LH, which concerns the computation of important properties of quantum systems at very low temperatures. Unfortunately, like MAX-SAT, LH is also believed intractable. This project hence asks the question: Can we compute approximate solutions to k-LH via the framework of approximation algorithms? <br/>The resolution of this question will yield deep insight into our ability to approximately compute properties of quantum systems in nature. The results obtained will be disseminated through a variety of avenues, including conferences, high school workshops, and engineering public lecture series aimed at exposing the general public to the frontiers of research.<br/> At a high level, this project aims to design polynomial-time approximation algorithms for a variety of classes of the local Hamiltonian problem (LH), from physically motivated special cases to more general settings. The techniques used are inspired primarily by ideas from the fields of approximation algorithms and quantum information theory. Among other results, a key aim of the project is to obtain insight into how well classically efficiently representable quantum states can approximate solutions to genuinely quantum problems involving ground spaces of local Hamiltonians."
"1740352","E2CDA: Type I: Collaborative Research: Energy-efficient analog computing with emerging memory devices","CCF","Energy Efficient Computing: fr","09/15/2017","07/18/2018","Dmitri Strukov","CA","University of California-Santa Barbara","Continuing grant","Sankar Basu","08/31/2020","$640,000.00","Timothy Sherwood, Yuan Xie","strukov@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","015Y","7945, 8089","$0.00","The main goal of this project is to develop analog computing circuits that will greatly exceed their digital counterparts in energy-efficiency, speed, and density by employing emerging nonvolatile memory devices. Though analog circuits have been around for a long time, their applications in computing have been rather limited, largely due to the lack of efficient implementations of analog weights. This impediment could be overcome now due to the rapid progress in the emerging nonvolatile memory devices, such as metal-oxide memristors, which are the focus in this project. The analog memory functionality of memristors, combined with high retention and sub-10-nm scaling prospects, might for the first time enable extremely fast and energy-efficient analog implementations of many core operations, such as vector-by-matrix multiplication, which are central to many existing and emerging future applications such as internet-of-the-things and sensor networks, robotics, and energy efficient neuromorphic systems.  The results of the proposed research will be integrated into educational curriculum and will help to train material science and electrical engineering students of all levels in this exciting field.<br/><br/>The main caveat of the considered analog circuits is their limited operation accuracy, primarily due to the noise and variability in memory devices. The mitigation of this challenge by several means will be one of the main focuses of the project, and will be addressed with highly-interconnected research effort across device, circuit, and architectural layers. At the device level, detailed electrical characterization of analog operation and ways to improve it via material engineering, optimization of electrical stress, and development of efficient tuning algorithms to cope with device variations will be explored. Guided by experimentally-verified device models, the design of several representative analog computing circuits will be optimized. Circuit modeling tools will be developed to capture rich design trade offs in area, speed, energy efficiency, and precision, calibrated on experimental results from wafer-scale integrated memristor circuits, and used for detailed comparison with state-of-the-art digital counterparts. Finally, accurate circuit models will guide exploration of circuit architectures that mitigate limitations of analog computing and assist with detailed system level simulations."
"1704860","AF: Large: Collaborative Research: Nonconvex Methods and Models for Learning: Toward Algorithms with Provable and Interpretable Guarantees","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","06/01/2017","09/17/2018","Sanjeev Arora","NJ","Princeton University","Continuing grant","Rahul Shah","05/31/2022","$1,026,472.00","Elad Hazan, Yoram Singer","arora@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","2878, 7796","7796, 7925, 7926","$0.00","Artificial Intelligence along with Machine Learning are perhaps the most dominant research themes of our times - with far reaching implications for society and our current life style. While the possibilities are many, there are also doubts about how far these methods will go - and what new theoretical foundations may be required to take them to the next level overcoming possible hurdles. Recently, machine learning has undergone a paradigm shift with increasing reliance on  stochastic optimization to train highly non-convex models -- including but not limited to deep nets. Theoretical understanding has lagged behind, primarily because most problems in question are provably intractable on worst-case instances. Furthermore, traditional machine learning theory is mostly concerned with classification, whereas much practical success is driven by unsupervised learning and representation learning. Most past theory of representation learning was focused on simple models such as k-means clustering and PCA, whereas  practical work uses vastly more complicated models like autoencoders, restricted Boltzmann machines and deep generative models. The proposal presents an ambitious agenda for extending theory to embrace and support these practical trends, with hope of influencing practice. Theoretical foundations will be provided for the next generation of machine learning methods and optimization algorithms. <br/><br/>The project may end up having significant impact on  practical machine learning, and even cause a cultural change in the field -- theory as well as practice -- with long-term ramifications. Given the ubiquity as well as  economic and scientific implications of machine learning today, such impact will extend into other disciplines, especially in (ongoing) collaborations with researchers in neuroscience. The project will train a new generation of machine learning researchers, through an active teaching and mentoring plan at all levels, from undergrad to postdoc. This new generation will be at ease combining cutting edge theory and applications. There is a pressing need for such people today, and the senior PIs played a role in training/mentoring several existing ones.<br/> <br/>Technical contributions will include new theoretical models of knowledge representation and semantics, and also frameworks for proving convergence of nonconvex optimization routines. Theory will be developed to explain and exploit the interplay between representation learning and supervised learning that has proved so empirically successful in deep learning, and seems to underlie new learning paradigms such as domain adaptation, transfer learning, and interactive learning. Attempts will be made to replace neural models with models with more ""interpretable""  attributes and performance curves.  All PIs have a track record of combining theory with practice. They  are also devoted to a heterodox research approach, borrowing from all the past phases of machine learning: interpretable representations from the earlier phases (which relied on logical representations, or probabilistic models), provable guarantees from the middle phase (convex optimization, kernels etc.), and an embrace of nonconvex methods from the latest deep net phase. Such eclecticism is uncommon in machine learning, and may give rise to new paradigms and new kinds of science."
"1523816","AF:  Small:  New Techniques for Private Information Retrieval and Locally Decodable Codes","CCF","ALGORITHMIC FOUNDATIONS, COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace","09/01/2015","07/20/2015","Zeev Dvir","NJ","Princeton University","Standard Grant","Tracy Kimbrel","08/31/2019","$427,093.00","","zdvir@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7796, 7797, 8060","7434, 7923, 7927, 7935","$0.00","Maintaining user privacy in a computerized world is a difficult challenge. Private Information Retrieval (PIR) schemes allow a user to retrieve a piece of information replicated among several servers without disclosing the precise nature of that information to each individual server.  The aim of this project is to develop new techniques for constructing such protocols with improved efficiency. In a recent breakthrough, the PI introduced a new technique that dramatically reduced the cost of the best PIR protocols. The main goals of the projects are to develop this technique further and to understand better the underlying mathematics that make it work. Pursuing the research goals of the project will require integration of research and education and training of gradate and undergraduate students. The PI will also contribute to education of high school students, including those from under-represented groups, by participating in a computer science summer school program. <br/><br/>PIR protocols guarantee information theoretic privacy in the setting where the same database is replicated among several non-communicating servers. The most interesting case is that of two servers (the smallest possible). The current state of the art (obtained in a recent work by the PI) gives sub-polynomial communication cost, improving the two-decades long record requiring  communication proportional to the cube root of the database size. The improvement comes from leveraging the connection between PIR and Locally Decodable Codes (LDCs) which are error correcting codes that allow for quick correction of a single codeword position by querying the code in only a few places.  The main goals of this project are to further study these two objects (PIR and LDCs) and to improve their known constructions. On the other hand, the project will aim to understand the limitation of PIR and LDCs in the form of provable lower bounds on their efficiency."
"1807575","SemiSynBio: Collaborative Research: Very Large-Scale Genetic Circuit Design Automation","CCF","SemiSynBio Semiconductor Synth, Genetic Mechanisms","10/01/2018","07/16/2018","Christopher Voigt","MA","Massachusetts Institute of Technology","Continuing grant","Mitra Basu","09/30/2021","$138,750.00","","cavoigt@gmail.com","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","061Y, 1112","7465","$0.00","The computing power of biology is incredible, evident in the natural world in the intricate patterns underlying materials and the body plan of animals.  Cells build these structures by using networks of interacting bio-molecules, encoded in their DNA, that function as microscopic computers, the power of which grows as many cells communicate to work together on a problem. The goal of this project is to significantly scale-up the ability to build these systems by design such that cells can be programmed to perform complex computational tasks.  This will be done by creating software that allows a user to write code, exactly as one would program a computer, which is then compiled to a DNA sequence. New theoretical tools will be applied to determine the power required by the cell to run these programs and how best to distribute tasks between circuits encoded in cells and conventional electronic systems. This research will broadly impact biotechnology, which is increasingly being used to commercially produce a wide range of products, from consumer goods to high-end advanced materials.  Current products do not harness the computational potential of cells; in other words, all the genes are turned on all the time.  This research will enable cells to be programmed to build chemicals and materials in multiple steps, both by performing the computations inside of the cells and also communicating across cells. This work is interdisciplinary and requires backgrounds in Biology, Chemistry, Mathematics, Biological Engineering, Electrical Engineering, and Computer Science. As such, the project includes the development of new educational platforms in anticipation of a need in industry for students trained at the interface between traditionally separated fields.  This includes a new undergraduate-level Synthetic Biology Design course, an industrial co-op, and curriculum material ""How to Grow Almost Anything,"" which will be made public at an international level.<br/><br/> To build the complexity of the natural world, cells use regulatory networks made up of interacting bio-molecules to control the timing and conditions for gene regulation. For the last 20 years, researchers have been able to build synthetic genetic circuits by artfully combining regulatory interactions. The problem is that the largest of such circuits only consist of ~10 regulators, far smaller than natural networks, which drastically limits the computation that can be performed. The proposed research will develop technologies that collectively enable a massive scale-up in computational complexity to ~10^5 regulators. The first objective seeks to increase the size of circuits within cells.  Logic gates based on Cas9 have enormous scale-up potential, but are limited by dCas9 toxicity and sequence repeats. A set of gates will be designed to fix these problems, guided by mathematical modeling.  A framework for design automation will be developed that enables a Verilog specification to be converted into a logic diagram, that is then divided up amongst many interacting cells. The second objective seeks to distribute a genetic circuit design across multiple communicating cells. The number and reliability of cell-cell communication signals will be improved by directed evolution to increase the number of channels from 2 to 8.  These will be implemented in living cells and non-living systems, thus enabling a broad range of applications inside and outside the bioreactor. Combined with 50 gates/cell, this platform offers the possibility of multicellular circuits containing 10^5+ gates. Some applications require deployment as a non-living system, for example when the application is outside of the lab, thus requiring containment. The third objective seeks to translate the parts developed in Objectives 1 and 2 to operate in multiple communicating lipid vesicles encapsulating cell-free protein extract. Cas9 gates and additional communication channels will be characterized to expand the computational potential. These will be characterized as gates and implemented using Electronic Design Automation tools to automate the design of large systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807461","SemiSynBio: Collaborative Research: Very Large-Scale Genetic Circuit Design Automation","CCF","SemiSynBio Semiconductor Synth, Genetic Mechanisms","10/01/2018","07/16/2018","Kate Adamala","MN","University of Minnesota-Twin Cities","Continuing grant","Mitra Basu","09/30/2021","$118,125.00","","kadamala@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","061Y, 1112","7465","$0.00","The computing power of biology is incredible, evident in the natural world in the intricate patterns underlying materials and the body plan of animals.  Cells build these structures by using networks of interacting bio-molecules, encoded in their DNA, that function as microscopic computers, the power of which grows as many cells communicate to work together on a problem. The goal of this project is to significantly scale-up the ability to build these systems by design such that cells can be programmed to perform complex computational tasks.  This will be done by creating software that allows a user to write code, exactly as one would program a computer, which is then compiled to a DNA sequence. New theoretical tools will be applied to determine the power required by the cell to run these programs and how best to distribute tasks between circuits encoded in cells and conventional electronic systems. This research will broadly impact biotechnology, which is increasingly being used to commercially produce a wide range of products, from consumer goods to high-end advanced materials.  Current products do not harness the computational potential of cells; in other words, all the genes are turned on all the time.  This research will enable cells to be programmed to build chemicals and materials in multiple steps, both by performing the computations inside of the cells and also communicating across cells. This work is interdisciplinary and requires backgrounds in Biology, Chemistry, Mathematics, Biological Engineering, Electrical Engineering, and Computer Science. As such, the project includes the development of new educational platforms in anticipation of a need in industry for students trained at the interface between traditionally separated fields.  This includes a new undergraduate-level Synthetic Biology Design course, an industrial co-op, and curriculum material ""How to Grow Almost Anything,"" which will be made public at an international level.<br/><br/> To build the complexity of the natural world, cells use regulatory networks made up of interacting bio-molecules to control the timing and conditions for gene regulation. For the last 20 years, researchers have been able to build synthetic genetic circuits by artfully combining regulatory interactions. The problem is that the largest of such circuits only consist of ~10 regulators, far smaller than natural networks, which drastically limits the computation that can be performed. The proposed research will develop technologies that collectively enable a massive scale-up in computational complexity to ~10^5 regulators. The first objective seeks to increase the size of circuits within cells.  Logic gates based on Cas9 have enormous scale-up potential, but are limited by dCas9 toxicity and sequence repeats. A set of gates will be designed to fix these problems, guided by mathematical modeling.  A framework for design automation will be developed that enables a Verilog specification to be converted into a logic diagram, that is then divided up amongst many interacting cells. The second objective seeks to distribute a genetic circuit design across multiple communicating cells. The number and reliability of cell-cell communication signals will be improved by directed evolution to increase the number of channels from 2 to 8.  These will be implemented in living cells and non-living systems, thus enabling a broad range of applications inside and outside the bioreactor. Combined with 50 gates/cell, this platform offers the possibility of multicellular circuits containing 10^5+ gates. Some applications require deployment as a non-living system, for example when the application is outside of the lab, thus requiring containment. The third objective seeks to translate the parts developed in Objectives 1 and 2 to operate in multiple communicating lipid vesicles encapsulating cell-free protein extract. Cas9 gates and additional communication channels will be characterized to expand the computational potential. These will be characterized as gates and implemented using Electronic Design Automation tools to automate the design of large systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1453261","CAREER: Algorithmic Aspects of Machine Learning","CCF","ALGORITHMIC FOUNDATIONS","07/01/2015","08/09/2018","Ankur Moitra","MA","Massachusetts Institute of Technology","Continuing grant","Tracy J. Kimbrel","06/30/2020","$396,320.00","","moitra@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796","1045, 7926","$0.00","Algorithms and complexity are the theoretical foundation and backbone of machine learning. Yet over the past few decades an uncomfortable truth has set in that worst-case analysis is not the right framework to study it: every model that is interesting enough to use in practice leads to computationally hard problems. The goal of the PI's research agenda is to move beyond worst-case analysis. This involves formalizing when and why heuristics -- such as alternating minimization and Gibbs sampling -- work as well as designing fundamentally new algorithms for some of the basic tasks in machine learning. This project has already had a number of successes such as provable algorithms for nonnegative matrix factorization, topic modeling and learning mixture models.<br/><br/>The PI will investigate several new directions in topics such as sparse coding, inference in graphical models, inverse problems for tensors, and semi-random models. These projects will leverage a wide range of modern tools to give new provable algorithms in each of these settings, and will involve making new connections between alternating minimization and approximate gradient descent, analyzing Gibbs sampling through correlation decay and coupling, connecting tensor completion and quantum complexity and rethinking the standard distributional models used in machine learning. These projects cut across several areas of computer science and applied mathematics and will build new bridges between them, as well as expanding the reach of theory into a number of domains where there is a serious gap in our current understanding. Bridging theory and practice has significant broader impact.  The PI will continue to mentor undergraduate and graduate students."
"1547999","EAGER: Collaborative Research: Algorithmic design principles for programmed DNA nanocages","CCF","ALGORITHMIC FOUNDATIONS, SOFTWARE & HARDWARE FOUNDATION","08/01/2015","08/07/2015","Mark Bathe","MA","Massachusetts Institute of Technology","Standard Grant","Mitra Basu","07/31/2019","$155,000.00","","mark.bathe@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7796, 7798","7916, 7931, 7946","$0.00","3D printing has revolutionized the ability to fabricate complex solid objects at the macroscopic scale using simple Computer-Aided Design (CAD) files as input. In this process, the user specifies the solid object using simple geometric primitives or surface-based meshes. Recent applications of this revolutionary technology include printing limb prosthetics and implants and tissue engineering scaffolds, as well as rapid prototyping of products in industries ranging from apparel and eyeware to automotive, aerospace, and art. A similar transformation in automated fabrication began in the 1970s using CAD for the design of complex electronics using very large scale integration (VLSI) to design circuits consisting of thousands of transistors. This CAD revolution also dramatically increased and broadened the participation of designers without detailed technical know-how needed to design and synthesize custom electrical circuits for diverse applications in industries ranging from mobile devices to biomedical implants. At the nanometer-scale, programmed self-assembly of synthetic DNA offers a similar ability to ""print"" complex 3D nanometer-scale objects with precisely defined 3D structural features. While the field of structural DNA nanotechnology is considerably younger than the preceding examples, recent technological and scientific advances have enabled the low-cost and reproducible synthesis of diverse structured DNA nano-objects, enabling numerous technological innovations including casting metallic nanoparticles for photonics and light-harvesting devices, fabricating therapeutic vectors that mimic viruses for drug and gene delivery, and developing nanoscale sensors for biomarker detection in disease diagnosis. <br/><br/>Structural DNA nanotechnology currently faces a similar bottleneck in the broad participation of designers due to the need for automated CAD-based design software for these nano-objects. Here, development of a next-generation CAD framework is proposed to enable the fully automated design of structured DNA assemblies at the nanometer scale. As a starting point, the development of a CAD program is proposed here for the synthesis of a unique class of DNA-based objects called DNA nanocages. DNA nanocages can be programmed to adopt nearly arbitrary symmetries and sizes on this scale. Further, these DNA-based particles may be functionalized chemically with proteins, RNAs, chromophores, and other small molecules for diverse applications in biomolecular science and technology. In addition, these nanoscale materials can be transformed into structured inorganic materials including metals and silicon dioxide. To realize the aim of transforming the ability to design and fabricate DNA-based nanomaterials, an open-source software package will be developed to prescribe geometrically from the top-down nanocage size and symmetry using a simple high-level language and CAD environment that is distributed worldwide through the world-wide web. Synthetic DNA sequences that self-assemble to form these CAD-specified structures will be automatically generated for nanocage fabrication. Validation of nanocage synthesis will be performed experimentally using high-resolution structural and folding assays. This work forms the starting point for a new high-level programming language to print 3D objects at the nanometer-scale using synthetic DNA that will broadly enable the use and application of these assemblies across diverse research and industrial applications. Future work may extend this framework to arbitrary 2D and 3D DNA-based assemblies, as well as molecularly functionalized DNA-assemblies that mimic, as well as extend far beyond, nature's evolutionary designs."
"1645381","EAGER: Understanding cooperation through the zero-error relay channel","CCF","COMM & INFORMATION FOUNDATIONS","07/15/2016","07/08/2016","Natasha Devroye","IL","University of Illinois at Chicago","Standard Grant","Phillip Regalia","06/30/2019","$249,999.00","","devroye@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","7797","7916","$0.00","The research seeks to obtain the zero-error capacity a fundamental performance limit quantifying the number of messages that may be reliably communicated with no error of the primitive relay channel. In this channel, a source communicates with a destination in the presence of an out-of-band relay. Understanding how the source and relay may cooperate in communicating messages is interesting theoretically as the relay channel capacity has been open for over 40 years.  The research looks at the zero-error capacity, which differs from the more commonly studied small-error capacity, and leads to an unexplored combinatorial problem involving new relaying ""compression"" graphs. Progress in understanding how to cooperate from a mathematical, structural perspective is expected to be made by attacking the relay channel from this novel combinatorial angle. Understanding how to best relay is important in extending coverage of, or adding reliability to, wireless networks. Relaying is a part of existing standards such as 802.16j, 802.11ah, and 4G LTE Advanced.  The PI's outreach and educational efforts focus on remaining active in ""women in science"" retention and recruiting events and broadening awareness of information theory through non-specialist talks and tutorials. <br/><br/>Technically, the research  will first determine when the zero-error capacity of the primitive relay channel is non-zero. When the capacity is positive, the structure of the graphs that characterize the overall capacity  of the primitive relay channel and how to best cooperate will be sought. These are expected to be difficult-to-compute combinatorial quantities; the goal will then be to simplify the expressions for certain classes of channels, and to understand the properties of these graphs as a function of the block length and number of nodes."
"1421467","SHF: Small: Disruptive Hardware for Energy-Starved Autonomous Nano-Systems","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2014","07/24/2014","Pinaki Mazumder","MI","University of Michigan Ann Arbor","Standard Grant","Sankar Basu","07/31/2019","$400,000.00","","mazum@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","7923, 7945","$0.00","Mobile autonomous robotic insects such as dragon flies and virtual bugs that chiefly rely on energy scavenging frequently languish due to the excessive power dissipation by digital processors while executing the underlying software algorithms used in navigation through dynamic environments. This NSF research will study the underlying adaptive control-theoretic software algorithms to replace them by energy-efficient hardware capable of brain-like reinforcement learning. The proposed biology-inspired hardware mounted on robotic insects will be beneficial to the defense related applications, first responders to disaster areas for remote monitoring, and mapping of hostile and hazardous environments. Such algorithmic hardware will also benefit applications in flight control, biochemical processes, power generators, and telecommunications. Educational improvement and research integration is a major goal of this integrative and cross-disciplinary research, thereby providing opportunity to upgrade the current Computer Science and Engineering curriculum and train engineering workforce of the future who will apply knowledge from multiple disciplines to design ultra-low-power autonomous nanosystems. <br/><br/>Adaptive dynamic programming (ADP) algorithms are used in many large-scale engineering applications involving adaptive optimal control systems and signal processing. However, the success of ADP on the microprocessor has limited its scope in mobile computing and autonomous robotic insects where the battery energy preservation is paramount. Reasonable ADP algorithms cannot run on portable low power machines because ADP needs a large memory bank and requires parallel processing for a reasonable runtime. This research aims to develop methods for porting higher-level algorithms from software implementation on the microprocessor to a mixed signal CMOS chip design at first and then an ultra-low-energy chip design by combining CMOS and memristor technologies."
"1442773","CyberSEES: TYPE 2: Sustainably Unlocking Energy from Municipal Solid Waste Using a Sensor-Driven Cyber-Infrastructure Framework","CCF","CyberSEES","09/01/2014","08/06/2014","Dimitrios Zekkos","MI","University of Michigan Ann Arbor","Standard Grant","Bruce K. Hamilton","08/31/2019","$1,199,600.00","Jerome Lynch, Edwin Olson","zekkos@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8211","8020, 8208","$0.00","ABSTRACT<br/>Our nation's practices in managing the growing amounts of Municipal Solid Waste (MSW) that are generated every year are unsustainable. The majority of MSW generated every year is still disposed of in landfills despite national and international efforts aimed to increase recycling. In modern landfills, MSW is treated as a material to be isolated and contained. Current MSW management strategies cause sub-optimal degradation of landfill waste resulting in the generation of biogases (primarily methane and carbon dioxide) that are mostly flared, vented or leaked to the atmosphere where they remain as greenhouse gases (GHG). As a result, landfills represent the second largest anthropogenic source of methane in the US. Fortunately, MSW has high energy potential that remains virtually untapped as a national energy resource. The overarching goal of this research is to revolutionize how MSW is managed to provide a transformative means of extracting utility-scale energy from waste using next-generation facilities to be termed Sustainable Energy Reactor Facilities (SERFs). This paradigm-shift is only recently possible through the adoption of innovative computing technologies such as high-performance computing for multi-domain process modeling, low-cost autonomous sensor networks, and unmanned autonomous vehicles (UAVs), all synergistically integrated within a customized cyber-environment.  This integration of in-situ SERF observation with high-performance computing allows the energy generation capacity of SERF to be maximized resulting in lower cost energy production with a dramatic reduction in GHG and carbon footprint compared to traditional dry-tomb landfills. <br/><br/>SERFs will be designed with two objectives: maximize energy recovery and minimize environmental impact. The explicit objective of maximizing energy generation will necessitate a significant deviation from modern MSW management practices which are based on empirical methods.  SERFs are only possible through environmental sensing and modeling of physical-chemical-biological processes occurring within a landfill. At the core of the SERF technology will be complex, multi-domain computational performance models (CPMs) that require execution in near real-time and consider these processes over varying spatial and temporal scales. CPM is enabled by high-performance computing platforms that can update and execute the CPMs using in-situ observations of MSW processes collected by field deployed wireless sensor networks. Model uncertainty can be further reduced through the introduction of ground-based and aerial mobile sensing platforms whose paths are optimally planned using CPM model uncertainty and platform constraints (e.g., energy) within the same minimizing objective function. With CPM models updated, energy generation can be predicted by SERF owners with energy extraction maximized by the injection of septage and leachate into the SERF. A multidisciplinary team of researchers with expertise in landfill design and modeling and researchers from computer science will work in close collaboration with an Industrial Advisory Board (IAB) of major waste industry stakeholders (i.e., waste management companies, industry consultants, and government regulators). Research, educational and outreach activities are integrated through a virtual ""hub"". The IAB will provide guidance on decisions pertaining to the project's research and education activities. Activities are planned to promote education of the society-at large, integrate undergraduate and graduate education and research, and nurture a well-equipped future domestic workforce to manage and advance SERF technology. An award-winning journalist will also be engaged in training engineering students in efficiently communicating with broad audiences complex engineering matters, and in evaluating the proposed web-based resources (videos and animations). In addition, the research team will partner with a team of Chinese researchers leading to international technology, education and cultural exchanges."
"1650069","EAGER: Constructive Univalent Foundations","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2016","08/04/2016","Robert Constable","NY","Cornell University","Standard Grant","Anindya Banerjee","08/31/2019","$294,998.00","","rc@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7798","7916, 7943","$0.00","Proof assistants can express any precise computing task in formal systems of logic. They specify exactly how hardware and software are required to operate and use the specifications to guide implementations. Computer scientists using these tools have achieved over the past two decades the highest levels of correctness known. Proof assistants draw on advanced mathematics; they have been used to check the validity of several deep mathematical results and discover new ones. In this process, leading mathematicians contributed powerful new concepts into the formal logics. Fields Medalist Vladimir Voevodsky proposed a new mathematical principle, called Univalence. It has been an open question whether this principle has computational interpretations. That question was affirmatively answered recently by the PI and his collaborators. The intellectual merits of this project consist in formal verification of the correctness of these interpretations using a proof assistant. The project's broader significance and importance are to enhance their impact on many critical applications of computing systems in science, engineering, and industry and to explore their use in teaching Euclidean geometry to high school students.<br/><br/>This research project on Constructive Univalent Foundations significantly raises the levels of abstraction that proof assistants can support, and increases the range of problems they help solve. Preliminary results by the PI and his team provide strong evidence that Univalent Foundations and the computational type theory using it will significantly improve the capabilities of proof assistants and thus the reliability and capabilities of hardware and software on which modern computing systems depend, from the desktop to the cloud. The project formally verifies the correctness of the computational interpretations of Univalence in the proof assistant Nuprl that has been developed by the PI and his collaborators over the last three decades."
"1563757","AF: Medium: Collaborative Research: The Power of Randomness for Approximate Counting","CCF","ALGORITHMIC FOUNDATIONS","09/01/2016","09/19/2017","Daniel Stefankovic","NY","University of Rochester","Continuing grant","Rahul Shah","08/31/2020","$311,118.00","","stefanko@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7796","7924, 7926","$0.00","The study of the complexity of counting problems has a long and rich history in theoretical computer science. Counting problems (and closely related sampling problems) arise naturally in many different fields, for example in statistical physics they correspond to partition functions and for studies of the equilibrium states of idealized models of physical systems, and in Bayesian inference they arise for the study of posterior distributions or maximum likelihood distributions. The specific questions addressed here are long-standing open problems, progress on which will be of wide interest. The project will develop new tools for approximate counting and is likely to make new and useful connections between statistical physics, probability and computational complexity. The research results will be disseminated via course notes, a summer school and workshops. Any practical algorithms that result will be made publicly available.<br/><br/>The overall goal of the project is to extend the known boundary of polynomial-time tractability for counting problems, to understand whether randomness is essential and how it could be eliminated, and to push the limits of the current fastest randomized algorithms towards practicality. Specific aims include: (1) Polynomial-time randomized approximation schemes for some fundamental problems that have thus far eluded efficient solutions, (2) Deterministic polynomial-time approximation schemes for some central problems that have celebrated randomized algorithms and (3) Faster randomized algorithms for classical counting problems."
"1422649","SHF: Small: Mainstream Transactional Memory","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2014","07/02/2014","Michael Scott","NY","University of Rochester","Standard Grant","Anindya Banerjee","08/31/2019","$499,415.00","","scott@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7798","7923, 7943","$0.00","Title: SHF: Small: Mainstream Transactional Memory<br/><br/>Transactional Memory (TM) is the union of two transformative ideas: first, that parallel programming will be easier if programmers can simply specify which operations in their code should be atomic, without specifying how to make them atomic; second, that this simplicity can be supported -- and performance often improved -- by a speculative implementation that executes atomic blocks in parallel, and backs out and retries when -- and only when -- those blocks conflict with one another.  After many years of research, TM is now entering widespread use.  Hardware support is commercially available from both IBM and Intel; software support is standard in Haskell and under consideration in several other programming languages -- notably C++.  The sponsored research extends the state of the art in transactional memory by focusing on (1) software acceleration of fast hardware transactions and (2) hardware acceleration of rich software transactions.<br/><br/>The intellectual merits in focus area 1 comprise compiler-based techniques to increase speculation success rates, by safely and automatically moving commonly conflicting operations out of transactions, and by ""pipelining"" execution to serialize the remaining causes of conflict.  The intellectual merits in focus area 2 comprise enhancements to the STM run-time system for the Haskell programming language, where hardware support can be used to accelerate transactions whose semantics are too complex to implement directly with commercial hardware. The broader impacts begin with easier construction of correct, efficient parallel code that will allow programmers of all skill levels to write that code more easily. Moreover, the work will impact computer science and allied fields by smoothing the transition to ubiquitous multithreading, thereby extending performance improvements through the next generation of computing. In summary, the work will lead to progress in almost any domain that is driven by parallel computing, across academia and industry."
"1439008","XPS: FULL: SDA: Collaborative Research: SCORE: Scalability-Oriented Optimization","CCF","Exploiting Parallel&Scalabilty","09/01/2014","07/17/2014","Emery Berger","MA","University of Massachusetts Amherst","Standard Grant","Anindya Banerjee","08/31/2019","$648,000.00","","emery@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","8283","","$0.00","Title: XPS: FULL: SDA: Collaborative Research: SCORE: Scalability-Oriented Optimization<br/><br/>Modern CPUs, which contain an increasingly large number of processing units or ""cores"", offer the promise of continued increases in performance as the number of cores increases. Unfortunately, it is notoriously difficult for programmers to fully take advantage of this processing power. Computations can be viewed as cars on a network of highways: we want traffic to flow as fast as possible without any crashes. Programming languages offer ""synchronization operations""---the programmer equivalent of traffic lights---which improve safety but reduce speed. For large programs, managing the tension between the twin goals of safety (more lights) and performance (fewer lights) can be out of reach for all but expert programmers. This project, SCORE (scalability-oriented optimization), lifts this burden by automatically maximizing program performance while maintaining correctness. The intellectual merits of this work are the development of a suite of techniques to identify bottlenecks in programs, and transform their code or execution environment to eliminate those bottlenecks. The project's broader significance and importance are to enable non-expert programmers to achieve high performance on modern, multicore platforms, and thus dramatically increase the performance and efficiency of existing and new software; contributing to the national software research infrastructure; and increasing access to science research opportunities and training for students.<br/><br/>As with optimizing compilation for sequential code, SCORE lifts the burden of concurrency optimization from programmers, letting them focus exclusively on getting the logic of their program right. By handling architectural and synchronization optimizations without programmer involvement, SCORE lets programmers deliver applications that portably and effectively harness a wide range of multicore architectures. SCORE comprises a suite of new dynamic analyses, static analyses, and runtime systems to enable scalability-oriented optimization. It uncovers bottlenecks and ranks them by the performance impact of removing them. This information guides a bottleneck-remediation dynamic analysis to identify a range of opportunities for concurrency optimizations. Finally, a code robustification phase augments the optimized code with lightweight checking and recovery code to ensure correct execution."
"1730088","Collaborative Research: EPiQC: Enabling Practical-Scale Quantum Computation","CCF","EXPERIMENTAL EXPEDITIONS","03/01/2018","08/01/2018","Danielle Harlow","CA","University of California-Santa Barbara","Continuing grant","Almadena Y. Chtchelkanova","02/28/2023","$141,668.00","","dharlow@education.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7723","7723","$0.00","Quantum computing sits poised at the verge of a revolution. Quantum machines may soon be capable of performing calculations in machine learning, computer security, chemistry, and other fields that are extremely difficult or even impossible for today's computers. Few of these limitless possibilities on the horizon that quantum computing could lead to are better drug discovery, more efficient photovoltaics, new nanoscale materials, and perhaps even more efficient food production. These benefits will be enabled by substantially improving the ability to solve computational problems in quantum chemistry, quantum simulation, and optimization. These dramatic improvements arise because each additional quantum bit doubles the potential computing power of a machine, accumulating exponential gains that could eventually eclipse the world's largest supercomputers. Quantum computing will also drive a new segment of the computing industry, providing new strategies for specific applications that increase computational power even as physical limits slow improvements in classical silicon-chip technology. This multi-institutional project, Enabling Practical-scale Quantum Computing (EPiQC) Expedition, will help bring the great potential of this new paradigm into reality by reducing the current gap between existing theoretical algorithms and practical quantum computing architectures. Over five years, the EPiQC Expedition will collectively develop new algorithms, software, and machine designs tailored to key properties of quantum device technologies with 100 to 1000 quantum bits. This work will facilitate profound new scientific discoveries and also broadly impact the state of high-performance computing. To prepare the U.S. workforce for this revolution in computing, we need to educate citizens to think about computing from a quantum perspective, integrating concepts such as probability and uncertainty into the digital lexicon. The EPiQC Expedition will design teaching curricula and distribute exemplar materials for students ranging from primary school to engineers in industry. EPiQC will also establish an academic-industry consortium which will share educational and research products and accelerate the pace of quantum computing design and applications.  <br/><br/>Because quantum computing is a new branch of computer science, it will require entirely new types of algorithms and software. In order to produce practical quantum computation in the near future, these elements cannot be developed in isolation. Instead, researchers must increase the efficiency of quantum algorithms running on quantum machines through the simultaneous design and optimization of algorithms, software and machines. New algorithms and software need to know what specific machine operations are easy or difficult in a given quantum technology and must be prepared to produce useful answers from imperfect results from imperfect machines. Software also needs to verify that the computation executed correctly as expected, an especially difficult task given that conventional machines cannot simulate even a modest-size quantum machine. The EPiQC Expedition unites experts on algorithms, software, architecture, and education to develop these elements in parallel. Overall, EPiQC will increase the efficiency of practical quantum computations by 100 to 1000 times, effectively bringing quantum computing out of the laboratory and into practical use 10-20 years sooner than through technology advances alone. The project identifies 4 thrusts: algorithmic innovations, compiler development, verification, and the broader impact tasks of developing education modules. The algorithmic tasks are organized into the subdomains of optimization, computational chemistry, and the discovery of separations between quantum and classical speedup. The compiler tasks are more milestone driven - development of technology libraries, development of various compilation techniques which leverages these libraries, as well as novel error correction schemes. The project will tie the tool chain closely to the underlying hardware and fault-tolerance mechanisms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1618475","CIF: Small: Collaborative Research: Perishable Network Information Flow","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/27/2016","Chih-Chun Wang","IN","Purdue University","Standard Grant","Phillip Regalia","06/30/2019","$249,854.00","","chihw@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7923, 7935","$0.00","End-to-end delay is an inherent important metric of all modern communication systems. As the end-to-end delay requirement is becoming more stringent (5G wireless technology is aiming for a latency of less than one millisecond) and as the improvement of local delay is quickly approaching the physical limits of the individual components, it is of paramount interest to study the fundamental limits of end-to-end delay from a network's perspective. Specifically, what is the largest throughput one can transmit over a multihop network while respecting a small end-to-end delay requirement? The research in this project will uncover new delay-aware fundamental limits of network communication and computation. Such results will have a basic impact on information theory, algorithmic graph theory, and communication complexity theory and promote bridging and sharing across technical disciplines that are normally studied by different scientific communities. The design of low-delay schemes will also have immediate and long-standing impact on many practical applications, including Internet of Things, cyber-physical systems, video conferencing, and mobile gaming. The project also includes plans for integrating the research into graduate-level lecture materials.<br/><br/>The research in this project is centered around answering the question ?what is the information-theoretic capacity of a networked system under a stringent (small) finite end-to-end delay requirement??. The distinct nature of this problem requires a cross-disciplinary approach spanning graph-theory, computer science, and information theory. This project develops various new tools including new traffic models, new probabilistic frameworks, and new distributive functional computation paradigms and provides a long overdue theoretic study that explores both the graph-theoretic and information-theoretic aspects of perishable network information flow. The research is divided into two thrusts: one focused on end-to-end delay-constrained multihop network capacity and the other focused on delay-constrained distributed function computation."
"1617745","CIF: Small: Collaborative Research: Perishable Network Information Flow","CCF","COMM & INFORMATION FOUNDATIONS","07/01/2016","06/27/2016","Pramod Viswanath","IL","University of Illinois at Urbana-Champaign","Standard Grant","Phillip Regalia","06/30/2019","$250,000.00","","pramodv@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797","7923, 7935","$0.00","End-to-end delay is an inherent important metric of all modern communication systems. As the end-to-end delay requirement is becoming more stringent (5G wireless technology is aiming for a latency of less than one millisecond) and as the improvement of local delay is quickly approaching the physical limits of the individual components, it is of paramount interest to study the fundamental limits of end-to-end delay from a network's perspective. Specifically, what is the largest throughput one can transmit over a multihop network while respecting a small end-to-end delay requirement? The research in this project will uncover new delay-aware fundamental limits of network communication and computation. Such results will have a basic impact on information theory, algorithmic graph theory, and communication complexity theory and promote bridging and sharing across technical disciplines that are normally studied by different scientific communities. The design of low-delay schemes will also have immediate and long-standing impact on many practical applications, including Internet of Things, cyber-physical systems, video conferencing, and mobile gaming. The project also includes plans for integrating the research into graduate-level lecture materials.<br/><br/>The research in this project is centered around answering the question ?what is the information-theoretic capacity of a networked system under a stringent (small) finite end-to-end delay requirement??. The distinct nature of this problem requires a cross-disciplinary approach spanning graph-theory, computer science, and information theory. This project develops various new tools including new traffic models, new probabilistic frameworks, and new distributive functional computation paradigms and provides a long overdue theoretic study that explores both the graph-theoretic and information-theoretic aspects of perishable network information flow. The research is divided into two thrusts: one focused on end-to-end delay-constrained multihop network capacity and the other focused on delay-constrained distributed function computation."
"1563744","SHF: Medium: Collaborative Research: Toward Extreme Scale Fault-Tolerance: Exploration Methods, Comparative Studies and Decision Processes","CCF","SOFTWARE & HARDWARE FOUNDATION","08/01/2016","07/21/2016","Thomas Herault","TN","University of Tennessee Knoxville","Standard Grant","Almadena Y. Chtchelkanova","07/31/2020","$530,000.00","","herault@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","7798","7924, 7942, 9150","$0.00","Current high-performance computing (HPC) research target computer systems with exaflop (1018 or a quintillion floating point operations per second) capabilities. Such computational power will enable new, important discoveries across all basic science domains. Application resilience to computer faults and failures is a major challenge to the realization of extreme scale computing systems. This project, Simulation and Modeling for Understanding Resilience and Faults at Scale (SMURFS), addresses this challenge by developing methods to improve our predictive understanding of the complex interactions amongst a given application, a given real or hypothetical hardware and software system environment, and a given fault-tolerance strategy at extreme scale. Specifically, SMURFS develops:<br/>1. New simulation and modeling capabilities for studying application resilience at scale;<br/>2. Capabilities to execute a comprehensive set of comparative fault-tolerance studies; and<br/>3. Effective prescriptions to guide application developers, hardware architects and system designers to realize efficient, resilient extreme scale capabilities.<br/><br/>SMURFS explores the impact of faults and failures, fault mitigation strategies and emerging technologies by providing new analytical and component models for predicting fault-tolerant application behavior at scale. The Iron simulation framework integrates these models for validation and comprehensive performance studies over a wide range of representative applications, application proxies, fault-tolerance protocols and hardware configurations. These studies inform a rule-based system for prescribing best fault-tolerance practices and configurations for new candidate applications and scenarios.<br/><br/>SMURFS renders (1) new simulation and analytical models that predict application performance at scale; (2) detailed understandings of how application features interplay with different fault-tolerance strategies and hardware technologies; (3) new knowledge about application behavior at scale; and (4) valuable insight and prescriptions for designing, developing and deploying future extreme scale HPC systems.<br/><br/>More broadly, artifacts like the Iron framework and the public suite of application traces will be valuable to the HPC research, engineering, development, procurement and administrative communities. Researchers can use these artifacts for their own research that can impact the HPC exploration and design space.  For example, this framework can be instrumental in the co-design of cohesive extreme scale applications, software environments and hardware platforms. Additionally, Iron-based research can inform and improve scientific computing practices, accelerating the rate of scientific discovery.  Finally, Iron will be useful as an instructional device to teach about HPC issues both in classroom and tutorial contexts and other programs that engage diverse populations of middle, high school and college students in New Mexico and Tennessee."
"1509178","AF: Medium: Algorithmic Complexity in Computation and Biology","CCF","ALGORITHMIC FOUNDATIONS","07/01/2015","06/09/2015","Leslie Valiant","MA","Harvard University","Standard Grant","Tracy J. Kimbrel","06/30/2019","$900,000.00","","valiant@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7924, 7927","$0.00","Computational complexity is the field that studies how much resources are needed for performing computational tasks. Its primary focus has been to understand how many steps and how much storage space is required for performing various important tasks on conventional computers. Biological processes, can also be viewed as computations to the extent that they consist of step-by-step processes that follow certain rules, and can then be also studied from the perspective of the theory of computational complexity. This proposal is concerned with studying both of these aspects. Its goal is that of proving, by mathematical means, upper or lower bounds on the resources needed for both general purpose and evolutionary computations.<br/><br/>While much is understood about the complexity of computations on general purpose computers, this understanding is pivoted around a small number of critical open questions, such as the P=?NP question, the answer to which would resolve the resource requirements of numerous important tasks. The first focus of this study will be algebraic approaches to these questions, in which the limitations on computations imposed by algebraic axioms is analyzed. Holographic algorithms have over the last decade yielded novel algorithms for a variety of problems, as well as new lower bound arguments, and also new techniques for proving computational equivalence among apparently dissimilar problems. The goal of the research is to understand the inherent limits of holographic algorithms and to use this understanding to develop efficient algorithms. Darwinian evolution can be also viewed as a computational process that uses quantifiable resources, here measured in terms of numbers of generations, size of populations, and the number of experiences of individuals. Recently it was shown that the Darwinian mechanism can be viewed as a form of machine learning, the field of computer science that studies systems in which most of the information is acquired from experience and not from a programmer. The goal of the research is to understand what classes of functions, such as those occurring in protein expression networks, can so evolve using practicable resources.  Graduate students will be involved in these projects."
"1525754","SHF: Small: Solving the Problems of Scalability and Portability while Maximizing Performance of Multiprecision Scalar and Vector Arithmetic on Clusters of GPUs","CCF","SOFTWARE & HARDWARE FOUNDATION","07/15/2015","07/01/2015","Charles Weems","MA","University of Massachusetts Amherst","Standard Grant","Almadena Y. Chtchelkanova","06/30/2019","$400,000.00","","weems@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7798","7923, 7942","$0.00","This project extends the PI's prior research into achieving high performance for multiprecision arithmetic utilizing commodity graphics processors (GPUs). Multiprecision (MP) arithmetic has important applications in science, engineering, and mathematics when computations require greater numerical precision than standard computer systems support. It is also an important part of cryptography used in secure internet communication. GPUs can accelerate MP arithmetic by more than two orders of magnitude. However, achieving this performance requires novel algorithms and software tools. The world-record performance for exponentiation achieved under the prior grant will be extended to include floating point vector arithmetic. A new code generation model will enable handling a wider range of precisions across newer generations of graphics processors. Support for clusters of GPUs to work together on larger problems, and practical demonstrations of the effectiveness of MP library such as showing how one GPU can offload decryption work from more than a hundred servers, with higher levels of security than are currently in common use, is being developed. <br/><br/>Each generation of GPU architecture requires extensive experimentation and reworking of multiprecision code to obtain a new optimum. Yet the potential benefits of a portable and scalable package could be transformational in certain application areas. This effort extends PI's prior work to include floating point and vectors, and begin the transition to GPU clusters. The result will be a publicly available multi-precision arithmetic package and implementation toolset that enables the scientific community to easily take full advantage of GPU scaling to obtain at least an order of magnitude improvement in performance per dollar and performance per watt over CPUs at the same technology step. The approach relies on a novel set of models for GPU storage that provide a higher level of abstraction over which the code generation tools can search for optimal combinations of algorithm, register/memory layout, and kernel launch geometry for a given precision size and GPU architectural generation to achieve maximum resource utilization."
"1619062","SHF: SMALL: Multiphysics Simulation Algorithms and Experimental Methods for the Development of Cu/Graphene/TMD Hybrid Interconnect Solution","CCF","SOFTWARE & HARDWARE FOUNDATION","06/15/2016","06/10/2016","Dan Jiao","IN","Purdue University","Standard Grant","Sankar Basu","05/31/2019","$450,000.00","Zhihong Chen","djiao@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","7923, 7945","$0.00","As integrated circuits (ICs) have progressed to finer feature sizes and higher levels of integration, interconnect has become a major challenge in IC technology development. Various interconnect solutions developed to date have not yet been able to replace Copper (Cu) interconnects due to their limitations in performance and/or difficulties in fabrication. In this project, novel Copper, Graphene, Transition Metal Dichalcogenide (TMD) hybrid interconnects will be demonstrated and explored by both simulations and experiments, which can compete with Cu nanowires with the same dimensions, and meanwhile have the potential to become the ultimate interconnect solution. The students who participate in this work will be trained with a broad range of skills to sustain future nanotechnology advancement. Education and outreach activities such as inclusive curriculum and culturally relevant teaching, Nano Days in the Birck Nanotechnology Center at Purdue, are also being developed to actively engage women and under-represented minorities into Science and Engineering.<br/><br/>The entire physical process that takes place in a Cu/Graphene/TMD interconnect is yet to be fully understood. The challenge is to understand intricate interactions between multiphysics phenomena involving circuits, electromagnetics, materials, electron transport, and thermal diffusion in a broad band of frequencies. Experimentally, a rapid low-temperature process is critical to preserve the structural integrity of the Cu nanowires during deposition and will be studied. To conquer the aforementioned challenges, multiphysics simulation algorithms and experimental methods will be pursued. On the experimental side, a low temperature chemical vapor deposition recipe, along with a plasma enhanced process, will be developed. Electrical and thermal transport measurements will be performed and correlated with simulations."
"1526335","AF:Small:Collaborative Research:Making Computational Geometry Polynomial in Derivation Length and in Dimension","CCF","ALGORITHMIC FOUNDATIONS","07/01/2015","06/26/2015","Victor Milenkovic","FL","University of Miami","Standard Grant","Rahul Shah","06/30/2019","$249,983.00","","vjm@cs.miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","7796","7923, 7929","$0.00","Computational geometry is the discipline that creates algorithms to<br/>design and manipulate shapes.  It has wide application in science,<br/>engineering, and industry -- CAD/CAM systems are a notable example.<br/>The problem is, where we see shapes and their spatial relations, the<br/>computer sees only numbers -- and usually, for speed, only<br/>approximate, ""floating-point"" numbers.  E.g., points on a line that<br/>are represented as numerical coordinates, when rounded to the<br/>number of digits the computer stores, may no longer lie on any common<br/>line.  Subsequent calculations that rely on a line being straight or<br/>two lines intersecting at most once can then go badly astray.  Error<br/>in numerical computation can sometimes be limited by rounding, but<br/>there is no practical technique for rounding three-dimensional shapes.<br/><br/>This project investigates shape rounding by encasement:<br/>High-complexity shapes are encased in approximating polyhedra with<br/>floating-point vertex coordinates.  Given an encasement, the PIs have<br/>described a rounding algorithm that projects input features to nearby<br/>encasement features. For dealing with intersections of surfaces, the<br/>project creates output-sensitive algorithms for another type of<br/>encasement, an isolating encasement that can have distant boundary<br/>vertices but cannot encase other features.<br/><br/>Encasement is only part of the solution, so the project also explores<br/>ways to compute topological structure using bounded-complexity<br/>arithmetic by avoiding numerical computation for the degenerate (zero)<br/>expressions.  It investigates using graph theory to analyze explicit<br/>expressions and algebraic techniques to analyze expressions involving<br/>roots of polynomials.<br/><br/>The outcome is to include a software library for implementing<br/>computational geometry algorithms with automated shape rounding.  The<br/>project integrates education and research through an introductory<br/>computational geometry course in which standard algorithms are taught<br/>and implemented using the library.  Previously, the computational cost<br/>of multi-step algorithms forced students to consider each algorithm in<br/>isolation."
"1815328","AF: Small: Approximate Counting, Stochastic Local Search and Nonlinear Dynamics","CCF","ALGORITHMIC FOUNDATIONS","06/01/2018","05/21/2018","Alistair Sinclair","CA","University of California-Berkeley","Standard Grant","Rahul Shah","05/31/2021","$500,000.00","","sinclair@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7796","7923, 7926","$0.00","Theoretical Computer Science (TCS) is concerned with core questions in algorithms and complexity theory, and also with many questions in other scientific disciplines viewed through the so-called ""computational lens."" This refers to the study of many scientific phenomena that are fundamentally computational in nature and can therefore benefit from a TCS perspective. This project addresses both core topics and connections with other disciplines, notably statistical physics and population genetics. A unifying theme in the project is techniques for the analysis of random and physical processes arising in all of these fields. The project will engage as collaborators experts in fields such as complex analysis, applied probability and statistical physics to work on these interface areas. The project affords many opportunities for graduate student and postdoctoral training and ideas from this research will find their way into course curricula. The investigator also maintains a strong interest in undergraduate and K-12 teaching, the latter through his involvement with the Berkeley Math Circle.<br/><br/>On a more technical level, the project contains three broad themes:<br/><br/>1. Approximate counting: the development of approximation algorithms for counting problems, with special emphasis on the emerging technique of ""geometry of polynomials"" and its connections to the more classical techniques of Markov chain Monte Carlo and correlation decay. Applications include generating functions in combinatorics, partition functions in statistical physics, the computation of volumes, and the analysis of graphical models in machine learning.<br/><br/>2. Stochastic local search algorithms: the study of a novel framework, arising from recent advances on the algorithmic Lovasz Local Lemma, for the design and analysis of stochastic local search algorithms, which search for combinatorial structures with desired properties by successively eliminating ""flaws"". The new framework is based on matrix norms and is inspired by linear time invariant analysis in control theory.<br/><br/>3. Nonlinear dynamics: the study of computational aspects of two distinct, but related nonlinear dynamical systems: mass action kinetics and the ""red-queen"" dynamics. Mass action kinetics is a standard model for chemical reaction networks, and also captures other processes such as the Boltzmann equation in statistical physics, genetic algorithms and recombination in population genetics. The more speculative red-queen dynamics is a novel model for the evolution of multiple species under selection.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1704656","AF: Large: Collaborative Research: Nonconvex Methods and Models for Learning: Towards Algorithms with Provable and Interpretable Guarantees","CCF","ALGORITHMIC FOUNDATIONS","06/01/2017","04/20/2018","Rong Ge","NC","Duke University","Continuing grant","Rahul Shah","05/31/2022","$202,246.00","","rongge@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796","7925, 7926, 9251","$0.00","Artificial Intelligence along with Machine Learning are perhaps the most dominant research themes of our times - with far reaching implications for society and our current life style. While the possibilities are many, there are also doubts about how far these methods will go - and what new theoretical foundations may be required to take them to the next level overcoming possible hurdles. Recently, machine learning has undergone a paradigm shift with increasing reliance on  stochastic optimization to train highly non-convex models -- including but not limited to deep nets. Theoretical understanding has lagged behind, primarily because most problems in question are provably intractable on worst-case instances. Furthermore, traditional machine learning theory is mostly concerned with classification, whereas much practical success is driven by unsupervised learning and representation learning. Most past theory of representation learning was focused on simple models such as k-means clustering and PCA, whereas  practical work uses vastly more complicated models like autoencoders, restricted Boltzmann machines and deep generative models. The proposal presents an ambitious agenda for extending theory to embrace and support these practical trends, with hope of influencing practice. Theoretical foundations will be provided for the next generation of machine learning methods and optimization algorithms. <br/><br/>The project may end up having significant impact on  practical machine learning, and even cause a cultural change in the field -- theory as well as practice -- with long-term ramifications. Given the ubiquity as well as  economic and scientific implications of machine learning today, such impact will extend into other disciplines, especially in (ongoing) collaborations with researchers in neuroscience. The project will train a new generation of machine learning researchers, through an active teaching and mentoring plan at all levels, from undergrad to postdoc. This new generation will be at ease combining cutting edge theory and applications. There is a pressing need for such people today, and the senior PIs played a role in training/mentoring several existing ones.<br/> <br/>Technical contributions will include new theoretical models of knowledge representation and semantics, and also frameworks for proving convergence of nonconvex optimization routines. Theory will be developed to explain and exploit the interplay between representation learning and supervised learning that has proved so empirically successful in deep learning, and seems to underlie new learning paradigms such as domain adaptation, transfer learning, and interactive learning. Attempts will be made to replace neural models with models with more ""interpretable""  attributes and performance curves.  All PIs have a track record of combining theory with practice. They  are also devoted to a heterodox research approach, borrowing from all the past phases of machine learning: interpretable representations from the earlier phases (which relied on logical representations, or probabilistic models), provable guarantees from the middle phase (convex optimization, kernels etc.), and an embrace of nonconvex methods from the latest deep net phase. Such eclecticism is uncommon in machine learning, and may give rise to new paradigms and new kinds of science."
"1834336","Inaugural TCS Women Meeting at Symposium of Theory of Computing 2018","CCF","SPECIAL PROJECTS - CCF, ALGORITHMIC FOUNDATIONS","05/01/2018","05/08/2018","Barna Saha","MA","University of Massachusetts Amherst","Standard Grant","Tracy J. Kimbrel","04/30/2019","$5,000.00","","barna@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","2878, 7796","7926, 7927, 9102","$0.00","This award will support student attendance at the Inaugural TCS Women Event which will be co-located with the 50th Annual Association for Computing Machinery (ACM) Symposium on the Theory of Computing (STOC), June 25-29, 2018. Scientists all over the world are now increasingly acknowledging the need for diversity. Many diversity-related workshops are being organized with the goal of building community, increasing camaraderie, and helping minorities to share their experience and receive mentoring. While these workshops and meetings of minority communities are extremely important, it is equally important to co-locate these meetings with major technical conferences whenever possible. In this way, the issues of the minorities will get heard by the broader research community. In addition,  the number of participants of women and other minorities are dwindling in many conferences. Instead of being an enjoyable and learning experience, attending conferences often becomes an intimidating experience for them, especially in their early career.<br/><br/>STOC is one of the flagship annual research conference in the field of Theoretical Computer Science (TCS). STOC features presentations covering all aspects of TCS and encourages papers that broaden the reach of theory. Moreover, the co-located theory festival organizes invited sessions with the best theoretical papers presented outside the TCS community. This award will provide partial support to approximately 10 students to attend this leading conference as well as the TCS Women event which will consist of a panel of senior female researchers and a research rump session, and will provide opportunities for network-building among women researchers in TCS.  It will contribute towards attracting more under-represented groups to TCS research and will provide a positive environment for their future professional development.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1652655","CAREER:  A Dynamic Program Monitoring Framework Using Neural Network Hardware","CCF","SOFTWARE & HARDWARE FOUNDATION","04/15/2017","09/20/2017","Abdullah Muzahid","TX","University of Texas at San Antonio","Continuing grant","Yuanyuan Yang","03/31/2022","$174,423.00","","abdullah.muzahid@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","7798","1045, 7941","$0.00","Software bugs and security attacks cripple US economy by costing more than $150 billion a year. However, there has been no major innovation in this context. This research project aims to change that fact with the help of neural network based hardware. If the project is successful, it will significantly affect current industry <br/>practices and spur a new trend. It will encourage companies to invest in new techniques for debugging and security attack analysis using neural network hardware and make a compelling use case for the hardware implementation, thereby influencing continuous investment in neural network hardware. In addition, the project will contribute to the research and educational activities of a minority serving institution. Students will be tightly integrated into the project through dissertation, thesis work, and undergraduate research work. The PI will incorporate emerging architecture design and its programming in undergraduate and graduate coursework. Moreover, the PI will involve local high school students in computer science related projects through summer internships.<br/><br/>Neural network is a machine learning technique that mimics human brain. Therefore, neural network hardware provides some unique capabilities that can be utilized in many different ways. This project proposes to utilize neural network hardware for ""program monitoring"". Program execution monitoring is often used to detect software bugs, performance issues, security attacks etc. Neural network hardware will learn the normal ""behavior"" of the program. Then it will detect any deviation of such behavior. Such deviation can be attributed to software bugs, performance issues or security attacks. The proposed approach provides a general framework for handling these issues. Due to online learning and testing capability of neural network hardware, the framework will be adaptive to any change in program inputs, code, and platforms."
"1809703","AF: Student Travel to Clay Mathematics Institute Complexity Workshop","CCF","INFORMATION TECHNOLOGY RESEARC, ALGORITHMIC FOUNDATIONS","04/01/2018","03/06/2018","Eric Allender","NJ","Rutgers University New Brunswick","Standard Grant","Tracy J. Kimbrel","03/31/2019","$10,000.00","","Allender@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1640, 7796","7556, 7927","$0.00","This award will support student and postdoctoral attendance at a workshop on computational complexity theory, organized by the Clay Mathematics Institute, and by the Computer Science Department at the University of Oxford.  Many leading figures in computational complexity have already made a commitment to attend the workshop, which will be held July 23-26 at the University of Oxford.  For these student attendees, the meeting serves as a valuable educational experience, both for the technical content of the talks and for the opportunities for networking that it provides. <br/><br/>The award will provide partial support to approximately seven students, partly defraying the cost of travel and lodging. Efforts will be made to support students from under-represented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1703925","AF: Medium: New Frontiers in Equilibrium Computation","CCF","ALGORITHMIC FOUNDATIONS","05/01/2017","05/07/2018","Mihalis Yannakakis","NY","Columbia University","Continuing grant","Tracy J. Kimbrel","04/30/2021","$599,082.00","Xi Chen","mihalis@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7924, 7927, 7932","$0.00","Concepts and methodologies from economics and social sciences have found numerous applications in the study of the Internet and e-commerce.  At the core of many such applications lies the notion of equilibria, which has been widely studied by game theorists and economists to model and predict the strategic behavior of selfish yet rational agents.  During the past decade, the computation of equilibria in both games and markets has been studied intensively and many of the exciting developments have brought new insights towards a much better understanding of equilibria.  At the same time, they opened up a window to many new research challenges.  The goal of the project is to explore some of the important directions and problems that have emerged in the new frontiers of equilibrium computation.  Research along directions pursued in the project will complement the already extensive literature in economics and social sciences on equilibria, by offering new perspectives through the lens of algorithms, approximation, and computational complexity.  In addition to curriculum development, mentoring PhD students, and involving undergraduate students in accessible projects, the PIs will actively pursue outreach activities and broadly disseminate results obtained in the project.<br/><br/>Some of the research directions that the PIs plan to explore include the following:  1) Deepen our understanding of the new exponential-time hypothesis for fixed-point computation employed by Rubinstein in his recent breakthrough, by exploring its connections with other natural conjectures on the exact complexity of fundamental equilibrium computation problems.  2) Explore connections between two problems that have been studied intensively in the literature, the problem of finding an approximate Nash equilibrium in an anonymous game with a polynomial precision and that in a two-player game with a constant precision.  3) Study the computation of equilibria in games and markets with a unique equilibrium.  Equilibrium as a prediction tool is more meaningful when a unique equilibrium exists.  4) Work towards a dichotomy theorem for Arrow-Debreu market equilibria that aims to classify every family of utilities into those that are easy to solve and those that are intractable."
"1848987","NSF Student Travel Grant for 2019 Annual International Conference on Research in Computational Molecular Biology (RECOMB 2019)","CCF","COMPUTATIONAL BIOLOGY","10/01/2018","08/28/2018","Max Alekseyev","DC","George Washington University","Standard Grant","Mitra Basu","09/30/2019","$10,000.00","","maxal@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","7931","7556","$0.00","Research in Computational Molecular Biology (RECOMB 2019) is an international scientific conference bridging the computational, mathematical, and biological sciences. The RECOMB conference series was founded in 1997 to provide a scientific forum for theoretical advances in computational biology and their applications in molecular biology and medicine. RECOMB 2019 will be the 23rd Annual Meeting, held in Washington, DC. RECOMB features keynote talks by preeminent scientists, together with presentations of refereed research papers in computational biology, special sessions and poster sessions. This project requests travel support for student conference participants from any US university, with priority going to graduate students who will be presenting paper or posters. Overall, students will be educated on cutting-edge developments that will further drive the research methods and results of the field of computational biology. Students and scientists are able to return to their labs to apply what they have learned as they advance their own research efforts or begin investigating new areas they were exposed to as a result of attending RECOMB. NSF investment in this project will help further students' careers and our nation's competitiveness in an expanding, global industry, and will benefit the public through new discoveries made by highly trained scientists.<br/><br/>The meeting and proposed travel fellowships will have particular value for educational purposes, creating a unique training opportunity for 10 U.S.-based students of computational biology. Awarded travel fellowships will cover registration, hotel, and travel costs for a selected group of student presenters. Preference will be given to students with proceedings papers that are accepted for oral presentation at the conference and secondarily to those making poster presentations of accepted abstracts. Women, minorities and persons with disabilities will be especially encouraged to apply with the goal of achieving diversity at the meeting and directly benefiting diverse groups. As much as possible, participant support cost requested in this grant will be used specifically to fund the travel expenses of students from these three underrepresented groups in order to increase their access to training opportunities. By attending the RECOMB meeting, students will gain the latest skills in computational biology, the field that has the potential to unlock the secrets for understanding life. The field comprises a vibrant, growing industry, with skilled computational scientists in high demand at pharmaceutical, agricultural, environmental, consumer products, biotech, software, hardware, and service companies. Students' participation is vital for the growth and continuation of the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1725566","SPX: Collaborative Research: SANDY: Sparsification-Based Approach for Analyzing Network Dynamics","CCF","SPX: Scalable Parallelism in t","09/01/2017","08/10/2018","Sanjukta Bhowmick","NE","University of Nebraska at Omaha","Continuing grant","Vipin Chaudhary","08/31/2020","$144,180.00","","Sanjukta.Bhowmick@unt.edu","6001 Dodge Street","Omaha","NE","681820210","4025542286","CSE","042Y","026Z, 9150","$0.00","The goal of this three-year project, Sparsification-based Approach for Analyzing Network Dynamics (SANDY), is to develop a suite of scalable parallel algorithms for updating dynamic networks for different problems that can be executed on a wide range of HPC platforms. Dynamic network analysis will enable researchers to study the evolution of complex systems in diverse disciplines, such as bioinformatics, social sciences, and epidemiology. The SANDY project is expected to initiate a new direction of research in developing parallel dynamic network algorithms that will benefit multiple analysis objectives (e.g., motif finding and network alignment) and application domains (e.g., epidemiology, health care). Research findings will be integrated into courses on network analysis, parallel algorithms, and bioinformatics offered at the three collaborating institutions. The PIs will collaborate with high schools to deliver talks on network theory, and encourage women and minority students to pursue IT-related careers. <br/><br/> <br/>To develop efficient and scalable parallel algorithms, the PIs propose to use an elegant technique, called graph sparsification, that expresses graph algorithms in a reduction-like fashion. The formal steps to parallelization, as guided by the graph sparsification framework, provide a template for creating provably correct parallel algorithms for dynamic networks. The proposed algorithms will address the dual needs of portability and performance optimization. The framework will further provide a mechanism for combining high level (e.g., static and dynamic graph partitioning) and low level (e.g., dataflow algorithms) tuning strategies to ensure high performance and scalability for various parallel architectures by considering such factors as scalability, time, memory, and energy efficiency."
"1725585","SPX:   Collaborative Research:   SANDY:  Sparsification-based Approach for Analyzing Network Dynamics","CCF","SPX: Scalable Parallelism in t","09/01/2017","08/10/2018","Boyana Norris","OR","University of Oregon Eugene","Continuing grant","Vipin Chaudhary","08/31/2020","$148,171.00","","norris@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","042Y","026Z","$0.00","The goal of this three-year project, Sparsification-based Approach for Analyzing Network Dynamics (SANDY), is to develop a suite of scalable parallel algorithms for updating dynamic networks for different problems that can be executed on a wide range of HPC platforms. Dynamic network analysis will enable researchers to study the evolution of complex systems in diverse disciplines, such as bioinformatics, social sciences, and epidemiology. The SANDY project is expected to initiate a new direction of research in developing parallel dynamic network algorithms that will benefit multiple analysis objectives (e.g., motif finding and network alignment) and application domains (e.g., epidemiology, health care). Research findings will be integrated into courses on network analysis, parallel algorithms, and bioinformatics offered at the three collaborating institutions. The PIs will collaborate with high schools to deliver talks on network theory, and encourage women and minority students to pursue IT-related careers. <br/><br/> <br/>To develop efficient and scalable parallel algorithms, the PIs propose to use an elegant technique, called graph sparsification, that expresses graph algorithms in a reduction-like fashion. The formal steps to parallelization, as guided by the graph sparsification framework, provide a template for creating provably correct parallel algorithms for dynamic networks. The proposed algorithms will address the dual needs of portability and performance optimization. The framework will further provide a mechanism for combining high level (e.g., static and dynamic graph partitioning) and low level (e.g., dataflow algorithms) tuning strategies to ensure high performance and scalability for various parallel architectures by considering such factors as scalability, time, memory, and energy efficiency."
"1150281","CAREER: Innovations in Markov Chains: Metrics, Duality and Liftings","CCF","ALGORITHMIC FOUNDATIONS, EPSCoR Co-Funding","08/01/2012","08/02/2016","Thomas Hayes","NM","University of New Mexico","Continuing grant","Rahul Shah","07/31/2019","$430,817.00","","hayes@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","7796, 9150","1045, 7926","$0.00","Markov chain simulation is a very general technique applied to a wide spectrum of problems in the physical sciences.  From explicit simulations of physical and dynamical processes, such as fluid dynamics and spin systems, to algorithms for sampling from probability distributions over enormous sets of combinatorial objects, Markov chains are ubiquitous.  This project will seek to improve the design and analysis of such algorithms, leading to faster running times.<br/><br/>Understanding the performance of a Markov Chain Monte Carlo algorithm involves proving bounds on how quickly it approaches its limiting, or ""stationary"", distribution.  Due to the inherent randomness in any Markov chain simulation, there is often no reliable empirical criterion for measuring this convergence; rather, one must rely on theoretical guarantees. The PI will focus on the twin long-standing problems of how to redesign Markov chains to actually converge faster, and of proving better convergence guarantees, both of which allow us to safely terminate MCMC simulations sooner. Over the course of this work, these goals will be approached using techniques from three main thematic groupings:<br/><br/>1.  In the ""Coupling Method"" for proving convergence bounds, one seeks to show that two copies of a Markov chain can be ""made to approach each other"" under some metric on the state space.  To improve this kind of analysis, the PI seeks to find better metrics, i.e., better definitions of the distance between two states.<br/><br/>2.  Several different mathematical notions of duality have played important roles in the analysis of Markov chains.  For example, the duality between the spin system and the cluster characterization of the Ising model, a standard model of magnetic materials, the high-temperature/low-temperature duality for the Potts model on a planar graph, and strong stationary duality, which underlies a recently introduced technique called the Evolving Sets method.<br/><br/>3. The PI will attempt to convert reversible Markov chains into non-reversible ""lifted"" Markov chains, by adding additional ""momentum"" information to the states.  These lifted chains allow sampling from the original distribution, but can run quadratically faster.<br/><br/>The project will include the creation and deployment of a free web resource, ""Markov Chains Central,"" which will include a collection of new and existing laboratory applets for simulating and experimenting with Markov chains and various measures of convergence. These applets will help students visualize Markov chains and understand them through experimentation and play. The project also features an integrated educational plan, which provides for wide dissemination of generated knowledge and educational materials. This work will support undergraduate and graduate student research and mentoring. Effort will be made to maximize involvement of women and minority students."
"1832952","CIF: Student Conference Travel Support for the 2018 North American School of Information Theory","CCF","COMM & INFORMATION FOUNDATIONS","05/01/2018","04/23/2018","Krishna Narayanan","TX","Texas A&M Engineering Experiment Station","Standard Grant","Phillip Regalia","04/30/2019","$10,000.00","Jean-Francois Chamberland","krn@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7797","7556, 7935","$0.00","The IEEE North American School for Information Theory is an annual event of the IEEE Information Theory Society. 2018 marks the eleventh year of the event. The School provides a supportive environment where foundations for learning and long-term future scientific collaborations are established. The School delivers interactive education for graduate students in the mathematical, engineering, and computer sciences. It presents students the opportunity to meet with and learn from senior researchers from academia and industry who present long-format tutorials. In addition, student and post-doc participants have the opportunity to present their own research results in poster sessions. The IEEE Information Theory Society Padovani Lecture is delivered at the school.<br/><br/>The IEEE North American School for Information Theory is hosted by different institutions each year, with the intent of increasing the geographic diversity of the participants. Student attendance at recent schools has varied from 40 to 80 student participants. Travel grants broaden the participation of students at the workshop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
