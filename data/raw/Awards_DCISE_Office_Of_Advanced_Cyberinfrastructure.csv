"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1445176","The Centrality of Advanced Digitally ENabled Science: CADENS","OAC","CYBERINFRASTRUCTURE, ETF","10/01/2014","07/15/2016","Donna Cox","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","09/30/2019","$1,799,376.00","","donnacox@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7231, 7476","026Z","$0.00","Computational data science is at a turning point in its history. Never before has there been such a challenge to meet the growing demands of digital computing, to fund infrastructure and attract diverse, trained personnel to the field. The methods and technologies that define this evolving field are central to modern science. In fact, advanced methods of computational and data-enabled discovery have become so pervasive that they are referred to as paradigm shifts in the conduct of science. A goal of this Project is to increase digital science literacy and raise awareness about the Centrality of Advanced Digitally ENabled Science (CADENS) in the discovery process. Digitally enabled scientific investigations often result in a treasure trove of data used for analysis. This project leverages these valuable resources to generate insightful visualizations that provide the core of a series of science education outreach programs targeted to the broad public, educational and professional communities. From the deep well of discoveries generated at the frontiers of advanced digitally enabled scientific investigation, this project will produce and disseminate a body of data visualizations and scalable media products that demonstrate advanced scientific methods. In the process, these outreach programs will give audiences a whole new look at the world around them. The project calls for the production and evaluation of two principal initiatives. The first initiative, HR (high-resolution) Science, centers on the production and distribution of three ultra-high-resolution digital films to be premiered at giant screen full-dome theaters; these programs will be scaled for wide distribution to smaller theaters and include supplemental educator guides. The second initiative, Virtual Universe, includes a series of nine high-definition (HD) documentary programs. Both initiatives will produce and feature data visualizations and the CADENS narratives to support an integrated set of digital media products. The packaged outreach programs will be promoted and made available to millions through established global distribution channels. Expanding access to data visualization is an essential component of the Project. Through a call for participation (CFP), the Project provides new opportunities for researchers to work with the project team and technical staff for the purpose of creating and broadly distributing large-scale data visualizations in various formats and resolutions. The project will feature these compelling, informative visualizations in the outreach programs described above. A Science Advisory Committee will participate in the CFP science selections and advise the Project team. The project calls for an independent Program Evaluation and Assessment Plan (PEAP) to iteratively review visualizations and the outreach programs that will target broad, diverse audiences. <br/><br/>The project launches an expansive outreach effort to increase digital science literacy and to convey forefront scientific research while expanding researchers access to data visualization. The project leverages and integrates disparate visualization efforts to create a new optimized large-scale workflow for high-resolution museum displays and broad public venues. The PEAP evaluations will measure progress toward project goals and will reveal new information about visualization's effectiveness to move a field forward and to develop effective outreach models. The project specifically targets broad audiences in places where they seek high-quality encounters with science: at museums, universities, K-16 schools, and the web. This distribution effort includes creating and widely disseminating the project outreach programs and supplemental educator guides. The project visualizations, program components, HD documentaries, educational and evaluation materials will be promoted, distributed and made freely available for academic, educational and promotional use. Dissemination strategies include proactively distributing to rural portable theaters, 4K television, professional associations, educators, decision-makers, and conferences. To help address the critical challenge of attracting women and underrepresented minorities to STEM fields, the Project will support a Broadening Participation in Visualization workshop and will leverage successful XSEDE/Blue Waters mechanisms to recruit under-represented faculty and students at minority-serving and majority-serving institutions and to disseminate the Project programs and materials among diverse institutions and communities."
"1636847","BD Spokes: Spoke: NORTHEAST: Collaborative: Grand Challenges for Data-Driven Education","OAC","BD Spokes -Big Data Regional I, INFORMATION TECHNOLOGY RESEARC, Core R&D Programs","09/01/2016","07/24/2017","Beverly Woolf","MA","University of Massachusetts Amherst","Standard Grant","Beth Plale","08/31/2019","$389,982.00","","bev@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","024Y, 1640, 7980","028Z, 043Z, 7433, 8083","$0.00","This project supports teachers, administrators and researchers to collaborate around online education resources and big data. It will increase the capacity of participants in Educational Big Data in the Northeast to analyze data from schools, students and administrators and to improve teaching and learning. However, as more refined data comes from online instructional systems and the use of data mining techniques, participants will learn to search for patterns and associations and to draw conclusions about student knowledge, performance and behavior. This research addresses several grand challenges in education: 1) Predict future student events, e.g., college attendance, college major, from existing large-scale longitudinal educational data sets involving the same thousands of students. 2) Help teachers to make sense of dense online data to influence their teaching, e.g., what should they say or do in response to student activity. 3) Provide personal instruction to each student based on using big data that represents student skills and behavior and infers students' cognitive, motivational, and metacognitive factors in learning. The project will improve the capacity in data-driven education by sharing educational databases, managing yearly data competitions, and conducting educational data science workshops and hackathons. Measurable results include studying gigabytes of data to: create actionable recommendations for classroom teachers; make effective and successful predictions about students; develop new AI methods for education; and create new data science tool sets. Key outcomes include introducing many researchers to educational big data, learning analytics and models of teaching interventions. The team intends to improve classroom learning and leverage the unique types of data available from digital education to better understand students, groups and the settings in which they learn.<br/><br/>Computers have been in classrooms for decades and yet educators have not identified the most effective ways of using them. Despite advances in evaluation methods to measure human learning, most researchers still use measures available 50 years ago. This project will leverage and extend state-of-the-art big data bases and technologies to measure online learning, especially features of student engagement and learning associated with improved student outcome. This project has the potential to reach millions of students (while learning), hundreds of researchers while measuring human learning (from education, cognitive science, learning sciences, psychology, and computer science) and a dozen other organizations (publishers, testing organizations, non-profit organizations, teachers, parents, and stakeholders). The team brings together a unique blend of researchers from data science (Baker, Heffernan); adaptive education technology and computer science (Woolf, Arroyo); and learning sciences (Arroyo, Heffernan). It includes women and minorities (Woolf, Arroyo), people who helped develop the largest educational database in the world (Baker), developers of data science teaching materials (Arroyo, Baker), and others who have developed online tutoring systems that achieve significant student success in learning (e.g., Heffernan, Arroyo, Woolf)."
"1923632","Ideas Labs: Data-Intensive Research in Science and Engineering","OAC","HDR-Harnessing the Data Revolu","03/01/2019","02/26/2019","James Crowley","PA","Society For Industrial and Applied Math (SIAM)","Standard Grant","Nandini Kannan","02/29/2020","$998,238.00","","jcrowley@siam.org","3600 Market St.","Philadelphia","PA","191042688","2153829800","CSE","099Y","","$0.00","In 2016, the National Science Foundation (NSF) unveiled a set of ""Big Ideas,"" 10 bold, long-term research and process ideas that identify areas for future investment at the frontiers of science and engineering.  The Big Ideas represent unique opportunities to position our Nation at the cutting edge of global science and engineering leadership by bringing together diverse disciplinary perspectives to support convergence research.  NSF's Harnessing the Data Revolution (HDR) Big Idea is a national-scale activity to enable new modes of data-driven discovery that will allow fundamental questions to be asked and answered at the frontiers of science and engineering.  This project describes a series of Ideas Labs on ""Data-Intensive Research in Science and Engineering (DIRSE)"".  Ideas Labs are intensive workshops focused on finding innovative and bold transdisciplinary solutions to grand challenge problems.  The overarching goal of the DIRSE Ideas Labs is to foster convergent approaches to enable data-intensive research in science and engineering through a series of facilitated activities bringing together scientists and engineers working on important data-intensive science and engineering problems with data scientists.<br/><br/>There are numerous science and engineering challenges that require, or will soon require, data science to help address research and technological questions.  Advancing knowledge in these areas requires solutions to many modeling and data challenges such as real-time sensing, learning, and decision making; social, political, and behavioral implications of machine learning and impacts of new data uses; issues related to ethics and fairness; and integrating heterogeneous data for explaining or predicting complex phenomena.  There is also a need for approaches that combine physical models with data driven models for learning and decision making.  Data science tools, such as signal and image processing, visualization, statistical modeling and inference, machine learning, and optimization, offer a starting point for solving important scientific and engineering challenges.  However, extracting new information and knowledge from data will benefit from new, convergent strategies that capitalize on existing NSF investments in data and cyberinfrastructure and that build synergy between the researchers with expertise in the generation or measurement of data and those with expertise in processing and analyzing that data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1253644","CAREER: Abstractions and Middleware for D3 Science on NSF Distributed Cyberinfrastructure","OAC","CAREER: FACULTY EARLY CAR DEV","03/01/2013","07/16/2014","Shantenu Jha","NJ","Rutgers University New Brunswick","Standard Grant","Sushil K Prasad","02/29/2020","$715,999.00","","shantenu.jha@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1045","1045, 9251","$0.00","Technical Description: This CAREER Award project will develop middleware to support Distributed Dynamic Data-intensive (D3) science on Distributed Cyberinfrastructure (DCI).  Existing NSF-funded CI systems, such as the Extreme Science and Engineering Discovery Environment (XSEDE) and the Open Science Grid (OSG), use distributed computing to substantially increase the computational power available to research scientists around the globe; however, such distributed systems face limitations in their ability to handle the large data-volumes being generated by today?s scientific instruments and simulations.  To address this challenge, the PI will develop and deploying extensible abstractions that will facilitate the integration of high-performance computing and large-scale data sets.  Building on previous work on pilot-jobs, these new abstractions will implement the analogous concept of ?pilot-data? and the linking principle of ?affinity.?  The result will be a unified conceptual framework for improving the matching of data and computing resources and for facilitating dynamic workflow placement and scheduling.  This research has the potential to significantly advance multiple areas of science and engineering, by generating production-grade middleware for accomplishing scalable big-data science on a range of DCI systems.<br/><br/>Broader Importance:  Increasingly, the high-performance computing resources available to scientific researchers are distributed across multiple machines in multiple locations.  The integration of these resources requires a fabric of ?middleware,? upon which a wide variety of user applications, tools and services can be built and run.  As more accurate and ubiquitous scientific instruments and models produce ever-larger volumes of data, however, this distributed cyberinfrastructure (DCI) is confronting unprecedented data-handling challenges that exceed the capabilities of existing DCI middleware.  In this project, the PI will develop, test and implement new middleware solutions, specifically designed for the coming era of big-data distributed supercomputing.<br/><br/>The project will also develop new curricula and new teaching and outreach materials for introducing secondary and college students, secondary school teachers, and the general public to the emerging field of distributed data-intensive science.  In partnership with FutureGrid, the PI will design simple and effective vehicles for sharing these resources with Historically Black Colleges and Universities (HBCUs) and other institutions where faculty might otherwise have relatively limited opportunity to develop advanced course materials.  The PI will also partner with the Douglass Program for Women in Science, and The Academy at Rutgers for Girls in Engineering and Technology (TARGET), to increase engagement of, and support for, female students in the DCI research community."
"1659348","CC* Networking Infrastructure: Improving Network Infrastructure to Enable Large Scale Scientific Data Flows and Collaboration","OAC","Campus Cyberinfrastrc (CC-NIE)","03/01/2017","12/08/2016","Klara Jelinkova","TX","William Marsh Rice University","Standard Grant","Kevin Thompson","02/29/2020","$499,646.00","Moshe Vardi, Keith Cooper, Jan Odegard, Paul Padley","klaraj@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","8080","","$0.00","Campus networks are required to protect information and inspect data flows to safeguard security and privacy. However, researchers need open and unfettered access to large data flows and instruments across the globe to reduce time to discovery.  The Rice University network is a shared resource that not only needs to support the administrative and teaching functions but also enable scientists to use that network in new and innovative ways for research.  Five key data-intensive application teams act as drivers of the new extension of network functionality and are providing feedback to the technical design staff.  These application areas include earth and atmospheric sciences; urban data science; computational biosciences and neuroengineering; particle physics and distributed cluster computing.  These applications build on long-term science investments aimed, amongst others, at understanding seismic events, the weather patterns in the Gulf regions and beyond, as well as urban trends in large, diverse cities such as Houston, TX. <br/><br/>The basic model adopted by the project is ""the science DMZ."" A Science DMZ is ""a portion of the network, built at or near the campus local network perimeter designed so that equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose business systems."" This approach allows Rice to aggressively upgrade its network capacity for greatly enhanced science data access. This project supports 100 GB/s flows between the data transfer facilities at our off-campus data center and national and international R&E data repositories and takes advantage of SDN (Openflow) mechanisms."
"1664119","Collaborative Research: SI2-SSI:  Cyberinfrastructure for Advancing Hydrologic Knowledge through Collaborative Integration of Data Science, Modeling and Analysis","OAC","SPECIAL INITIATIVES, EAR, XC-Crosscutting Activities Pro, Software Institutes, EarthCube","10/01/2017","09/14/2017","Shaowen Wang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","09/30/2021","$699,999.00","","shaowen@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1642, 6898, 7222, 8004, 8074","026Z, 7433, 8004, 8009","$0.00","Researchers across the country and around the world expend tremendous resources to gather and analyze vast stores of hydrologic data and populate a myriad of models to better understand hydrologic phenomena and find solutions to vexing water problems. Each of those researchers has limited money, time, computational capacity, data storage, and ability to put that data to productive use. What if they could combine their efforts to make collaboration easier? What if those collected data sets and processed model outputs could be used collaboratively to help advance hydrologic understanding beyond their original purpose? HydroShare is a system to advance hydrologic science by enabling the scientific community to more easily and freely share products resulting from their research, not just the scientific publication summarizing a study, but also the data and models used to create the scientific publication. HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation and to improve reproducibility and community trust in scientific findings through transparency. As a platform for collaboration and running models on advanced computational infrastructure, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is designed to help researchers easily meet the sharing requirements of data management plans while at the same time providing value added functionality that makes metadata capture more effective and helps researchers improve their work productivity. This project will extend the capabilities of the HydroShare cyberinfrastructure to enhance support for scientific methods, advance the social capabilities of HydroShare to enable improved collaborative research, integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage. and establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Empowering HydroShare users with the ability to rapidly develop web application programs opens the door to unforeseen, innovative combinations of data and models. WRF-Hydro, the framework for the NOAA National Water Model, will be used as a use case for collaboration on model development. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. Collectively, this functionality will provide a computing framework for transforming the practice of broad science communities to leverage advances in data science and computation and accelerate discovery.<br/><br/>HydroShare is a system for sharing hydrologic data and models aimed at giving hydrologists the cyberinfrastructure needed to manage data, innovate and collaborate in research to solve water problems. It addresses the challenges of sharing data and hydrologic models to support collaboration and reproducible hydrologic science through the publication of hydrologic data and models. With HydroShare users can: (1) share data and models with colleagues; (2) manage who has access to shared content; (3) share, access, visualize and manipulate a broad set of hydrologic data types and models; (4) use the web services interface to program automated and client access; (5) publish data and models to meet the requirements of research project data management plans; (6) discover and access data and models published by others; and (7) use web apps to visualize, analyze, and run models on data. This project will extend the capabilities of HydroShare to: (1) enhance support for scientific methods enabling systematic data and model analysis and hypothesis testing; (2) advance the social capabilities of HydroShare to enable improved collaborative research; (3) integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage; and (4) establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Under development since 2012 and first released in 2014, HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation. As a platform for collaboration and cloud based computation on network servers remote from the user, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is innovative from a computer science and CI perspective in the way computation and data sharing are framed as a network computing platform that integrates data storage, organization, discovery, and programmable actions through web applications (web apps). Support for these three key elements of computation allows researchers to easily employ services beyond the desktop to make data storage and manipulation more reliable and scalable, while improving ability to collaborate and reproduce results. The generation of new understanding, through integration of information from multiple sources and reuse and collaborative enrichment of research data and models, will be enhanced. Structured and systematic model process intercomparisons and alternative hypothesis testing will be enabled, bringing, through user friendly CI, the latest thinking in advancing hydrologic modeling to a broad community of earth science researchers, thereby transforming research practices and the knowledge generated from this research. Interoperability with consumer cloud storage will greatly ease entry of content into HydroShare and support its sustainability. This meshing of the rigorous metadata model of HydroShare with consumer file sharing will enhance reproducibility as well as provide an innovative mechanism for sharing and collaboration. Empowering HydroShare users with the ability to rapidly develop web apps opens the door to unforeseen, innovative combinations of data and models. WRF- Hydro will be used as a use case for collaboration on model development. WRF-Hydro provides a reach-based high resolution representation of hydrologic processes, and offers the potential to bring together scientists working at scales from research catchments on the order of 1 to 100s of square kilometers as well as those working at regional to continental scales and cut across disciplines from environmental engineering to aquatic ecologists. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. This project will adapt current best practices in CI for interoperability and extensibility to serve this multidisciplinary community of scientists. HydroShare has already had a broader impact, with documented rapid growth in use and uptake by other projects including in EarthCube. It will become sustainable community CI through operation as part of the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Water Data Center (WDC) facility. The use of WRF- Hydro/NWM, as a driving use case, will advance CI for community based model improvement. Through the Summer Young Innovators Program at the National Water Center (NWC), supported by the National Weather Service (NWS) and operated by CUAHSI, a pathway already exists to translate research findings to the operational needs of federal agencies participating in the NWC. HydroShare already touches a broad and diverse community, with user base including Native American tribes, hydrologic science students, and faculty researchers across the U.S. This proposal builds on the success of HydroShare to extend its capabilities and broaden model hypothesis testing, collaborative data sharing, and open app development across earth science research and education."
"1664061","Collaborative Research: SI2-SSI: Cyberinfrastructure for Advancing Hydrologic Knowledge through Collaborative Integration of Data Science, Modeling and Analysis","OAC","HYDROLOGIC SCIENCES, SPECIAL INITIATIVES, EAR, Software Institutes, EarthCube","10/01/2017","08/23/2018","David Tarboton","UT","Utah State University","Standard Grant","Stefan Robila","09/30/2021","$2,809,998.00","Alva Couch, Daniel Ames, Jeffery Horsburgh, Martyn Clark","dtarb@usu.edu","Sponsored Programs Office","Logan","UT","843221415","4357971226","CSE","1579, 1642, 6898, 8004, 8074","026Z, 7433, 7556, 8004, 8009","$0.00","Researchers across the country and around the world expend tremendous resources to gather and analyze vast stores of hydrologic data and populate a myriad of models to better understand hydrologic phenomena and find solutions to vexing water problems. Each of those researchers has limited money, time, computational capacity, data storage, and ability to put that data to productive use. What if they could combine their efforts to make collaboration easier? What if those collected data sets and processed model outputs could be used collaboratively to help advance hydrologic understanding beyond their original purpose? HydroShare is a system to advance hydrologic science by enabling the scientific community to more easily and freely share products resulting from their research, not just the scientific publication summarizing a study, but also the data and models used to create the scientific publication. HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation and to improve reproducibility and community trust in scientific findings through transparency. As a platform for collaboration and running models on advanced computational infrastructure, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is designed to help researchers easily meet the sharing requirements of data management plans while at the same time providing value added functionality that makes metadata capture more effective and helps researchers improve their work productivity. This project will extend the capabilities of the HydroShare cyberinfrastructure to enhance support for scientific methods, advance the social capabilities of HydroShare to enable improved collaborative research, integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage. and establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Empowering HydroShare users with the ability to rapidly develop web application programs opens the door to unforeseen, innovative combinations of data and models. WRF-Hydro, the framework for the NOAA National Water Model, will be used as a use case for collaboration on model development. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. Collectively, this functionality will provide a computing framework for transforming the practice of broad science communities to leverage advances in data science and computation and accelerate discovery.<br/><br/>HydroShare is a system for sharing hydrologic data and models aimed at giving hydrologists the cyberinfrastructure needed to manage data, innovate and collaborate in research to solve water problems. It addresses the challenges of sharing data and hydrologic models to support collaboration and reproducible hydrologic science through the publication of hydrologic data and models. With HydroShare users can: (1) share data and models with colleagues; (2) manage who has access to shared content; (3) share, access, visualize and manipulate a broad set of hydrologic data types and models; (4) use the web services interface to program automated and client access; (5) publish data and models to meet the requirements of research project data management plans; (6) discover and access data and models published by others; and (7) use web apps to visualize, analyze, and run models on data. This project will extend the capabilities of HydroShare to: (1) enhance support for scientific methods enabling systematic data and model analysis and hypothesis testing; (2) advance the social capabilities of HydroShare to enable improved collaborative research; (3) integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage; and (4) establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Under development since 2012 and first released in 2014, HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation. As a platform for collaboration and cloud based computation on network servers remote from the user, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is innovative from a computer science and CI perspective in the way computation and data sharing are framed as a network computing platform that integrates data storage, organization, discovery, and programmable actions through web applications (web apps). Support for these three key elements of computation allows researchers to easily employ services beyond the desktop to make data storage and manipulation more reliable and scalable, while improving ability to collaborate and reproduce results. The generation of new understanding, through integration of information from multiple sources and reuse and collaborative enrichment of research data and models, will be enhanced. Structured and systematic model process intercomparisons and alternative hypothesis testing will be enabled, bringing, through user friendly CI, the latest thinking in advancing hydrologic modeling to a broad community of earth science researchers, thereby transforming research practices and the knowledge generated from this research. Interoperability with consumer cloud storage will greatly ease entry of content into HydroShare and support its sustainability. This meshing of the rigorous metadata model of HydroShare with consumer file sharing will enhance reproducibility as well as provide an innovative mechanism for sharing and collaboration. Empowering HydroShare users with the ability to rapidly develop web apps opens the door to unforeseen, innovative combinations of data and models. WRF- Hydro will be used as a use case for collaboration on model development. WRF-Hydro provides a reach-based high resolution representation of hydrologic processes, and offers the potential to bring together scientists working at scales from research catchments on the order of 1 to 100s of square kilometers as well as those working at regional to continental scales and cut across disciplines from environmental engineering to aquatic ecologists. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. This project will adapt current best practices in CI for interoperability and extensibility to serve this multidisciplinary community of scientists. HydroShare has already had a broader impact, with documented rapid growth in use and uptake by other projects including in EarthCube. It will become sustainable community CI through operation as part of the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Water Data Center (WDC) facility. The use of WRF- Hydro/NWM, as a driving use case, will advance CI for community based model improvement. Through the Summer Young Innovators Program at the National Water Center (NWC), supported by the National Weather Service (NWS) and operated by CUAHSI, a pathway already exists to translate research findings to the operational needs of federal agencies participating in the NWC. HydroShare already touches a broad and diverse community, with user base including Native American tribes, hydrologic science students, and faculty researchers across the U.S. This proposal builds on the success of HydroShare to extend its capabilities and broaden model hypothesis testing, collaborative data sharing, and open app development across earth science research and education."
"1730390","CyberTraining: DSE: Cyber Carpentry:  Data Life-Cycle Training using the Datanet Federation Consortium Platform","OAC","CyberTraining - Training-based, DATANET","11/01/2017","06/28/2017","Arcot Rajasekar","NC","University of North Carolina at Chapel Hill","Standard Grant","Sushil K Prasad","10/31/2020","$499,641.00","Hao Xu, Melanie Feinberg","rajaseka@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","044Y, 7726","7361, 7433","$0.00","The emergence of massive data collections has ushered a paradigm shift in the way scientific research is conducted and new knowledge is discovered.  This shift necessitates students to be trained in team-based, interdisciplinary, complex data-oriented approaches designed to translate scientific data into new solutions in order to promote the progress of science; to advance the national health, prosperity and welfare, and to secure the national defense.  The proliferation of cyberinfrastructure (CI) tools necessitate addressing the needs of domain scientists from multiple angles, including data access, metadata management, large-scale analytics and workflows, data and application discovery and sharing, and data preservation.  Training with such a holistic perspective is indeed daunting with a tool and solution landscape that is still fragmented.  Integrated solutions, such as the Datanet Federation Consortium (DFC) Platform, provide a way to ease this overload and help touch upon all of these needed functionalities.  The aim of this project is to make it easier for next generation workforce in STEM disciplines to learn all aspects of data-intensive computing environment and, more importantly, to work together with other researchers with complementary expertise.<br/><br/>Students in STEM disciplines need to be educated in (i) practices of data organization, (ii) importance of provenance, metadata and ontology, (iii) conformance to authentication, authorization and access control protocols, (iv) models for data sharing, discovery and curation, (v) necessity for reproducible data science workflows, (vi) practices in dealing with large-scale data computation using super computers and cloud computing, and (vii) distributed data management practices.  The Datanet Federation Consortium (DFC) is an NSF-funded project that has implemented a data-centered cyber platform that has integrated tools for end-to-end data life-cycle management and data-intensive high performance computation.  This project aims to use the DFC Platform to provide training for STEM graduate students in leading-edge data-intensive practices, in all aspects of data-intensive computing environments.  Their training workshops will be multi-disciplinary, including  earth system sciences, biological sciences, social and information sciences, marine sciences and engineering.  The short term goal of the project is to provide intensive, short duration training discipline-centric workshops, called Cyber Carpentries.  These workshops will lead to Certificates in Data Science, preparing a better scientific workforce with advanced data-intensive CI capabilities.  For the long-term, project plans to develop self-paced tutorials and a sequence of courses that can be adapted in different STEM disciplines with concentration in data life-cycle management and data-intensive computing.  The practicums will involve large datasets from multiple science data repositories including several NSF-funded large-scale cyberinfrastructure such as iRODS, CyVerse, DataONE, SEAD, TerraPop, DataVerse and HydroShare - all of which are integrated through the DFC Platform.  Project will recruit students from HBCUs and MSIs for its workshops and will work closely with faculty from these universities to help them adopt the courses developed through this project.  All material developed as part of the project will be made available as open course material."
"1443054","CIF21 DIBBs: Middleware and High Performance Analytics Libraries for Scalable Data Science","OAC","EDUCATION AND WORKFORCE, DATANET","10/01/2014","06/11/2018","Geoffrey Fox","IN","Indiana University","Standard Grant","Amy Walton","09/30/2019","$5,208,043.00","Madhav Marathe, Shantenu Jha, Judy Qiu, Fusheng Wang","gcf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7361, 7726","7433, 8048, 9251","$0.00","Many scientific problems depend on the ability to analyze and compute on large amounts of data.  This analysis often does not scale well; its effectiveness is hampered by the increasing volume, variety and rate of change (velocity) of big data.  This project will design, develop and implement building blocks that enable a fundamental improvement in the ability to support data intensive analysis on a broad range of cyberinfrastructure, including that supported by NSF for the scientific community. The project will integrate features of traditional high-performance computing, such as scientific libraries, communication and resource management middleware, with the rich set of capabilities found in the commercial Big Data ecosystem. The latter includes many important software systems such as Hadoop, available from the Apache open source community.  A collaboration between university teams at Arizona, Emory, Indiana (lead), Kansas, Rutgers, Virginia Tech, and Utah provides the broad expertise needed to design and successfully execute the project.  The project will engage scientists and educators with annual workshops and activities at discipline-specific meetings, both to gather requirements for and feedback on its software.  It will include under-represented communities with summer experiences, and will develop curriculum modules that include demonstrations built as 'Data Analytics as a Service.'<br/><br/>The project will design and implement a software Middleware for Data-Intensive Analytics and Science (MIDAS) that will enable scalable applications with the performance of HPC (High Performance Computing) and the rich functionality of the commodity Apache Big Data Stack.  Further, this project will design and implement a set of cross-cutting high-performance data-analysis libraries; SPIDAL (Scalable Parallel Interoperable Data Analytics Library) will support new programming and execution models for data-intensive analysis in a wide range of science and engineering applications.   The project addresses major data challenges in seven different communities: Biomolecular Simulations, Network and Computational Social Science, Epidemiology, Computer Vision, Spatial Geographical Information Systems, Remote Sensing for Polar Science, and Pathology Informatics.  The project libraries will have the same beneficial impact on data analytics that scientific libraries such as PETSc, MPI and ScaLAPACK have had for supercomputer simulations.  These libraries will be implemented to be scalable and interoperable across a range of computing systems including clouds, clusters and supercomputers."
"1829740","CyberTraining: CIU: The LSST Data Science Fellowship Program","OAC","CyberTraining - Training-based, SPECIAL PROGRAMS IN ASTRONOMY","08/01/2018","06/29/2018","Adam Miller","IL","Northwestern University","Standard Grant","Sushil Prasad","07/31/2021","$499,251.00","","amiller@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","044Y, 1219","026Z, 062Z, 1207, 7361, 9179","$0.00","This National Science Foundation (NSF) Training-based Workforce Development for Advanced Cyberinfrastructure award supplements graduate education in astronomy by providing in-depth training in the skills necessary to make scientific discoveries using big data. Ongoing and future surveys, such as the NSF's flagship optical telescope project, the Large Synoptic Survey Telescope (LSST), are producing data at an unprecedented rate. The sheer size of these data sets requires new working practices: sophisticated computational software and data mining procedures are necessary to fully exploit the rich information present in the data. However, these skills are not typically a core component of the astronomy and astrophysics graduate curriculum. The LSST Data Science Fellowship Program (DSFP) supplements traditional educational programs by training students in a variety of data science methods to work with and ultimately analyze big data. DSFP students are selected from a wide variety of universities using an innovative admissions procedure that increases the participation of students from underrepresented groups. Furthermore, DSFP students are trained in science communication and receive a certification in teaching data science so they can tutor peers and lead training workshops in the material learned as part of the program. The project serves the national interest, as stated by the National Science Foundation's mission: to promote the progress of science, by training the next generation of astronomers to have the computing skills necessary to derive scientific insights from the largest telescopic surveys that have ever been conducted.<br/><br/>DSFP students attend six week-long sessions over the course of two years as part of their program training. Each session is hosted by a different institution and designed to focus on a single topic including: the basics of managing and building code, statistics, machine learning, scalable programming, data management, image processing, visualization, and science communication. This curriculum empowers trainees to ask broader questions of their data, prepares them for the technical challenges associated with LSST, and exposes them to the tools and methods necessary to advance fundamental science research. Student participants spread the adoption of data science tools, methods, and resources via the aforementioned teaching workshops, fostering new pathways to discovery in the broader research community. Students must work in collaborative groups, which in conjunction with their science communication training, enhances their leadership and mentoring skills. To reach a broad audience, all materials developed as part of the program are made available to the public, and a guide to convert the material into a semester-long course at the undergraduate or graduate level is provided. This program prepares students for success in a wide range of careers, providing education in data science methodologies, domain-specific<br/>considerations, and professional skill development in research, teaching, and communication.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841471","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Kyle Cranmer","NY","New York University","Standard Grant","William Miller","09/30/2020","$486,879.00","Heiko Mueller","kyle.cranmer@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841448","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Michael Hildreth","IN","University of Notre Dame","Standard Grant","William Miller","09/30/2020","$422,981.00","","hildreth.2@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829724","CyberTraining: CIU: Preparing the Public Sector Research Workforce to Impact Communities through Data Science","OAC","CyberTraining - Training-based","09/01/2018","06/29/2018","Libby Hemphill","MI","University of Michigan Ann Arbor","Standard Grant","Sushil Prasad","08/31/2021","$498,778.00","Clifford Lampe, Lynette Hoelter, Christopher Brooks","libbyh@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","044Y","026Z, 062Z, 7361","$0.00","The ability to share data among researchers, citizens, government agencies, and  educators creates potential for new research collaborations with significant real-world impact. However, cities are often unprepared to use cyberinfrastructure to support research that would impact their citizens and communities, and researchers often do not have access to or awareness of the kinds of data and questions that are relevant for communities. This project develops innovative and scalable instructional materials, for both in-person and online courses, to increase data science literacy to meet the public sector's emerging needs for experts in computational and data science. The materials emphasize the types of data necessary for communities to make informed decisions (e.g., administrative data on land use, constituent service requests, and crime statistics) and applies them to pressing issues presented by community partners, providing a real-world context for learning. The project leverages the University of Michigan School of Information's Citizen Interaction Design program and the Summer Program in Quantitative Methods of Social Research at the Inter-university Consortium for Political and Social Research (ICPSR) to train undergraduate students, graduate students, and public sector researchers in collecting, extracting, cleaning, annotating, and analyzing data generated and used by government organizations. The project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; by enabling cities to conduct research that improves their communities and making the instructional materials and data used in the courses available for use by other interested educators, communities, and citizens. <br/><br/>The project addresses bottlenecks in scientific and engineering research workforce development by developing and offering three instructional activities: (a) a project-based course in which students work directly with Michigan communities to design cyberinfrastructure tools, and (b) two massive, open online courses (MOOCs) in which students learn the fundamentals of data science for work in the public sector. These courses provide scalable training and education programs to increase cyberinfrastructure-enabled research in the public sector that leverages administrative data. Course development occurs in collaboration between educators and community partners in cities throughout the Midwest, in order to ensure diversity of topics and audiences, timeliness, relevance, and direct application. This reliance on real data and immediate application to community issues is a novel approach in data science instruction. Despite being offered virtually, the MOOCs retain the pedagogical benefits of working on meaningful projects with real stakeholders. All three courses will be offered and evaluated at least once over the course of the project, and the course materials will be made publicly available through the University of Michigan?s School of Information, ICPSR, and other forums (e.g., the University's institutional repository, Deep Blue) for maximum impact. The long-term goals are to broaden engagement between researchers and communities to leverage advanced cyberinfrastructure to support public sector STEM research and to contribute to the infrastructure for online, dynamic, personalized lessons and certifications. Through partnership with the Midwest Big Data Hub, the project ensures that communities in the Midwest have access to resources for workforce development and opportunities to refine educational materials to serve their specific needs now and in the future.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818253","Computation for the Endless Frontier","OAC","PETASCALE - TRACK 1","09/01/2018","08/31/2018","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Edward Walker","08/31/2023","$60,000,000.00","Dhabaleswar Panda, Omar Ghattas, Tommy Minyard, John West","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7781","026Z","$0.00","Computation is critical to our nation's progress in science and engineering. Whether through simulation of phenomena where experiments are costly or impossible, large scale data analysis to sift the enormous quantities of digital data scientific instruments can produce, or machine learning to find patterns and suggest hypothesis from this vast array of data, computation is the universal tool upon which nearly every field of science and engineering relies upon to hasten their advance. This project will deploy a powerful new system, called ""Frontier"", that builds upon a design philosophy and operations approach proven by the success of the Texas Advanced Computing Center (TACC) in delivering leading instruments for computational science. Frontier provides a system of unprecedented scale in the NSF cyberinfrastructure that will yield productive science on day one, while also preparing the research community for the shift to much more capable systems in the future.  Frontier is a hybrid system of conventional Central Processing Units (CPU) and Graphics Processing Units (GPU), with performance capabilities that significantly exceeds prior leadership-class computing investments made by NSF.  Importantly, the design of Frontier will support the seamless transition of current NSF leadership-class computing applications to the new system, as well as enable new large-scale data-intensive and machine learning workloads that are expected in the future.  Following deployment, the project will operate the system in partnership with ten academic partners.  In addition, the project will begin planning activities in collaboration with leading computational scientists and technologists from around the country, and will leverage strategic public-private partnerships to design a leadership-class computing facility with at least ten times more performance capabilities for Science and Engineering research, ensuring the economic competitiveness and prosperity for our nation at large.<br/><br/>TACC, in partnerships with Dell EMC and Intel, will deploy Frontier, a hybrid system offering 39 PF (double precision) of Intel Xeon processors, complemented by 11 PF (single precision) of GPU cards for machine learning applications. In addition to 3x the per node memory of NSF's prior leadership-class computing system primary compute nodes, Frontier will have 2x the storage bandwidth in a storage hierarchy that includes 55PB of usable disk-based storage and 3PB of 'all flash' storage, to enable next generation data-intensive applications and support for the data science community.  Frontier will be deployed in TACC's state-of-the-art datacenter which is configured to supply 30% of the system's power needs from renewable energy.  Frontier will include support for science and engineering in virtually all disciplines through its software environment support for application containers, as well as through its partnership with ten academic institutions providing deep computational science expertise in support of users on the system. The project planning effort for a Phase 2 system with at least 10x performance improvement will incorporate a community-driven process that will include leading computational scientists and technologists from around the country and leverage strategic public-private partnerships.  This process will ensure the design of a future NSF leadership-class computing facility that incorporates the most productive near-term technologies, and anticipates the most likely future technological capabilities for all of science and engineering requiring leadership-class computational and data-analytics capabilities.  Furthermore, the project is expected to develop new expertise and techniques for leadership-class computing and data-driven applications that will benefit future users worldwide through publications, training, and consulting.  The project will leverage the team's unique approach to education, outreach, and training activities to encourage, educate, and develop the next generation of leadership-class computational science researchers. The team includes leaders in campus bridging, minority-serving institute (MSI) outreach, and data technologies who will oversee efforts to use Frontier to increase the diversity of groups using leadership-class computing for traditional and data-driven applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829622","CyberTraining: CIU: SJSU Data Science for All Seminar Series","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Leslie Albert","CA","San Jose State University Foundation","Standard Grant","Sushil Prasad","08/31/2021","$410,060.00","Esperanza Huerta, Scott Jensen","leslie.albert@sjsu.edu","210 North Fourth Street","San Jose","CA","951125569","4089241400","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","The Nation's research enterprise faces a shortage of data scientists. Expanding the pipeline of data science students, particularly from underrepresented populations, requires educational institutions to increase awareness of data science and inspire a passion for data in students as they begin their academic careers. Currently, few community colleges or undergraduate programs provide training in cyberinfrastructure tools or data science techniques to a broad student population. This project takes a novel approach to augmenting the Nation's data science workforce by training community college and undergraduate students to provide data analytics support to data scientists through a series of ""Data Science for All"" extracurricular seminars. The seminars require no prior data science knowledge, emphasize transferable skills, and present a feasible path into data science-related research and other careers for students from a broad array of disciplines and from underrepresented groups without extending their time to graduation. By increasing the Nation's data science capabilities and the diversity of its data science research workforce, the project serves the national interest, as stated by NSF's mission: to promote progress of science and advance the prosperity and welfare of the Nation. <br/><br/>The goals of this project are to increase undergraduate student awareness of data-driven science and to grow and diversify the population of students trained to perform data wrangling - the data acquisition, transformation, cleaning, and profiling required to prepare data for analysis. According to industry experts, data wrangling is the ""heavy lifting"" of data science, constituting up to 80% of a data scientist's daily work. Shifting this time-consuming effort to trained data analysts free data scientists to focus more of their time on research. The project achieves its goals through the development and delivery of widely consumable, extracurricular seminars providing interactive training on data science concepts and industry-leading data wrangling tools to undergraduate and community college students. Initial seminar topics, selected in collaboration with the project's advisory board, include Python, Jupyter notebooks, Apache Spark, Tableau, and demystifying artificial intelligence (AI). The seminars' focus on data wrangling also introduces students to data preparation documentation - capturing the data provenance needed for reproducible science. This project's contribution to the Nation's data science workforce is broadened through the free and open distribution of its seminar materials and supplemental resources and its online instructor support community. To encourage adoption at Bay Area community colleges and universities, instructor training is provided through co-instruction and a teaching-the-teacher model. The project contributes to pedagogical research by identifying instructional approaches most effective in teaching data science to a diverse population of undergraduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836650","S2I2: Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2018","03/06/2019","G.J. Peter Elmer","NJ","Princeton University","Cooperative Agreement","Bogdan Mihaila","08/31/2023","$12,500,000.00","Gordon Watts, Brian Bockelman","gelmer@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","1253, 7244, 8004","026Z, 062Z, 7569, 8211","$0.00","The quest to understand the fundamental building blocks of nature and their interactions is one of the oldest and most ambitious of human scientific endeavors. In Elementary Particles Physics, the most successful theory to date is known as the ""Standard Model"" of particle physics. Facilities such as CERN's Large Hadron Collider (LHC) represent a huge step forward in this quest as evidenced by the discovery of the Higgs boson. The next phase of this global scientific project will be the High-Luminosity LHC (HL-LHC), which will collect data starting circa 2026 and continue into the 2030's. The primary science goal at the HL-LHC is to search for physics beyond the Standard Model. In the HL-LHC era, the ATLAS and CMS experiments will record 10 times as much data from 100 times as many collisions as were used to discover the Higgs boson. As such, significant R&D advances must be achieved in the software for acquiring, managing, processing and analyzing HL-LHC data to realize the scientific potential of the upgraded accelerator and detectors and the planned physics program. In this context, the Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP) will play a leading role to meet the software and computing challenges of the HL-LHC. <br/><br/>The Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP) addresses key elements of the international ""Roadmap for HEP Software and Computing R&D for the 2020s"" and implements the ""Strategic Plan for a Scientific Software Innovation Institute (S2I2) for High Energy Physics"" submitted to the NSF in December 2017. IRIS-HEP will advance R&D in three high-impact areas: (1) development of innovative algorithms for data reconstruction and triggering; (2) development of highly performant analysis systems that reduce `time-to-insight' and maximize the HL-LHC physics potential; and (3) development of data organization, management and access (DOMA) systems for the community's upcoming Exabyte era. IRIS-HEP will sustain investments in today's distributed high-throughput computing (DHTC) and build an integration path to deliver its R&D activities into the distributed production infrastructure. As an intellectual hub, IRIS-HEP will lead efforts to (1) build convergence research between HEP and the Cyberinfrastructure, Data Science and Computer Science communities for novel approaches to address the compelling software and computing challenges of HL-LHC era HEP experiments, (2) engage broadly with researchers and students from U.S. Universities and labs emphasizing professional development and training, and (3) sustain HEP software and underlying knowledge related to the algorithms and their implementations over the two decades required. In addition to enabling the best possible HL-LHC science, IRIS-HEP will bring together the larger Cyberinfrastructure and HEP communities to address the complex issues at the intersection of Exascale high-throughput computing and Exabyte-scale datasets in ways broadly relevant to many research domains with emerging data-intensive needs. The education and training provided by the Institute in the form of summer schools and a fellows program will contribute to a highly qualified STEM workforce as most students and even most post-docs move into the private sector taking their skills with them. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"", one of the 10 Big Ideas for Future NSF Investments. <br/><br/>This project is supported by the Office of Advanced Infrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Physics in the Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1730286","CyberTraining: CDL: Security-Assured Data Science Workforce Development in Pennsylvania","OAC","CyberTraining - Training-based, CYBERCORPS: SCHLAR FOR SER","09/01/2017","08/14/2017","Balaji Palanisamy","PA","University of Pittsburgh","Standard Grant","Sushil K Prasad","08/31/2020","$476,903.00","James Joshi, Konstantinos Pelechrinis, Brian Stengel","bpalan@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","044Y, 1668","7254, 7361, 7434, 9178, 9179, SMET","$0.00","Data is growing exponentially at unprecedented levels.   In the recent years, we have been witnessing a huge proliferation of digital data capturing our lives and our surroundings.  While this provides promising opportunities for path-breaking innovation using data science and data-driven technologies, ensuring data security and data privacy has become a critical barrier. Cybersecurity is a key component of data science and security-assured data science impacts all aspects of personal, organizational, community and national life.  The workforce capable of impacting security-assured data science is significantly understaffed, while the demand for workforce with required skills is fast increasing.  This project creates a new model and curricula for foundational education in security-assured data science to young professionals that provides an opportunity to create a workforce trained to make innovative solutions leading to smarter health care, improved social good, smart transportation, advancements in social, behavioral, and economic sciences and smarter analytics for security.  The project thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense. <br/><br/>To address this growing national challenge to enable the next generation data-enabled innovation and research with qualified talent, it is critical that regional collaborations be formed to streamline and support sharing of data science and cybersecurity resources in the context of security-assured data science training and education. This project expands upon existing partnerships with local school districts to increase digital literacy and cybersecurity awareness at the secondary level and induce high school students to initiate activities that would enable them to pursue cybersecurity and data science education at the postsecondary level.  The project aims to collaboratively develop an integrated curriculum on Security Assured Data Science Education and Training (SADET), an undergraduate level curriculum to be offered to high school students as part of Pitt's College in High School (CHS) program.  The key goals and activities of the project include (a) developing interdisciplinary curriculum components focused on security-assured data Science education to be offered through University of Pittsburgh's College in High School (CHS) program, (b) enhancing and widening University of Pittsburgh's existing offering of introductory cybersecurity and information science courses in this area, (c) promoting and increasing the adoption of SADET modules through the CHS program, (d) conducting a security-assured data science materials competition for high school teachers to develop additional educational and training materials for use in classroom teaching, and (e) creating an online repository and a regional alliance to share security and data science training resources and expertise, and to increase the awareness, training and education in the western Pennsylvania region through workshops.  The proposed framework will provide a model for more regional collaborations at the intersection of data science and cybersecurity areas in the context of security-assured data science education, training and research.  The framework will also provide a basis to reach to broader communities including schools, colleges, industries and government related organizations.  Long-term impact of the project includes bridging the gaps between high school and college level curricula in this rapidly emerging field.  The project also provides a model for increasing the participation in data science and data security related fields at the undergraduate level leading to a focused workforce development in this important area."
"1664018","Collaborative Research: SI2-SSI:  Cyberinfrastructure for Advancing Hydrologic Knowledge through Collaborative Integration of Data Science, Modeling and Analysis","OAC","SPECIAL INITIATIVES, EAR, XC-Crosscutting Activities Pro, Software Institutes, EarthCube","10/01/2017","09/14/2017","Ray Idaszak","NC","University of North Carolina at Chapel Hill","Standard Grant","Stefan Robila","09/30/2021","$540,000.00","Hong Yi, Michael Stealey","rayi@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1642, 6898, 7222, 8004, 8074","026Z, 7433, 8004, 8009","$0.00","Researchers across the country and around the world expend tremendous resources to gather and analyze vast stores of hydrologic data and populate a myriad of models to better understand hydrologic phenomena and find solutions to vexing water problems. Each of those researchers has limited money, time, computational capacity, data storage, and ability to put that data to productive use. What if they could combine their efforts to make collaboration easier? What if those collected data sets and processed model outputs could be used collaboratively to help advance hydrologic understanding beyond their original purpose? HydroShare is a system to advance hydrologic science by enabling the scientific community to more easily and freely share products resulting from their research, not just the scientific publication summarizing a study, but also the data and models used to create the scientific publication. HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation and to improve reproducibility and community trust in scientific findings through transparency. As a platform for collaboration and running models on advanced computational infrastructure, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is designed to help researchers easily meet the sharing requirements of data management plans while at the same time providing value added functionality that makes metadata capture more effective and helps researchers improve their work productivity. This project will extend the capabilities of the HydroShare cyberinfrastructure to enhance support for scientific methods, advance the social capabilities of HydroShare to enable improved collaborative research, integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage. and establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Empowering HydroShare users with the ability to rapidly develop web application programs opens the door to unforeseen, innovative combinations of data and models. WRF-Hydro, the framework for the NOAA National Water Model, will be used as a use case for collaboration on model development. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. Collectively, this functionality will provide a computing framework for transforming the practice of broad science communities to leverage advances in data science and computation and accelerate discovery.<br/><br/>HydroShare is a system for sharing hydrologic data and models aimed at giving hydrologists the cyberinfrastructure needed to manage data, innovate and collaborate in research to solve water problems. It addresses the challenges of sharing data and hydrologic models to support collaboration and reproducible hydrologic science through the publication of hydrologic data and models. With HydroShare users can: (1) share data and models with colleagues; (2) manage who has access to shared content; (3) share, access, visualize and manipulate a broad set of hydrologic data types and models; (4) use the web services interface to program automated and client access; (5) publish data and models to meet the requirements of research project data management plans; (6) discover and access data and models published by others; and (7) use web apps to visualize, analyze, and run models on data. This project will extend the capabilities of HydroShare to: (1) enhance support for scientific methods enabling systematic data and model analysis and hypothesis testing; (2) advance the social capabilities of HydroShare to enable improved collaborative research; (3) integrate with 3rd party consumer data storage systems to provide more flexible and sustainable data storage; and (4) establish an application testing environment to empower researchers to develop their own computer programs to act on and work with data in HydroShare. Under development since 2012 and first released in 2014, HydroShare supports the sharing and publication of hydrologic data and models. This capability is necessary for community model development, execution, and evaluation. As a platform for collaboration and cloud based computation on network servers remote from the user, HydroShare enhances the capability for data intensive research in hydrology and other aligned sciences. HydroShare is innovative from a computer science and CI perspective in the way computation and data sharing are framed as a network computing platform that integrates data storage, organization, discovery, and programmable actions through web applications (web apps). Support for these three key elements of computation allows researchers to easily employ services beyond the desktop to make data storage and manipulation more reliable and scalable, while improving ability to collaborate and reproduce results. The generation of new understanding, through integration of information from multiple sources and reuse and collaborative enrichment of research data and models, will be enhanced. Structured and systematic model process intercomparisons and alternative hypothesis testing will be enabled, bringing, through user friendly CI, the latest thinking in advancing hydrologic modeling to a broad community of earth science researchers, thereby transforming research practices and the knowledge generated from this research. Interoperability with consumer cloud storage will greatly ease entry of content into HydroShare and support its sustainability. This meshing of the rigorous metadata model of HydroShare with consumer file sharing will enhance reproducibility as well as provide an innovative mechanism for sharing and collaboration. Empowering HydroShare users with the ability to rapidly develop web apps opens the door to unforeseen, innovative combinations of data and models. WRF- Hydro will be used as a use case for collaboration on model development. WRF-Hydro provides a reach-based high resolution representation of hydrologic processes, and offers the potential to bring together scientists working at scales from research catchments on the order of 1 to 100s of square kilometers as well as those working at regional to continental scales and cut across disciplines from environmental engineering to aquatic ecologists. Since WRF-Hydro is used by NOAA as part of the National Water Model (NWM), this collaboration opens possibilities for transfer of research to operations. This project will adapt current best practices in CI for interoperability and extensibility to serve this multidisciplinary community of scientists. HydroShare has already had a broader impact, with documented rapid growth in use and uptake by other projects including in EarthCube. It will become sustainable community CI through operation as part of the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) Water Data Center (WDC) facility. The use of WRF- Hydro/NWM, as a driving use case, will advance CI for community based model improvement. Through the Summer Young Innovators Program at the National Water Center (NWC), supported by the National Weather Service (NWS) and operated by CUAHSI, a pathway already exists to translate research findings to the operational needs of federal agencies participating in the NWC. HydroShare already touches a broad and diverse community, with user base including Native American tribes, hydrologic science students, and faculty researchers across the U.S. This proposal builds on the success of HydroShare to extend its capabilities and broaden model hypothesis testing, collaborative data sharing, and open app development across earth science research and education."
"1841456","Collaborative Research: Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Mark Neubauer","IL","University of Illinois at Urbana-Champaign","Standard Grant","William Miller","09/30/2020","$499,872.00","Daniel Katz","msn@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7684","020Z, 062Z","$0.00","The National Science Foundation (NSF) has made significant investments in major multi-user research facilities (MMURFs), which are the foundation for a robust data-intensive science program. Extracting scientific results from these facilities involves the comparison of ""real"" data collected from the experiments with ""synthetic"" data produced from computer simulations. There is wide growing interest in using new machine learning (ML) and artificial intelligence (AI) techniques to improve the analysis of data from these facilities and improve the efficiency of the simulations. The SCAILFIN project will use recently developed algorithms and computing technologies to bring cutting-edge data analysis techniques to such facilities, starting with the data from the international Large Hadron Collider. One result of these advancements will be that research groups at smaller academic institutions will more easily be able to access to the necessary computing resources which are often only available at larger institutions. Removing access barriers to such resources democratizes them, which is key to developing a diverse workforce. This effort will also contribute to workforce development through alignment of high-energy physics data analysis tools with industry computing standards and by training students in high-value data science skills.<br/><br/>The main goal of the SCAILFIN project is to deploy artificial intelligence and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) that is developed to be integrated into existing CI elements, such as the REANA system. The  analysis of LHC data is the project's primary science driver, yet the technology is sufficiently generic to be widely applicable. The LHC experiments generate tens of petabytes of data annually and processing, analyzing, and sharing the data with thousands of physicists around the world is an enormous challenge. To translate the observed data into insights about fundamental physics, the important quantum mechanical processes and response of the detector to them need to be simulated to a high-level of detail and accuracy. Investments in scalable CI that empower scientists to employ ML approaches to overcome the challenges inherent in data-intensive science such as simulation-informed inference will increase the discovery reach of these experiments. The development of the proposed scalable CI components will catalyze convergent research because 1) the abstract LFI problem formulation has already demonstrated itself to be the ""lingua franca"" for a diverse range of scientific problems; 2) the current tools for many tasks are limited by lack of  scalability for data-intensive problems with computationally-intensive simulators; 3) the tools the project is developing are designed to be scalable and immediately deployable on a diverse set of computing resources due to the design; and 4) the integration of additional commonly-used workflow languages to drive the optimization of ML components and to orchestrate large-scale workflows will lower the barrier-to-entry for researchers from other domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829717","CyberTraining:CIU:Computational and Data Science Literacy Curriculum Exchange","OAC","CyberTraining - Training-based","09/01/2018","06/29/2018","Katharine Cahill","OH","Ohio State University","Standard Grant","Sushil Prasad","08/31/2021","$499,734.00","Linda Akli, Ana Gonzalez, Asamoah Nkwanta, Dinadayalane Tandabany","kcahill@osc.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","044Y","026Z, 062Z, 7361, 9102, 9150","$0.00","There is a well-established national need for a research workforce with suitable knowledge of computational and data science (CDS). Although, some strides have been made in integrating CDS competencies into the university curriculum, the pace of change has been slow. Department boundaries, course requirements for STEM majors, and resource constraints all contribute to the slow pace of CDS curriculum implementation across STEM disciplines. Ohio Supercomputer Center (OSC) in collaboration with Bethune Cookman University (BCU), Clark Atlanta University (CAU), Morgan State University (Morgan), Southeastern Universities Research Association (SURA), Southern University and A&M College (SUBR), and the University of Puerto Rico at Mayaguez (UPRM) are implementing a pilot Computational and Data Science (CDS) Curriculum Exchange (C2Exchange) to address the challenges experienced by resource constrained institutions and, in particular, those faced by Minority Serving Institutions (MSIs). This project serves the national interest, as stated by NSF's mission: to promote the progress of science and to serve the national health, prosperity, and welfare by advancing STEM education and improving the computational and data science skills of our future research workforce.<br/> <br/>The curriculum exchange approach is intended to minimize the faculty preparation time required to deliver new and updated courses, increase the number of CDS courses offered at each institution, and allow courses to be offered that may not initially have large enrollments at an individual institution by drawing students from all of the participating institutions. This pilot develops the foundation for a CDS Curriculum Exchange that can increase the capacity of small under-resourced MSIs to deliver undergraduate CDS curriculum, minors, and certificates with low investment and be extensible for future participation by additional resource constrained institutions. Students in these institutions will benefit by having access to a broader selection of computational science courses. Training in these highly sought after skills aims to increase their chances of being part of the STEM workforce or continuing on to graduate school and becoming effective researchers. Each academic institution is contributing and receiving courses, such as Introduction to Modeling and Simulation, Computational Chemistry, as well as Data Visualization, with the goal of providing a sequence of CDS courses that can form part of a certificate or minor program at each institution. The consortium delivers courses through a blended online learning model that has been tested and shown to be effective for technical subjects. This model is flexible enough to reduce preparation time for local instructors, respond to students' difficulties with material, and to accommodate institutions' varied schedules. If successful, this exchange model of instruction will be extensible to a wide range of institutions as well as to specialized instruction at the graduate level.  The project explores important insights into both the institutional and pedagogical questions associated with the implementation of shared course networks in resource constrained environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1751161","CAREER: Building an Advanced Cyberinfrastructure for the Data-Driven Design of Chemical Systems and the Exploration of Chemical Space","OAC","CAREER: FACULTY EARLY CAR DEV, Theory, Models, Comput. Method","03/01/2018","02/08/2018","Johannes Hachmann","NY","SUNY at Buffalo","Standard Grant","Sushil K Prasad","02/28/2023","$561,685.00","","hachmann@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1045, 6881","026Z, 062Z, 1045, 8084, 9263","$0.00","Innovation in chemistry and materials is a key driver of economic development, prosperity, and a rising standard of living.  It also offers solutions to pressing problems on energy, environmental sustainability, and resources that shape our society.  This research program is designed to boost the chemistry community's capacity to address these challenges by transforming the process that creates underlying innovation. The research promotes a shift away from trial-and-error searches and towards rational design.  These combine traditional chemical research with modern data science by introducing tools such as machine learning into the chemical context.  This project enables and advances this emerging field by building a cyberinfrastructure that makes data-driven research a viable and widely accessible proposition for the chemistry community, and thereby an integral part of the chemical enterprise.  Tools and methods developed in this research provide the means for the large-scale exploration of chemical space and for a better understanding of the hidden mechanisms that determine the behavior of complex chemical systems.  These insights can potentially accelerate, streamline, and ultimately transform the chemical development process.  The project also tackles the concomitant need to adapt education to this new research landscape in order to adequately equip the next generation of scientists and engineers, to build a competent and skilled workforce for the cutting-edge R&D of the future, and to ensure the competitiveness of US students in the international job market.  By promoting minority participation in this promising field, it contributes to a sustained push towards equal opportunity in our society.  This project thus promotes the progress of science and advances prosperity and welfare as stated by NSF's mission.<br/><br/>While there is growing agreement on the value of data-driven discovery and rational design, this approach is still far from being a mainstay of everyday research in the chemistry community.  This work addresses three key obstacles: (i) data-driven research is beyond the scope and reach of most chemists due to a lack of available and accessible tools, (ii) many fundamental and practical questions on how to make data science work for chemical research remain unresolved, and (iii) data science is not part of the formal training of chemists, and much of the community thus lacks the necessary experience and expertise to utilize it.  This research centers around the creation of an open, general-purpose software ecosystem that fuses in silico modeling, virtual high-throughput screening, and big data analytics (i.e., the use of machine learning, informatics, and database technology for the validation, mining, and modeling of resulting data sets) into an integrated research infrastructure.  A key consideration is to make this ecosystem as comprehensive, robust, and user-friendly as possible, so that it can readily be employed by interested researchers without the need for extensive expert knowledge.  It also serves as a development platform and testbed for innovation in the underlying methods, algorithms, and protocols, i.e., it allows the community to systematically and efficiently evaluate the utility and performance of different techniques, including new ones that are being introduced as part of this project.  A meta machine learning approach is being developed to establish guidelines and best practices that provide added value to the cyberinfrastructure.  The work is driven by concrete molecular design problems, which serve to demonstrate the efficacy of the overall approach.  The educational challenges that arise from the qualitative novelty of data-driven research and its inherent interdisciplinarity are addressesed by leveraging a new graduate program in Computational and Data-Enabled Science and Engineering for cross-cutting course and curricular developments, the creation of interactive teaching materials, and a skill-building hackathon initiative.  This award is jointly made with the Division of Chemistry's, Chemical Theory, Models and Computational Methods Program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835717","Elements: Software: HDR: A knowledge base of deep time to facilitate automated workflows in studying the co-evolution of the geosphere and biosphere","OAC","XC-Crosscutting Activities Pro, CESER-Cyberinfrastructure for, EarthCube","12/01/2018","09/06/2018","Xiaogang Ma","ID","University of Idaho","Standard Grant","Stefan Robila","11/30/2021","$596,975.00","","xgmachina@gmail.com","Office of Sponsored Programs","MOSCOW","ID","838443020","2088856651","CSE","7222, 7684, 8074","026Z, 062Z, 077Z, 7923, 8004, 9150","$0.00","This project will result in the creation of a software that will support research in the Earth's deep time history. The co-evolution of the geosphere and biosphere is one of the fundamental questions for the 21st century Earth science. The multi-disciplinary characteristics of the research questions on co-evolution are reflected in the various subjects of datasets that need to be integrated. In the past decades, many open data facilities have been built through the support from NSF and other sources. However, the shortage of efficient methods for accessing and synthesizing multi-source datasets hamper the data-intensive co-evolution research. Geologic time is an essential topic in the co-evolving geosphere and biosphere, and can be used as a common reference to connect various parameters among the data silos. This project will improve the machine readability and alignment of various global, local and regional geologic time standards and build a knowledge base of deep time and its service on the Web. All the deliverables will be well-documented and offered under open-access to promote a national cyberinfrastructure ecosystem. The planned tasks and activities will leverage the usage of existing data facilities, facilitate executable and reproducible workflows, generate best practices of cross-disciplinary data science, generate state-of-the-art materials to education programs, and engage the participation of female and underrepresented groups. Shared in the national cyberinfrastructure, the knowledge base built in the project will be able to support a broad range of research, education and outreach programs, which will benefit not only science and engineering but also the society at large.<br/><br/>The research question to be addressed is the heterogeneity of geologic time concepts that hamper the data synthesis among multiple data facilities. Accordingly, the objective of this project is to build a knowledge base of deep time to automate geoscience data access and integration in the open data environment, and to support data synthesis in executable workflows for data-intensive scientific discovery. The development approach will include both top-down and bottom-up tracks to leverage previous works on geologic time ontologies and address end user needs through use case analyses. With carefully designed activities and work plan, deliverables from this project will include a machine-readable knowledge base of aligned geologic time standards, services and packages for accessing and querying the knowledge base, and best practices of data synthesis in workflow platforms for studying the co-evolution. The developed knowledge base of deep time will provide powerful support to co-evolution researchers to tackle data heterogeneity issues. Robust services the knowledge base will be built to support automated data synthesis in workflow platforms to advance the co-evolution research. The source code and metadata of the knowledge base will be released on GitHub and registered on community repositories to enable reuse and adaptation. <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences, and the OAC Cyberinfrastructure for Emerging Science and Engineering Research (CESER) program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828187","MRI: Acquisition of an HPC System for Data-Driven Discovery in Computational Astrophysics, Biology, Chemistry, and Materials Science","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC","10/01/2018","08/24/2018","Srinivas Aluru","GA","Georgia Tech Research Corporation","Standard Grant","Stefan Robila","09/30/2021","$3,699,317.00","Surya Kalidindi, Charles Sherrill, Deirdre Shoemaker, Richard Vuduc","aluru@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1189, 1640","026Z, 1189","$0.00","The project funds the purchase of a high-performance computing and storage system at the Georgia Institute of Technology. This computing instrument will support data-driven research in astrophysics, biosciences, computational chemistry, materials and manufacturing, and computational science. These projects contribute to national initiatives in big data, strategic computing, materials genome, and manufacturing partnership; and NSF supported observatories such as the gravitational wave observatory and the South Pole neutrino observatory. The system also serves as a springboard for developments of codes, software prototyping, and scalability studies prior to using national supercomputers. Advances made in computational methods and scientific software are disseminated in the form of open-source codes and data analysis portals. Over 33 faculty, 54 research scientists/postdocs, 195 graduate students, and 56 undergraduate students will immediately benefit from the instrument. In addition, the system provides training opportunity at all levels from undergraduate students to early career researchers, in important interdisciplinary areas of national need. A fifth of the system capacity is utilized to enable research activities of regional partners, researchers from minority serving institutions, and other users nationally through XSEDE participation. The project involves undergraduate student participation from historically black colleges from Atlanta metropolitan area. Public outreach efforts are planned through videos of public interest and local events such as the Atlanta Science Festival.<br/><br/>The cluster will combine regular compute nodes with others configured to emphasize one of the following: big memory, big local storage, solid state storage, Graphics Processing Units (GPU), and ARM processors. In doing so, the system can be employed by a diversity of projects. In astrophysics, the instrument bolsters data-driven research including detection of gravitational waves, astrophysical neutrinos, and gamma rays. It does it by leveraging data from leading astroparticle observatories and contributing to their mission. It also leads to improved insights into formation of supermassive black holes and large-scale structure of the universe. The computing system also aids the development of parallel software in computational genomics, systems biology, and health analytics. Important applications in assembly and network analysis of plant genomes, and environmental metagenomics are pursued. The instrument also enables next generation algorithms and software for computational chemistry and expands the boundaries of molecular simulation. The system enables advances in density function theory, enhances studies of crystal defects and nanostructures, and injects novel use of machine learning techniques in computational chemistry. It also fosters the development of data science methodologies to identify building blocks of materials at multiple scales, thus significantly reducing the development and deployments cycles for new materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1560168","REU Site: iCER ACRES: iCER Advanced Computational Research Experience for Students","OAC","","08/01/2016","06/24/2016","Kenneth Merz","MI","Michigan State University","Standard Grant","Sushil Prasad","07/31/2019","$360,000.00","Brian O'Shea","merzjrke@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","p226","7736, 9250","$0.00","This research experience for undergraduates (REU) project aims to engage undergraduates in learning experiences that increase student's interest and ability to conduct computational and data science research in a range of scientific domains. The ability to effectively solve problems on high-performance computational resources is an area of increasing national importance. The educational process supported by iCER ACRES will enable students to address a range of computational projects of significant benefit to the US. In particular, computational science is now important in improving the health of the country, advancing our common prosperity and welfare and advancing the cyber-defense of the US. Projects provided through this site will advance the overall national skillset in the computational and data sciences field through hands-on educational activities. In addition to providing undergraduates with research learning experiences, this project will broaden the participation of underrepresented populations in STEM research, provide early HPC exposure to students from institutions with limited access to HPC resources, especially women and underrepresented groups and motivate students to pursue careers in computational fields. <br/><br/>The overarching goal of the program is to increase students' technical skillset to address in a broad range of computational fields. iCER ACRES achieves this goal by: (i) engaging undergraduates in original interdisciplinary collaborative research in data science and scientific computing domains; (ii) introducing students to a broad set of ""classical"" programming algorithms and their applications for problem-solving in data science and scientific computing; and (iii) exposing undergraduates to high performance computing (HPC) resources used to conduct research and software engineering practices applied to increase the quality of scientific applications. These program elements are designed to:<br/>- Allow students to experience and learn about the research process through working on an unsolved, open-ended research problem;<br/>- Increase participants' disciplinary knowledge and their understanding of how that knowledge may be applied;<br/>- Assist students in defining and refining their research and career interests;<br/>- Provide students with an introduction to the world of academia and graduate school; and<br/>- Provide a forum for collegial interaction with a faculty member.<br/>Research projects provided through this site focus on the development and enhancement of algorithms, models, and software for applications in multiple domains that require high performance resources.  These domains include but are not limited to: computational chemistry, biology, astrophysics, mathematics, big data science, computational electromagnetics and cybersecurity. This site is supported by the Department of Defense in partnership with the NSF REU program."
"1829752","Collaborative Research: CyberTraining: CIU: Toward Distributed and Scalable Personalized Cyber-Training","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Prasun Dewan","NC","University of North Carolina at Chapel Hill","Standard Grant","Sushil Prasad","08/31/2021","$439,366.00","Sreekalyani Bhamidi, Alison LaGarry","dewan@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","This project is addressing the challenge of providing distributed, scalable, and personalized training of cyberinfrastructures - systems that offer state-of-the-art cloud services for storing, sharing, and processing scientific data. Today, personalized training of these rapidly evolving, and hence relatively undocumented, systems requires trainer-supervised, hands-on use of these systems. These training sessions require trainees and trainers to be co-located and provide personalized training to a relatively small number of trainees. The project is developing new (a) domain-independent technologies in distributed collaboration and machine learning to reduce all three problems in a concerted manner, and (b) domain-dependent training material targeted at trainees in statistics, physical sciences, computer science, humanities, and medicine. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>A key technical insight in this work is that a cyberinfrastructure should not only support data science, but also make use of data science. The project is exploring two related innovations based on this insight: (1) Collaboration technologies that log, visualize and share the work of remote and local trainees to allow trainers to determine the need for remote or face-to-face assistance.  (2) Machine-learning technologies that mine trainee and trainer interactions so that trainees can be automatically instructed on how to solve their problems based on similar problems that have been previously solved by trainers and other trainees. The project is leveraging existing technologies and training techniques developed for a widely used NSF-supported cyberinfrastructure, called CyVerse. This system is domain-independent, but so far, its training material has been targeted mainly at plant-science research.  The project is extending the command interpreters and GUIs provided by CyVerse. The extended user-interfaces allow (a) trainees to announce difficulties and request recommendations, and (b) trainers to be aware of the progress of remote and local trainees, and remotely intervene when necessary. The functionality behind the user-interfaces is implemented by CyVerse-independent servers based on a general model of cyberinfrastructures, which includes the concepts of sharing and visualization of protected files, creation and execution of parameterized commands composed in workflows, and shareable, persistent work spaces. The project is adapting the CyVerse training material to cover new research domains including Geoscience, Political Science, and Biomedical Engineering. This expanded training material is being used to evaluate the proposed training technologies through training sessions for (a) students in a Statistics, Computer Science, Political Science, and interdisciplinary course, (b) attendees at three conferences targeted at Geoscientists, women, and Hispanics and Native Americans, respectively, (c) subjects in controlled lab studies, and (d) members of research groups at multiple institutes. The proposed qualitative and quantitative evaluation data gathered from these sessions are being used to assess not only the proposed technologies and training material, but also CyVerse and cyberinfrastructures in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829701","Collaborative Research: CyberTraining: CIU: Towards Distributed and Scalable Personalized Cyber-Training","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Blake Joyce","AZ","University of Arizona","Standard Grant","Sushil Prasad","08/31/2021","$60,631.00","","bjoyce3@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","This project is addressing the challenge of providing distributed, scalable, and personalized training of cyberinfrastructures - systems that offer state-of-the-art cloud services for storing, sharing, and processing scientific data. Today, personalized training of these rapidly evolving, and hence relatively undocumented, systems requires trainer-supervised, hands-on use of these systems. These training sessions require trainees and trainers to be co-located and provide personalized training to a relatively small number of trainees. The project is developing new (a) domain-independent technologies in distributed collaboration and machine learning to reduce all three problems in a concerted manner, and (b) domain-dependent training material targeted at trainees in statistics, physical sciences, computer science, humanities, and medicine. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/><br/>A key technical insight in this work is that a cyberinfrastructure should not only support data science, but also make use of data science. The project is exploring two related innovations based on this insight: (1) Collaboration technologies that log, visualize and share the work of remote and local trainees to allow trainers to determine the need for remote or face-to-face assistance.  (2) Machine-learning technologies that mine trainee and trainer interactions so that trainees can be automatically instructed on how to solve their problems based on similar problems that have been previously solved by trainers and other trainees. The project is leveraging existing technologies and training techniques developed for a widely used NSF-supported cyberinfrastructure, called CyVerse. This system is domain-independent, but so far, its training material has been targeted mainly at plant-science research.  The project is extending the command interpreters and GUIs provided by CyVerse. The extended user-interfaces allow (a) trainees to announce difficulties and request recommendations, and (b) trainers to be aware of the progress of remote and local trainees, and remotely intervene when necessary. The functionality behind the user-interfaces is implemented by CyVerse-independent servers based on a general model of cyberinfrastructures, which includes the concepts of sharing and visualization of protected files, creation and execution of parameterized commands composed in workflows, and shareable, persistent work spaces. The project is adapting the CyVerse training material to cover new research domains including Geoscience, Political Science, and Biomedical Engineering. This expanded training material is being used to evaluate the proposed training technologies through training sessions for (a) students in a Statistics, Computer Science, Political Science, and interdisciplinary course, (b) attendees at three conferences targeted at Geoscientists, women, and Hispanics and Native Americans, respectively, (c) subjects in controlled lab studies, and (d) members of research groups at multiple institutes. The proposed qualitative and quantitative evaluation data gathered from these sessions are being used to assess not only the proposed technologies and training material, but also CyVerse and cyberinfrastructures in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835643","RUI: Framework: Data - An Open Semantic Data Framework for Data-Driven Discovery","OAC","DATANET, Big Data Science &Engineering","01/01/2019","09/07/2018","Stuart Chalk","FL","University of North Florida","Standard Grant","Amy Walton","12/31/2021","$605,849.00","","schalk@unf.edu","1 UNF Drive","JACKSONVILLE","FL","322242645","9046202455","CSE","7726, 8083","026Z, 062Z, 077Z, 7925, 9229","$0.00","The project makes contributions to the ease of annotating, sharing, and searching heterogeneous data sets.  It focuses upon undergraduate training, emphasizing data science capabilities applied to a range of science problems.  <br/><br/>The project enables aggregation, search, and inference with heterogeneous datasets using a structured framework allowing data and metadata to be linked by encoding the framework as a JavaScript Object Notation (JSON) for Linked Data (JSON-LD) document.  The approach builds on existing developments such as the Scientific Data (SciData) framework and associated ontology that has been developed by the PI, and Shape Constraint Language (SHACL) shapes to provide efficient searching, browsing, and visualization of data.  The result extends existing approaches to link data and metadata and make data easily discoverable.  The activity emphasizes Research in Undergraduate Institutions (RUI), training more than 30 undergraduates, graduate students and a post-doctoral student in the application of data science techniques to an array of science problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835791","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","EarthCube","01/01/2019","08/27/2018","Charles Meertens","CO","UNAVCO, Inc.","Standard Grant","Amy Walton","12/31/2022","$830,728.00","David Mencin, Scott Baker","meertens@unavco.org","6350 Nautilus Dr.","Boulder","CO","803015394","3033817500","CSE","8074","062Z, 077Z, 7925","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835661","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","XC-Crosscutting Activities Pro, DATANET","01/01/2019","08/27/2018","Diego Melgar","OR","University of Oregon Eugene","Standard Grant","Amy Walton","12/31/2022","$405,494.00","","dmelgarm@uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","7222, 7726","062Z, 077Z, 7925","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849625","Toward a common digital continuum platform for big data and extreme-scale computing (BDEC2)","OAC","CYBERINFRASTRUCTURE","10/01/2018","08/28/2018","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Sushil Prasad","09/30/2020","$203,406.00","Daniel Reed, Peter Beckman, Geoffrey Fox","dongarra@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","7231","026Z, 062Z","$0.00","By the end of this decade, the world's store of digital data is projected to reach 40 zetabytes (10 to the power 21 bytes), while the number of network-connected devices (sensors, actuators, instruments, computers, and data stores) is expected to reach 20 billion. While these devices vary dramatically in their capabilities and number, taken collectively they represent a vast ""digital continuum"" of computing power and prolific data generators that scientific, economic, social, governmental and military concerns of all kinds will want and need to utilize. The diverse set of powerful forces propelling the growth of this digital continuum are prompting calls from various quarters for a next generation network computing platform - a digital continuum platform (DCP) - for creating distributed services in a world permeated by devices and saturated by digital data. But experience shows how challenging the creation of such a future-defining platform is likely to be, especially if the goal is to maximize its acceptance and use, and thereby the size of the community of interoperability it supports. Focusing on the strategically important realm of scientific research, broadly conceived, this project is staging six international workshops (two each in the United States, Europe, and Asia over a two-year period) to enable transnational research communities in a wide range of disciplines to converge on a common DCP to meet this challenge. Building on a decade of leadership in cyberinfrastructure planning, the Big Data and Extreme-scale Computing (BDEC2) community is attacking this problem by pursuing three complementary objectives: <br/>1. Draft a design for a ""digital continuum platform"" (DCP) to serve as shared software infrastructure for the growing continuum of computing devices and data sources on which future science will rely; <br/>2. Organize and plan an international demonstration of the feasibility and potential of the DCP; and<br/>3. Develop a corresponding ""shaping strategy"" that addresses all relevant stakeholders and moves the community toward convergence on a standard DCP specification. <br/>Thus, the project serves the national interest, as stated by NSF's mission: to promote the progress of science and to secure the national defense.<br/><br/>Creating a common digital continuum platform represents a grand challenge problem for the global cyberinfrastructure community. To address this monumental challenge and achieve its objectives, the BDEC2 community is organizing around four distinct but complementary activities:<br/>1. Surveying and analyzing the spectrum of application/workflow needs across diverse research and engineering communities who will use the digital continuum; <br/>2. Developing a reference design for a DCP system architecture that is able to manage the trade offs involved in using a widely shared infrastructure to satisfy diverse application community requirements; <br/>3. Strengthening cooperative and crosscutting efforts among stakeholders (e.g., research communities, commercial vendors, software developers, resource providers) in the ""cyber ecosystem"" of science and engineering; and <br/>4. Formulating a strategy for building community consensus on a common digital continuum within this same collection of stakeholders.<br/>To help researchers converge on critical problems for important user communities, foster and focus collaboration to solve those problems, and better coordinate software research and data sharing, the BDEC2 community process engages both international software research and data science communities. The process also includes inter-meeting working groups (e.g., for application/workflow analysis and DCP architecture). Combined with the work of the international meetings themselves, these working groups are intended to accelerate community-wide discussion and collaborative activities needed to address the multi-dimensional challenges of the emerging digital continuum. By achieving its goals, this project intends to supply the different stakeholder communities with the kind of well-defined vision and consensus building strategy necessary to realize a common, open, and interoperable DCP for digital continuum era. The project actively promotes participation by talented young scientists (up to 15 across the series) drawn from the academic community and with special attention to women and minorities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835692","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","DATANET, EarthCube, Big Data Science &Engineering","01/01/2019","08/27/2018","Ivan Rodero","NJ","Rutgers University New Brunswick","Standard Grant","Amy Walton","12/31/2022","$899,139.00","Juan Jose Villalobos","irodero@cac.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7726, 8074, 8083","062Z, 077Z, 7925, 8083","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1852538","REU Site: Cyberinfrastructure (CI) Research 4 Social Change","OAC","RSCH EXPER FOR UNDERGRAD SITES","03/01/2019","02/04/2019","Rosalia Gomez","TX","University of Texas at Austin","Standard Grant","Sushil K Prasad","02/28/2022","$323,689.00","Kelly Gaither","rosie@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1139","9102, 9250","$0.00","The world is increasingly more dependent on computational power, augmented by larger and larger networks of sensors and instruments. To fuel innovation across multiple disciplines, advanced computing (data, high performance computing, analytics) is required to keep pace with and accelerate the rate of scientific discoveries. In response to the National Science Foundation's mission to promote the progress of science; advance the national health, prosperity and welfare; and secure the national defense, the Cyberinfrastructure (CI) Research 4 Social Change REU project is actively engaging ten undergraduate students each summer for nine-weeks in solving real-world problems of national relevance, teaching them to not only be critical thinkers, but to be creative and reflective as well. The REU project is preparing the future scientific workforce to use advanced CI resources, thus building capacity in research areas that support major advances in understanding across a broad range of societal challenges. Students have a strong desire to have an impact in their communities, a strong desire to have a sense of belonging, and a strong desire to be materially part of something larger than themselves. Connecting social change principals to real-world problem solving is key to long-term success for recruiting and retaining underrepresented communities to computational fields and for preparing students to leverage the national CI.<br/> <br/>The REU aims to meet three objectives: (1) train students to use the national CI by integrating learning of computational science, data-enabled science and multi-disciplinary science in preparation for graduate programs and the workforce; (2) train students to apply advanced computational skills, critical thinking, and creativity to address problems relevant to society; and (3) increase the number of diverse and computationally competent students in the STEM pipeline. The enriching and transformative experience at a world-class supercomputing center includes: training in High Performance Computing (HPC), visualization, and data intensive computing; mentoring by The University of Texas (UT) at Austin researchers; social and team-building activities on the UT Austin campus; professional development and graduate school preparation; and leadership development and opportunities to develop and enhance communication skills. Research projects emphasize advanced computing as a tool to power discoveries that will impact social change for future generations. Using the CI, students conduct cutting-edge research in engineering,  science, and computational medicine: helping automate processes in support of cleaner energy sources for production of biofuel; enhancing research resources for modeling/prediction of porous material properties in fields of petroleum, civil and environmental engineering as well as geology; using data science and visualization to illustrate interfaces between science, traditional culture, and pressing societal problems in the Pacific region; developing and analyzing numerical algorithms for multiphysics, multiscale flow and transport problems in storm surge, tides, and coastal circulation; and developing and analyzing machine learning algorithms for individualized medicine. The REU recruits at least 50% underrepresented students. Targeted recruitment also includes women, first-generation college students, and majority students from institutions with limited research opportunities, including Minority-Serving Institutions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1664172","SI2-SSI: LIMPID: Large-Scale IMage Processing Infrastructure Development","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2017","09/14/2017","Bangalore Manjunath","CA","University of California-Santa Barbara","Standard Grant","Vipin Chaudhary","09/30/2022","$3,400,000.00","Tresa Pollock, Amit Roy Chowdhury, Nirav Merchant, Robert Miller","manj@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","1712, 8004","054Z, 7433, 8004, 8009, 9216, 9251","$0.00","Scientific imaging is ubiquitous: From materials science, biology, neuroscience and brain connectomics, marine science and remote sensing, to medicine, much of the big data science is image centric. Currently,interpretation of images is usually performed within isolated research groups either manually or as workflows over narrowly defined conditions with specific datasets. This LIMPID (Large-scale IMage Processing Infrastructure Development) project will have a transformative impact on such discipline-centric workflows through the creation of an extensive and unique resource for the curation, distribution and sharing of scientific image analysis methods. The project will create an image processing marketplace for use by a diverse community of researchers, enabling them to discover, test, verify and refine image analysis methods within a shared infrastructure.As a freely available, cloud-based resource, LIMPID will facilitate participation of underrepresented groups and minority-serving institutions, as well as international scientists, allowing them to addressquestions that would otherwise require expensive software. The potential impacts of the projectare significant: from wide dissemination of novel processing methods, to development of automaticmethods that can leverage data and human feedback from large datasets for software training andvalidation.For the broader scientific community, this immediately provides a resource for joint data and methods publication, with provenance control and security. This in turn will facilitate faster development and deployment of tools and foster new collaborations between computer scientists developing methods and scientific users. The project will prepare a diverse cadre of students and researchers, including women and members of under-represented groups, to tackle complex problems in an interdisciplinary environment. Through workshops, participation at scientific meetings, and summer undergraduate research internships, a broad community of users will be engaged to actively contribute to all aspects of research, development, and training during the course of this project.<br/>    <br/><br/>The primary goal is to create alarge scale distributed image processing infrastructure, the LIMPID, though a broad,interdisciplinary collaboration of researchers in databases, image analysis, and sciences. In order to create a resource ofbroad appeal, the focus will be on three types of image processing: simple detection and labelling of objects based on detection of significant features and leveraging recent advances in deep learning, semi-custom pipelines and workflows based onpopular image processing tools, and finally fully customizable analysis routines. Popular image processing pipeline tools will be leveraged to allow users to create or customize existing pipeline workflows and easily test these on large-scale cloud infrastructure from their desktop or mobile devices. In addition, a core cloud-based platform will be created where custom image processing can be created,shared, modified, and executed on large-scale datasets and applynovel methods to minimize data movement. Usage test cases will be created for three specific user communities: materials science, marine science and neuroscience. An industry supported consortium will be established at the beginning of the projecttowards achieving long-term sustainability of the LIMPID infrastructure.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate for Mathematical and Physical Sciences."
"1461549","SEES Fellows: Building Informatics: Utilizing Data-Driven Methodologies to Enable Energy Efficiency and Sustainability Planning of Urban Building Systems","OAC","EDUCATION AND WORKFORCE, SEES Fellows, CyberSEES","09/01/2014","07/27/2017","Rishee Jain","CA","Stanford University","Standard Grant","Sushil Prasad","08/31/2019","$628,179.00","","risheej@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7361, 8055, 8211","018Z, 7231, 7361, 8055, 8060, 8232, 9102, 9179","$0.00","The project is supported under the NSF Science, Engineering and Education for Sustainability Fellows (SEES Fellows) program, with the goal of helping to enable discoveries needed to inform actions that lead to environmental, energy and societal sustainability while creating the necessary workforce to address these challenges. Sustainability science is an emerging field that addresses the challenges of meeting human needs without harm to the environment, and without sacrificing the ability of future generations to meet their needs. A strong scientific workforce requires individuals educated and trained in interdisciplinary research and thinking, especially in the area of sustainability science. With the SEES Fellowship support, this project will enable a promising early career researcher to establish himself in an independent research career related to sustainability. <br/><br/>This project addresses the fact that buildings are responsible for over 40% of all energy consumption and GHG emissions in the US. The impact of building systems is even more pronounced in dense urban areas like New York City (NYC) where over 75% of GHG emissions come from energy used in buildings. Any transition towards a sustainable NYC will require addressing this energy and pollution intensive built environment. This research proposal aims to integrate data-driven methodologies from information science, social science, network science and urban planning with engineering to increase the energy efficiency and sustainability of urban building systems. Specifically, this project will identify district level energy efficiency opportunities for urban buildings systems, characterize the interdependencies between the built, natural and human environments and model the impact of energy policy instruments to enhance the sustainability and resilience of NYC and Mumbai, India. As a part of this project, the PI will build upon his expertise in civil engineering and building systems by incorporating data science, social science and urban planning into his exploration of energy and sustainability challenges facing urban building systems."
"1626217","MRI: Acquisition of a Shared High Performance Computational (HPC) Cluster for Research, Training and Institutional Use","OAC","MAJOR RESEARCH INSTRUMENTATION, EPSCoR Co-Funding","08/01/2016","06/29/2017","Gopinath Subramanian","MS","University of Southern Mississippi","Standard Grant","Stefan Robila","07/31/2019","$622,986.00","Jeremy Wiggert, Chaoyang Zhang, Jacob Schaefer, Jeffrey Wiggins, Bandana Kar","Gopinath.Subramanian@usm.edu","2609 WEST 4TH ST","Hattiesburg","MS","394015876","6012664119","CSE","1189, 9150","1189, 9150","$0.00","The University of Southern Mississippi (USM) will acquire a supercomputer to support research and training in computational and data-enabled science and engineering (CDS&E) in a variety of Science, Technology, Engineering and Mathematics (STEM) disciplines. With this supercomputer, computational resources will be made available to students, staff, and faculty at both USM and regional community colleges and high schools, thereby providing enhanced capabilities for training and education. This instrumentation will also include an institutional repository for archiving data sets and scholarly output, and disseminating these resources to the scientific community and to the public.<br/><br/>This instrumentation will establish critical cyberinfrastructure needed to facilitate student learning and research in four main thrusts whose common denominator is computing: (i) Materials Science, (ii) Biological Sciences, (iii) Coastal and Marine Sciences, and (iv) Data Mining, Bioinformatics, and Geoinformatics. The instrumentation will comprise standard compute nodes, high-core compute nodes, large memory nodes, and nodes equipped with graphics processing units (GPUs). The instrumentation will enable academicians, researchers, and students to pursue research and education in areas of national and international importance, such as climate change, coastal hazard mitigation and resilience building efforts, advanced manufacturing, natural resource management, the materials genome initiative, and big data science. The instrumentation will reduce researchers' reliance on external cyberinfrastructure, and will aid their transition to national user facilities such as the NSF's eXtreme Science and Engineering Discovery Environment (XSEDE)."
"1829799","CyberTraining: CIC: The Texas A&M University Computational Materials Science Summer School (CMS3)","OAC","CyberTraining - Training-based","09/01/2018","07/09/2018","Ahmed-Amine Benzerga","TX","Texas A&M Engineering Experiment Station","Standard Grant","Sushil K Prasad","08/31/2021","$499,822.00","Raymundo Arroyave, Honggao Liu, Lisa Perez","benzerga@aero.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","Advances in both hardware and software infrastructure are quickly making it possible to carry out realistic simulations of materials and materials phenomena that can lead to a better understanding of their behavior. Simulations constitute one of the key ingredients necessary to realize the Materials Genome Initiative (MGI), which calls for the reduction in time and resources necessary to develop the materials needed to enable potentially transformative technologies. To push the thriving field of Computational Materials Science (CMS) forward it is necessary to provide the next generation research workforce with a broad exposure to as many computational techniques as possible.  This project brings together researchers in materials science and engineering, and in advanced cyber-infrastructure (CI) to establish the Computational Materials Science Summer School (CMS3) that aims to train graduate students and junior scientists and engineers in some of the most advanced and widely used computational materials science simulation tools. The training provided by CMS3 will equip participating students with the knowledge and skills necessary to push the frontiers of simulation-enabled materials research. Furthermore, CMS3 will contribute to MGI's mission of maintaining the Nation's overall economic competitiveness and security by training the scientists and engineers that will discover and develop the materials that will make technologies of national importance possible.<br/><br/>This award leverages existing cyber-infrastructure to expand participation, including from underrepresented groups, through local and remote training. Expert instructors are selected from academic, domestic and international institutions, and national laboratories to cover three modules: continuum micromechanics, mesoscopic simulation and atomistic modeling, along with an overarching theme of data science. The school targets 20 on-site and up to 80 remote graduate and post-doctoral researchers, including industry participants. The activities of CMS3 are to (i) establish a CMS network among national laboratories and academic institutions to leverage the expertise of the CMS community at large in developing the next-generation workforce in materials research across multiple scales; (ii) develop a CMS curriculum, commensurate with a summer school format, that involves utilizing and supporting advanced CI for effective scale-up of a series of practica; (iii) organize shorter seed programs in specialized areas, such as dislocation dynamics, phase-field modeling and data-enabled materials science; (iv) integrate the theoretical foundations and practical training into the graduate curriculum and continuing education at Texas A&M University and elsewhere; and (v) introduce immersive visualization through virtual and augmented reality tools to help students with different backgrounds and learning styles interpret complex material data. Course material and software codes associated with this project not already disseminated by the original developers are maintained in a GitHub repository. This ensures that all released materials and packages are preserved to maintain historical context and broad access.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839201","EAGER: Towards the Web of Biodiversity Knowledge: Understanding Data Connectedness to Improve Identifier Practices","OAC","NSF Public Access Initiative","10/01/2018","07/31/2018","Jose Fortes","FL","University of Florida","Standard Grant","William Miller","09/30/2020","$299,973.00","","fortes@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7414","7916","$0.00","Biodiversity research investigates the variety and variability of life on Earth. This field of science crosses many research disciplines such as genetics, studies of organisms, plants and animals, habitats and ecosystems, and their interactions. A long-standing challenge for biodiversity researchers is to find, access, ""mine"", and integrate complex and diverse information from those disciplines. New approaches have now become possible with the increasing availability of ""big data"" techniques and infrastructure. This project will explore and employ such advanced techniques for retrieval and mining of a wide range of available open biodiversity data sources, with the aim of generating an improved holistic picture or ""knowledge graph"" of Earth's biodiversity. The project will also identify the data practices and discovered relationships that were needed to accomplish this graph-building task, with the aim of informing the development of future data systems and training on these techniques. <br/><br/>Many attempts have been made to link together biodiversity knowledge using linked identifiers coupled with data standards and taxonomies, but satisfactory results with such ""exact matching"" approaches have been elusive. This project aims to develop new methods of relating records across datasets that do not rely on matching identifiers but instead employ inferred rather than explicit relationships between data records. This is an experimental approach that has not yet been attempted at scale.  Linkages between publicly available biodiversity, genetic, literature, and other data will be explored; and software infrastructure will be developed to combine and link multiple biodiversity datasets. Another goal is to quantify the relationship between identifier practices and the ability to construct links between available biodiversity, genetic, literature, and other data. This project will draw on and complement other large ongoing collaborative efforts that contribute to broad integration of biodiversity knowledge, data science, and infrastructure such as the Encyclopedia of Life (EOL) and the NSF-supported iDigBio project. The ultimate aim is to understand which data practices provide the most value to the biodiversity community and thereby inform policy, standards, and training on identifiers. This, in turn, can enable the exploration of new fundamental and cross-disciplinary research questions, and potentially improve practices of a wide range of US and international data aggregators and data producers. <br/><br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1531752","MRI: Acquisition of Conflux, A Novel Platform for Data-Driven Computational Physics","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE","09/01/2015","08/27/2015","Karthikeyan Duraisamy","MI","University of Michigan Ann Arbor","Standard Grant","Stefan Robila","08/31/2019","$2,422,972.00","August Evrard, Krishnakumar Garikipati, Barzan Mozafari, Carlos Alberto","kdur@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1189, 1640, 7231","1189","$0.00","This project develops an instrument, called ConFlux, hosted at the University of Michigan (UM), specifically designed to enable High Performance Computing (HPC) clusters to communicate seamlessly and at interactive speeds with data-intensive operations.  The project establishes a hardware and software ecosystem to enable large scale data-driven modeling of multiscale physical systems.  ConFlux will produce advances in predictive modeling in several disciplines including turbulent flows, materials physics, cosmology, climate science and cardiovascular flow modeling. <br/><br/>A wide range of phenomena exhibit emergent behavior that makes modeling very challenging. In this project, physics-constrained data-driven modeling approaches are pursued to account for the underlying complexity. These techniques require HPC applications (running on external clusters) to interact with large data sets at run time. ConFlux provides low latency communications for in- and out-of-core data, cross-platform storage, as well as high throughput interconnects and massive memory allocations. The file-system and scheduler natively handle extreme-scale machine learning and traditional HPC modules in a tightly integrated workflow---rather than in segregated operations--leading to significantly lower latencies, fewer algorithmic barriers and less data movement.  <br/><br/>Course material developed from the usage of ConFlux is being integrated into the educational curriculum via several degree and certificate programs offered by two UM institutes dedicated to computational and data sciences. Use of the ConFlux cluster will be extended to research groups outside of UM utilizing a number of Extreme Science and Engineering Discovery Environment (XSEDE) bridging tools and file-systems. Connections established through UM's Office of Outreach and Diversity are being leveraged to extend the use of ConFlux to minority serving institutions and Historically Black Colleges and Universities. Using the programs developed by the Society of Women Engineers at UM, middle and high school students will be engaged in hands-on educational modules in computing, physics and data."
"1835566","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","EarthCube","01/01/2019","08/27/2018","Kristy Tiampo","CO","University of Colorado at Boulder","Standard Grant","Amy Walton","12/31/2022","$432,326.00","","kristy.tiampo@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8074","062Z, 077Z, 7925","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1726260","MRI: Acquisition of a Multidisciplinary Beowulf Cluster","OAC","MAJOR RESEARCH INSTRUMENTATION","10/01/2017","09/15/2017","Joel Adams","MI","Calvin College","Standard Grant","Stefan Robila","09/30/2020","$259,731.00","Mark Muyskens, Douglas Vander Griend, Jason VanHorn, Rebecca (Becky) Haney","adams@calvin.edu","3201 BURTON ST SE","GRAND RAPIDS","MI","495464301","6165266000","CSE","1189","1189","$0.00","This project will replace Calvin College's aging computational cluster with a newer, more flexible cluster for multidisciplinary research. The new cluster is designed to facilitate six research projects, as well as research training activities at Calvin, including a nascent Data Science program.  The resource will also offer the flexibility to support research activities beyond the local campus, specifically at Macalester, St. Olaf, and Wheaton Colleges.  By enabling new, as well as enhancing existing research and research training activities at Calvin, the cluster is expected to improve the technological capabilities of the United States cyber-workforce by equipping hundreds of students each year with domain-specific high-performance computing skills in computer science, data science, mathematics, statistics, and the sciences.<br/><br/>Specifically, the cluster will enable research in: a thread safe graphics library (TSGL), used to create real-time visualizations of parallel algorithms; an agent-based economic model to evaluate the effects of different policies during transitions from non-renewable to renewable energy sources; new computational chemistry models used to explore the photophysical properties of common coumarins; accuracies of various economic models; 3D routing systems for first-responders and improved 3D sniper-line-of-sight models used by homeland security personnel in emergency situations; and enhancing Sivvu.org, a new web service for ERFA (Equilibrium Restricted Factor Analysis), a popular computational chemistry technique.  In addition, faculty and student research projects at Calvin, Macalester, St Olaf, and Wheaton colleges will also use the new cluster.<br/><br/>Furthermore the cluster will enable new degree programs in Data Science and Statistics at Calvin. Students in these programs will use the new cluster to store and process large data sets.   Additionally, the cluster will enhance existing courses, such as CS 374, a course in High Performance Computing (HPC) where students will use the cluster to learn to program with MPI, OpenMP, CUDA, and similar HPC tools."
"1840003","CICI: SSC: Securing Science Gateway Cyberinfrastructure with Custos","OAC","Cyber Secur - Cyberinfrastruc","08/15/2018","08/16/2018","Marlon Pierce","IN","Indiana University","Standard Grant","Micah Beck","07/31/2021","$997,672.00","James Basney, Suresh Marru, Enis Afgan","marpierc@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8027","","$0.00","Science gateways are web-based portals for scientists, educators, and students to easily access advanced computing infrastructure, perform reproducible research, and share scientific data. Science gateways are used by tens of thousands of researchers worldwide; thus, gateways are an increasingly attractive target for a wide range of cybersecurity attacks. In cybersecurity terms, science gateways are a collection and integration of a rich set of assets, including user identities, access to a wide range of third party computing and storage resources, sensitive data, licensed or otherwise restricted software, and generated scientific results. The consequences of compromises to science gateways are wide ranging, including unauthorized access to computing resources and data as well as more subtle attacks such as alterations of computed results that may require retraction of publications and otherwise undermine the integrity of the scientific enterprise.  <br/><br/>Through the integration and extension of best-of-breed software, the Custos project provides a single, open source software solution that improves science gateways' management of users identities, secrets such as security tokens used to access a wide range of resources needed for scientific research, and groups and sharing permissions on digital research objects created by science gateway users. Custos software is used to operate a secure, online service with a language-independent programming interface for the science gateway community, providing both an operational platform and a proving ground for research into multi-tenanted, science-focused cyberinfrastructure. The entire Custos environment, including provisioning and operations tools, is openly governed following the meritocracy and diversity principles of the Apache Software Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829729","Collaborative Research: CyberTraining: CIC: Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP)","OAC","CyberTraining - Training-based, COMPUTATIONAL PHYSICS","08/01/2018","07/02/2018","G.J. Peter Elmer","NJ","Princeton University","Standard Grant","Sushil K Prasad","07/31/2021","$375,041.00","Ian Cosden","gelmer@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","044Y, 7244","026Z, 062Z, 7361, 7569, 9102, 9179","$0.00","High-energy physics (HEP) aims to understand the fundamental building blocks of nature and their interactions by using large facilities such as the Large Hadron Collider (LHC) at the European Laboratory for Particle Physics (CERN) in Switzerland and the Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino Experiment (DUNE) planned for the 2020s at Fermilab, in Illinois, as well as many smaller experiments. These experiments generate ever increasing amounts of data and rely on a sophisticated software ecosystem consisting of tens of millions of lines of code that is critical to mine this data and produce physics results. People are the key to developing, maintaining, and evolving this software ecosystem for HEP experiments over many decades. Building the necessary software requires a workforce with a mix of HEP domain knowledge and advanced software skills. The Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP) project provides a training path from a researcher's first steps through active contribution to software training and workforce development. The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure and impacts STEM disciplines in terms of much needed and sought after software training.<br/><br/>The FIRST-HEP project directly organizes training activities and works with partners to leverage and bring synergy to disparate existing efforts in order to maximize their collective impact. It brings together an extended set of partners from the community to build not only missing basic training elements like introductory programming skills in Python, git and Unix but  also use of HEP data formats like ROOT and advanced topics including parallel programming, performance tuning, machine learning and data science for Ph.D. students. It works to build a community of instructors and experiments around the software training material and transforms the approach for research software training in HEP. It builds the workforce required for the cyberinfrastructure challenges of running and planned HEP facilities and experiments in the coming years.The FIRST-HEP education and training activities include specific goals to educate minorities in HEP, K- 12 educators and the broader STEM workforce. The K-12 teachers learn very basic skills of Unix including file management, programming languages, such as C+ and shell scripting. FIRST-HEP harnesses the potential of the underrepresented groups and works to ensure that the pool meets or exceeds the diversity in the larger HEP graduate student population when selecting both training participants and instructors for the HEP fundamental training sessions and the advanced computing schools. FIRST-HEP includes a dedicated outreach activity on cybertraining to the local Puerto Rico public. FIRST-HEP leverages engagement with the Software Carpentries to host training of  K-12 teachers at UPRM in basic Software Carpentry skills and who in turn train their students. This encourages the teachers and school authorities to consider incorporating the basic carpentries into the high school curriculum. The training and cyber skills gained during the FIRST-HEP fundamental training courses directly contribute to the broader STEM workforce and trains students to pursue data science careers and other research areas besides HEP, such as Astronomy, where similar software skills are required.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829707","Collaborative Research: CyberTraining: CIC: Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP)","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Sudhir Malik","PR","University of Puerto Rico Mayaguez","Standard Grant","Sushil Prasad","07/31/2021","$124,342.00","","sudhir.malik@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","044Y","026Z, 062Z, 7361, 9102, 9150","$0.00","High-energy physics (HEP) aims to understand the fundamental building blocks of nature and their interactions by using large facilities such as the Large Hadron Collider (LHC) at the European Laboratory for Particle Physics (CERN) in Switzerland and the Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino Experiment (DUNE) planned for the 2020s at Fermilab, in Illinois, as well as many smaller experiments. These experiments generate ever increasing amounts of data and rely on a sophisticated software ecosystem consisting of tens of millions of lines of code that is critical to mine this data and produce physics results. People are the key to developing, maintaining, and evolving this software ecosystem for HEP experiments over many decades. Building the necessary software requires a workforce with a mix of HEP domain knowledge and advanced software skills. The Framework for Integrated Research Software Training in High Energy Physics (FIRST-HEP) project provides a training path from a researcher's first steps through active contribution to software training and workforce development. The project serves the national interest as stated by NSF's mission to promote the progress of science by preparing a workforce trained in cyberinfrastructure and impacts STEM disciplines in terms of much needed and sought after software training.<br/><br/>The FIRST-HEP project directly organizes training activities and works with partners to leverage and bring synergy to disparate existing efforts in order to maximize their collective impact. It brings together an extended set of partners from the community to build not only missing basic training elements like introductory programming skills in Python, git and Unix but  also use of HEP data formats like ROOT and advanced topics including parallel programming, performance tuning, machine learning and data science for Ph.D. students. It works to build a community of instructors and experiments around the software training material and transforms the approach for research software training in HEP. It builds the workforce required for the cyberinfrastructure challenges of running and planned HEP facilities and experiments in the coming years.The FIRST-HEP education and training activities include specific goals to educate minorities in HEP, K- 12 educators and the broader STEM workforce. The K-12 teachers learn very basic skills of Unix including file management, programming languages, such as C+ and shell scripting. FIRST-HEP harnesses the potential of the underrepresented groups and works to ensure that the pool meets or exceeds the diversity in the larger HEP graduate student population when selecting both training participants and instructors for the HEP fundamental training sessions and the advanced computing schools. FIRST-HEP includes a dedicated outreach activity on cybertraining to the local Puerto Rico public. FIRST-HEP leverages engagement with the Software Carpentries to host training of  K-12 teachers at UPRM in basic Software Carpentry skills and who in turn train their students. This encourages the teachers and school authorities to consider incorporating the basic carpentries into the high school curriculum. The training and cyber skills gained during the FIRST-HEP fundamental training courses directly contribute to the broader STEM workforce and trains students to pursue data science careers and other research areas besides HEP, such as Astronomy, where similar software skills are required.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839022","Encouraging Data Sharing and Reuse in the Field of Collective Behavior through Hackathon-Style Collaborative Workshops","OAC","NSF Public Access Initiative","10/01/2018","08/30/2018","James Curley","TX","University of Texas at Austin","Standard Grant","Beth Plale","09/30/2020","$25,000.00","","curley@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7414","7556","$0.00","The investigators will bring together diverse researchers who work in the field of collective and emergent behavior. Collective and emergent behavior is the study of complex biological and social systems, ranging from bacterial colonies to human groups.  The hackathon-style workshop draws researchers around identifying best practice mechanisms for sharing data, communicating methods of data analysis, and reusing publicly available data.   The investigators propose a series of two workshops where teams of 2-5 participants work on a specific project during the duration of the 3-day event. An objective of the workshop is to foster novel collaborations between researchers in biology, data science, mathematics, computer science and physics, all of whom have an interest in collective behavior.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743191","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FLUID DYNAMICS, SPECIAL INITIATIVES, Software Institutes, CDS&E","03/01/2018","06/27/2018","Robert Moser","TX","University of Texas at Austin","Continuing grant","Stefan Robila","08/31/2019","$45,000.00","","rmoser@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743185","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FLUID DYNAMICS, SPECIAL INITIATIVES, Software Institutes, CDS&E","03/01/2018","06/27/2018","Mark Shephard","NY","Rensselaer Polytechnic Institute","Continuing grant","Stefan Robila","08/31/2019","$65,000.00","Onkar Sahni, Cameron Smith","shephard@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9102, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743180","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FLUID DYNAMICS, SPECIAL INITIATIVES, Software Institutes, CDS&E","03/01/2018","06/27/2018","Beverley McKeon","CA","California Institute of Technology","Continuing grant","Stefan Robila","08/31/2019","$44,284.00","","mckeon@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9102, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743179","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FLUID DYNAMICS, SPECIAL INITIATIVES, Software Institutes, CDS&E","03/01/2018","06/27/2018","Charles Meneveau","MD","Johns Hopkins University","Continuing grant","Stefan Robila","08/31/2019","$22,821.00","","meneveau@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1626364","MRI: Acquisition of the Kentucky Research Informatics Cloud (KyRIC)","OAC","MAJOR RESEARCH INSTRUMENTATION, CYBERINFRASTRUCTURE","08/01/2016","02/14/2019","GQ Zhang","KY","University of Kentucky Research Foundation","Standard Grant","Stefan Robila","07/31/2019","$2,240,000.00","James Griffioen, Hunter Moseley, Vincent Kellen, Christina Payne","gqatcase@gmail.com","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","1189, 7231","1189, 9150","$0.00","This project will create a big data cloud infrastructure, the Kentucky Research Informatics Cloud (KyRIC), to accelerate data-driven discovery and computational research education across multiple disciplines. Scientific discovery today is being enabled through computational and data intensive research that exploits enormous amounts of available data.  KyRIC will advance a number of exciting research programs across many disciplines, such as Bioinformatics and System Biology Algorithms, Large Graph and Evolutionary Network Analysis, Image Processing, and Computational Modeling and Simulation. Breakthroughs in KyRIC-enabled research will have important societal benefits in a number of areas, such as increasing agricultural yields, improving economic competitiveness, and creating new products and markets. <br/><br/>KyRIC will use a hybrid architecture to support massively parallel applications that will address exciting and challenging new data and memory intensive research in big data science. The KyRIC hybrid system will consists of two subsystems: a 50 nodes cluster, each with 4 10-core processors, 3TB RAM, and an 8TB SSD array; and a Peta-scale storage system providing 2 PB of object-based storage. KyRIC will employ leading-edge cloud management software that will allow nodes to be reconfigured, scheduled, and loaded with problem-specific applications software based on the current mix of big data jobs being executed by users.  As a result, the project will enable and support a wide range of new research activities, each with its own unique characteristics that are beyond the capacity of our existing infrastructure. KyRIC will be readily accessed by researchers across the state utilizing our latest high-performance network, with multiple 100GB/s links from Lexington to Louisville and Cincinnati. KyRIC will also join XSEDE to better integrate the University of Kentucky (UK) with national multi-petascale capabilities.<br/><br/>KyRIC will provide intuitive access, rapid infrastructure customizations, and higher bandwidth and lower latency between the desktop and resources like XSEDE to facilite improved algorithm design, software development, and interactive data analysis. KyRIC will be used by over 1000 UK researchers (faculty, staff, and students) and by computational research collaborators across the state of Kentucky, notably University of Louisville (UL), Northern Kentucky University (NKU), and Kentucky State University (KSU).  The resource will make exciting data-intensive projects possible, enhance computational research education for graduate and undergraduate students, help attract and retain talented younger faculty, and promote big data science and technology, thus impacting Kentucky's and the nation's economic development."
"1743178","Collaborative Research: NISC SI2-S2I2 Conceptualization of CFDSI: Model, Data, and Analysis Integration for End-to-End Support of Fluid Dynamics Discovery and Innovation","OAC","FLUID DYNAMICS, SPECIAL INITIATIVES, Software Institutes, CDS&E","03/01/2018","06/27/2018","Kenneth Jansen","CO","University of Colorado at Boulder","Continuing grant","Stefan Robila","08/31/2019","$321,838.00","Alireza Doostan, John Farnsworth, John Evans, Jed Brown","jansenke@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1443, 1642, 8004, 8084","026Z, 8004, 8005, 9263","$0.00","Fluid dynamics is a broad field spanning a large number of science and engineering problem domains that are critical to a wide variety of important applications including manufacturing, climate analysis, environment, health, transportation, propulsion, and power generation. To support the fluid dynamics research and applications community, this project seeks to engage the community in order to conceptualize a future institute, the Computational Fluid Dynamics Software Infrastructure (CFDSI), that will broadly develop, share, and apply computational tools for the generation and analysis of fluid dynamics data from both experimental and computational sources.  After its conceptualization, CFDSI will create and extend tools for problem definition, solution, and analysis of both computational and experimental investigations. The primary objective is to facilitate the sharing of computational tools and data resources through a rich and extensible set of software components that can be integrated into a wide range of existing fluid dynamics analysis tools. By improving the efficiency of tools and their ease of use, the ability for scientists and engineers to accurately predict and understand how complex fluid flows behave will be enhanced, having a significant impact on design, innovation, and discovery across the vast range of applications where fluid dynamics plays a role. CFDSI even has the potential to impact on K-12, undergraduate and graduate education by making a wide variety of resources available to students for fluid dynamics investigations.<br/><br/>The conceptualized institute will make a wide variety of powerful simulation, data, and analysis resources available to the fluid dynamics research community by lowering or eliminating barriers associated with the adoption and use of these resources. The software infrastructure will have a number of positive impacts on the fluid dynamics research community. To do so, CFDSI will connect the best research in fluid dynamics to the best research in data science/analytics within a highly sustainable software development environment. Specifically, CFDSI will: 1) enhance the dissemination of fluid dynamics data resources and advances in CFD modeling, 2) facilitate collaboration in fluid dynamics research, especially between computational and experimental researchers, 3) enable detailed comparisons between different data sources and detailed validation of computational models, 4) ease the use of advanced CFD models, methods, and codes in new and complex applications, 5) facilitate advanced analytics, such as uncertainty quantification, data compression, and optimization, 6) provide students access to advanced CFD methods and data resources, both computational and experimental, to enhance both graduate and undergraduate education in fluid dynamics, 7) improve the sustainability of current and future CFD software, and 8) facilitate the management of the growing body of fluid dynamics data sets. These outcomes will greatly enhance the effectiveness and productivity of research in fluid dynamics. In particular, they will transform the conduct of fluid dynamics research by: 1) making it more collaborative, 2) enhancing the credibility of research results, 3) enabling discovery, 4) reducing the cost of pursuing new research questions, and 5) diversifying and widening the fluid dynamics community through lowering the barriers associated with accessing and adopting CFD codes and large data sets. Software components will be designed for both analysis of experimental and computational databases as well as direct integration into CFD codes. The latter will enable in situ data analytics to address the growing chasm between data creation rate (solver performance) and data storage rate/volume (I/O resources). After conceptualization and implementation, CFDSI will enable more effective fluid dynamics research and thus impact the wide variety of application domains in which fluid dynamics is critical including manufacturing climate, environment, health, transportation, propulsion, and power generation (including conventional, alternative, and nuclear sources) which will, in turn, strongly impact our economy. Additionally, CFDSI will provide the capability for immersive simulations and experiments that will close the loop on idea, insight, discovery, and design through establishing links to in situ data analytics and problem redefinition during ongoing simulations or experiments. Finally, CFDSI will impact other problem domains governed by partial differential equations (e.g. solid mechanics) by serving as a model and starting point for similar domain-specific software infrastructures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642411","SI2-SSE: STAMLA: Scalable Tree Algorithms for Machine Learning Applications","OAC","Software Institutes","01/01/2017","09/08/2016","Vincent Reverdy","IL","University of Illinois at Urbana-Champaign","Standard Grant","Micah Beck","12/31/2019","$499,992.00","Robert Brunner","vreverdy@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004","7433, 8004, 8005","$0.00","The voluminous growth in data together with the burgeoning field of data science have greatly increased the demand for machine learning, a field of computer science that focuses on the development of programs that can learn and change in response to new data. With increasing access to large volumes of data, practitioners often resort to machine learning to construct more precise models of nature, or to learn fundamentally new concepts. For example, machine learning can help improve the accuracy of weather and climate predictions, model the efficacy of drugs and their interactions, and identify specific features, such as a face, from a large set of images or videos. But with the growth of data volumes, the speed with which machine can learn from data is decreasing. As a result, new techniques are required to accelerate learning algorithms so that they can be applied to larger and more complex data sets. This work will develop new approaches to improve the performance of a wide class of machine learning algorithms. Specifically, this work will leverage the C++ programming language and recent research into fundamental bit operations to make fast tree-like data structures that underlie many of the most commonly used implementations of machine learning algorithms. In particular, algorithms in the scikit-learn library, the most widely used machine learning library written in the Python programming language, will be accelerated by using our these new tree data structures. Given the widespread adoption of the scikit learn library, this work will impact diverse fields from Astronomy to Biology to Geoscience to Physics. The scikit learn library is also one of the more popular libraries for teaching (and understanding) machine learning. With an explosion of books, blogs, and tutorials that use scikit learn algorithms and pipelines to demonstrate specific types of machine learning such as classification, regression, clustering, and feature extraction, this project will immediately impact a wide range of people from seasoned practitioners, engineers gaining additional training, and students at universities and colleges across the nation. In addition, these tree data structures will be submitted for inclusion in the C++ standard, which would impact millions of developers world-wide. Finally, the algorithms will be implemented under an open source license in a public forum.<br/><br/><br/>The STAMLA project aims at developing efficient and scalable tree algorithms inspired from high performance simulation codes for machine learning. Over the last few years, machine learning has become a popular technique in data mining to extract information from data sets, build models and make predictions across a wide range of application areas. However, current tools have been built in high-level languages with more focus on functionalities than on pure performance. But as scientific experiments are accumulating more and more data, and as complex models are requiring larger and larger training sets, scalability issues are emerging. At the same time, in high performance computing, petascale simulations have shown that fundamental data structure optimizations can have a significant impact on overall code performance. In particular, by replacing straightforward tree implementations with implicit trees based on hash tables, simulation codes are able to make the most of modern architecture, leveraging cache and vectorization. This research project will apply this knowledge to machine learning algorithms in order to overcome the limitations of existing libraries and make analyses of extremely large data sets possible. The proposed research includes the development of three library layers, built on top of each other. The first layer is a library of fast bit manipulation tools for tree indexing. It extends existing research that has already demonstrated two to three orders of magnitude improvements compared to standard solutions provided by compilers. The second layer is a tree building blocks library developed using generative programming techniques. This layer will provide developers with generic tools to build efficient implicit trees for specific domains and optimized at compile time to make the most of the targeted architecture. Finally, the third layer consists in a contribution package to the scikit-learn library to leverage the data structures introduced in the second layer. Together, these three layers form a consistent set that propagates low level optimizations based on high performance computing practices to one of the most widely used high level machine learning library. As machine learning is domain independent, the results of this project have the potential to impact all data intensive applications relying on machine learning algorithms based on tree data structures. Moreover, in addition to being developed in an open source framework via a public repository, the three library layers will be released through different channels: 1) the bit manipulation tools will aim at standardization in the C++ language through a collaboration with the ISO C++ Standards Committee 2) the tree building blocks will be proposed for inclusion in the Boost C++ libraries and 3) the machine learning algorithms will be published as a contribution package of the scikit-learn library. These channels will ensure a large adoption of the tools developed throughout this project, and their long-term support by well established communities."
"1838955","Encouraging Data Sharing and Reuse in the Field of Collective Behavior through Hackathon-Style Collaborative Workshops","OAC","NSF Public Access Initiative","10/01/2018","08/30/2018","Simon Garnier","NJ","New Jersey Institute of Technology","Standard Grant","Beth Plale","09/30/2020","$24,997.00","Jason Graham","garnier@njit.edu","University Heights","Newark","NJ","071021982","9735965275","CSE","7414","7556","$0.00","The investigators will bring together diverse researchers who work in the field of collective and emergent behavior. Collective and emergent behavior is the study of complex biological and social systems, ranging from bacterial colonies to human groups.  The hackathon-style workshop draws researchers around identifying best practice mechanisms for sharing data, communicating methods of data analysis, and reusing publicly available data.   The investigators propose a series of two workshops where teams of 2-5 participants work on a specific project during the duration of the 3-day event. An objective of the workshop is to foster novel collaborations between researchers in biology, data science, mathematics, computer science and physics, all of whom have an interest in collective behavior.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1852454","REU Site: Interdisciplinary Research Experience in Computational Sciences","OAC","RSCH EXPER FOR UNDERGRAD SITES","03/01/2019","02/21/2019","Juana Moreno","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Sushil K Prasad","02/28/2022","$358,900.00","","moreno@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1139","9102, 9150, 9250","$0.00","Computational simulations are rapidly emerging as a co-equal branch of inquiry with experiment and theory. In parallel, automated instruments store massive amounts of measurements. However, computational science will only fulfill its full potential if advances in undergraduate and graduate education accompany the advances in hardware. The 30 undergraduate students participating in this Research Experience for Undergraduates (REU) program are engaged with varied computational science projects, learn how to use state-of-the-art cyberinfrastructure tools, manage large amounts of data, experience activities that characterize research careers, and work in interdisciplinary research teams. The Center for Computation & Technology (CCT) at Louisiana State University provides an ideal setting for the REU student to become familiar with interdisciplinary research. For ten weeks in the summer, the students work collaboratively with their mentors on a wide variety of computational science projects. With research groups exploring gravitational waves, complex emergent phenomena in material science, or computational music and arts, the participants work on cutting edge research in computational sciences. By preparing the next generation of students in the computational sciences, this project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.<br/><br/><br/>There is a clear need for training the workforce on computing and computational science. But, currently, the majority of students learn little, if any, Computational Science (CSci) in the classroom and are not prepared for CSci research or data science. This proposal provides an evidence-based approach to address these issues and prepare the next generation of students. One of the current barriers in CSci education is that while CSci is intrinsically multidisciplinary, traditional academic departments set rigid boundaries between disciplines. Because LSU Center for Computation & Technology is not an academic unit, it provides an ideal setting for the REU student to become familiar with interdisciplinary research by participating in a multidisciplinary research team. During ten weeks the participants experience activities that characterize research careers, such as presenting their research in scholarly forums, and working on the soft- and hard-skills needed in interdisciplinary research. Faculty mentors work with the students in individual and group meetings, and they provide activities that help students appreciate the nature of multidisciplinary research and the value of working as a team. At the conclusion of the REU, students are encouraged to continue their CSci research and present their work at their home institution, informing others about computational science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640386","Exploring the Transition of Research-Derived Cyber-Threat Data","OAC","","10/01/2016","09/12/2017","Phillip Porras","CA","SRI International","Standard Grant","Kevin L. Thompson","09/30/2019","$829,655.00","","porras@csl.sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","6508592651","CSE","P419, P420, Q293","7434, 8237","$0.00","Transitioning later stage cybersecurity research into operational capabilities remains a national priority. Both the 2011 and 2016 Federal Cybersecurity Research and Development Strategic Plans, collaboratively authored by agencies which participate in the NITRD (Networking and Information Technology R&D) program, explicitly identify Transition to Practice (TTP) as a goal set by the Federal agencies that fund Cybersecurity research, including NSF. The 2016 Plan states that agencies should: ""Assess barriers and identify incentives that could accelerate the transition of evidence-validated effective and efficient cybersecurity research results into adopted technologies, especially for emerging technologies and threats."" This project considers the unique challenges in the transition of large-scale data or analytic results that are embodied as data feeds, distilled threat intelligence, or online portal services. With the growing attention by U.S. agencies in sponsoring cross-disciplinary research from the Data Science and cybersecurity research communities, the study is both timely and important.<br/><br/>Novel security applications can be derived through the use of many newly emerging massively scalable machine learning technologies. Organized cyber-adversaries regularly mount sophisticated large-scale attacks and malware-driven campaigns that target large Universities, research organizations, industries, nations, and even the Internet itself. The dire need for deep reasoning cyber-threat-relevant applications now coincides with the growing availability of massive Internet datasets and the data mining solutions to capitalize on this data. This proposal explores the issues involved in transitioning the results of such large-scale data analytics into practical use."
"1738981","CICI: CE: Enhancing Cybersecurity for Broadening Data-Driven Research and Partnerships","OAC","Cyber Secur - Cyberinfrastruc","10/01/2017","07/14/2017","Sonia Fahmy","IN","Purdue University","Standard Grant","Micah Beck","09/30/2019","$841,506.00","Nicole Key, Bruno Ribeiro, Ida Ngambeki, Xiao Zhu","fahmy@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8027","8027, 9102","$0.00","As more disciplines leverage computational and data-driven modeling, the security of campus cyberinfrastructure is becoming increasingly important in order to protect intellectual property and secure a competitive advantage for researchers. Often, facilities and projects funded by industrial partners and US government agencies create new institutional requirements for information security and assurance. Designed for open science, the Purdue University campus cyberinfrastructure does not currently have infrastructure in place to support the wide variety of security requirements stemming from this growing volume of aerospace, defense, and industrial projects being conducted on campus. The rapid pace of expanding partnerships has outpaced the campus cybersecurity framework, limiting the opportunities for researchers and students to conduct research. This project brings together engineering and research teams to develop a cyber attack detection and response capability for the Purdue University campus research network.<br/><br/>The project enhances the Purdue campus network to develop a security-assured research network infrastructure, and deploys novel adaptive monitoring tools as part of the campus cyberinfrastructure. Enhanced campus cyberinfrastructure empowers domain scientists to conduct research with heightened security requirements. The project also provides cybersecurity researchers with production network traffic data to develop and evaluate adaptive intrusion detection and response and flow-level anomaly detection. The proposed research will reduce the overhead of network security mechanisms, and improve their effectiveness, agility, and scalability. The project supports cybersecurity education and training by engaging students in deployment and operation of networking hardware and software. It provides students with a unique opportunity to apply their conceptual knowledge to practical situations and better prepares them to be part of the future workforce in cybersecurity. The highly interdisciplinary project team consists of security, networking and data science researchers, cybersecurity educators, campus Information Technology professionals and the engineering community."
"1827098","CC* Networking Infrastructure: Jackson State University (JSU)-Research Network","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/22/2018","Deborah Dent","MS","Jackson State University","Standard Grant","Kevin Thompson","06/30/2020","$500,000.00","Tor Kwembe, Natarajan Meghanathan, Hung-Chung Huang","deborah.f.dent@jsums.edu","1400 J R LYNCH ST.","Jackson","MS","392170002","6019792008","CSE","8080","9150","$0.00","This project provides Jackson State University (JSU) researchers with a campus cyber infrastructure (CI) capability allowing expansion of big data and other network intensive collaborative research that depend on high-speed network access to local, regional, cloud and national compute and storage resources.  The project increases research network capacity to 100G, re-architects the campus research network, and strategic relocation of equipment to utilize a Science DMZ (Demilitarized zone - i.e., free from firewalls and other friction devices).  The research network includes centrally pooled High-Performance/Throughput Computing (HPC/HTC) resources designed to meet immediate and future research needs.  An increased focus on domain scientist talent is possible due to management and monitoring of the CI assets by a centralized, well-trained enterprise information technology team. This project enhances the end-to-end network services for researchers and is an important catalyst for the growing campus-wide interdisciplinary data science program. <br/><br/>Application-specific network and computational needs for big data analytics, visualization, nanotoxicity, complex network analysis, science drivers and education are addressed.  Faculty, students, and the IT staff on campus and across campuses are engaged to leverage the new environment for their research, education, and operational needs. The project is disseminated through outreach workshop activities with Historical Black Colleges and Universities and other universities or the community college systems within the state of Mississippi who may be planning similar campus network upgrades.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835825","Collaborative Research: Elements: Software NSCI: Constitutive Relation Inference Toolkit (CRIKit)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2018","08/31/2018","Jed Brown","CO","University of Colorado at Boulder","Standard Grant","Bogdan Mihaila","08/31/2021","$293,441.00","","Jed.Brown@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1712, 8004","026Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Constitutive relations are mathematical models that describe the way materials respond to local stimuli such as stress or temperature change, and are essential to the study of biological tissues in biomechanics, ice and rock in geosciences, plasmas in high-energy physics and many other science and engineering applications. This project seeks to infer constitutive relations from practical observations without requiring isolation of the material in conventional laboratory experiments, which are often expensive and difficult to apply to volatile materials such as liquid foams or materials such as sea ice that exhibit homogenized behavior only at large scales.  The investigators and their students will develop underlying algorithms and the Constitutive Relation Inference Toolkit (CRIKit), a new community software package to leverage recent progress in machine learning and physically-based modeling to infer constitutive relations from noisy, indirect observations, and disseminate the results as citable research products for use in a range of open source and extensible commercial simulation environments.  This development will create new opportunities and increase accessibility at the confluence of data science and high-fidelity physical modeling, which the investigators will highlight through community outreach and educational activities.<br/><br/>The CRIKit software will integrate parallel partial differential equation (PDE) solvers like FEniCS/dolfin-adjoint with machine learning (ML) packages like TensorFlow to infer constitutive relations from noisy indirect or in-situ observations of material responses.  The forward simulation is post-processed to create synthetic observations which are compared to real observations by way of a loss function, which may range from simple least squares to advanced techniques such as ML-based image analysis.  This approach results in a nonlinear regression problem for the constitutive relation (formulated to satisfy invariants and free energy compatibility requirements) and relies on well-behaved and efficiently computable gradients provided by PDE solvers using compatible discretizations with adjoint capability.  The inference problem exposes parallelism within each forward model and across different experimental realizations and facilitates research in optimization.  The research enables constitutive models to be readily updated with new experimental data as well as reproducibility and validation studies. CRIKit's models will improve simulation capability for scientists and engineers by providing ready access to the cutting edge of constitutive modeling.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835792","Collaborative Research: Elements: Software: NSCI: Constitutive Relation Inference Toolkit (CRIKit)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2018","08/31/2018","Tobin Isaac","GA","Georgia Tech Research Corporation","Standard Grant","Bogdan Mihaila","08/31/2021","$299,136.00","","tisaac@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1712, 8004","026Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Constitutive relations are mathematical models that describe the way materials respond to local stimuli such as stress or temperature change, and are essential to the study of biological tissues in biomechanics, ice and rock in geosciences, plasmas in high-energy physics and many other science and engineering applications. This project seeks to infer constitutive relations from practical observations without requiring isolation of the material in conventional laboratory experiments, which are often expensive and difficult to apply to volatile materials such as liquid foams or materials such as sea ice that exhibit homogenized behavior only at large scales. The investigators and their students will develop underlying algorithms and the Constitutive Relation Inference Toolkit (CRIKit), a new community software package to leverage recent progress in machine learning and physically-based modeling to infer constitutive relations from noisy, indirect observations, and disseminate the results as citable research products for use in a range of open source and extensible commercial simulation environments. This development will create new opportunities and increase accessibility at the confluence of data science and high-fidelity physical modeling, which the investigators will highlight through community outreach and educational activities.<br/><br/>The CRIKit software will integrate parallel partial differential equation (PDE) solvers like FEniCS/dolfin-adjoint with machine learning (ML) packages like TensorFlow to infer constitutive relations from noisy indirect or in-situ observations of material responses. The forward simulation is post-processed to create synthetic observations which are compared to real observations by way of a loss function, which may range from simple least squares to advanced techniques such as ML-based image analysis. This approach results in a nonlinear regression problem for the constitutive relation (formulated to satisfy invariants and free energy compatibility requirements) and relies on well-behaved and efficiently computable gradients provided by PDE solvers using compatible discretizations with adjoint capability. The inference problem exposes parallelism within each forward model and across different experimental realizations and facilitates research in optimization. The research enables constitutive models to be readily updated with new experimental data as well as reproducibility and validation studies. CRIKit's models will improve simulation capability for scientists and engineers by providing ready access to the cutting edge of constitutive modeling.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829554","CyberTraining: CIC: Widening the CI Workforce On-ramp by Exposing Undergraduates to Heterogeneous Computing","OAC","CyberTraining - Training-based","09/01/2018","07/31/2018","David Bunde","IL","Knox College","Standard Grant","Sushil K Prasad","08/31/2021","$132,026.00","","dbunde@knox.edu","2 East South Street","Galesburg","IL","614014999","3093358860","CSE","044Y","026Z, 062Z, 7361, 9229","$0.00","In keeping with NSF's mission of promoting scientific progress, this project strengthens the workforce of future cyberinfrastructure researchers and professionals by preparing undergraduate students to program and employ heterogeneous computing systems. The need for increased performance per watt and demands of processing diverse workloads have triggered a major industry shift towards systems containing different kinds of specialized components. These heterogeneous architectures are quickly becoming the dominant platform in high-performance computing (HPC), cloud computing, and Internet of Things (IoT) networks. As such, it is imperative that the scientific workforce in advanced cyberinfrastructure have a deep understanding of heterogeneity in computing systems. Current undergraduate computer science curricula lack sufficient coverage of heterogeneous computing concepts and there is an imminent risk that tomorrow's scientific workforce will be ill-equipped to program the complex heterogeneous systems of the future.  This project addresses this by developing and disseminating modules covering heterogeneous computing concepts. Module development is complemented with out-of-class training camps that will serve as a springboard for undergraduates to pursue further training and career opportunities.  <br/><br/>The project takes an early-and-often, module-driven approach to curricular integration. A collection of modules is being designed to cover a range of heterogeneous computing concepts including heterogeneous architectures, hybrid algorithms, and heterogeneous programming models such as CUDA and OpenCL. For easy adoption, modules are designed to be self-contained with all requisite teaching material including in-class interactive exercises, problem sets and solutions, and pedagogical notes.  Each module is designed to exploit heterogeneous context within the curriculum and introduce topics at an appropriate level of abstraction. The summer training camps are designed to be immersive experiences for undergraduates that (i) reinforce and extend the material covered in the modules, (ii) present the role of cyberinfrastructure in advancing scientific research, and (iii) expose students to opportunities in the fields of computational and data science.  A key component of this project is forging alliances with industry partners, who are engaged throughout the design process.  A detailed evaluation plan is in place to evaluate student learning outcomes and engagement.  Both internal and external evaluators are involved in carrying out the evaluation plan.  The modules and training activities are expected to impact over 1,100 computer science undergraduates enrolled at Texas State, Knox College, and Concordia University Texas.  Extensive dissemination efforts through faculty training workshops are expected to further broaden the impact of this work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829644","CyberTraining: CIC: Widening the CI Workforce On-ramp by Exposing Undergraduates to Heterogeneous Computing","OAC","CyberTraining - Training-based, SPECIAL PROJECTS - CCF","09/01/2018","07/31/2018","Apan Qasem","TX","Texas State University - San Marcos","Standard Grant","Sushil Prasad","08/31/2021","$225,791.00","","apan@txstate.edu","601 University Drive","San Marcos","TX","786664616","5122452314","CSE","044Y, 2878","026Z, 062Z, 7361, 9102","$0.00","In keeping with NSF's mission of promoting scientific progress, this project strengthens the workforce of future cyberinfrastructure researchers and professionals by preparing undergraduate students to program and employ heterogeneous computing systems. The need for increased performance per watt and demands of processing diverse workloads have triggered a major industry shift towards systems containing different kinds of specialized components. These heterogeneous architectures are quickly becoming the dominant platform in high-performance computing (HPC), cloud computing, and Internet of Things (IoT) networks. As such, it is imperative that the scientific workforce in advanced cyberinfrastructure have a deep understanding of heterogeneity in computing systems. Current undergraduate computer science curricula lack sufficient coverage of heterogeneous computing concepts and there is an imminent risk that tomorrow's scientific workforce will be ill-equipped to program the complex heterogeneous systems of the future.  This project addresses this by developing and disseminating modules covering heterogeneous computing concepts. Module development is complemented with out-of-class training camps that will serve as a springboard for undergraduates to pursue further training and career opportunities.  <br/><br/>The project takes an early-and-often, module-driven approach to curricular integration. A collection of modules is being designed to cover a range of heterogeneous computing concepts including heterogeneous architectures, hybrid algorithms, and heterogeneous programming models such as CUDA and OpenCL. For easy adoption, modules are designed to be self-contained with all requisite teaching material including in-class interactive exercises, problem sets and solutions, and pedagogical notes.  Each module is designed to exploit heterogeneous context within the curriculum and introduce topics at an appropriate level of abstraction. The summer training camps are designed to be immersive experiences for undergraduates that (i) reinforce and extend the material covered in the modules, (ii) present the role of cyberinfrastructure in advancing scientific research, and (iii) expose students to opportunities in the fields of computational and data science.  A key component of this project is forging alliances with industry partners, who are engaged throughout the design process.  A detailed evaluation plan is in place to evaluate student learning outcomes and engagement.  Both internal and external evaluators are involved in carrying out the evaluation plan.  The modules and training activities are expected to impact over 1,100 computer science undergraduates enrolled at Texas State, Knox College, and Concordia University Texas.  Extensive dissemination efforts through faculty training workshops are expected to further broaden the impact of this work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849273","NSF Student Travel Grant for 2018 Forum on Infrastructural Resilience to Low-Level Seismicity in Oklahoma","OAC","INFORMATION TECHNOLOGY RESEARC, DATANET","10/01/2018","09/13/2018","Priyank Jaiswal","OK","Oklahoma State University","Standard Grant","Amy Walton","09/30/2019","$19,600.00","","priyank.jaiswal@okstate.edu","101 WHITEHURST HALL","Stillwater","OK","740781011","4057449995","CSE","1640, 7726","062Z, 7556, 9150","$0.00","This award provides travel support to students from across the country to attend the 2018 Forum on Infrastructural Resilience to Low-level Seismicity, to be held in Stillwater, Oklahoma.  Undergraduate and graduate students in data science, geoscience and engineering will be able to take an active role in understanding the interdisciplinary problems and potential solutions.<br/><br/>The forum includes an opportunity for hands-on interaction with the latest data and sensing capabilities.  Dr. Priyank Jaiswal recently received a Cyberinfrastructure for Sustained Innovation (CSSI) award (#1835371): Oklahoma State University and UCSD are collaborating on the comparative characteristics of earthquakes induced by wastewater injection.  The project demonstrates an approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the September 2016 Pawnee earthquake.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1626262","MRI: Acquisition of a Computing Cluster to Enable Transformative Research across Disciplines","OAC","MAJOR RESEARCH INSTRUMENTATION","08/01/2016","08/03/2016","Megan Olsen","MD","Loyola University Maryland, Inc.","Standard Grant","Stefan Robila","07/31/2019","$280,120.00","David Binkley, Birgit Albrecht, Jeremy Schwartz","mmolsen@loyola.edu","4501 N CHARLES ST","Baltimore","MD","212102601","4016172561","CSE","1189","","$0.00","Loyola University Maryland will acquire a computing cluster with central processing and graphical processing units (CPU and GPU). The computing cluster will be utilized for computationally expensive research projects spanning multiple fields, including computer science, materials engineering, electrical engineering, physics, chemistry, and economics. As a primarily undergraduate institution, Loyola is also committed to training the next generation of researchers via undergraduate research opportunities. Through the acquisition of its first major computing cluster, Loyola will expand opportunities to introduce undergraduate students to modern computationally-intensive tasks and multidisciplinary research, preparing them for graduate school and computational work within their chosen fields. Additionally, it will support projects within the university's new interdisciplinary Data Science M.S. degree program.<br/><br/>The initial eleven research projects will impact areas as diverse as software development, medicine, and social policy.  Example impacts include developing techniques leading to drugs to fight SARS; understanding protein-DNA interactions for biotechnology applications; providing policymakers with a better understanding of how job competition and human capital allocation influence the optimal design of unemployment insurance; improving software quality by helping programmers identify and avoid the introduction of dependence clusters; improving techniques for coordination in pursuing a moving target, such as in military operations, autonomous automobile police chases, or surveillance programs; new methods for the creation of fossil fuel alternatives; providing secure communications for mobile devices in the Internet of Things; and improving techniques for nondestructive evaluation of electromagnetic materials, which is of critical importance in agriculture, bio-electromagnetics, aerospace, and the design of integrated circuits. This high performance computing cluster will significantly increase the shared computing resources on campus, thereby expanding opportunities for faculty research, faculty recruitment, and student research."
"1642053","Collaborative Research: CICI: Secure and Resilient Architecture: Scientific Workflow Integrity with Pegasus","OAC","Cyber Secur - Cyberinfrastruc","09/01/2016","08/22/2016","Ewa Deelman","CA","University of Southern California","Standard Grant","Micah Beck","08/31/2020","$290,000.00","","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8027","9102","$0.00","Scientists use computer systems to analyze and store their scientific data, sometimes in a complex process across multiple machines. This process can be tedious and error-prone, which has led to the development of software known as a ""workflow management system"". Workflow management systems allow scientists to describe their process in a human-friendly way and then the software handles the details of the processing for the scientists, dealing with tedious and repetitive steps and handling errors. One popular workflow management system is Pegasus, which, over the past three years, was used to run over 700,000 workflows by scientists in a number of domains including astronomy, bioinformatics, earthquake science, gravitational wave physics, ocean science, and neuroscience. The ""Scientific Workflow Integrity with Pegasus"" project enhances Pegasus with additional security features. The scientist's description of their desired work is protected from tampering and the data processed by Pegasus is checked to ensure it hasn't been accidentally or maliciously modified. Such tamper protection is attained by cryptographic techniques that ensure data integrity. These changes allow scientists, and our society, to be more confident of scientific findings based on collected data.<br/><br/>The Scientific Workflow Integrity with Pegasus project strengthens cybersecurity controls in the Pegasus Workflow Management System in order to provide assurances with respect to the integrity of computational scientific methods. These strengthened controls enhance both Pegasus' handling of science data and its orchestration of software-defined networks and infrastructure. The result is increased trust in computational science and increased assurance in our ability to reproduce the science by allowing scientists to validate that data has not been changed since a workflow completed and that the results from multiple workflows are consistent. The focus on Pegasus is due to its popularity in the scientific community as a method of computation and data management automation. For example, LIGO, the NSF-funded gravitational-wave physics project, recently used the Pegasus Workflow Management System to structure and execute the analyses that confirmed and quantified its historic detection of a gravitational wave, confirming the prediction made by Einstein 100 years ago. The proposed project has established collaborations with LIGO and additional key NSF infrastructure providers and science projects to ensure broadly applied results."
"1659210","CC* Networking Infrastructure: Building a Science DMZ Network for University of California Merced","OAC","Campus Cyberinfrastrc (CC-NIE)","01/01/2017","06/05/2018","Suzanne Sindi","CA","University of California - Merced","Standard Grant","Kevin Thompson","12/31/2019","$432,460.00","Michael Spivey, Thomas DeFanti, Ashlie Martini, Nicola Lercari","ssindi@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092598670","CSE","8080","9251","$0.00","As the nation's first new research university of the 21st Century, the University of California Merced (UC Merced) has quickly established itself as a regional leader in emerging research across a wide variety of areas. Many of these research activities involve very large data sets, data-intensive computation and data collaboration.<br/><br/>These endeavors require a reliable, robust and fast network that is different in its construction, management and use from existing internet networks. This new type of network is built for science data flows, has appropriate security measures and is dedicated to its special purpose: research. The Energy Sciences Network (ESNet) has developed a model for such a network called a ""Science DMZ"".  UC Merced is building a Science DMZ network, distinct from the 'regular' network and dedicated for these advanced science applications. This new network cuts data transfer times, allows for seamless data collaboration and sets the stage for new discoveries on the UC Merced campus and beyond.<br/><br/>The project includes a dedicated Science DMZ campus edge router which allows for 10, 40 and 100 Gbps connections across campus and out to the wider scientific community. Each academic building has dedicated fiber optic cables connecting many devices (data servers, clusters and even science instruments) at speeds hundreds of times faster and much more reliably than current networks provide. This project will employ a different security model: by carefully controlling the physical topology of the network, security can be provided without the use of a firewall, vastly improving performance."
"1541349","CC*DNI DIBBs: The Pacific Research Platform","OAC","DATANET","10/01/2015","06/11/2018","Larry Smarr","CA","University of California-San Diego","Cooperative Agreement","Amy Walton","09/30/2020","$6,149,262.00","Philip Papadopoulos, Frank Wuerthwein, Thomas DeFanti, Camille Crittenden","lsmarr@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7726","7433, 8048","$0.00","Research in data-intensive fields is increasingly multi-investigator and multi-institutional, depending on ever more rapid access to ultra-large heterogeneous and widely distributed datasets. The Pacific Research Platform (PRP) is a multi-institutional extensible deployment that establishes a science-driven high-capacity data-centric 'freeway system.' The PRP spans all 10 campuses of the University of California, as well as the major California private research universities, four supercomputer centers, and several universities outside California. Fifteen multi-campus data-intensive application teams act as drivers of the PRP, providing feedback to the technical design staff over the five years of the project. These application areas include particle physics, astronomy/astrophysics, earth sciences, biomedicine, and scalable multimedia, providing models for many other applications.<br/> <br/>The PRP builds on prior NSF and Department of Energy (DOE) investments. The basic model adopted by the PRP is 'The Science DMZ,' being prototyped by the DOE ESnet. (A Science DMZ is defined as 'a portion of the network, built at or near the campus local network perimeter that is designed such that the equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose business systems'). In the last three years, NSF has funded over 100 U.S. campuses through Campus Cyberinfrastructure - Network Infrastructure and Engineering (CC-NIE) grants to aggressively upgrade their network capacity for greatly enhanced science data access, creating Science DMZs within each campus.  The PRP partnership extends the NSF-funded campus Science DMZs to a regional model that allows high-speed data-intensive networking, facilitating researchers moving data between their laboratories and their collaborators' sites, supercomputer centers or data repositories, and enabling that data to traverse multiple heterogeneous networks without performance degradation over campus, regional, national, and international distances. The PRP's data sharing architecture, with end-to-end 10-40-100Gb/s connections, provides long-distance virtual co-location of data with computing resources, with enhanced security options."
"1550463","SI2-SSI: Advancing and Mobilizing Citizen Science Data through an Integrated Sustainable Cyber-Infrastructure","OAC","AISL, Software Institutes","11/01/2016","08/03/2016","Gregory Newman","CO","Colorado State University","Standard Grant","Stefan Robila","10/31/2020","$1,000,000.00","Melinda Laituri, Stacy Lynn, Louis Liebenberg","Gregory.Newman@ColoState.Edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7259, 8004","043Z, 7433, 8004, 8009","$0.00","Citizen science engages members of the public in science. It advances the progress of science by involving more people and embracing new ideas. Recent projects use software and apps to do science more efficiently. However, existing citizen science software and databases are ad hoc, non-interoperable, non-standardized, and isolated, resulting in data and software siloes that hamper scientific advancement. This project will develop new software and integrate existing software, apps, and data for citizen science - allowing expanded discovery, appraisal, exploration, visualization, analysis, and reuse of software and data. Over the three phases, the software of two platforms, CitSci.org and CyberTracker, will be integrated and new software will be built to integrate and share additional software and data. The project will: (1) broaden the inclusivity, accessibility, and reach of citizen science; (2) elevate the value and rigor of citizen science data; (3) improve interoperability, usability, scalability and sustainability of citizen science software and data; and (4) mobilize data to allow cross-disciplinary research and meta-analyses. These outcomes benefit society by making citizen science projects such as those that monitor disease outbreaks, collect biodiversity data, monitor street potholes, track climate change, and any number of other possible topics more possible, efficient, and impactful through shared software. <br/><br/>The project will develop a cyber-enabled Framework for Advancing Buildable and Reusable Infrastructures for Citizen Science (Cyber-FABRICS) to elevate the reach and complexity of citizen science while adding value by mobilizing well-documented data to advance scientific research, meta-analyses, and decision support. Over the three phases of the project, the software of two platforms, CitSci.org and CyberTracker, will be integrated by developing APIs and reusable software libraries for these and other platforms to use to integrate and share data and software. Using participatory design and agile methods over four years, the project will: (1) broaden the inclusivity, accessibility, and reach of citizen science; (2) elevate the value and rigor of citizen science software and data; (3) improve interoperability, usability, scalability and sustainability of citizen science software and data; and (4) mobilize data to allow cross-disciplinary research and meta-analyses. These outcomes benefit society by making citizen science projects and any number of other possible topics more possible, efficient, and impactful through shared software and data. Adoption of Cyber-FABRICS infrastructure, software, and services will allow anyone with an Internet or cellular connection, including those in remote, underserved, and international communities, to contribute to research and monitoring, either independently or as a team.  This project is also being supported by the Advancing Informal STEM Learning (AISL) program, which seeks to advance new approaches to, and evidence-based understanding of, the design and development of STEM learning in informal environments."
"1642070","Collaborative Research: CICI: Secure and Resilient Architecture: Scientific Workflow Integrity with Pegasus","OAC","Cyber Secur - Cyberinfrastruc","09/01/2016","02/06/2018","Von Welch","IN","Indiana University","Standard Grant","Micah Beck","08/31/2019","$479,855.00","Raquel Hill, Steven Myers","vwelch@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8027","","$0.00","Scientists use computer systems to analyze and store their scientific data, sometimes in a complex process across multiple machines. This process can be tedious and error-prone, which has led to the development of software known as a ""workflow management system"". Workflow management systems allow scientists to describe their process in a human-friendly way and then the software handles the details of the processing for the scientists, dealing with tedious and repetitive steps and handling errors. One popular workflow management system is Pegasus, which, over the past three years, was used to run over 700,000 workflows by scientists in a number of domains including astronomy, bioinformatics, earthquake science, gravitational wave physics, ocean science, and neuroscience. The ""Scientific Workflow Integrity with Pegasus"" project enhances Pegasus with additional security features. The scientist's description of their desired work is protected from tampering and the data processed by Pegasus is checked to ensure it hasn't been accidentally or maliciously modified. Such tamper protection is attained by cryptographic techniques that ensure data integrity. These changes allow scientists, and our society, to be more confident of scientific findings based on collected data.<br/><br/>The Scientific Workflow Integrity with Pegasus project strengthens cybersecurity controls in the Pegasus Workflow Management System in order to provide assurances with respect to the integrity of computational scientific methods. These strengthened controls enhance both Pegasus' handling of science data and its orchestration of software-defined networks and infrastructure. The result is increased trust in computational science and increased assurance in our ability to reproduce the science by allowing scientists to validate that data has not been changed since a workflow completed and that the results from multiple workflows are consistent. The focus on Pegasus is due to its popularity in the scientific community as a method of computation and data management automation. For example, LIGO, the NSF-funded gravitational-wave physics project, recently used the Pegasus Workflow Management System to structure and execute the analyses that confirmed and quantified its historic detection of a gravitational wave, confirming the prediction made by Einstein 100 years ago. The proposed project has established collaborations with LIGO and additional key NSF infrastructure providers and science projects to ensure broadly applied results."
"1642090","Collaborative Research: CICI: Secure and Resilient Architecture: Scientific Workflow Integrity with Pegasus.","OAC","Cyber Secur - Cyberinfrastruc","09/01/2016","08/22/2016","Ilya Baldin","NC","University of North Carolina at Chapel Hill","Standard Grant","Micah Beck","08/31/2019","$230,000.00","","ibaldin@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8027","","$0.00","Scientists use computer systems to analyze and store their scientific data, sometimes in a complex process across multiple machines. This process can be tedious and error-prone, which has led to the development of software known as a ""workflow management system"". Workflow management systems allow scientists to describe their process in a human-friendly way and then the software handles the details of the processing for the scientists, dealing with tedious and repetitive steps and handling errors. One popular workflow management system is Pegasus, which, over the past three years, was used to run over 700,000 workflows by scientists in a number of domains including astronomy, bioinformatics, earthquake science, gravitational wave physics, ocean science, and neuroscience. The ""Scientific Workflow Integrity with Pegasus"" project enhances Pegasus with additional security features. The scientist's description of their desired work is protected from tampering and the data processed by Pegasus is checked to ensure it hasn't been accidentally or maliciously modified. Such tamper protection is attained by cryptographic techniques that ensure data integrity. These changes allow scientists, and our society, to be more confident of scientific findings based on collected data.<br/><br/>The Scientific Workflow Integrity with Pegasus project strengthens cybersecurity controls in the Pegasus Workflow Management System in order to provide assurances with respect to the integrity of computational scientific methods. These strengthened controls enhance both Pegasus' handling of science data and its orchestration of software-defined networks and infrastructure. The result is increased trust in computational science and increased assurance in our ability to reproduce the science by allowing scientists to validate that data has not been changed since a workflow completed and that the results from multiple workflows are consistent. The focus on Pegasus is due to its popularity in the scientific community as a method of computation and data management automation. For example, LIGO, the NSF-funded gravitational-wave physics project, recently used the Pegasus Workflow Management System to structure and execute the analyses that confirmed and quantified its historic detection of a gravitational wave, confirming the prediction made by Einstein 100 years ago. The proposed project has established collaborations with LIGO and additional key NSF infrastructure providers and science projects to ensure broadly applied results."
"1829698","CyberTraining CIP: Cyberinfrastructure Expertise on High-throughput Networks for Big Science Data Transfers","OAC","CyberTraining - Training-based, Campus Cyberinfrastrc (CC-NIE)","10/01/2018","07/02/2018","Jorge Crichigno","SC","University of South Carolina at Columbia","Standard Grant","Sushil Prasad","09/30/2021","$499,959.00","Nasir Ghani, Elias Bou-Harb","jcrichigno@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","044Y, 8080","026Z, 062Z, 7361, 9150","$0.00","This project establishes the Cyberinfrastructure Network of Expertise (CNE) for teaching, training, and research on networking technologies including Science Demilitarized Zone (Science DMZ). The Science DMZ is a network specifically designed to facilitate the transfer and sharing of very large scientific data (big data) across geographically separated sites. The project serves the national interest, as it addresses the shortage of skilled research personnel with specialized skills to support networks carrying big science data among research institutions, universities, and national laboratories. This initiative also fosters skills toward the development of a national ""cyber-highway"" system to better facilitate the sharing of big science data, hence promoting collaboration and national competitiveness in science and engineering, aligned with NSF's mission. CNE is composed of universities, industry organizations, national and state agencies in multiple states, and a national laboratory. This broad initiative ensures a strong adoption of best cyberinfrastructure (CI) practices in sharing terabytes of data and more, thereby enabling new modes of discovery and collaboration. Additionally, the project develops advanced instructional material for Science DMZ, which is being integrated into undergraduate courses in information technology, computer science, and engineering in multiple universities. <br/><br/>CNE is anchored at the University of South Carolina (USC) and has several thrusts. First, the instructional material is enriched with a large number of virtual laboratories (vLabs), designed in partnership with domain experts and the Network Development Group (NDG), a company in virtualized training. The material covers key aspects of the Science DMZ, including wide area networks, routers and switches, TCP attributes for big data transfers, and cybersecurity for friction-free environments. Second, the CNE partnership also includes Florida Atlantic University (FAU) and University of South Florida (USF), which are incorporating the vLabs and companion material into their degree programs. These enriched programs provide skills to fulfill pressing demands in both South Carolina and Florida. Third, in conjunction with local industry and Savannah River National Laboratory, CNE incorporates internships and industry-sponsored capstone projects in student training to strengthen the cooperation between academia and industry and to bolster national security related to big data transfers. Fourth, CNE organizes workshops to provide critical training and education on Science DMZs in partnership with Internet2 and state agencies, namely South Carolina Cyber (SC Cyber) and the Florida Center for Cybersecurity (FC2). Finally, by leveraging the extensive dissemination channels of all project partners, CNE plans to make vLabs and manuals available to the wider CI community for online self-paced self-training delivery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1724728","CIF21 DIBBs: EI: Creating a Digital Environment for Enabling Data-Driven Science (DEEDS)","OAC","DATANET","08/01/2017","02/12/2019","Ann Christine Catlin","IN","Purdue University","Standard Grant","Amy Walton","07/31/2021","$3,456,281.00","Connie Weaver, Joseph Francisco, Muhammad Alam, Maria Sepulveda","acc@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7726","7433, 8048","$0.00","This project creates a platform that simplifies knowledge extraction from diverse scientific datasets, by integrating multimodal data, computing software, and interactive tools for search and exploration.The effort provides a collaborative environment for sharing research data and computational and statistical algorithms.During research investigations, the environment uploads and organizes data, executes and tracks computational jobs, and connects input and output to analysis results. When investigations are complete, the environment helps publish data, algorithms and investigative workflows for public access. The environment also simplifies the discovery, search, and exploration of published datasets - providing interactive tools for viewing and analyzing data. Four distinct use cases (chemistry, electrical engineering, nutrition science, and environmental science) provide a testbed for platform usability and functionality. The use cases produce important datasets that are intended for use by HHS, USDA, EPA, DOE, and other government agencies to support decision-making about policy and regulations.<br/><br/>The project builds upon a 2014 DIBBs pilot demonstration award (#1443017 - DataHub) which created a data management platform for the publishing and discovery of scientific research datasets. The team extends this platform to support the full research investigation process by 1) connecting data to computational and statistical modeling software, 2) tracking research workflows to link data, algorithms, and results, 3) automatically capturing metadata and classifying data by type, and 4) providing interfaces to define complex hierarchical, structured data and operate on the data using analytical toolkits. Published datasets are explored with interactive tools that interpret data types for advanced navigation, viewing, search, analysis, and visualization. Collaborations with research projects in chemistry, electrical engineering, nutrition science, and environmental science produce system requirements and guarantee that the general, discipline-neutral platform provides end-to-end support for their use cases. The project helps researchers curate their own findings, and also facilitates the sharing of data and findings for the purposes of preservation, replication, and extension."
"1664162","SI2-SSI: Pegasus: Automating Compute and Data Intensive Science","OAC","Software Institutes","05/15/2017","05/08/2017","Ewa Deelman","CA","University of Southern California","Standard Grant","Stefan Robila","04/30/2022","$2,500,000.00","Miron Livny","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8004","7433, 8004, 8009","$0.00","This project addresses the ever-growing gap between the capabilities offered by on-campus and off-campus cyberinfrastructures (CI) and the ability of researchers to effectively harness these capabilities to advance scientific discovery. Faculty and students on campuses struggle to extract knowledge from data that does not fit on their laptops or cannot be processed by an Excel spreadsheet and they find it difficult to efficiently manage their computations. The project sustains and enhances the Pegasus Workflow Management System, which enables scientist to orchestrate and run data- and compute-intensive computations on diverse distributed computational resources. Enhancements focus on the automation capabilities provided by Pegasus to support workflows handling large data sets, as well as usability of Pegasus that lowers the barrier of its adoption. This effort expands the reach of the advanced capabilities provided by Pegasus to researchers from a broader spectrum of disciplines that range from gravitational-wave physics to bioinformatics, and from earth science to material science.<br/><br/>For more than 15 years the Pegasus Workflow Management System has been designed, implemented and supported to provide abstractions that enable scientists to focus on structuring their computations without worrying about the details of the target CI. To support these workflow abstractions Pegasus provides automation capabilities that seamlessly map workflows onto target resources, sparing scientists the overhead of managing the data flow, job scheduling, fault recovery and adaptation of their applications. Automation enables the delivery of services that consider criteria such as time-to-solution, as well as takes into account efficient use of resources, managing the throughput of tasks, and data transfer requests. The power of these abstractions was demonstrated in 2015 when Pegasus was used by an international collaboration to harness a diverse set of resources and to manage compute- and data- intensive workflows that confirmed the existence of gravitational waves, as predicted by Einstein's theory of relativity. Experience from working with diverse scientific domains - astronomy, bioinformatics, climate modeling, earthquake science, gravitational and material science - uncover opportunities for further automation of scientific workflows. This project addresses these opportunities through innovation in the following areas: automation methods to include resource provisioning ahead of and during workflow execution, data-aware job scheduling algorithms, and data sharing mechanisms in high-throughput environments. To support a broader group of ""long-tail"" scientists, effort is devoted to usability improvements as well as outreach, education, and training activities. The proposed work includes the implementation and evaluation of advanced frameworks, algorithms, and methods that enhance the power of automation in support of data-intensive science. These enhancements are delivers as dependable software tools integrated with Pegasus so that they can be evaluated in the context of real-life applications and computing environments. The data-aware focus targets new classes of applications executing in high-throughput and high-performance environments. Pegasus has been adopted by researchers from a broad spectrum of disciplines that range from gravitational-wave physics to bioinformatics, and from earth science to material science. It provides and enhances access to national CI such as OSG and XSEDE, and as part of this work it will be deployed within Chameleon and Jetstream to provide broader access to NSF's CI investments. Through usability improvements, engagement with CI and community platform providers such as HubZero and Cyverse, combined with educational, training, and tutorial activities, this project broadens the set of researchers that leverage automation for their work. Collaboration with the Gateways Institute assures that Pegasus interfaces are suitable for vertical integration within science gateways and seamlessly supports new scientific communities."
"1826997","CC* Integration: Delivering a Dynamic Network-Centric Platform for Data-Driven Science (DyNamo)","OAC","CISE RESEARCH RESOURCES","08/01/2018","07/19/2018","Anirban Mandal","NC","University of North Carolina at Chapel Hill","Standard Grant","Deepankar Medhi","07/31/2020","$1,000,000.00","Ewa Deelman, Michael Zink, Ivan Rodero, Cong Wang","anirban@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","2890","9102","$0.00","Computational science today depends on many complex, data-intensive applications operating on datasets that originate from a variety of scientific instruments and data stores. A major challenge for data-driven science applications is the integration of data into the scientist's workflow. Recent advances in dynamic, networked cloud infrastructures provide the building blocks to construct integrated, reconfigurable, end-to-end infrastructure that has the potential to increase scientific productivity. However, applications and workflows have seldom taken advantage of these advanced capabilities.<br/>Dynamo will allow atmospheric scientists and hydrologists to improve short- and long-term weather forecasts, and aid the oceanographic community to improve key scientific processes like ocean-atmosphere exchange, turbulent mixing etc., both of which have direct impact on our society.<br/> <br/>The Dynamo project will develop innovative network-centric algorithms, policies and mechanisms to enable programmable, on-demand access to high-bandwidth, configurable network paths from scientific data repositories to national CyberInfrastructure facilities, and help satisfy data, computational and storage requirements of science workflows. This will enable researchers to test new algorithms and models in real time with live streaming data, which is currently not possible in many scientific domains. Through enhanced interactions between Pegasus, the network-centric platform, and new network-aware workflow scheduling algorithms, science workflows will benefit from workflow automation and data management over dynamically provisioned infrastructure. The system will transparently map application-level, network Quality of Service expectations to actions on programmable software defined infrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659088","CC* Networking Infrastructure: Enabling Data Intensive Science at Cornell University","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2017","12/21/2016","David Lifka","NY","Cornell University","Standard Grant","Kevin L. Thompson","06/30/2019","$376,714.00","","lifka@cac.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","8080","","$0.00","This project, CC* Networking Infrastructure: Enabling Data Intensive Science at Cornell University, is upgrading network connectivity from the University's main data center to Internet2 to create a transformative impact on Cornell research that was previously bandwidth limited. The upgraded networking infrastructure is advancing knowledge and discovery by providing Cornell faculty and student researchers with faster and higher volume access to remote data sets, instrumentation, and computational and data analysis resources. Campus researchers in data-driven disciplines - including astronomy, computer science, high energy physics, life sciences, and material science  - are using the upgraded networks to improve time to science and researcher productivity. This upgrade is also preparing Cornell and its national collaborators for future data growth driven by the Internet of Things, more powerful scientific instruments, and cloud computing models that share large-scale data and resources between institutions.<br/> <br/>Research enabled by Cornell's 100G (gigabits per second) components impacts individual health and well-being in analysis of genetic variations in the human genome, water resource management, analysis of atmospheric and aerosol particle data), as well as global health and biodiversity. Campus backbone performance is monitored by the project's perfSONAR infrastructure and that information is available to internal and external users wanting to share data and resources via Internet2. A public web portal provides real-time data on the health of the 100G path. Project success is measured in terms of the myriad research benefits made possible by the availability of this increased bandwidth."
"1341711","Wrangler: A Transformational Data Intensive Resource for the Open Science Community","OAC","INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE, EQUIPMENT ACQUISITIONS, DATANET, Cyber Secur - Cyberinfrastruc","11/01/2013","08/09/2018","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Robert Chadduck","10/31/2019","$12,392,996.00","Christopher Jordan, Tommy Minyard","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1640, 7231, 7619, 7726, 8027","7433, 7434, 7619, 7726","$0.00","This award is to fund a transformational data intensive resource for the open science community called Wrangler. Big data is creating tremendous new scientific opportunities, but also many new challenges for data-driven science. The computational needs of large-scale data-driven science vary across domains and applications but there are some requirements that are widely applicable: capacious, high performance, reliable data storage; support for diverse data types and access methods; and support for embedded analytics that eliminate costly data movement.  Wrangler is a high performance system with an innovative embedded data analytics capability that far exceeds the capabilities available to the open science community today.  It contains massive data storage, which can be expanded if required, high performance, and support for both structured and unstructured data.  The storage is configured for ultra-high reliability using replication at two locations, unprecedented analytics capabilities and innovative NAND flash storage.  The resource contains 3000 next generation Intel Haswell cores, offering the most powerful embedded analytics capabilities in the world for a wide range of data intensive science. Wrangler will be connected at 100 Gbps to Internet2, the fastest available connection to the biggest research network. The project will also offer a data docking service for receiving and ingesting data shipped on physical media. Augmented by Stampede & XSEDE, Wranglers capabilities will be enhanced through tight integration to TACC?s Stampede supercomputer, and through TACC to other XSEDE resources."
"1841487","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Robert Gardner","IL","University of Chicago","Standard Grant","William Miller","09/30/2020","$400,000.00","","rwg@hep.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841475","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Laura Cadonati","GA","Georgia Tech Research Corporation","Standard Grant","William Miller","09/30/2020","$146,937.00","","cadonati@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841546","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Peter Couvares","CA","California Institute of Technology","Standard Grant","William Miller","09/30/2020","$55,094.00","","peter.couvares@ligo.org","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841530","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","Frank Wuerthwein","CA","University of California-San Diego","Standard Grant","William Miller","09/30/2020","$696,085.00","","fkw@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841479","Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/07/2018","David Schultz","WI","University of Wisconsin-Madison","Standard Grant","William Miller","09/30/2020","$197,221.00","","david.schultz@icecube.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7684","020Z, 062Z","$0.00","In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of ""Multi-Messenger Astrophysics,"" an exciting new field supported by one of NSF's ten Big Ideas, ""Windows on the Universe"".<br/><br/>The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835660","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Madhav Marathe","VA","Virginia Polytechnic Institute and State University","Standard Grant","Vipin Chaudhary","10/31/2023","$2,880,000.00","Edward Fox, Naren Ramakrishnan, Catherine Amelink, Christopher Kuhlman","mvm7hz@virginia.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835631","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Geoffrey Fox","IN","Indiana University","Standard Grant","Vipin Chaudhary","10/31/2023","$500,000.00","","gcf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835598","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Jurij Leskovec","CA","Stanford University","Standard Grant","Vipin Chaudhary","10/31/2023","$540,000.00","","jure@cs.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835441","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Albert Esterline","NC","North Carolina Agricultural & Technical State University","Standard Grant","Vipin Chaudhary","10/31/2023","$40,000.00","","esterlin@ncat.edu","1601 E. Market Street","Greensboro","NC","274110001","3363347995","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835439","Collaborative Research: Framework: Software: CINES: A Scalable Cyberinfrastructure for Sustained Innovation in Network Engineering and Science","OAC","Software Institutes","11/01/2018","08/28/2018","Richard Alo","MS","Jackson State University","Standard Grant","Vipin Chaudhary","10/31/2023","$40,000.00","Natarajan Meghanathan","richard.alo@famu.edu","1400 J R LYNCH ST.","Jackson","MS","392170002","6019792008","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Networks are ubiquitous and are a part of our common vocabulary. Network science and engineering has emerged as a formal field over the last twenty years and has seen explosive growth.  Ideas from network science are central to companies such as Akamai, Twitter, Google, Facebook, and LinkedIn.  The concepts have also been used to address fundamental problems in diverse fields (e.g., biology, economics, social sciences, psychology, power systems, telecommunications, public health and marketing), and are now part of most university curricula. Ideas and techniques from network science are widely used in making scientific progress in the disciplines mentioned above.  Networks are now part of the public vocabulary, with news articles and magazines frequently using the term ""networks"" to refer to interconnected entities.  Yet, resources for effective use of techniques from network science are largely dispersed and stand-alone, of small scale, home-grown for personal use, and/or do not cover the broad range of operations that need to be performed on networks.  Compositions of these diverse capabilities are rare.  Furthermore, many researchers who study networks are not computer scientists.  As a result, they do not have easy access to computing and data resources; this creates a barrier for researchers. This project will develop a sophisticated cyberinfrastructure that brings together various resources to provide a unifying ecosystem for network science that is greater than the sum of its parts. The resulting cyberinfrastructure will benefit researchers and students from various disciplines by facilitating access to various tools for synthesizing and analyzing large networks, and by providing access points for contributors of new software and data. An important benefit of the system is that it can be readily used even by researchers who have no formal training in computer programming.  The cyberinfrastructure resulting from this work will foster multi-disciplinary and multi-university research and teaching collaborations. As part of this project, comprehensive education and outreach programs will be launched by the participating institutions, spanning educators and K-12 students. These programs will include network science courses with students from minority and under-represented groups, and students at smaller institutions who do not have easy access to high performance computing resources.<br/><br/><br/>Resources for doing network science are largely dispersed and stand-alone (in silos of isolated tools), of small scale, or home-grown for personal use.  What is needed is a cyberinfrastructure to bring together various resources, to provide a unifying ecosystem for network science that is greater than the sum of its parts. The primary goal of this proposal is to build self-sustaining cyberinfrastructure (CI) named CINES (Cyberinfrastructure for Sustained Innovation in Network Engineering and Science) that will be a community resource for network science.  CINES will be an extensible and sustainable platform for producers and consumers of network science data, information, and software.  CINES will have: (1) a layered architecture that systematically modularizes and isolates messaging, infrastructure services, common services, a digital library, and APIs for change-out  and updates; (2) a robust and reliable infrastructure that---for applications (apps)---is designed to accommodate technological advances in methods, programming languages, and computing models; (3) a resource manager to enable jobs to run on target machines for which they are best suited; (4) an engine to enable users to create new workflows by composing available components and to distribute the resulting workload across computing resources; (5) orchestration among system components to provide CI-as-a-service (CIaaS) that scales under high system load to networks with a billion or more vertices; (6) a digital library with 100,000+ networks of various kinds that allows rich services for storing, searching, annotating, and browsing; (7) structural methods (e.g., centrality, paths, cuts, etc.) and dynamical models of various contagion processes; (8) new methods to acquire data, build networks, and augment them using machine learning techniques; (9) a suite of industry- recognized tools such as SNAP, NetworkX, and R-studio that make it easier for researchers, educators, and analysts to do network science and engineering; (10) a suite of APIs that allows developers to add new web-apps and services, based on an app-store model, and allows access to CINES from third party software; and (11) metrics and a Stack Overflow model, among other features, for producers and consumers to interact (in real-time) and guide the evolution of CINES. CINES will enable fundamental changes in the way researchers study and teach complex networks.  The use of state-of-the-art high-performance computing (HPC) resources to synthesize, analyze, and reason about large networks will enable researchers and educators to study networks in novel ways. CINES will allow scientists to address fundamental scientific questions---e.g., biologists can use network methods to reason about genomics data that is now available in large quantities due to fast and effective sequencing and the NIH Microbiome Program.  It will enable educators to harness HPC technologies to teach Network Science to students spanning various academic levels, disciplines, and institutions.  CINES, which will be useful to researchers supported by many NSF directorates and divisions, will be designed for scalability, usability, extensibility, and sustainability. This project will also advance the fields of digital libraries and cloud computing by stretching them to address challenges related to Network Science.  Given the multidisciplinary nature of the field, CINES will provide a collaborative space for scientists from different disciplines, leading to important cross fertilization of ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835379","Elements: Bringing Montage To Cutting Edge Science Environments","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","03/15/2019","03/11/2019","Graham Berriman","CA","California Institute of Technology","Standard Grant","Micah Beck","02/28/2022","$598,369.00","","gbb@ipac.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","Astronomy is undergoing a transformation in the way data are acquired, and this is driving a corresponding transformation in the way astronomers process these data.  Telescopes and sky surveys that are operating now or will begin to operate in the coming years will deliver data that are too large and complex to analyze by the traditional method of downloading data to desktops and local clusters.  Thus a transformation is underway to use new technologies to process data. Astronomers have embraced the Python language for analysis because it provides the necessary flexible building blocks to handle complex data, and are embracing new Python-based technologies to manage and control processing that allows the software to run next to the data themselves. The Montage image mosaic engine, a toolkit already used widely by astronomers, will join this transformative community and deliver high-performance, next generation image processing capabilities for astronomers and computer scientists.  It will allow astronomers to create large-scale images of the sky, and study these images with the many powerful tools available in Python.<br/><br/>Python has become the language of choice for astronomy, and environments such as JupyterLabs and JupyterHub are almost certainly the science environments of the future. The LSST is committed to using such an environment for its science platform, which will be the primary way LSST users will discover, access and analyze data. Astronomy science archives are actively building similar platforms. NOAO has deployed their DataLab, which supports datasets acquired at Kitt Peak and CTIO. We will incorporate the functionality of the Montage image mosaic engine - a scalable toolkit written in ANSI-C and in wide use in astronomy and information technology - into environments such as these to unleash its full power when applied to large and complex new datasets. Moreover, the same functionality can be incorporated into a single desktop platform, or into a new scalable environment built to support a new project or mission, or into a distributed scalable environment such the Amazon Elastic Cloud (""bringing the code to the data"").  As a component-based toolkit, Montage will be well positioned to respond to the rapid changes expected as these new platforms develop and contribute substantially to understanding their performance and usefulness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640834","CIF21 DIBBs: EI: Virtual Data Collaboratory: A Regional Cyberinfrastructure for Collaborative Data Intensive Science","OAC","DATANET","09/01/2016","01/31/2018","Ivan Rodero","NJ","Rutgers University New Brunswick","Standard Grant","Amy Walton","08/31/2020","$4,000,000.00","Vasant Honavar, Jenni Evans, Grace Agnew, James von Oehsen","irodero@cac.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7726","7433, 8048","$0.00","This project develops a virtual data collaboratory that can be accessed by researchers, educators, and entrepreneurs across institutional and geographic boundaries, fostering community engagement and accelerating interdisciplinary research.  A federated data system is created, using existing components and building upon existing cyberinfrastructure and resources in New Jersey and Pennsylvania.  Seven universities are directly involved (the three Rutgers University campuses, Pennsylvania State University, the University of Pennsylvania, the University of Pittsburgh, Drexel University, Temple University, and the City University of New York); indirectly, other regional schools served by the New Jersey and Pennsylvania high-speed networks also participate.  The system has applicability to a several science and engineering domains, such as protein-DNA interaction and smart cities, and is likely to be extensible to other domains.  The cyberinfrastructure is to be integrated into both graduate and undergraduate programs across several institutions.   <br/><br/>The end product is a fully-developed system for collaborative use by the research and education community.   A data management and sharing system is constructed, based largely on commercial off-the-shelf technology.  The storage system is based on the Hadoop Distributed File System (HDFS), a Java-based file system providing scalable and reliable data storage, designed to span large clusters of commodity servers.  The Fedora and VIVO object-based storage systems are used, enabling linked data approaches.  The system will be integrated with existing research data repositories, such as the Ocean Observatories Initiative and Protein Data Bank repositories.  Regional high-performance computing and network infrastructure is leveraged, including New Jersey's Regional Education and Research Network (NJEdge), Pennsylvania's Keystone Initiative for Network Based Education and Research (KINBER), the Extreme Science and Engineering Discovery Environment (XSEDE) computing capabilities, Open Science Grid, and other NSF Campus Cyberinfrastructure investments.  The project also develops a custom site federation and data services layer; the data services layer provides services for data linking, search, and sharing; coupling to computation, analytics, and visualization; mechanisms to attach unique Digital Object Identifiers (DOIs), archive data, and broadly publish to internal and wider audiences; and manage the long-term data lifecycle, ensuring immutable and authentic data and reproducible research."
"1548562","XSEDE 2.0:  Integrating, Enabling and Enhancing National Cyberinfrastructure with Expanding Community Involvement","OAC","ETF","09/01/2016","09/14/2018","John Towns","IL","University of Illinois at Urbana-Champaign","Cooperative Agreement","Robert Chadduck","08/31/2021","$98,317,919.00","Ralph Roskies, Kelly Gaither, Nancy Wilkins-Diehr","jtowns@ncsa.illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7476","026Z","$0.00","This award supports the continuation and evolution of NSF project 1053575 - XSEDE: eXtreme Science and Engineering Discovery Environment. The goal of XSEDE is to accelerate open scientific discovery by enhancing the productivity and capability of researchers, engineers, and scholars, and by broadening their participation in science and engineering. It does so by making advanced computational resources easier to use, integrating existing resources into new, powerful services and building the community of users and providers. XSEDE is a virtual organization that provisions complex distributed infrastructure, support services, and technical expertise. A prominent opportunity for XSEDE is the growing, diverse collection of advanced computing, high-end visualization, data analysis, and other resources and services available to researchers, engineers, and scholars; these resources have the potential to help understand and solve the most important and challenging problems facing the nation and world. The challenge for XSEDE, as a virtual organization, is to organize these disparate resources, creating integrated services and a coordinated environment that serves the end user needs. The challenge also includes fostering awareness of, and training for, full utilization of the capabilities offered by XSEDE and its associated resources, as well as catalyzing workforce developments. All these tasks need to be accomplished in light of evolving user requirements, resources, and NSF strategies.<br/><br/>The XSEDE 2 project will be executed by the principal investigator (PI) and staff of the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign and by the co-PIs and staff of the partner organizations at the Pittsburgh Supercomputing Center (PSC, Carnegie Mellon University and University of Pittsburgh), San Diego Supercomputing Center (SDSC, University of California San Diego), and Texas Advanced Computing Center (TACC, University of Texas at Austin), as well as 15 other partner organizations.<br/><br/>For the next five years, in pursuit of its overall goals of enhancing user productivity and broadening participation of the CDS&E community, XSEDE 2 will provide an adaptive and streamlined framework that anticipates the opportunities afforded by advances in technology, responds to users' abilities to make effective use of new capabilities, and enables the current and next generation in using these technologies to advance their fields. The three strategic goals remain unchanged from the original XSEDE project:<br/><br/>* To deepen and extend use of the ecosystem of national cyberinfrastructure (CI) by both existing computational researchers and new communities of scientists and students where the use of computation and large-scale data is transforming their respective fields;<br/>* To advance the national CI ecosystem by creating an open and evolving infrastructure, and by enhancing the array of technical expertise and support services offered; and<br/>* To sustain the national CI ecosystem by maintaining a secure, reliable and efficient infrastructure.<br/><br/>XSEDE 2 will reorganize into five goal-driven focus areas that will provide a more agile and responsive program designed to accelerate progress toward the strategic goals:<br/><br/>* The Resource Allocation Service (RAS), led by the National Center for Atmospheric Research (NCAR) and four other partners, will continue to manage the process of receiving, evaluating and awarding proposals for computational resources. In doing so, it will fulfill XSEDE 2's crucial role of neutral arbiter in allocating resources from the service-provider ecosystem to the research community. RAS will also identify new opportunities for allocation innovations by increased transparency, open reporting of user trends, and adapting the allocation process to new technologies.<br/>* The revised XSEDE Community Infrastructure (XCI) service, led by Cornell University and six other partners, will identify, evaluate, test, and make available new software capabilities. Governance is in place to ensure that these activities are driven by the needs of both users and providers of cyberinfrastructure.<br/>* Community Engagement & Enrichment (CEE), led by the University of Texas and 12 other partners, will build on the XSEDE tradition of outstanding user services, and engage a new generation of diverse computational researchers. In addition to education, training, and outreach activities, CEE will connect to campus HPC communities, to help researchers access both local and national resources.<br/>* The Extended Collaborative Support Service (ECSS), led by the San Diego Supercomputer Center, the Pittsburgh Supercomputing Center and eight other partners, will maximize the effectiveness of HPC resources through its large staff of computational experts who will directly participate in research teams, providing advanced assistance to science projects.<br/>* Finally, XSEDE Operations, led by the University of Tennessee and five other partners, will maintain and evolve an integrated HPC capability of national scale. Operations provides a ""one-stop-shop"" experience for users across the XSEDE-coordinated HPC ecosystem.<br/><br/>While continuity in providing these services is essential for the large and further-growing user community, XSEDE 2 will also respond to the evolving needs and opportunities of science and technology. To this end, XSEDE 2 will develop novel ways to connect to and collaborate with other national, regional and campus cyberinfrastructure organizations. The project will continue to innovate the use of ""e-science portals"" (also known as Science Gateways). Science gateways provide interfaces and services that are customized to a domain science and have an increasing role with facilities and research centers, collaborating on large research undertakings (e.g., Advanced LIGO, Polar Geospatial Center). This approach facilitates broad community access to advanced compute and data resources. Science gateways are now serving more than 50% of the user community. XSEDE 2 will also incorporate new methods to serve users interested in cloud computing resources and big-data projects. Furthermore, by analyzing trends in usage and technology, the renewal project will be even better positioned to respond to the evolving needs of its stakeholders and to emerging opportunities in new compute and data resources."
"1642101","Collaborative Research: CICI: Secure and Resilient Architecture: S3D: A New SDN-Based Security Framework for the Science DMZ","OAC","Cyber Secur - Cyberinfrastruc","11/01/2016","12/13/2018","Douglas Swany","IN","Indiana University","Standard Grant","Kevin Thompson","10/31/2019","$309,999.00","","swany@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8027","9251","$0.00","The Science DMZ (SDMZ) is a key foundational element in building state-of-the-art scientific research infrastructure. The SDMZ is a portion of the network, built at the campus or laboratory's edge, that is designed such that the equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose business systems or enterprise computing. SDMZs are increasingly being implemented by research agencies, campuses and national labs. In order to improve the throughput of scientific research data, NSF has funded many Science DMZ implementations on campuses by upgrading research network connectivity and encouraging installation of a SDMZ. However, the SDMZ has characteristics that separate it as a unique ecosystem which cannot simply adopt existing enterprise and cloud based network security technologies and policies. This project designs and prototypes an integrated Software Defined Network (SDN) security framework for managing data-intensive science applications utilizing the Science DMZ (SDMZ) model. It offers one of the first demonstrations of how fine-grained security controls can co-exist within a high performance data-intensive network. This project produces significant advancements in the trustworthiness and reliability of large-scale data-intensive scientific research infrastructures.<br/><br/>This project evaluates the current state of the SDMZ security architecture, then identifies the current shortcomings in its existing security services. The new proposed framework: 1) defines fine-grained network flow controls using dynamically deployable security services that are migratable and science-application aware; 2) defines a new class of network privilege management policies that can revoke or divert flows that violate SDMZ policies or that differ from user-defined, application-specific usage expectations; 3) establishes high-performance virtual circuits that enable data intensive applications to register and fast-path their authenticated flows across the SDMZ. Furthermore, this project introduces a unified security policy engine to dramatically simplify the control of the above three services. The policy engine offers a valuable and user-friendly abstraction to meet the domain-specific needs of the SDMZ."
"1642129","Collaborative Research: CICI: Secure and Resilient Architecture: S3D: A New SDN-Based Security Framework for the Science DMZ","OAC","Cyber Secur - Cyberinfrastruc","11/01/2016","08/18/2016","Guofei Gu","TX","Texas A&M Engineering Experiment Station","Standard Grant","Micah Beck","10/31/2019","$350,000.00","","guofei@cse.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","8027","","$0.00","The Science DMZ (SDMZ) is a key foundational element in building state-of-the-art scientific research infrastructure. The SDMZ is a portion of the network, built at the campus or laboratory's edge, that is designed such that the equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose business systems or enterprise computing. SDMZs are increasingly being implemented by research agencies, campuses and national labs. In order to improve the throughput of scientific research data, NSF has funded many Science DMZ implementations on campuses by upgrading research network connectivity and encouraging installation of a SDMZ. However, the SDMZ has characteristics that separate it as a unique ecosystem which cannot simply adopt existing enterprise and cloud based network security technologies and policies. This project designs and prototypes an integrated Software Defined Network (SDN) security framework for managing data-intensive science applications utilizing the Science DMZ (SDMZ) model. It offers one of the first demonstrations of how fine-grained security controls can co-exist within a high performance data-intensive network. This project produces significant advancements in the trustworthiness and reliability of large-scale data-intensive scientific research infrastructures.<br/><br/>This project evaluates the current state of the SDMZ security architecture, then identifies the current shortcomings in its existing security services. The new proposed framework: 1) defines fine-grained network flow controls using dynamically deployable security services that are migratable and science-application aware; 2) defines a new class of network privilege management policies that can revoke or divert flows that violate SDMZ policies or that differ from user-defined, application-specific usage expectations; 3) establishes high-performance virtual circuits that enable data intensive applications to register and fast-path their authenticated flows across the SDMZ. Furthermore, this project introduces a unified security policy engine to dramatically simplify the control of the above three services. The policy engine offers a valuable and user-friendly abstraction to meet the domain-specific needs of the SDMZ."
"1823385","Collaborative Research: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, INFORMATION TECHNOLOGY RESEARC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2018","04/23/2018","Von Welch","IN","Indiana University","Standard Grant","William Miller","04/30/2020","$74,410.00","","vwelch@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642150","Collaborative Research: CICI: Secure and Resilient Architecture: S3D: A New SDN-Based Security Framework for the Science DMZ","OAC","Cyber Secur - Cyberinfrastruc","11/01/2016","08/18/2016","Phillip Porras","CA","SRI International","Standard Grant","Micah Beck","10/31/2019","$349,798.00","Vinod Yegneswaran","porras@csl.sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","6508592651","CSE","8027","","$0.00","The Science DMZ (SDMZ) is a key foundational element in building state-of-the-art scientific research infrastructure. The SDMZ is a portion of the network, built at the campus or laboratory's edge, that is designed such that the equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose business systems or enterprise computing. SDMZs are increasingly being implemented by research agencies, campuses and national labs. In order to improve the throughput of scientific research data, NSF has funded many Science DMZ implementations on campuses by upgrading research network connectivity and encouraging installation of a SDMZ. However, the SDMZ has characteristics that separate it as a unique ecosystem which cannot simply adopt existing enterprise and cloud based network security technologies and policies. This project designs and prototypes an integrated Software Defined Network (SDN) security framework for managing data-intensive science applications utilizing the Science DMZ (SDMZ) model. It offers one of the first demonstrations of how fine-grained security controls can co-exist within a high performance data-intensive network. This project produces significant advancements in the trustworthiness and reliability of large-scale data-intensive scientific research infrastructures.<br/><br/>This project evaluates the current state of the SDMZ security architecture, then identifies the current shortcomings in its existing security services. The new proposed framework: 1) defines fine-grained network flow controls using dynamically deployable security services that are migratable and science-application aware; 2) defines a new class of network privilege management policies that can revoke or divert flows that violate SDMZ policies or that differ from user-defined, application-specific usage expectations; 3) establishes high-performance virtual circuits that enable data intensive applications to register and fast-path their authenticated flows across the SDMZ. Furthermore, this project introduces a unified security policy engine to dramatically simplify the control of the above three services. The policy engine offers a valuable and user-friendly abstraction to meet the domain-specific needs of the SDMZ."
"1841399","Collaborative: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, INFORMATION TECHNOLOGY RESEARC, COMPUTATIONAL PHYSICS, Software Institutes","06/01/2018","07/21/2018","Michela Taufer","TN","University of Tennessee Knoxville","Standard Grant","William Miller","04/30/2020","$75,000.00","","taufer@utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084, 9150","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823405","Collaborative Research: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, INFORMATION TECHNOLOGY RESEARC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2018","04/23/2018","Ewa Deelman","CA","University of Southern California","Standard Grant","William Miller","04/30/2020","$75,000.00","","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1823378","Collaborative Research: EAGER: Exploring and Advancing the State of the Art in Robust Science in Gravitational Wave Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, INFORMATION TECHNOLOGY RESEARC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2018","04/23/2018","Duncan Brown","NY","Syracuse University","Standard Grant","William Miller","04/30/2020","$75,000.00","","dabrown@physics.syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","1253, 1640, 7244, 8004","026Z, 040Z, 7569, 7916, 8004, 8084","$0.00","Science is increasingly based on computation for science simulations, data management and analysis, instrument control and collaboration. For scientific results generated through computation to be considered robust and become widely accepted, the computational techniques should be automated, reproducible and trustworthy. By exploring the practices of gravitational-wave astronomy researchers working on the Laser Interferometer Gravitational-Wave Observatory (LIGO) project, this project seeks to create a set of case studies documenting broadly applicable methods for reproducible computational science. Specifically, the project will explore and articulate what reproducibility, automation, and trust mean with respect to computation-based research in gravitational-wave astronomy, identify, implement and validate a set of experimental practices, that will include computational techniques, and finally, evaluate how these experimental practices can be extended to other science domains. <br/><br/>Robust computational science builds on rigorous methods and is composed of three key elements: (1) reproducibility, which enables the verification and leveraging of scientists' findings; (2) automation, which speeds up the exploration of alternative solutions and the processing of large amounts of data while reducing the introduction of errors; and (3) trust, providing security and reliability for software and data, while supplying the necessary attributes for confidence in the scientist's own results and results from others. This project explores robust science in the LIGO project through the following activities within the context of gravitational-wave astronomy: (1) articulating the roles of reproducibility, automation, and trust in gravitational-wave astronomy; (2) identifying, implementing and validating a set of experimental practices, including computational techniques; and (3) advancing towards the project's vision of general computational methods for robust science by evaluating how the experimental practices can be extended to other science domains. The project will develop and use a survey to collect information about LIGO workflows that are composed of a series of experimental, computational, and data manipulation steps. The analysis of the survey will result in a document that describes what reproducibility means in the LIGO context and help identify potential improvements in LIGO's practices. The project will generalize these findings by documenting a mapping of LIGOs original and enhanced approach to other science workflows including those of the molecular dynamics and bioinformatics communities. The final project document will target a broad audience that includes researchers and students at various levels of education, with the goal of introducing them to the concept of robust computational research, and the underlying concepts of reproducibility, automation and trust, teaching them to access code, data, and workflow information to regenerate findings, learn about the scientific methods, and to engage in STEM research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835704","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","Software Institutes, EarthCube","10/01/2018","09/12/2018","Catherine Olschanowsky","ID","Boise State University","Standard Grant","Stefan Robila","09/30/2022","$700,000.00","Alejandro Flores","cathie@cs.boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","8004, 8074","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835794","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","Software Institutes, EarthCube","10/01/2018","09/12/2018","Laura Condon","AZ","University of Arizona","Standard Grant","Stefan Robila","09/30/2022","$699,587.00","Michelle Strout","lecondon@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","8004, 8074","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835855","Collaborative Research: Framework: Software: NSCI : Computational and Data Innovation Implementing a National Community Hydrologic Modeling Framework for Scientific Discovery","OAC","Software Institutes","10/01/2018","09/12/2018","Ilkay Altintas","CA","University of California-San Diego","Standard Grant","Stefan Robila","09/30/2022","$397,684.00","","altintas@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835903","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","XC-Crosscutting Activities Pro, Software Institutes","10/01/2018","09/12/2018","Reed Maxwell","CO","Colorado School of Mines","Standard Grant","Stefan Robila","09/30/2022","$901,394.00","","rmaxwell@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","7222, 8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835569","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","Software Institutes","10/01/2018","09/12/2018","David Tarboton","UT","Utah State University","Standard Grant","Stefan Robila","09/30/2022","$339,985.00","","dtarb@usu.edu","Sponsored Programs Office","Logan","UT","843221415","4357971226","CSE","8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1550488","SI2-SSI: Integrating the NIMBLE Statistical Algorithm Platform with Advanced Computational Tools and Analysis Workflows","OAC","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Software Institutes","10/01/2016","02/13/2017","Perry de Valpine","CA","University of California-Berkeley","Standard Grant","Micah Beck","09/30/2020","$999,709.00","Duncan Temple Lang, Christopher Paciorek, Benjamin Shaby","pdevalpine@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","1253, 1269, 8004","4444, 7433, 8004, 8009, 8251","$0.00","The software developed in this project will enable scientists to learn more from complex data and to share new analysis methods more easily.  Increasingly, scientists in many fields aim to draw sound conclusions from large and complex data sets.  Such fields include environmental biology, political science, education research, atmospheric and oceanic science, climate science, and many others.  Data may be complex because many related variables are measured and/or because some measurements are not independent from each other.  Non-independence can arise when some variables are measured repeatedly through time; or when measurements are made at nearby locations; or when measurements are made on groups of related individuals; or for a combination of those and other similar reasons.  For such cases, general statistical methods have been developed to allow researchers to tailor their analysis to each data set in order to account for the relationships among the data.  Such methods rely on computer algorithms to explore the range of possible conclusions given the uncertainties inherent in limited data. Within those general methods there are many varieties of specific approaches that have been and continue to be developed.  Thus, a major software gap has emerged: Many new and evolving methods are not easily available for application by a wide range of scientists because there has not been a software framework that makes them easy to program and disseminate.  This project will support continued development of the NIMBLE software to help fill that gap.   As a result, scientists will be able to use computational analysis methods more flexibly, to combine and compare different algorithms more easily, to integrate such algorithms into other software workflows, and to gain better computational performance.  This will enable more advanced and more routine use of some modern computational methods for analyzing complex data.<br/><br/>The existing NIMBLE framework for hierarchical statistical models and algorithms comprises a model specification language, a language for programming model-generic algorithms within the R statistical environment, and a compiler that generates, compiles and interfaces to model- and algorithm-specific C++ for efficient execution.  These enable general implementation and dissemination of methods such as Markov chain Monte Carlo, sequential Monte Carlo, and many related methods. In this project NIMBLE will be extended and generalized to be more powerful and flexible, enabling use in a variety of software workflows.  Extensions to NIMBLE's core capabilities will include harnessing automatic differentiation and parallelization in generated C++, enhancements to its existing linear algebra capabilities, more efficient implementation of large statistical models including those with structural uncertainty such as latent group membership, and extensions to the statistical modeling language.  Enhancements to facilitate integration of NIMBLE-generated models and algorithms with other software will include generation of stand-alone executables, generation of clearly defined application-programmer interfaces such as for use by Python, features to call user-provided libraries from algorithm code, features to load and save data via standard formats such as JSON and NetCDF, and separation of NIMBLE components into distinct packages. The project will include substantial outreach, training, and user community development.  These activities will include development of uses cases in fields such as population and ecosystem ecology, oceanography, climate science, political science, and education. They will also include workshops, user meetings, key-user visits, and training material. This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Mathematical and Physical Sciences (Division of Mathematical Sciences)."
"1649703","EAGER:   Collaborative Research:   Supporting Public Access to Supplemental Scholarly Products Generated from Grant Funded Research","OAC","NSF Public Access Initiative","09/01/2016","09/08/2016","Kerstin Lehnert","NY","Columbia University","Standard Grant","Beth Plale","08/31/2019","$178,874.00","Vicki Ferrini","lehnert@ldeo.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7414","7916","$0.00","This EAGER project addresses the urgent need to better understand the research community's Data Management Plan (DMP) requirements and, based on this understanding, provides an open software tool that helps investigators generate structured and machine-readable Data Management Plans that fulfill both the researcher's need to easily deliver a standardized set of information to the funder, and the funder's need to analyze the information contained in DMPs. This allows funders to identify trends in data and software submission, repository use patterns, and carry out other analyses that can assist in understanding community use patterns and needs.  The Principal Investigators (PIs) will leverage an existing DMP Tool built for the geosciences community by initially assessing the DMPs not only of the geosciences, but also the biological and social, behavioral, and economic sciences and upgrading the DMP Tool to serve those communities as well.  Ultimately, the team will work to determine if this upgraded DMP Tool is extendable and scalable to all science, engineering, and educational research funded by the National Science Foundation.  If successful, this will ultimately enhance the reproducibility and reuse of scientific research and help improve public access to supplementary scholarly products from federally funded research.<br/><br/>The National Science Foundation has required Data Management Plans (DMPs) for all grant proposals submitted for review since 2011. The DMPs submitted thus far are mostly free text and do not follow any specified format or structure and because of this limitation, current DMPs are not easy to compare or analyze. Consistent and comprehensive structured and machine-readable DMPs may substantially advance understanding of the data management landscape and address gaps that will improve data access which will lead to enhanced re-use and more reproducible science.  The PIs propose to modify and upgrade the DMP Tool that has been developed and operated by IEDA (Interdisciplinary Earth Data Alliance) to serve the broadest research communities possible. This DMP Tool gathers relevant data management planning information from investigators in a structured manner into a relational database that can be mined and analyzed. As part of this project, they will analyze the information from the more than 1,350 DMPs that have already been generated with the IEDA DMP Tool to understand gaps, successes, and patterns of use. They will initially focus on the DMP requirements of science communities funded by the National Science Foundation's GEO, BIO, and SBE directorates, and use the results of this research to guide the development and prototyping of the extended version of the IEDA DMP Tool. They will subsequently focus on other directorates."
"1649545","EAGER:   Collaborative Research:   Supporting Public Access to Supplemental Scholarly Products Generated from Grant Funded Research","OAC","NSF Public Access Initiative","09/01/2016","09/08/2016","Helen Berman","NJ","Rutgers University New Brunswick","Standard Grant","Beth Plale","08/31/2019","$60,000.00","","berman@rcsb.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7414","7916","$0.00","This EAGER project addresses the urgent need to better understand the research community's Data Management Plan (DMP) requirements and, based on this understanding, provides an open software tool that helps investigators generate structured and machine-readable Data Management Plans that fulfill both the researcher's need to easily deliver a standardized set of information to the funder, and the funder's need to analyze the information contained in DMPs. This allows funders to identify trends in data and software submission, repository use patterns, and carry out other analyses that can assist in understanding community use patterns and needs.  The Principal Investigators (PIs) will leverage an existing DMP Tool built for the geosciences community by initially assessing the DMPs not only of the geosciences, but also the biological and social, behavioral, and economic sciences and upgrading the DMP Tool to serve those communities as well.  Ultimately, the team will work to determine if this upgraded DMP Tool is extendable and scalable to all science, engineering, and educational research funded by the National Science Foundation.  If successful, this will ultimately enhance the reproducibility and reuse of scientific research and help improve public access to supplementary scholarly products from federally funded research.<br/><br/>The National Science Foundation has required Data Management Plans (DMPs) for all grant proposals submitted for review since 2011. The DMPs submitted thus far are mostly free text and do not follow any specified format or structure and because of this limitation, current DMPs are not easy to compare or analyze. Consistent and comprehensive structured and machine-readable DMPs may substantially advance understanding of the data management landscape and address gaps that will improve data access which will lead to enhanced re-use and more reproducible science.  The PIs propose to modify and upgrade the DMP Tool that has been developed and operated by IEDA (Interdisciplinary Earth Data Alliance) to serve the broadest research communities possible. This DMP Tool gathers relevant data management planning information from investigators in a structured manner into a relational database that can be mined and analyzed. As part of this project, they will analyze the information from the more than 1,350 DMPs that have already been generated with the IEDA DMP Tool to understand gaps, successes, and patterns of use. They will initially focus on the DMP requirements of science communities funded by the National Science Foundation's GEO, BIO, and SBE directorates, and use the results of this research to guide the development and prototyping of the extended version of the IEDA DMP Tool. They will subsequently focus on other directorates."
"1659259","CC* Network Design: Network Infrastructure for Improved Student Engagement in Science Discovery and Innovation","OAC","Campus Cyberinfrastrc (CC-NIE)","08/01/2017","07/13/2017","Carol Smith","IN","DePauw University","Standard Grant","Kevin L. Thompson","07/31/2019","$210,000.00","Pascal Lafontant, Gloria Townsend, David Berque, Daniel Gurnon","clsmith@depauw.edu","313 South Locust Street","Greencastle","IN","461350037","7656584800","CSE","8080","","$0.00","DePauw University has a strong history of undergraduate participation in science research and education through its rich science curriculum, experiential programs in science and technology, high-impact teaching practices, and nationally recognized teaching and mentoring. DePauw graduates in science, technology, and mathematics are well-prepared to successfully enter graduate school and careers, many receiving prestigious postgraduate scholarships or fellowships.<br/><br/>This project improves DePauw's cyberinfrastructure and internet connectivity in three areas to reduce limitations that hinder effective data exchange and use of remote applications in the process of scientific discovery and education, by increasing the campus connection to national research networks through Indiana's regional network provider, ILight, from 1Gpbs to 10Gbps; creating internet redundancy via a secondary fiber optic link to I-Light; and implementing a re-architected network border prioritizing science data flows with high performance data transfer and PerfSonar nodes.<br/><br/>These improvements address immediate research needs for DePauw projects in computer science, biology, and chemistry that are improving understanding of tools for developing interactive documents that blend text and data-driven arguments, adding to general understanding of heart regeneration with the potential to influence treatments for heart-attacks, helping the scientific community better understand how cutting-edge DNA sequencing technology can help patients with rare genetic diseases, and supporting women computer science students to persist in computational fields by decreasing feelings of isolation through online interaction with mentors and role models. Further, these improve access to national research networks and resources that enables DePauw to expose a greater number of undergraduate students to participation in innovative science research and discovery."
"1835864","Collaborative Research: Framework: Software: NSCI: Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","EarthCube","10/01/2018","09/12/2018","Paul Constantine","CO","University of Colorado at Boulder","Standard Grant","Stefan Robila","09/30/2022","$350,000.00","","paul.constantine@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8074","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835818","Collaborative Research: Framework: Software: NSCI : Computational and data innovation implementing a national community hydrologic modeling framework for scientific discovery","OAC","Software Institutes","10/01/2018","09/12/2018","Jerad Bales","MA","Consortium of Universities for the Advancement of Hydrologic Sci","Standard Grant","Stefan Robila","09/30/2022","$196,235.00","","jdbales@cuahsi.org","150 Cambridge Park Drive","Cambridge","MA","021402479","3392215400","CSE","8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","This award supports the design and implementation of a software framework to simulate the movement of water at various scales. Understanding the movement and availability of water locally and across the country is of paramount importance to economic productivity and human health of our nation. Hydrologic scientists, are actively tackling these challenges using increasingly complex computational methods. However, modeling advances have not been easily translated to the broader community of scientists and professionals due to technical barriers to entry. This software platform draws from computer models and employs supercomputers capable of analyzing big data to provide unprecedented simulations of water movement over the continental US. Combining hydrologists and computer scientists the team behind the project envision a broad community of users who will have multiple ways to interact with the software framework. For the hydrologic scientist who is interested in generating their own scenarios the framework will facilitate direct interaction with the hydrologic models and the ability to generate simulations on the fly. Conversely, the framework will also provide a set of static output and a range of tools for a broader set of users who would like to evaluate hydrologic projections locally or extract model data for use in other analyses.<br/><br/>Continental scale simulation of water flow through rivers, streams and groundwater is an identified grand challenge in hydrology. Decades of model development, combined with advances in solver technology and software engineering have enabled large-scale, high-resolution simulations of the hydrologic cycle over the US, yet substantial technical and communication challenges remain. With support from this award, an interdisciplinary team of computer scientists and hydrologists is developing a framework to leverage advances in computer science transforming simulation and data-driven discovery in the Hydrologic Sciences and beyond. This project is advancing the science behind these national scale hydrologic models, accelerating their capabilities and building novel interfaces for user interaction. The framework brings computational and domain science (hydrology) communities together to move more quickly from tools (models, big data, high-performance computing) to discoveries. It facilitates decadal, national scale simulations, which are an unprecedented resource for both the hydrologic community and the much broader community of people working in water dependent systems (e.g., biological system, energy and food production). These simulations will enable the community to address scientific questions about water availability and dynamics from the watershed to the national scale. Additionally, this framework is designed to facilitate multiple modes of interaction and engage a broad spectrum of users outside the hydrologic community. We will provide easy-to-access pre-processed datasets that can be visualized and plotted using built-in tools that will require no computer science or hydrology background. Recognizing that most hydrology training does not generally include High Performance Computing and data analytics or software engineering, this framework will provide a gateway for computationally enhanced hydrologic discovery. Additionally, for educators we will develop packaged videos and educational modules on different hydrologic systems geared towards K-12 classrooms.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839900","CICI: SSC: Integrity Introspection for Scientific Workflows (IRIS)","OAC","Cyber Secur - Cyberinfrastruc","09/01/2018","08/22/2018","Anirban Mandal","NC","University of North Carolina at Chapel Hill","Standard Grant","Micah Beck","08/31/2021","$999,575.00","Ewa Deelman, Von Welch, Cong Wang","anirban@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8027","","$0.00","Scientists use computer systems to analyze and store their scientific data, sometimes in a complex process across multiple machines in different geographical locations. It has been observed that sometimes during this complex process, scientific data is unintentionally modified or accidentally tampered with, with errors going undetected and corrupt data becoming part of the scientific record. The IRIS project tackles the problem of detecting and diagnosing these unintentional data errors that might occur during the scientific processing workflow. The approach is to collect data relevant to the correctness and integrity of the scientific data from various parts of the computing and network system involved in the processing, and to analyze the collected data using machine learning techniques to uncover errors in the scientific data processing. The solutions are integrated into Pegasus, a popular ""workflow management system"" - a software used to describe the complex process in a user-friendly way and that handles the details of processing for the scientists. The research methods will be validated on national computing resources with exemplar scientific applications from gravitational-wave physics, earthquake science, and bioinformatics. These solutions will allow scientists, and our society, to be more confident of scientific findings based on collected data.<br/><br/>Data-driven science workflows often suffer from unintentional data integrity errors when executing on distributed national cyberinfrastructure (CI). However, today, there is a lack of tools that can collect and analyze integrity-relevant data from workflows and thus, many of these errors go undetected jeopardizing the validity of scientific results. The goal of the IRIS project is to automatically detect, diagnose, and pinpoint the source of unintentional integrity anomalies in scientific workflows executing on distributed CI. The approach is to develop an appropriate threat model and incorporate it in an integrity analysis framework that collects workflow and infrastructure data and uses machine learning (ML) algorithms to perform the needed analysis. The framework is powered by novel ML-based methods developed through experimentation in a controlled testbed and validated in and made broadly available on NSF production CI. The solutions will be integrated into the Pegasus workflow management system, which is used by a wide variety of scientific domains. An important part of the project is the engagement with science application partners in gravitational-wave physics, earthquake science, and bioinformatics to deploy the analysis framework for their workflows, and to iteratively fine tune the threat models, ML model training, and ML model validation in a feedback loop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838990","Leveraging Scientific Societies for Open and FAIR Scientific Data","OAC","NSF Public Access Initiative","11/01/2018","07/29/2018","Shelley Stall","DC","American Geophysical Union","Standard Grant","Beth Plale","10/31/2019","$50,000.00","","sstall@agu.org","2000 FLORIDA AVE NW","Washington","DC","200099123","2024626900","CSE","7414","7556","$0.00","Scientific societies are a critical stakeholder and leader in bringing about cultural change toward open data across the sciences. The American Geophysical Union (AGU) has played a key role in being a change agent in open science specifically by leading the Earth and space science communities in enabling research data to be findable, accessible, interoperable, and reusable (FAIR). The AGU proposes to host a workshop and one-on-one follow-up conversations with targeted societies within the Earth and space sciences and in adjacent disciplines. The goal of the proposal is to engage and empower more scientific societies on their way to adoption of open science principles through education, tools, materials, and consultation. The intellectual merit is in evaluation of the effectiveness of the open and FAIR data tool suite for societies and their members and in moving societies forward in open science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1443062","Beyond Data Discovery: Shared Services for Community Metadata Improvement","OAC","DATANET, EarthCube","05/01/2015","04/27/2015","Ray Habermann","IL","The HDF Group","Standard Grant","Amy Walton","04/30/2019","$1,498,604.00","Matthew Jones","thabermann@hdfgroup.org","410 E University Ave","Champaign","IL","618203871","2175316100","CSE","7726, 8074","7433, 8048","$0.00","Science data and results must be well documented in order to be reproducible and re-usable.  Metadata -- ancillary contextual information such as science objectives, data provenance, and uncertainty estimates at each step -- is a fundamental part of the research documentation, reuse, and collaboration process.<br/><br/>This project develops flexible tools for evaluating metadata, using consistent measurement systems that encourage community engagement, integrate guidance for improvement, and are a critical element in cross-community metadata improvement efforts.  Provision of these new metadata and data evaluation services across communities will improve the ability to integrate and reuse trustworthy data for crosscutting synthesis and analysis across science communities.  The focus on use metadata rather than discovery metadata is a significant shift in focus.  Use metadata is a fundamental building block needed to allow effective scientific analysis workflows.  The team builds a significant collaboration with several interdisciplinary partner organizations that provide guidance to this project."
"1839021","EAGER: Environmental drivers of biodiversity: leveraging a history of NSF-funded research to test models of butterfly responses to global change","OAC","NSF Public Access Initiative","10/01/2018","07/31/2018","Leslie Ries","DC","Georgetown University","Standard Grant","Beth Plale","09/30/2020","$299,989.00","","Leslie.Ries@georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","7414","7916","$0.00","Our most pressing ecological priority is to determine how human activity is driving global shifts in biodiversity and how we can balance preserving ecosystem function with the needs of a growing human population. The PI proposes new ecological research on the Pierid family of butterflies that have been the subject of research on thermal responses over the last 40 years.  The overarching goal of this research is to extend and generalize Species Distribution Models (SDM), the dominant modeling approach to understanding large-scale shifts of biodiversity in the face of global change.   The PI will leverage that 40 year legacy of thermal response research to develop general models of how butterflies respond to changing environments.  In order to verify how well these models perform and their ability to make both species-specific and general predictions, models will be validated with large-scale monitoring data sets.  There is a growing resource of citizen-science monitoring data but there are also several NSF-funded academic monitoring programs that have occurred over the years.  Another aspect of this project is to bring together those academic and citizen science data into a unified publicly-available data set.  The research is expected to advance macrosystems ecology study of thermal ecology and responses of biodiversity, especially ectotherms while also serving as a strong demonstration of data reuse by focusing on a well-studied and wide-spread group of butterfly species, the Pierids.<br/> <br/><br/>Research in macrosystems ecology requires two disparate ecological data types that are rarely generated or employed by the same research community: mechanistic, experimental data and data from large spatiotemporally-replicated monitoring programs. The two dominant environmental factors driving the distribution and abundance of butterflies are thermal environment (impacted by climate) and host-plant availability (impacted by land use change and climate). There are 14 previous and current NSF-funded projects focused on primarily thermal, but also nutritional drivers of performance in one family of butterflies, Pierids. Thermal environments have several impacts on butterfly performance. Indirectly, temperature drives the seasonal timing and distribution of host-plant (food) resources. It also provides energy for growth for developing caterpillars. Host-plants are also a key component of butterfly development, not just for the obvious reason that they acquire all their nutrition for growth from these plants, but the quality of the plants also determines their growth rate. There has been a long legacy of research on the thermal and nutritional constraints of growth for many insects, but these models have not yet been fully employed to make range-wide predictions for butterflies. We propose to bring together a legacy of NSF-funded research on butterfly responses to environmental change with long-term monitoring data that can be used, respectively, to generate and test mechanistic SDMs over multiple spatiotemporal scales.<br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827184","CC* Networking Infrastructure: Integrating Big Data Instrumentation into Campus Cyberinfrastructure","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/29/2018","Xiaohui Carol Song","IN","Purdue University","Standard Grant","Kevin L. Thompson","06/30/2020","$323,327.00","Wen Jiang, Robin Tanamachi, Preston Smith","carolxsong@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8080","","$0.00","Rapid advancements in data-intensive scientific instrumentation have greatly outpaced the capability of campus networking infrastructures to effectively connect big data-producing facilities to powerful computing and storage systems. This delays the analysis, use and dissemination of a huge amount of valuable scientific data for discovery and innovation in multiple science and engineering domains. Purdue's project bridges this gap by improving the campus high-speed network infrastructure between five big data instrument facilities and the centrally supported cyberinfrastructure (CI) consisting of supercomputers, large storage systems and network connections to outside the campus. The facilities support accelerated research in new materials, understanding brain functions and viruses, monitoring lower atmospheric weather, employing geospatial data in teaching and public engagement, and secure computing. The project builds upon a strong partnership between campus information technology experts and domain scientists. It also helps prepare the next generation of CI professionals by pairing student workers with the campus CI experts.<br/><br/>The project adds high-speed Science DMZ connections to the five selected big data facilities enabling high-volume, high-velocity, or interactive science data flows to both the campus research cyberinfrastructure and off campus facilities. Existing Cisco Nexus 7710 network switches at the distribution layer are augmented with new 40-Gigabit networking modules, increasing bandwidth to 20-80 Gigabits per second. This should yield a significant increase in research productivity, and more timely publication and dissemination of research data and results, through faster data transfer, easier access to the central computing infrastructure, and more effectively utilizing Purdue's 100-Gigabit WAN connections and research network peerings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833846","EAGER: The Xpert Network: Synergizing National Expert-Assistance and Tool-Support Teams for Computational and Data-Intensive Science","OAC","Software Institutes","10/01/2018","08/15/2018","Rudolf Eigenmann","DE","University of Delaware","Standard Grant","Vipin Chaudhary","09/30/2020","$298,521.00","","eigenman@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","8004","026Z, 7916, 8004, 9150","$0.00","Today's science frontiers are being advanced, to an increasing degree, by researchers who use large amounts of computational power to simulate models of our world's processes and to analyze large volumes of observed and collected data. Doing so requires expertise not only in the involved domain sciences, such as physics, chemistry, and biology, but also in the computational tools that implement the needed simulation and data analysis methods.  Many research projects have recognized the importance of pairing domain scientists with computational experts.  While the domain scientists pursue their individual research objectives, there is much commonality is the supporting software and hardware methods as well as in the needed cyberinfrastructure tools and resources. This project aims to increase the productivity of the many computational support teams - referred to as Xperts - through (i) the exchange of best practices and discussion of open problems, and (ii) the provision of advanced programming support environments.<br/><br/>To achieve the first objective - creating synergy among Xpert teams - this award will engage computational experts from some of the large, national cyberinfrastructure projects, including XSEDE and the NSF Software Institutes, as well as from university campus efforts that have recognized the value of computational research support teams. Events for exchanging best practices and discussing open issues will be organized at special workshops, through online meetings, and at established annual symposia, such as the Supercomputing and PEARC conferences. The same gatherings will be used to bring together users and developers of programming environments, pursuing the second objective. Among these groups are those who develop automatic, and semi-automatic program optimization instruments, performance analysis environments, and data management tools. This effort will not only expose Xpert teams to today's advanced programming support environments, but also gather important feedback from key user groups - those who push today's science frontiers - to help advance the programming environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841531","Virtual Data Set Services Enabling New Science at NSF Facilities","OAC","CESER-Cyberinfrastructure for","10/01/2018","09/15/2018","Ian Foster","IL","University of Chicago","Standard Grant","William Miller","09/30/2020","$1,500,000.00","Steven Tuecke","foster@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7684","020Z, 062Z","$0.00","Scientific facilities supported by the National Science Foundation such as the Daniel K. Inouye Solar Telescope (DKIST), National Center for Atmospheric Research (NCAR), and National Ecological Observatory Network (NEON) collect enormous quantities of valuable data about the world in which we live. These data can be used for scientific and societal benefit: to make breakthrough discoveries about sun's behavior and magnetic field, and our changing environment; to improve the speed and accuracy of forecasting of severe storms and destructive wildfires; to predict disruptions to electrical systems from solar flares; and many other purposes. Before such vital scientific data can be used effectively, they must be delivered rapidly, efficiently, and reliably to the people who need them. Researchers need interactive community access to hard-to-obtain data located in large data archives. Because the NCAR, DKIST, and NEON data archives cannot feasibly provide the computing resources needed for all analyses, end-user scientists need to be able to define, navigate, download, and analyze data subsets. Current web-based tools are not up to these tasks. The Virtual Data Set Services Enabling New Science at NSF Facilities project will tackle this challenge by developing new methods for organizing, packaging, and rapidly transporting data. <br/><br/>A key innovation will be the development of methods for defining, sharing, and manipulating ""virtual data sets,"" data collections extracted ""on the fly"" from the vast holdings of scientific facilities for a specific purpose. A researcher may define a virtual data set much as a shopper assembles products in an online ""shopping cart."" Once defined, a virtual data set can then be transferred to a remote computer for analysis, shared with colleagues, or extended for future projects. We will develop new services to (a) enable definition of, navigation over, and selective access to virtual data sets from petascale data archives of the scientific facilities, and (b) ensure reliable, automated, efficient, and secure replication and access of entire data sets or data subsets between a petascale data archive and other locations, to include both end user computers and remote mirrors intended to accelerate data access by community members. These new services will be constructed on top of the Globus platform, already heavily used within NCAR's Research Data Archive (RDA) and many other research data centers. The ultimate aim is to integrate the new services into operational systems in collaboration with DKIST, NEON, and NCAR/RDA. The results will be evaluated in the context of demanding science applications in partnership with solar physics, atmospheric science, and ecology researchers.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826574","EAGER: Empirical Software Engineering for  Computational  Science","OAC","SOFTWARE & HARDWARE FOUNDATION, Software Institutes","05/01/2018","04/16/2018","Timothy Menzies","NC","North Carolina State University","Standard Grant","Stefan Robila","07/31/2019","$124,628.00","","tim.menzies@gmail.com","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7798, 8004","026Z, 7916, 7944, 8004","$0.00","Science has become increasingly reliant on Computational Science methods implemented in software. These methods are complex, and therefore the software that implements them is prone to errors. This projects seeks to transformatively improve the state of the practice in the development of Computational Science software by applying systematic, data-driven methods (known as empirical methods) to evaluate how software is being developed and to suggest improvements. Improving the software engineering methods of Computational Science would result in higher quality software, and consequently increase our confidence in the research in scientific phenomena conducted by Computational Scientists, <br/><br/>Much of the work in Computational Science is related to the software that implements it. In this project, the researcher will apply state of the art empirical software engineering methods to Computational Science software. Qualitative methods will be applied to conduct large scale surveys of computational science. Quantitative data mining tools (classifiers, intelligent data preprocessor, automatic hyperparameter optimizers) will be used to can learn predictive models of time series of SE data such as ""Where in this system should we look for current bugs?"" and ""How many bugs are left on the system?"". These models can be used to guide developer effort in building new code or maintaining old code.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835256","Element: Software: Data-Driven Auto-Adaptive Classification of Cryospheric Signatures as Informants for Ice-Dynamic Models","OAC","POLAR CYBERINFRASTRUCTURE, CESER-Cyberinfrastructure for, EarthCube","01/01/2019","09/06/2018","Ute Herzfeld","CO","University of Colorado at Boulder","Standard Grant","Stefan Robila","12/31/2021","$593,624.00","","uch5678@gmail.com","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","5407, 7684, 8074","026Z, 062Z, 072Z, 077Z, 1079, 7923, 8004","$0.00","The objective of this project is to develop and automatize a connection between Earth observation data and numerical models of Earth system processes. Both collection of Earth observation data from satellites and modeling of physical processes have seen unprecedented advances in recent years. However, data-derived information is not used to inform modeling in a systematic and automated fashion. This creates a bottleneck that is growing with the data revolution. The award supports the development of a software cyberinfrastructure aimed at reducing this bottleneck by automating classification and parameterization. The proposed cyberinfrastructure will be implemented in a general and transportable way, but its functionality will be demonstrated by addressing a concrete open problem in glaciology: the acceleration during a glacier surge, which is characterized by an increase to 100-200 times the flow normal velocity. Glacial accelerations are important, because they constitute the largest uncertainty in sea-level-rise assessment. The team, from University of Colorado will combine their expertise of field work and data collection with their background in software development to generate a high-quality application that will be made available under an open source license to the broader scientific community. The project will engage graduate and undergraduate students in the software development, thus contributing to the development of future generations of scientists and cyberinfrastructure professionals.  The results will also be used to inform activities in K-12 schools and other outreach efforts.<br/><br/>The proposed data-driven auto-adaptive classification system is expected to provide a tool to the Earth Sciences community that allows it to employ the unprecedented detail in satellite image and SAR data (WordView, Sentinel-1 and Sentinel-2) necessary to extract information on surface properties and processes that were previously indiscernible. In that the classification will automatically adapt to changing conditions in time and space, it will provide a consistent parameterization of spatial processes that can be used to drive numerical simulations of Earth system processes. Thus, a direct connection will be established between data analysis and modeling. In a pilot study, the data-modeling connection will be demonstrated through classification of crevasse patterns, which result from deformation, and optimization of the basal sliding parameter in a three-dimensional model of a glacier surge. Hence the pilot study will advance understanding of ice dynamics. A second application is a sea-ice classification system, aimed to aid in mapping and understanding the changing Arctic sea-ice cover. The planned automated connection between data-driven automated classification and optimization of model parameters is expected to lay the foundation for a transformation of the data-modeling world in Earth sciences, atmospheric and polar sciences. The project will also advance machine learning and spatial statistics through realization of a science-driven approach to computer science and cyberinfrastructure.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure (OAC) is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, The OAC Cyberinfrastructure for Emerging Science and Engineering Research (CESER) program and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the OAC.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740315","SI2-SSE: MetPy - A Python GEMPAK Replacement for Meteorological Data Analysis","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","09/01/2017","08/29/2017","Ryan May","CO","University Corporation For Atmospheric Res","Standard Grant","Stefan Robila","08/31/2020","$499,693.00","Kevin Goebbert, John Leeman","rmay@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","1525, 8004, 8074","4444, 7433, 8004, 8005","$0.00","The MetPy project aims to make atmospheric science research and teaching easier and more reproducible by providing a set of well-tested and modern software tools. Meteorologists require many specialized calculations and maps in order to understand the weather and make reliable predictions. The tools they use must provide correct results, since lives and property depend on accurate forecasts and research. This project will port the bulk of the functionality from a widely used and trusted -- but aging and minimally supported -- software program called GEMPAK (the GEneral Meteorological PAcKage) into MetPy, developed using the Python programming language, and with a well-designed, new software architecture. Python has been selected as the language of choice because it has become very popular in many scientific communities. MetPy will be the meteorological community's entry into this growing scientific software ecosystem. In addition to making GEMPAK's functionality available in MetPy, this project will implement a better user-interface, which will help students and researchers get started more easily. The software team will use software development best practices in its development of MetPy, and ensure that it can work with all common meteorological data sources. Every relevant aspect of MetPy will be documented in an easy to digest way on the MetPy project webpage. The development team will work with university instructors to help revise their course materials to integrate MetPy. In addition, the team will teach MetPy and Python training workshops each year, allowing university professors, students, and professionals to get hands-on training on how to do their research in a faster and more robust way. <br/><br/>This project seeks to fill a need within the atmospheric science community by bringing key functional elements of a foundational software program, GEMPAK, to the innovation-rich Python ecosystem. By devoting software development resources to increasing the number of data types and file formats MetPy can work with, improving the underlying data model, and reaching feature parity with GEMPAK, MetPy can be positioned as a community-supported replacement for the older package. This effort leverages the entire Python ecosystem, and supports the movement (already well under way) of the atmospheric science community to Python-driven reproducible workflows. This transition will provide a number of community benefits. By bringing needed functionality from GEMPAK to the Python ecosystem, this project will allow atmospheric scientists to: simplify the process of exploratory analysis, have a cross-platform toolchain that can be carried from the classroom to the workforce, simplify the research workflow to make science easier and more reproducible, provide a tested library of domain-specific calculations with literature references, and create publication-quality data visualizations. Educators and researchers will be able to replace their use of legacy software, which is no longer being developed and is increasingly hard to maintain, with a modern toolkit that allows increased flexibility and reproducibility within atmospheric science research. Sustainability of the atmospheric science software workflow will be enhanced by the inclusion of modern automated software build-and-test tools, robust community-supported documentation and learning materials, and the ability to quickly incorporate new sources of environmental data. Finally, modernizing the atmospheric science toolchain opens the door to the use of innovations like web-based tools (Jupyter notebooks, for example) that would be difficult or impossible to take advantage of when using legacy software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1739025","CICI: CE: Improving the Security of a Science DMZ","OAC","Cyber Secur - Cyberinfrastruc","10/01/2017","08/04/2018","Matt Bishop","CA","University of California-Davis","Standard Grant","Micah Beck","09/30/2019","$754,094.00","Dipak Ghosal, Viji Murali","mabishop@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8027","9251","$0.00","The term ""big science"" refers to science that involves massive amounts of data. Moving this data through ordinary networks is very slow, in part because the amount of data is more than ordinary networks are designed to handle, and in part because of the checking that network security mechanisms perform. The ""science DMZ"" (DeMilitarized Zone) is a special network designed to move massive amounts of data very quickly. The need to do so results in compromises affecting security; checking is done on entry, and only for the first part of the connection. If the connection is suspicious, it is blocked; otherwise, it is allowed through. This project extends the checking by sampling the data in the connection throughout the lifetime of the connection. Then, if something suspicious is detected in the samples, appropriate action can be taken. A second goal of this project is to determine what procedural or technical actions are most effective when malicious flows are detected. The final goal is to determine how to speed up the security analysis so the impact on the throughput is both minimal and acceptable. The overall goal is to secure the science DMZ network without sacrificing speed.<br/><br/>More specifically, on a science DMZ, large amounts of data are moved, making it very difficult to secure, so data is sampled only when a connection starts and is analyzed without blocking the connection. If the analysis shows it is malicious, a filter rule is added to the router to drop packets in that connection. This project samples not just at the beginning, but at various times while the connection is active. If something malicious starts during the connection, it can be detected. What to do when suspicious flows are identified is less clear; the project will examine both technical and procedural methods, for example slowing down the flow to where it can be analyzed thoroughly, and then if indeed malicious, provide information to the Chief Information Security Officer?s (CISO) office to enable them to take action. If the flow is not malicious (i.e., a false positive, confirmed by the more detailed analysis), the rate reduction will cease. A key question is how to reduce the time needed for the sampling analysis. Part of this project is to examine how to speed up intrusion detection systems by using GPUs in order to do a quicker analysis."
"1827116","CC* Integration: Service Analysis and Network Diagnosis (SAND)","OAC","Campus Cyberinfrastrc (CC-NIE)","08/15/2018","08/13/2018","Brian Bockelman","NE","University of Nebraska-Lincoln","Standard Grant","Kevin Thompson","07/31/2020","$499,673.00","Robert Gardner, Shawn McKee","bbockelman@morgridge.org","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","CSE","8080","9150","$0.00","Increasingly, science has become a ""team sport"" with projects less likely to be conducted by a single investigator or university group but with multi-institution collaborations spanning many universities and national laboratories.  As these collaborations have become data-intensive, highly-performant network links connecting their distributed science platforms have become increasingly important.  While true across all science, some collaborations, like those at the Large Hadron Collider (LHC) at CERN, transfer millions of gigabytes across global networks today and anticipate orders of magnitude increases within the next decade. Without performant networks and efficient data transfer and access services, the time to science will be greatly compromised.  In this context, high-speed, long-distance networks see their performance degrade surprisingly quickly in the face of modest error rates.  Accordingly, network engineers and researchers use sophisticated tools to monitor the network and transfer services.  Without a means to aggregate and correlate network tests, performance measurements, and application response, they at best can only reveal a small piece of the overall problem.  This project focuses on techniques that better combine, visualize, and analyze disparate network monitoring and service logging data, providing a comprehensive picture critical to the engineers and scientists relying on the network.  This will allow problems to be located and fixed more quickly, reducing the time to science. <br/><br/>The ""CC* Integration: Service Analysis and Network Diagnosis (SAND)"" project brings together an experienced team that has been working for more than a decade on wide area data transfers for large scale science.  The project develops a network monitoring archive and analytics platform, SAND-NMA, which integrates widely used data analytics tools (such as ElasticSearch, Kibana and JupyterLab) with infrastructure components (perfSONAR, HTCondor) and application sensors. Data from disparate sources are published to a messaging bus providing low-latency metrics describing the performance of research platforms on a global scale. Exploratory work is performed to provide engineers with pragmatic tools to identify and locate problems and perform analytics to understand the long-term evolution of network and higher level service performance.  Programming interfaces allow external cyberinfrastructure (such as workflow management systems) to incorporate the network monitoring feeds into their decision making engines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1724821","CIF21 DIBBs: EI: SLATE and the Mobility of Capability","OAC","COMPUTATIONAL PHYSICS, DATANET","07/01/2017","07/25/2018","Robert Gardner","IL","University of Chicago","Continuing grant","Amy Walton","06/30/2021","$3,498,494.00","Shawn McKee, Joseph Breen III","rwg@hep.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7244, 7726","7433, 7569, 8048, 8084","$0.00","Much of science today is propelled by multi-institutional research collaborations that require computing environments that connect instrumentation, data, and computational resources. These resources are distributed among university research computing centers, national-scale high performance computing facilities, and commercial cloud service providers. The scale of the data and complexity of the science drive this diversity, and the need to aggregate resources from many sources into scalable computing systems. The heterogeneity of resources causes scientists to spend more time on the technical aspects of computation and data management than on discoveries and knowledge creation, while computing support staff are required to invest more effort integrating domain specific software stacks with limited applicability beyond the community served. Services Layer At The Edge (SLATE) provides technology that simplifies connecting university and laboratory data center capabilities to the national cyberinfrastructure ecosystem and thus expands the reach of domain-specific science gateways and multi-site research platforms.<br/><br/>SLATE implements 'cyberinfrastructure as code' by augmenting the canonical Science DMZ pattern with a generic, programmable, secure and trusted underlayment platform. This platform hosts advanced container-centric services needed for higher-level capabilities such as data transfer nodes, software and data caches, workflow services and science gateway components. SLATE uses best-of-breed data center virtualization components, and where available, software defined networking, to enable distributed automation of deployment and service lifecycle management tasks by domain experts. As such it simplifies creation of scalable platforms that connect research teams, institutions and resources to accelerate science while reducing operational costs and development cycle times. Since SLATE needs only commodity components for its functional layers, it is used in building distributed systems across all data center types and scales thus enabling creation of ubiquitous, science-driven cyberinfrastructure.<br/> <br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Computational Physics within the NSF Directorate for Mathematical and Physical Sciences."
"1829585","Collaborative Research: CYBER Training:  CIU: Data Streams, Model Workflows, and Educational Pipelines for Hydrologic Sciences","OAC","CyberTraining - Training-based, SPECIAL INITIATIVES","09/01/2018","07/09/2018","Christina Bandaragoda","WA","University of Washington","Standard Grant","Sushil Prasad","08/31/2021","$446,392.00","Anthony Arendt, Bart Nijssen, Erkan Istanbulluoglu","cband@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","044Y, 1642","026Z, 062Z, 7361, 9102, 9179","$0.00","Studies of water and environmental systems are becoming increasingly complex and require integration of knowledge across multiple domains. At the same time, technological advances have enabled the collection of massive quantities of data for studying earth system changes. Fully leveraging these datasets and software tools requires fundamentally new approaches in the way researchers store, access and process data. The project serves the national interest by motivating a culture shift within the hydrologic and more broadly earth science communities toward open and reproducible software practices that will enhance interdisciplinary collaboration and increase capacity for addressing complex science challenges around the availability, risks and use of water. Project's CyberTraining approach provides virtual learning experiences throughout an academic year, with online learning modules oriented around a one-week in-person workshop (WaterHackWeek) that will focus on hands-on real-world research projects. These research projects are designed to serve the national interest by preparing for natural hazards such as floods, hurricanes and climate change, and to advance the nation's health by making tools and data accessible to health researchers, local governments, and citizens.<br/><br/>New cyberinfrastructure that emphasizes data sharing and open, reproducible software practices is currently in development, but requires a mode of knowledge transfer, or CyberTraining, that extends beyond currently available university curriculum.  Project's aim is to ensure successful use of community cyberinfrastructure to 1) publish large datasets, 2) run numerical models, 3) organize collaborative research projects, and 4) meet journal requirements to follow open data standards. The activities take advantage of HydroShare, a National Science Foundation funded cyberinfrastructure platform, operated by the Consortium of Universities Allied for Hydrologic Sciences (CUAHSI), for sharing hydrologic data and models.  The short-term goals are to develop new CyberTraining modules; the long-term goals are to have an annually recurring WaterHackWeek, to distribute curriculum CUAHSI to more than 130 member universities, and advance cyberinfrastructure education for the broader geoscience community. The use of the hackweek educational model extends the use of cyberinfrastructure to promote the progress of science by including a specific emphasis on graduate student training as instructors, training coordinators, and building research networks with data providers who are stakeholders outside of academia. For example, case studies include data and resource management by Native American tribal governments, Hurricane Maria data archive for research in Puerto Rico, improving flood forecasting, and tool-building using complex numerical models such as the National Water Model.  This project allows to test the educational model in the water research community, in addition to connecting team's research and curriculum to annually recurring hackweeks in neuro, astro, ocean, and geo sciences. The team of researchers is actively engaged in experimenting with this new model, and in testing its efficacy through robust evaluation metrics. The proposed activities encourage collaboration and support for use of cyberinfrastructure at all stages of the educational pipeline and provides participants with opportunities for networking, career development, community building and design of open-source software tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829744","Collaborative Research: CYBER Training: CIU: Data Streams, Model Workflows, and Educational Pipelines for Hydrologic Sciences","OAC","CyberTraining - Training-based","09/01/2018","07/09/2018","Anthony Castronova","MA","Consortium of Universities for the Advancement of Hydrologic Sci","Standard Grant","Sushil Prasad","08/31/2021","$53,540.00","","acastronova@cuahsi.org","150 Cambridge Park Drive","Cambridge","MA","021402479","3392215400","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","Studies of water and environmental systems are becoming increasingly complex and require integration of knowledge across multiple domains. At the same time, technological advances have enabled the collection of massive quantities of data for studying earth system changes. Fully leveraging these datasets and software tools requires fundamentally new approaches in the way researchers store, access and process data. The project serves the national interest by motivating a culture shift within the hydrologic and more broadly earth science communities toward open and reproducible software practices that will enhance interdisciplinary collaboration and increase capacity for addressing complex science challenges around the availability, risks and use of water. Project's CyberTraining approach provides virtual learning experiences throughout an academic year, with online learning modules oriented around a one-week in-person workshop (WaterHackWeek) that will focus on hands-on real-world research projects. These research projects are designed to serve the national interest by preparing for natural hazards such as floods, hurricanes and climate change, and to advance the nation's health by making tools and data accessible to health researchers, local governments, and citizens.<br/><br/>New cyberinfrastructure that emphasizes data sharing and open, reproducible software practices is currently in development, but requires a mode of knowledge transfer, or CyberTraining, that extends beyond currently available university curriculum.  Project's aim is to ensure successful use of community cyberinfrastructure to 1) publish large datasets, 2) run numerical models, 3) organize collaborative research projects, and 4) meet journal requirements to follow open data standards. The activities take advantage of HydroShare, a National Science Foundation funded cyberinfrastructure platform, operated by the Consortium of Universities Allied for Hydrologic Sciences (CUAHSI), for sharing hydrologic data and models.  The short-term goals are to develop new CyberTraining modules; the long-term goals are to have an annually recurring WaterHackWeek, to distribute curriculum CUAHSI to more than 130 member universities, and advance cyberinfrastructure education for the broader geoscience community. The use of the hackweek educational model extends the use of cyberinfrastructure to promote the progress of science by including a specific emphasis on graduate student training as instructors, training coordinators, and building research networks with data providers who are stakeholders outside of academia. For example, case studies include data and resource management by Native American tribal governments, Hurricane Maria data archive for research in Puerto Rico, improving flood forecasting, and tool-building using complex numerical models such as the National Water Model.  This project allows to test the educational model in the water research community, in addition to connecting team's research and curriculum to annually recurring hackweeks in neuro, astro, ocean, and geo sciences. The team of researchers is actively engaged in experimenting with this new model, and in testing its efficacy through robust evaluation metrics. The proposed activities encourage collaboration and support for use of cyberinfrastructure at all stages of the educational pipeline and provides participants with opportunities for networking, career development, community building and design of open-source software tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1261582","CIF21 DIBBs: Brown Dog","OAC","DATANET","10/01/2013","04/14/2017","Kenton McHenry","IL","University of Illinois at Urbana-Champaign","Cooperative Agreement","Amy Walton","09/30/2019","$10,519,716.00","Jong Lee, Praveen Kumar, Michael Dietze, Barbara Minsker","kmchenry@ncsa.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7726","7433, 8048","$0.00","The information age has made it trivial for anyone to create and then share vast amounts of digital data.  This includes unstructured collections made of data such as images, video, and audio to collections of born digital content made up of data such as documents and spreadsheets.  While the creation and sharing of content has been made easy, its inverse, the ability to search and use the contents of digital data, has been made exponentially more difficult.  In the physical analogue librarians have used the process of curation to standardize the format by which information is stored and diligently index holdings with metadata to allow both current and future generations to find information.  Digitally this does not happen as that curation overhead is an unwelcomed bottleneck to the creation of more data.  Though popular services such as modern search engines give the illusion that this is being done, this is largely over the portion of digital data that is text based and/or containing text metadata.  Unstructured collections and contents trapped behind difficult to read file formats, however, make up a significant part of our collective digital data assets and are largely not accessible. <br/>Science today not only uses but relies on software and digital content.  It is well known that science is not only responsible for a significant amount of our digital data holdings but that also much of this is un-curated data, what the scientific community currently refer to as ""long-tail"" data.  As such contemporary science, which relies on digital data and software, software which evolves and disappears quickly as underlying technology changes, is entering a realm where scientific results are no-longer easily reproducible and as such in essence no longer a science as science hinges on the fact that a documented procedure will result in the same result each time."
"1835822","Framework: Data: HDR: Extensible Geospatial Data Framework towards FAIR (Findable, Accessible, Interoperable, Reusable) Science","OAC","DATANET","10/01/2018","08/09/2018","Xiaohui Carol Song","IN","Purdue University","Standard Grant","Amy Walton","09/30/2023","$4,571,811.00","Jian Jin, Uris Lantz Baldos, Venkatesh Merwade, Jack Smith","carolxsong@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7726","062Z, 077Z, 7925","$0.00","This project provides seamless connections among platforms, data and tools, making large scientific and social geospatial datasets directly usable in scientific models and tools.  Users with little or no programming experience will be able to create and build data pipelines that collect and process data at multiple scales and convert such data into usable results.  Four case studies demonstrate the capability of the data framework: flood hazard prediction, plant phenotyping, water quality monitoring and sustainable development.<br/><br/>The project creates an extensible geospatial data framework (GeoEDF) to address prevalent geospatial data challenges, support domain science needs, and contribute to a national geospatial data and software ecosystem. Specific objectives include: <br/> - development of a plug-and-play data framework (GeoEDF),<br/> - use of the framework to address domain science needs, <br/> - development of interoperability with other cyberinfrastructures, and <br/> - dissemination of GeoEDF to the broader community. <br/>Use of modular plug-and-play application programming interfaces (APIs) would enable integration of domain-specific geospatial data in a meaningful and accessible manner, leveraging an existing cyberinfrastructure capability (HUBzero) to facilitate adoption and dissemination without reinventing existing components. The project engages a substantial number of domain scientists from a variety of stakeholder communities; by incorporating the framework into HUBzero-powered sites, the team anticipates having access to more than 750,000 users.  Training will be provided to the next-generation of researchers and professionals; internship programs are planned for undergraduate and underrepresented groups. The project allows users / scientists to connect a range of data sources, potentially increasing interdisciplinary work.  The ultimate goal is to put easy-to-use tools and platforms into the hands of researchers and students to conduct scientific investigations using findable, accessible, interoperable, and reusable (FAIR) science principles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827153","CC* Networking Infrastructure: High Performance Research Data Infrastructure at the American Museum of Natural History","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/18/2018","Juan Montes","NY","American Museum Natural History","Standard Grant","Kevin L. Thompson","06/30/2020","$499,722.00","Rebecca Oppenheimer, Michael Benedetto","jmontes@amnh.org","Central Park West at 79th St","New York","NY","100245192","2127695975","CSE","8080","","$0.00","Through the National Science Foundation?s Campus Cyberinfrastructure (CC*) program, this project provides a major data network upgrade for the American Museum of Natural History (AMNH) that makes scientific data flows a priority. AMNH conducts scientific research and education activities spanning astrophysics, geosciences, and genomics. Scientific collaborations require increasing network capacity among scientific instruments, collaborators and researchers. These data networking improvements to AMNH directly support these research activities. AMNH's ability to move large data sets quickly between its campus and other sites across the nation and throughout the world is critical to the success of the Museum's research program.<br/><br/>In the project, AMNH implements a high-speed Science DMZ, a network specifically tuned for large data transfers. Through the Science DMZ, AMNH connects to the NYSERNet Research and Education Network, which provides pathways to the higher education community in New York State, Internet2, and beyond. Using purpose-built Data Transfer Nodes (DTNs) based on the FIONA concept, AMNH scientists can move diverse, large-scale data sets between research facilities, supercomputing centers, and collaborators via a dedicated 10Gb/s connection. The efficiencies gained enable greater collaboration, more intelligent and timely coordination of research, and increased research throughput and quality. To further facilitate collaboration, the project leverages the GLOBUS file transfer system to easily orchestrate data transfers and for disseminating AMNH's research to the broader research community. Additionally, AMNH integrates with the InCommon Federation, utilizing a common framework to provide researchers secure access to online resources. Using perfSONAR, the Science DMZ is continually monitored to ensure that throughput, latency, and performance targets are met, and data can flow unimpeded. This allows Museum researchers to focus on science rather than the logistical details of moving data. Additionally, the work of AMNH scientists informs the Museum's educational and curatorial programs, directly benefitting AMNH students and the public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826994","CC* NPEO: The Research and Science Engagement Center: A Production Platform for Operations, Applied Training, Monitoring, and R&E Support","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/25/2018","Jennifer Schopf","IN","Indiana University","Continuing grant","Kevin L. Thompson","06/30/2021","$1,166,667.00","Jason Zurawski, David Jent","jmschopf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8080","9102","$0.00","The scientific community has experienced an unprecedented shift in the way<br/>research is performed and how discoveries are made. Highly sophisticated experimental instruments are<br/>creating massive datasets for diverse scientific communities and hold the potential for new insights that<br/>will have long-lasting impacts on society. However, scientists cannot make effective use of this data if<br/>they are unable to move, store, and analyze it. This project establishes the Research and Science<br/>Engagement Center (ReSEC) as a collaborative focal point for operational expertise and analysis.<br/>This project will assist scientists in routinely, reliably, and robustly transferring their data.<br/>ReSEC will deliver end-to-end user support and network engineering solutions,<br/>and become a central community hub ready to provide personalized expertise and assistance on an ongoing basis.<br/><br/>ReSEC proposes four primary execution thrusts: 1) a Roadside Assistance center to reactively<br/>respond to immediate problems with science data transfers; 2) proactive network observation<br/>using tools such as perfSONAR and NetSage; 3) assistance with design & deployment of campus<br/>networking assets such as Science DMZs; and 4) training for campus network administrators.<br/>ReSEC will scale operations broadly by relying on Regional Network, Infrastructure and Science Community partners.<br/>ReSEC will deliver expertise and assistance on a sustainable, ongoing basis, with a particular emphasis<br/>on serving educational institutions with relatively limited local network administrative resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1743184","SI2-S2I2 Conceptualization: Geospatial Software Institute","OAC","METHOD, MEASURE & STATS, Software Institutes","10/01/2017","08/30/2017","Shaowen Wang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Vipin Chaudhary","09/30/2019","$500,000.00","Donna Cox, Daniel Katz, Paul Morin, Margaret Palmer","shaowen@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1333, 8004","026Z, 7433, 8004, 8211, 9102","$0.00","Throughout the globe, changes and concerns such as emergency management, population growth, and rapid urbanization are creating scientific and societal challenges that are both localized and interdependent across space and time. Data related to location (i.e. geospatial data) collected and used for academic, governmental, and industrial purposes urgently needs innovative geospatial software to allow such data to be transformed into valuable insights and significant scientific knowledge. Fields such as agriculture, ecology, emergency management, environmental engineering and sciences, hydrology, geography and spatial sciences, geosciences, national security, public health, and social sciences all require geospatial data and software to make important advances.  This project seeks to conceptualize a Geospatial Software Institute (GSI) as a long-term hub of excellence in software infrastructure that will bring together the diverse research communities that use advanced geospatial analysis software to do their research. This project will develop a strategic plan for such a Geospatial Software Institute, that is, develop the vision and roadmap of the Institute by mobilizing the relevant communities and stakeholders. The central goal of this project is to understand how to structure and implement a GSI as a long-term hub of excellence in advanced geospatial software infrastructure, that can bring together and serve the advanced and diverse geospatial software and science research and education communities. The project will bring together the perspectives of diverse academic, governmental, and industrial institutions as well as international partners.to better understand the requirements for a GSI, identify potential software contributors, and then develop the mission, vision and plan for the GSI.  A community report will additionally be produced, that will assess critical science and engineering needs as well as promising solutions for high-performance geospatial software. Finally, this project will make useful contributions to the strategic objectives of the National Strategic Computing Initiative (NSCI). <br/><br/>The project will conduct a number of activities, consisting of three workshops as well as a number of engagement and outreach activities. These activities will consist of an initial, in depth survey of science drivers and user needs to be completed within the first month of the project, with the goal of identifying a preliminary list of community needs and requirements. This survey will help focus the three workshops; the project team will use the results of the survey to develop the key foci, agenda, and desired outcomes of each workshop. This information will also be used to advertise the workshops and solicit applications for participation. The project will reach out to underrepresented groups, target a small set of minority institutions (e.g. Harris-Stowe State College in St. Louis) and communities (e.g. social sciences) by leveraging existing activities by at the National Center for Supercomputing Applications (NCSA) and the National Socio-Environmental Synthesis Center (SESYNC), engage national museum programs, and collaborate with professional organizations (e.g., IEEE TCPP and UCGIS Body of Knowledge Committee) to understand the needs of education and workforce development, as well as develop effective mechanisms for leveraging these organizations to ensure that the findings and recommendations of the project related to curriculum and education materials are disseminated to broad communities. Finally, this project is well-aligned with the strategic objectives of the National Strategic Computing Initiative (NSCI) and will contribute to increasing coherence between technology for modeling and simulation and data analytics, increasing the capacity and capability of an enduring national HPC ecosystem, and to to developing U.S. government, industry, and academic collaborations.<br/> <br/>This award, supported by the Office of Advanced Cyberinfrastructure and the Division of Social and Economic Sciences, reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1738962","CICI: CE: SciTokens: Capability-Based Secure Access to Remote Scientific Data","OAC","Cyber Secur - Cyberinfrastruc","07/15/2017","07/12/2017","James Basney","IL","University of Illinois at Urbana-Champaign","Standard Grant","Micah Beck","06/30/2019","$1,000,000.00","Todd Tannenbaum, Duncan Brown, Brian Bockelman, Alexander Withers","jbasney@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8027","","$0.00","The management of security credentials such as passwords and secret keys for computational science workflows is a burden for scientists and information security officers. Problems with security credentials (e.g., expiration, privilege mismatch) cause the workflows to fail to fetch needed input data or store valuable scientific results, distracting scientists from their research by requiring them to diagnose the problems, re-run their computations, and wait longer for their results. In an effort to avoid these problems, scientists often use long-lived, highly-privileged credentials (e.g., enabling the workflow to fully impersonate their identity), increasing risks to their accounts and to the underlying computational infrastructure and resulting in complexity for information security officers managing the infrastructure. The SciTokens project delivers open source software to help scientists manage their security credentials more reliably and securely. The project includes participants from the Laser Interferometer Gravitational-Wave Observatory (LIGO) Scientific Collaboration and the Large Synoptic Survey Telescope (LSST) project to ensure relevance and facilitate adoption. Integration with the widely-used HTCondor software and collaboration with Open Science Grid and the Extreme Science and Engineering Discovery Environment (XSEDE) facilitates adoption by the wider scientific community.<br/><br/>To address the challenges and risks of managing security credentials for scientific workflows, the SciTokens project delivers an open source software infrastructure that uses IETF-standard Open Authorization (OAuth) tokens for capability-based secure access to remote scientific data. SciTokens uses OAuth refresh tokens, maintained securely on the submission node, to delegate short-lived, least-privilege OAuth access tokens to scientific workflows, to enable their remote data access. The access tokens convey the specific authorizations needed by the workflows, rather than general-purpose authentication impersonation credentials. These least-privilege authorization tokens help to address the risks of scientific workflows running on distributed infrastructure including NSF resources (e.g., LIGO Data Grid, Open Science Grid, XSEDE) and public clouds (e.g., Amazon Web Services, Google Cloud, Microsoft Azure). By improving the interoperability and security of scientific workflows, the SciTokens project 1) enables use of distributed computing for scientific domains that require greater data protection and 2) enables use of more widely distributed computing resources by reducing the risk of credential abuse on remote systems."
"1751143","CAREER: Reliable and Efficient Data Encoding for Extreme-Scale Simulation and Analysis","OAC","CAREER: FACULTY EARLY CAR DEV","04/15/2018","04/09/2018","Seung Woo Son","MA","University of Massachusetts Lowell","Continuing grant","Sushil K Prasad","03/31/2023","$334,234.00","","SeungWoo_Son@uml.edu","600 Suffolk Street","Lowell","MA","018543643","9789344170","CSE","1045","026Z, 062Z, 1045","$0.00","Transformative research in science and engineering to address challenges of our time, such as designing new combustion systems, depends on progressively sophisticated computational models and simulations that operate on high performance computing systems.  These simulations and analyses are increasingly constrained by the massive volumes of data that they must use, generate, and analyze.  To manage this enormous amount of data, this project explores innovative mechanisms to optimize the performance of these simulations by reducing data movement and maximizing the use of computing power, while minimizing errors and information loss.  Such performance improvements support NSF's mission to advance emerging, data-intensive science discovery and contribute to solving the world's most pressing and complex contemporary science and engineering problems.  This project implements comprehensive outreach and education to train the next-generation of professional workers and researchers in the latest computing architectures and programming methodologies, and provides rich opportunities for student engagement, research, and employment.  It leverages multiple campus and national resources and implements proven, research-based interventions to attract, retain, and educate female and underrepresented minority populations in computer engineering, which furthers the US national goal of increased participation in engineering. <br/><br/>The research goal of this project is to adapt techniques and formats for compressing video data to the investigation of novel data encoding and decoding schemes to optimize data movement and computation in data-intensive simulation and analyses.  Innovative new mechanisms have the potential to efficiently reduce the volume of data generated and transferred while also enabling rapid execution of various analysis kernels using compressed data, and permitting seamless scaling of their performance on current and future extreme-scale platforms.  The research objectives are to investigate data encoding/decoding of scientific datasets and harness encoded data, employ and scale encoded datasets seamlessly within current extreme-scale scientific workflows, and optimize machine learning and data mining algorithms with the goal of maximizing the use of computing power while minimizing errors.  These new mechanisms are applied to an evaluation framework and validated on multiple extreme-scale data-driven scientific applications, including climate, multiphysics, and fluid dynamics.  This approach is expected to transform data representation and encoding while incurring minimal disturbance to existing applications, responding to the trends in hardware architecture and dataset characteristics.  It is anticipated to improve the overall performance of computational scientists' workloads by reducing defensive and productive I/O costs, respectively, up to 100x and 200x data reduction spatially and temporally, potentially resulting in up to an overall 50x I/O cost improvement.  The project leverages multiple collaborations in order to establish the governing principles for system co-design and scalable system software layers for better data encoding within world-class computational infrastructures.  This project strengthens the University of Massachusetts Lowell computer engineering curriculum, broadens participation in computer engineering, and creates a collaborative, interdisciplinary research program geared toward exploiting ever-evolving computing paradigms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1800946","Workshop on Clusters, Clouds, and Data Analytics in Scientific Computing","OAC","Software Institutes","08/01/2018","08/04/2018","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Vipin Chaudhary","07/31/2019","$19,278.00","","dongarra@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","8004","026Z, 7556, 8004","$0.00","This workshop will be held in La Maison Des Contes, Dareize, France, and will bring together international experts from the US, France, and other countries to discuss advancements in cyberinfrastructure (CI). The specific focus will be on commonalities that exist between cluster, cloud, and high-end data analytics. CI has become of growing importance to nearly all NSF research, which makes increasing use of computational simulation and large-data analysis methods. The results will be published in an open report as well as in several journal papers. They will contribute to and leverage new ideas in cluster, cloud, and data analytics computing for the benefit of the research community.<br/> <br/><br/>The architectural similarities between the above issues, and the fact that high performance clusters typically make up the major compute nodes of computational grids and clouds, means that deployment, operational, and usage issues surrounding computational clouds form a superset of the issues that revolve around clusters. The workshop will focus on five tasks: (1) survey and analyze the key deployment, operational, and usage issues for clusters, clouds, and data analytics-- focusing especially on discontinuities produced by multicore and hybrid architectures, data-intensive science, and the increasing need for wide-area/local-area interaction; (2) document the current state-of-the-art in each of these areas, identifying interesting questions and limitations; (3) discuss experiences with clusters, clouds, and data relative to the research communities and science domains benefiting from the technology; (4) explore interoperability among disparate clouds and between various clouds and grids and their impact on domain sciences; and (5) explore directions for future research and development against the background of disruptive trends and technologies and the recognized gaps in the current state-of-the-art.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1547611","S2I2: Impl: The Science Gateways Community Institute (SGCI) for the Democratization and Acceleration of Science","OAC","Software Institutes","08/01/2016","07/20/2018","Nancy Wilkins-Diehr","CA","University of California-San Diego","Cooperative Agreement","Vipin Chaudhary","07/31/2021","$11,999,000.00","Marlon Pierce, Maytal Dahan, Michael Zentner, Katherine Lawrence","wilkinsn@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8004","026Z, 7433, 8004, 8211","$0.00","Science gateways are user-friendly web portals that make advanced computing, data, networking and scientific instrumentation accessible and easily usable by scientists at all levels, including students, thereby revolutionizing how research and education is done in science. For example, scientists are conducting biomedical studies through Galaxy, a science gateway for data intensive biomedical research, as well as engaging citizens in investigating lion density using Snapshot Serengeti, a science gateway for citizen science. By being easily accessible via the Web, science gateways expand and democratize  access to supercomputers, telescopes, sensor networks, unique data collections, collaborative spaces that enable the multidisciplinary collaborations needed to solve complex problems, and analysis capabilities. Thus, science gateways expand and broadening participation in science - an important goal of the National Science Foundation (NSF). By increasing participation, science gateways increase the NSF's return on investment in advanced technologies and facilities. The Science Gateways Community Institute (SGCI) will speed the development and application of robust, cost-effective, sustainable gateways to address the needs of scientists and engineers across the sciences. The work of the institute will increase the number as well as the effectiveness and usability of gateways to science and engineering. This will result in broader gateway use and more widespread conduct of science ranging from professionals to citizen scientists, thus, directly amplifying the impact of the SGCI. Further, and very importantly, the Institute's community engagement and exchange activities will, over time, increase the audience for its services, and its partnerships with minority professional organizations will ensure involvement in training and workforce development from underrepresented groups.<br/><br/><br/>Science gateways are user-friendly web portals that make advanced computing, data, networking and scientific instrumentation accessible and easily usable by scientists at all levels, including students, thereby revolutionizing how research and education is done in science. Gateways enable scientists to test their assumptions more quickly, providing them more time for deeper thinking about the types of problems that have yet to be solved. In this way, gateways become ""research amplifiers"". They also enable synthetic science - by using modelling and simulation tools powered by high-performance computing - across ecosystems, geographic distances, methodologies, and disciplines. However, and despite the presence of gateways for many years, development of these environments is often done with ad-hoc processes, limiting success, resource efficiency, and long-term impact. Developers of gateways are often unaware that others have solved similar  challenges before, and do not know where to turn for advice or expertise. Thus, projects waste money and time re-implementing the more basic functions rather than building the value-added features for their unique audience. Many gateway efforts fail. Some fail early by not understanding how to build communities of users; others fail later by not developing plans for sustainability. The Science Gateways Community Institute (SGCI) has been designed to address the above limitations while  providing career paths for gateway developers and for students. The five-component design of the SGCI is the result of several years of studies, including many focus groups and a 5,000-person survey of the research community. Its Incubator component will provide shared expertise in business and sustainability planning, cybersecurity, user interface design, and software engineering practices. The Extended Developer Support component will provide expert developers for up to one year to projects that request assistance as well as demonstrating the potential to achieve impacts on their research communities. The Scientific Software Collaborative component will offer a component-based, open-source, extensible framework for gateway design, integration, and services, including gateway hosting and capabilities for external developers to integrate their own software into Institute offerings. The Community Engagement and Exchange component will provide a forum for communication and shared experiences among gateway developers, within NSF, across federal agencies, and internationally. Finally, with its training programs the Workforce Development component will increase the pipeline of gateway developers, with special emphasis on recruiting underrepresented minorities, and by helping universities form gateway support groups. In short, the work of the institute will increase the number, ease of use, and effective application of gateways to science and engineering, resulting in broader gateway use and more widespread conduct of science ranging from professionals to citizen scientists. The Institute's community engagement and exchange activities over time will increase the audience for its services, and its partnerships with minority professional organizations will ensure involvement in training and workforce development from underrepresented groups."
"1730250","CyberTraining: DSE: Cross-Training of Researchers in Computing, Applied Mathematics and Atmospheric Sciences using Advanced Cyberinfrastructure Resources","OAC","CyberTraining - Training-based, COMPUTATIONAL MATHEMATICS, PHYSICAL & DYNAMIC METEOROLOGY, CLIMATE & LARGE-SCALE DYNAMICS","09/01/2017","07/19/2017","Jianwu Wang","MD","University of Maryland Baltimore County","Standard Grant","Sushil K Prasad","08/31/2020","$499,970.00","Matthias Gobbert, Aryya Gangopadhyay, Zhibo Zhang","jianwu@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","044Y, 1271, 1525, 5740","026Z, 4444, 7361, 9263","$0.00","This project will develop a training program for cross-training of participants, including graduate students, postdocs, as well as junior faculty, from three disciplines (Computing, Mathematics, and Atmospheric Sciences) to foster multidisciplinary research and education using advanced cyberinfrastructure (CI) resources and techniques.  The training focus will be Atmospheric Sciences topics that require knowledge and skills of high performance computing (HPC) and Big Data.  The impacts of this ""Data + Computing + Atmospheric Sciences"" training program include 1) prepare a better scientific workforce for advanced CI; 2) broaden CI adoption and accessibility for trainees recruited nationwide; 3) complement curricular offerings through multidisciplinary research training and team-based projects.  The project, thus, serves the national interest, as stated by NSF's mission, by promoting the progress of science and advancing the national prosperity and welfare.<br/><br/>The training program will focus on modeling and analysis of atmospheric radiation budget.  Clouds play an important role in Earth's climate system, particularly its radiative energy budget.  New technical advances of cloud simulation in numerical global climate models (GCM) usually come with high computational cost, which makes HPC an indispensable tool.  The advances of satellite-based remote sensing techniques have made a significant change in our way to observe the state of the atmosphere and evaluate GCM, and have led to growing amount of large datasets.  Because of these advances and changes, more than ever, HPC and Big Data have become parts of essential knowledge and skills to tackle some of the most challenging questions in Atmospheric Sciences.  The project will conduct cross-training of participants on a wide range of levels, including graduate students, postdocs, as well as junior faculty from the areas of Computing, Applied Mathematics, and Atmospheric Sciences.  The goal of the project is to foster multidisciplinary workforce development and collaboration using advanced CI resources and techniques.  The training program includes 1) customized course design for three disciplines with commonalities and differences; 2) online instruction on selected topics in HPC, Big Data, Applied Mathematics, and Atmospheric Sciences; 3) faculty-assisted team-based research projects."
"1450975","IRNC: AMI: Collaborative Research: Software-Defined and Privacy-Preserving Network Measurement Instrument and Services for Understanding Data-Driven Science Discovery","OAC","INTERNATIONAL RES NET CONNECT","04/01/2015","03/30/2015","Gabriel Ghinita","MA","University of Massachusetts Boston","Standard Grant","Kevin L. Thompson","03/31/2019","$416,574.00","","gabriel.ghinita@umb.edu","100 Morrissey Boulevard","Dorchester","MA","021253300","6172875370","CSE","7369","7369","$0.00","Data intensive science discovery at a global scale has imposed new requirements on the speed and management of international research and education networks. At the connection points of these international networks, it is critical to measure the network data flows to understand network traffic patterns, identify network anomalies and provide insights to network control and planning. However, the ever-increasing network speed, the massive amount of network flows and the changing measurement objectives have made the flow-level measurement on very high-speed networks extremely challenging. The Advanced Measurement Instrument and Services (AMIS) project leverages many-core, programmable network processors to prototype and deploy an advanced measurement instrument to enable services for accurate network monitoring and in-depth traffic analysis. The instrument supports flow-granularity measurement at line rate up to 100Gbps and software application programming interfaces to examine selected flows, with no impact to the performance of user traffic. With scalable hardware and an open source software stack, the measurement services equip network operators with effective tools to quantify flow-level network performance and study network flows through privacy-preserving computational analytics. This project is built on a consortium of academia, industrial partners, network operators and international alliances, who bring unique expertise and resources to achieve the objectives of high performance, programmable flow-granularity network measurement. The outcomes from this project will significantly benefit data driven science discovery, such as astronomy and space weather studies, and will promote broadened participation of underrepresented groups (such as Hispanic and female students) through the involvement of multiple universities, including an EPSCoR university and a Hispanic Serving Institution."
"1450996","IRNC: AMI: Collaborative Research: Software-Defined and Privacy-Preserving Network Measurement Instrument and Services for Understanding Data-Driven Science Discovery","OAC","INTERNATIONAL RES NET CONNECT","04/01/2015","03/30/2015","Yan Luo","MA","University of Massachusetts Lowell","Standard Grant","Kevin Thompson","03/31/2020","$1,254,935.00","","Yan_Luo@uml.edu","600 Suffolk Street","Lowell","MA","018543643","9789344170","CSE","7369","7369","$0.00","Data intensive science discovery at a global scale has imposed new requirements on the speed and management of international research and education networks. At the connection points of these international networks, it is critical to measure the network data flows to understand network traffic patterns, identify network anomalies and provide insights to network control and planning. However, the ever-increasing network speed, the massive amount of network flows and the changing measurement objectives have made the flow-level measurement on very high-speed networks extremely challenging. The Advanced Measurement Instrument and Services (AMIS) project leverages many-core, programmable network processors to prototype and deploy an advanced measurement instrument to enable services for accurate network monitoring and in-depth traffic analysis. The instrument supports flow-granularity measurement at line rate up to 100Gbps and software application programming interfaces to examine selected flows, with no impact to the performance of user traffic. With scalable hardware and an open source software stack, the measurement services equip network operators with effective tools to quantify flow-level network performance and study network flows through privacy-preserving computational analytics. This project is built on a consortium of academia, industrial partners, network operators and international alliances, who bring unique expertise and resources to achieve the objectives of high performance, programmable flow-granularity network measurement.  The outcomes from this project will significantly benefit data driven science discovery, such as astronomy and space weather studies, and will promote broadened participation of underrepresented groups (such as Hispanic and female students) through the involvement of multiple universities, including an EPSCoR university and a Hispanic Serving Institution."
"1450997","IRNC: AMI: Collaborative Research: Software-Defined and Privacy-Preserving Network Measurement Instrument and Services for Understanding Data-Driven Science Discovery","OAC","INTERNATIONAL RES NET CONNECT","04/01/2015","03/30/2015","Michael McGarry","TX","University of Texas at El Paso","Standard Grant","Kevin L. Thompson","03/31/2019","$175,430.00","","mpmcgarry@utep.edu","ADMIN BLDG RM 209","El Paso","TX","799680001","9157475680","CSE","7369","7369","$0.00","Data intensive science discovery at a global scale has imposed new requirements on the speed and management of international research and education networks. At the connection points of these international networks, it is critical to measure the network data flows to understand network traffic patterns, identify network anomalies and provide insights to network control and planning. However, the ever-increasing network speed, the massive amount of network flows and the changing measurement objectives have made the flow-level measurement on very high-speed networks extremely challenging. The Advanced Measurement Instrument and Services (AMIS) project leverages many-core, programmable network processors to prototype and deploy an advanced measurement instrument to enable services for accurate network monitoring and in-depth traffic analysis. The instrument supports flow-granularity measurement at line rate up to 100Gbps and software application programming interfaces to examine selected flows, with no impact to the performance of user traffic. With scalable hardware and an open source software stack, the measurement services equip network operators with effective tools to quantify flow-level network performance and study network flows through privacy-preserving computational analytics. This project is built on a consortium of academia, industrial partners, network operators and international alliances, who bring unique expertise and resources to achieve the objectives of high performance, programmable flow-granularity network measurement. The outcomes from this project will significantly benefit data driven science discovery, such as astronomy and space weather studies, and will promote broadened participation of underrepresented groups (such as Hispanic and female students) through the involvement of multiple universities, including an EPSCoR university and a Hispanic Serving Institution."
"1450937","IRNC: AMI: Collaborative Research: Software-Defined and Privacy-Preserving Network Measurement Instrument and Services for Understanding Data-Driven Science Discovery","OAC","INTERNATIONAL RES NET CONNECT","04/01/2015","03/30/2015","Vernon Bumgardner","KY","University of Kentucky Research Foundation","Standard Grant","Kevin L. Thompson","03/31/2019","$502,104.00","","cody@uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7369","7369, 9150","$0.00","Data intensive science discovery at a global scale has imposed new requirements on the speed and management of international research and education networks. At the connection points of these international networks, it is critical to measure the network data flows to understand network traffic patterns, identify network anomalies and provide insights to network control and planning. However, the ever-increasing network speed, the massive amount of network flows and the changing measurement objectives have made the flow-level measurement on very high-speed networks extremely challenging. The Advanced Measurement Instrument and Services (AMIS) project leverages many-core, programmable network processors to prototype and deploy an advanced measurement instrument to enable services for accurate network monitoring and in-depth traffic analysis. The instrument supports flow-granularity measurement at line rate up to 100Gbps and software application programming interfaces to examine selected flows, with no impact to the performance of user traffic. With scalable hardware and an open source software stack, the measurement services equip network operators with effective tools to quantify flow-level network performance and study network flows through privacy-preserving computational analytics. This project is built on a consortium of academia, industrial partners, network operators and international alliances, who bring unique expertise and resources to achieve the objectives of high performance, programmable flow-granularity network measurement. The outcomes from this project will significantly benefit data driven science discovery, such as astronomy and space weather studies, and will promote broadened participation of underrepresented groups (such as Hispanic and female students) through the involvement of multiple universities, including an EPSCoR university and a Hispanic Serving Institution."
"1827243","CC* Network Design: Improve Network on Campus for Research and Education in Agriculture, Science, and Engineering at Prairie View A&M University","OAC","Campus Cyberinfrastrc (CC-NIE)","08/15/2018","02/25/2019","Suxia Cui","TX","Prairie View A & M University","Standard Grant","Kevin Thompson","07/31/2020","$515,964.00","Seungchan Kim, Hua-Jun Fan, Shumon Alam, Yoonsung Jung","sucui@pvamu.edu","P.O. Box 519","Prairie View","TX","774460519","9362611689","CSE","8080","9251","$0.00","Prairie View A&M University (PVAMU), a Historically Black College and University, is implementing a Science DMZ to Improve Network on Campus for Research and Education in Agriculture, Science, and Engineering (INCREASE), which is an upgrade to current cyberinfrastructure that allows researchers to effectively work in the fields of Big Data and Data Intensive science. This INCREASE project enables data driven research in the areas of cyber security, high performance computing, computational chemistry, brain imaging, genomics, and bioinformatics. It provides better visualization and collaboration mechanisms for PVAMU researchers from seven departments in three colleges. It leads to better educational resources to train students at a minority serving institution. The INCREASE project specifically ties researchers from four research centers closely together to form a new foundation of campus cyberinfrastructure.<br/><br/>The INCREASE network, bypassing PVAMU-IT's firewall, integrates with existing infrastructure and provides improved connectivity between PVAMU and Internet2 via Lonestar Education and Research Network (LEARN). It also provides high bandwidth interconnectivity to other research universities and High Performance Computing resources nationwide. As part of the network design, the need for Software Defined Networking solutions that can selectively steer Science DMZ traffic around the campus firewall solution as well as look into implementing a deep-packet inspection and Intrusion Prevention Service to route unregistered connections in and out of the Science DMZ is considered. The study and exploration of the INCREASE network can provide guidance for universities of similar scale to pursue and utilize research resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1541335","CC*DNI DIBBs: Multi-Institutional Open Storage Research InfraStructure (MI-OSiRIS)","OAC","DATANET","09/01/2015","08/23/2018","Shawn McKee","MI","University of Michigan Ann Arbor","Cooperative Agreement","Amy Walton","08/31/2020","$4,958,411.00","Kenneth Merz, Douglas Swany, Patrick Gossman","smckee@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7726","062Z, 7433, 8048, 9251","$0.00","Every field of science generates and utilizes data in various forms: programs, instrument outputs, papers, notes, applications, simulations, video and audio recordings, etc. The continuing and evolving challenge for scientists is how to store, access, transform, manage and curate the variety of data required to effectively conduct their research, and transparently share it with other researchers across campus or at other institutions. The MI-OSiRIS project is addressing that challenge by combining an object-based software-defined storage technology with a monitored, managed network infrastructure to give scientists a distributed storage system which allows them to directly access their data from resources at any of the participating institutions.   Furthermore, MI-OSiRIS utilizes each institution's existing authentication infrastructure to allow scientists to provide controlled access to their data across all participating institutions.  By documenting and publishing designs, code, and operational experiences, the MI-OSiRIS project serves as a replicable model for supporting data-intensive, multi-institutional science collaborations.<br/><br/>MI-OSiRIS implements a Ceph-based petabyte-scale distributed data system by deploying object storage servers at each participating institution, connecting them via a managed high speed network, and distributing data based on the specific requirements of each science research domain. Ceph, an open source storage platform, supports multiple data access methods (traditional file, native object, and block), and allows configuration of access, replication, distribution, and integrity on a per-research-domain basis. MI-OSiRIS is built on low-cost, commodity hardware and can deliver gigabytes per second of I/O bandwidth per node. The system monitors and manages the network paths between its partner institutions, science users and Ceph storage components by strategically deploying perfSONAR instances which have been augmented with a network discovery, monitoring, and management platform (Network Management Abstraction Layer).  Globus Online servers provide access to data from outside MI-OSiRIS.  In addition, MI-OSiRIS leverages Ceph's software defined storage aspects to automate some data-lifecycle management tasks."
"1841694","Exploring Introduction of High Performance Computing and Big Data in High Schools","OAC","EDUCATION AND WORKFORCE","08/01/2018","08/04/2018","Yumei Huo","NY","CUNY College of Staten Island","Standard Grant","Sushil Prasad","07/31/2019","$50,000.00","Feng Gu","Yumei.Huo@csi.cuny.edu","2800 Victory Boulevard","Staten Island","NY","103146609","7189822254","CSE","7361","026Z, 062Z, 7556, 9102","$0.00","Many high schools across the nation offer programs in science, technology, mathematics, and engineering (STEM), and a lot of high school students pursue related majors in the post-secondary study. To meet the urgent workforce need, early exposure to high performance computing and big data in high schools can help attract more students to choose related majors in colleges. This requires the stakeholders including academia, industry, high school administration, and instructors to work together to address the concerns. This project has the experts from academia provide training support to high school instructors in high performance computing and big data, and the professionals from industry helping bring the cutting-edge technologies and real life examples and experience to high school administration and instructors. High school instructors are expected to develop and offer high performance computing and big data courses in their schools after the training. By inviting all the stakeholders and training high school instructors, the workshop aims to collect feedback about contents of materials, training effectiveness, and covered topics for developing a complete training model for high school instructors.  This project, thus, aligns with NSF's mission to promote the progress of science by exploring scientific workforce development pipeline.<br/><br/>The workshop invites high school administration and instructors from New York Metropolitan Area in an effort to provide exposure of high performance computing and big data, especially instructors from science, technology, mathematics, and engineering, through carefully designed tutorials. In addition to improving the awareness via tutorials and invited talks, the workshop also aims to prepare for developing an informal and cross-training model for high school instructors and designing the related advanced placement or elective courses for high school students. By convening three parties including academia, industry, and high schools and looking at the problem from different perspectives, the organizers aim to gain new insight into the education of high performance computing and big data at high school level and obtain crucial evidences to design and implement the cross-training model. The trained high school teachers are expected to infuse the training materials into their computer science related courses and/or develop new advanced placement or elective high performance computing and big data courses. The findings from the workshop are broadly disseminated in the research and educational communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826915","CC* Networking Infrastructure: Building a Friction-free High-speed Science Network and DMZ to Support Data-intensive Research and Education at The College of New Jersey","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/25/2018","Sharon Blanton","NJ","The College of New Jersey","Standard Grant","Kevin L. Thompson","06/30/2020","$499,987.00","Jeffrey Osborn","blantons@tcnj.edu","P.O. Box 7718","Ewing","NJ","086280718","6097713255","CSE","8080","","$0.00","This project proposes a program of high-speed network upgrades at The College of New Jersey (TCNJ) to support cutting-edge computational research and education. Insufficient access to TCNJ's High Performance Computing cluster previously created a bottleneck that limited scientific research and training. These enhancements to TCNJ's science network will expand research capacity and efficiency, particularly in computationally intensive fields, including biochemistry, astrophysics, phylogenomics, high-resolution microscopy, and undergraduate science education, and will provide students with greater access to research opportunities in laboratories and classrooms while contributing to the preparation of a computationally literate workforce. The upgrades enable faster data acquisition and propagation and enhancing the research and external collaborations of the College's many computationally intensive researchers, including both faculty members and undergraduate students. Finally, as part of this project, a new data visualization short course is exposing a greater number of undergraduate students to TCNJ's computing resources.<br/> <br/>This project implements a new high-speed science network and DMZ at The College of New Jersey (TCNJ) to support cutting-edge computational research and education where insufficient access to TCN's High Performance Computing cluster previously created a bottleneck that limited scientific research and training. This project directly connects TCNJ's science buildings to its High Performance Computing cluster via a new 80 Gigabit per second (Gbps) backbone and 10 Gbps connections to targeted labs, classrooms, and offices. A new Science DMZ that uses the SciPass OpenFlow application is also being implemented. SciPass supports friction-free interconnectivity by automatically allowing ""good"" data flows to bypass the Science Network firewall in real-time. Network performance is being assessed and nationally benchmarked using perfSONAR, and a new collaboration with Open Science Grid is allowing TCNJ to connect directly to this distributed computing federation's national infrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834807","Elements:  Software - Harnessing the InSAR Data Revolution: GMTSAR","OAC","XC-Crosscutting Activities Pro, EarthCube","10/01/2018","08/15/2018","David Sandwell","CA","University of California-San Diego Scripps Inst of Oceanography","Standard Grant","Stefan Robila","09/30/2021","$281,747.00","","dsandwell@ucsd.edu","8602 La Jolla Shores Dr","LA JOLLA","CA","920930210","8585341293","CSE","7222, 8074","026Z, 062Z, 072Z, 077Z, 7923, 8004","$0.00","The award supports the development of a software tool aimed at lowering the barriers for the use of Interferometric Synthetics Aperture Radar (InSAR) data. Natural processes such as earthquakes, volcanoes, landslides, and glaciers cause deformation of the surface of the earth that can now be monitored to 10 mm precision globally.  These surface measurements provide a new tool for investigating processes in the interior of the Earth. InSAR is a powerful and low-cost way to monitor subsurface magma movement and is especially useful when combined with other tools such as GPS and seismic data.  InSAR is also used to understand subsurface fluid movement, such as caused by groundwater withdrawal, geothermal production, or in oil and gas fields. A wide variety of new SAR satellites are currently operating and the US will have an even more capable mission in the near future.  These massive data sets need to be transformed into information to promote the progress of science, mitigate natural  hazards, and enhance use of underground resources. The software, named GMTSAR will develop robust and sustainable software to take full advantage of the satellite generated data for both science and applications.  The main innovation of this project is to develop open and robust software to simplify the InSAR data processing and enable routine processing of thousands of SAR images on state-of-the-art computer facilities.  Open distribution of software, and the GMTSAR theoretical basis document provide a foundation for education in the field of space geodesy and ensures availability to a diverse audience.<br/><br/>This proposed investigation will use standard software engineering practices to harden the GMTSAR code, improve the geodesy, and make it more accessible to novice and advanced users on a wide array of UNIX platforms.  This will be achieved through improved and automated testing, partial redesign and simplification of the work flows, UNAVCO short courses, user feedback, and eventual migration of the code distribution and maintenance to a national facility. Work will be performed by a postdoctoral researcher in collaboration with the GMTSAR and GMT development teams and with assistance from the XSEDE program. The expected outcome is to provide sustainable, open, geodetically-accurate software to move InSAR time series analysis from the intermediate-scale methods published today to large spatial and time scale analyses that are becoming possible using the new data streams.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program of the Division of Earth Sciences within the NSF Directorate for Geosciences, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1532133","MRI: Development of an Urban-Scale Instrument for Interdisciplinary Research","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC","10/01/2015","02/15/2019","Charles Catlett","IL","University of Chicago","Standard Grant","Stefan Robila","09/30/2019","$3,110,488.00","Peter Beckman, Michael Papka, Kathleen Cagney, Daniel Work","catlett@anl.gov","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1189, 1640","1189","$0.00","This Major Research Instrumentation (MRI) grant will enable the creation of a new tool - The Array of Things - for continuously measuring many aspects of the physical environment of urban areas at the city block scale. Data collected will fill gaps in understanding across many disciplines such as how air pollution is controlled by urban form and traffic patterns, how different materials and surroundings affect the magnitude of the urban heat island, what correlations exist between weather, noise, pollution and traffic and social and behavioral trends, and how urban infrastructure faults and failures can be better detected and predicted. The instrument opens an opportunity to experiment with new sensors and data collection strategies such as will be critical to understanding the urban microbiome or tracking disease outbreaks. Ultimately, the objective is to not only better understand elements of the built and natural infrastructure, but also the interactions of infrastructure systems with people and the environment, to understand complex city dynamics.<br/><br/>The instrument will comprise 500 nodes deployed in the City of Chicago, each with power, Internet, and a base set of sensing and embedded information systems capabilities, with capacity to evolve annually to incorporate new technologies and experiments from an already growing scientific partner community. The project will directly support scientific data infrastructure services for specific communities - engineering, physical, life, social, and climate sciences - with a data repository and tools to support workflows that enable combination with other urban data sources, analytics, and calibration and validation of computational models. In addition, the instrument's open data and application programming interface architectures will support science and education communities incorporating the data into their existing algorithms, tools, and methodologies, to merge the data with their community data resources, and to develop applications that directly access the instrument data in real time.  The instrument will provide better spatial and temporal resolution of multiple phenomena simultaneously and at larger scales than what is available from other sensing modalities in urban infrastructure networks.  The instrument will be the first instance of a general-purpose research infrastructure that allows researchers to rapidly deploy networks of sensors, embedded systems, computing, and communication systems at scale in urban environments.<br/><br/>This instrument award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Computer & Information Science & Engineering, and the NSF Engineering Directorate (Division of Chemical, Bioengineering, Environmental, and Transport Systems, and Division of Civil, Mechanical & Manufacturing Innovation)."
"1541450","CC*DNI DIBBS: Merging Science and Cyberinfrastructure Pathways: The Whole Tale","OAC","DATANET","03/01/2016","04/17/2018","Bertram Ludaescher","IL","University of Illinois at Urbana-Champaign","Cooperative Agreement","Amy Walton","02/28/2021","$4,986,951.00","Victoria Stodden, Niall Gaffney, Matthew Turk, Kyle Chard","ludaesch@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7726","7433, 8048","$0.00","Scholarly publications today are still mostly disconnected from the underlying data and code used to produce the published results and findings, despite an increasing recognition of the need to share all aspects of the research process.  As data become more open and transportable, a second layer of research output has emerged, linking research publications to the associated data, possibly along with its provenance.  This trend is rapidly followed by a new third layer: communicating the process of inquiry itself by sharing a complete computational narrative that links method descriptions with executable code and data, thereby introducing a new era of reproducible science and accelerated knowledge discovery.  In the Whole Tale (WT) project, all of these components are linked and accessible from scholarly publications. The third layer is broad, encompassing numerous research communities through science pathways (e.g., in astronomy, life and earth sciences, materials science, social science), and deep, using interconnected cyberinfrastructure pathways and shared technologies. <br/><br/>The goal of this project is to strengthen the second layer of research output, and to build a robust third layer that integrates all parts of the story, conveying the holistic experience of reproducible scientific inquiry by (1) exposing existing cyberinfrastructure through popular frontends, e.g., digital notebooks (IPython, Jupyter), traditional scripting environments, and workflow systems; (2) developing the necessary 'software glue' for seamless access to different backend capabilities, including from DataNet federations and Data Infrastructure Building Blocks (DIBBs) projects; and (3) enhancing the complete data-to-publication lifecycle by empowering scientists to create computational narratives in their usual programming environments, enhanced with new capabilities from the underlying cyberinfrastructure (e.g., identity management, advanced data access and provenance APIs, and Digital Object Identifier-based data publications).  The technologies and interfaces will be developed and stress-tested using a diverse set of data types, technical frameworks, and early adopters across a range of science domains."
"1640818","CIF21 DIBBS:  EI: VIFI:Virtual Information-Fabric Infrastructure (VIFI) for Data-Driven Decisions from Distributed Data","OAC","DATANET","10/01/2016","01/17/2018","Ashit Talukder","NC","University of North Carolina at Charlotte","Standard Grant","Amy Walton","09/30/2020","$3,999,531.00","Stanislav Djorgovski, Mirsad Hadzikadic, Yong Tao, Ehab Al-Shaer","atalukde@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","7726","7433, 8048","$0.00","Data discovery and data analytics often rely on the use of multiple data sources and data residing in distributed locations.  This project builds infrastructure that encourages data-driven discovery from distributed, fragmented datasets without requiring movement of massive amounts of data and without exposing sensitive raw datasets to end users.  The capability will be applied to a wide range of science topics: to the large sky surveys of astronomy, for which the collecting instruments are distributed nationally and internationally;  to classify Earth science satellite data; for the management of sickle-cell disease and antimicrobial resistance surveillance studies; and to integrate the highly distributed and fragmented data sources needed for multi-hazard mitigation and for sustainable and resilient human-building ecosystem research. The project outlines an ambitious and will enable interdisciplinary training in multiple universities and institutions, and contribute to the training of early career researchers<br/><br/>A Virtual Information-Fabric Infrastructure (VIFI) is created, allowing scientists to search, access, manipulate, and evaluate fragmented, distributed data in the information 'fabric' (the infrastructure to facilitate data sharing) without directly accessing or moving large amounts of data.  The system addresses the challenges of coordinating loosely federated infrastructure, distributed data management, security and privacy.  The architecture combines a set of loosely coupled components representing some proven capabilities with several emerging components. The VIFI infrastructure includes a novel orchestration layer for on-site analytics and hybrid-infrastructure (GPU, CPU) management, a dynamic secure container-based infrastructure which enables online adaptive analytics from unshareable data at distributed locations, and enhanced data and code management tools.  The layer also provides search, access and query based on improvements using persistent identifiers and automated semantic descriptions (or metadata) of raw data using semantic data mining techniques.  By integrating several NSF-funded components into a coherent whole, VIFI allows researchers to search, access, manipulate and evaluate data elements without requiring detailed familiarity with the data infrastructure itself.  The system contributes to and expands the sets of resources serving diverse communities, and is extensible to additional communities.  The project contains a substantial outreach effort, including training of early career scientists."
"1835632","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, DATANET","01/01/2019","08/27/2018","Subhashini Sivagnanam","CA","University of California-San Diego","Standard Grant","Amy Walton","12/31/2021","$63,538.00","","sivagnan@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1430508","DataONE (Data Observation Network for Earth)","OAC","DATANET","10/01/2014","03/01/2018","William Michener","NM","University of New Mexico","Cooperative Agreement","Amy Walton","09/30/2019","$15,000,000.00","Matthew Jones, David Vieglais, Suzanne Allard, Patricia Cruse, Amber Budden","wmichene@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","7726","7433, 7726, 8048, 9150","$0.00","DataONE will create new cyberinfrastructure (CI) that will resolve many of the key challenges that hinder the realization of more global, open, and reproducible science. We will do so through four interrelated CI activities that are supported by the DataONE team of developers and the CI Working Group. <br/>First, we will significantly expand the volume and diversity of data available to researchers through the DataONE Federation of repositories (i.e., Member Nodes) for large-scale scientific innovation and discovery. DataONE will create lightweight and easily deployed ""Slender Node"" software and develop DataONE compatibility for common repository software systems (e.g. DSpace and others) that are already deployed in hundreds of high-value repositories worldwide. <br/>Second, we will incorporate innovative and high-value features into the DataONE CI.  These new features include: 1) measurement search to leverage semantic technologies and enable highly precise data discovery and recall of data needed by researchers; 2) tracking the data through creation, all transformations, and analyses (provenance) to enable more reproducible science by storing and indexing provenance trace information that can be used to both reproduce scientific data processing and analysis steps and to discover specific data sources by examining the documented workflows; and 3) data extraction, sub-setting and processing services to enable researchers at any location to more easily participate in  ?big data? initiatives (e.g. working with data from large environmental observatories and participating in broad-scale synthesis and modeling endeavors). These three new sets of features will dramatically improve data discovery; further support reproducible and open science; and enable scientists from any institution, independent of networking capacity, to extract subsets of large data sets held in DataONE-affiliated repositories for processing and interpretation. <br/>Third, we will maintain and improve core CI software and services (e.g., Coordinating and Member Node software stacks and key components of the Investigator Toolkit) so that the user experience continues to improve, new services can be easily added over time, and the CI can be readily upgraded as operating system and other supporting software systems continue to evolve.   <br/>Fourth, we will increase the number of Member Nodes (size of the Federation) while maintaining cybersecurity and trust. Both of these activities respond to the need for DataONE network continuity and reliability that are critical to maintaining community trust and enabling researchers to achieve their science objectives. <br/>Four working groups that are each comprised of a small number of experts from computer and information sciences, domain sciences, and cyber-enabled learning will guide and contribute to DataONE CI development and usability, sustainability, and education and outreach. The CI Working Group will coordinate core CI research and development, including the addition of new services such as provenance tracking and semantically enabled measurement search. The Usability and Assessment Working Group will help DataONE understand community needs and expectations, and constantly improve the CI via feedback from usability analysis.  The Community Engagement and Outreach Working Group will ensure that community needs are met and that education activities and materials achieve optimal impact. The Sustainability and Governance Working Group will empower the community to drive the organization?s governance structure and sustainability strategies, ensuring that DataONE can sustain services and evolve to meet the needs of researchers, libraries, sponsors, and other stakeholders for decades to come.<br/>In addition to developing robust and powerful infrastructure, DataONE aims to change the scientific culture by promoting good data stewardship practices.  Our specific goals are to: 1) build a community of stakeholders through active engagement with data repositories and the broad community of scientists; and 2) educate scientists about good data life cycle practices through effective education, outreach and training activities and experiences.  Community engagement in the biweekly Member Node Forum and the annual meeting of the DataONE Users Group will support expansion of the data content and services provided to and needed by the research community. A new DataONE webinar series and education resources (e.g., best practices and software tools, learning modules) will enable researchers to better steward their data and take advantage of the myriad services and tools available through DataONE. The DataONE Summer Internship Program will actively involve students in CI development and related DataONE activities such as creating and providing web-based educational resources.<br/>"
"1835890","Framework: Software: HDR Globus Automate: A Distributed Research Automation Platform","OAC","Software Institutes","11/01/2018","08/27/2018","Ian Foster","IL","University of Chicago","Standard Grant","Vipin Chaudhary","10/31/2021","$2,000,000.00","Kyle Chard, Blase Ur","foster@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Rapid increases in data volumes and velocities are overwhelming finite human capabilities. Continued progress in science and engineering demands that we automate a broad spectrum of currently manual research data manipulation tasks, from transfer and sharing to acquisition, publication, indexing, analysis, and inference. To address this need, which arises across essentially all scientific disciplines, this project will work with scientists in astronomy, engineering, geosciences, materials science, and neurosciences to develop and apply Globus Automate, a distributed research automation platform. Its purpose is to increase productivity and research quality across many science disciplines by allowing scientists to offload the management of a broad range of data acquisition, manipulation, and analysis tasks to a cloud-hosted distributed research automation platform. By thus enabling scientists to hand off responsibility for managing frequently performed tasks, such as acquiring, analyzing, and storing data, Globus Automate will increase the productivity of scientific instruments and the scientists that use them.<br/><br/>This project will expand the capabilities and reach of the highly successful Globus research data management platform. Globus combines a professionally operated cloud-hosted management service with Globus Connect software deployed on more than 12,000 storage system endpoints, spanning most research universities, NSF-funded compute facilities, and NSF disciplines. Users employ Globus web interfaces and APIs to drive data movement, synchronization, and sharing tasks at and among endpoints. This ability to hand off responsibility for such tasks to cloud-hosted management logic has enabled substantial increases in data management efficiency, and spurred development of a wide range of innovative data management applications. Globus Automate will extend Globus capabilities to produce a full-featured distributed research automation platform that will enable the reliable, secure, and efficient automation of a wide range of research data management and manipulation activities. It will extend intuitive trigger-action programming models, suitable for non-programming users, to enable the specification and execution of a series of actions. It will provide for the detection of data events both at Globus storage system endpoints (e.g., creation or modification of new data files, extraction of new metadata) and at other sources (e.g., completion or failure of Globus transfer tasks); the propagation of such events to a cloud-hosted orchestration engine for reliable, efficient, and secure processing; and the invocation of remote actions on Globus endpoints and other resources. The project will leverage these basic event mechanisms to implement solutions to challenging science problems associated with partner science projects, and create a library of automation flows, both general-purpose (e.g., data publication and data replication) and domain-specific (e.g., feature detection in experimental data). These data event mechanisms will be made available on all storage systems relevant to research (Globus already supports most on-premises and cloud systems) and integrated with the Python language and JupyterLab environment that have become popular in science, so that researchers can define and share data automation behaviors as simple Python programs. A quantitative and qualitative research agenda will analyze the usability and adoption of both the platform and the research automation paradigm.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835410","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, DATANET","01/01/2019","08/27/2018","Sarah Benson-Amram","WY","University of Wyoming","Standard Grant","Amy Walton","12/31/2021","$85,284.00","Jeff Clune","sbensona@uwyo.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835272","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, DATANET","01/01/2019","08/27/2018","Laura Trouille","IL","Adler Planetarium","Standard Grant","Amy Walton","12/31/2021","$610,311.00","","ltrouille@adlerplanetarium.org","1300 S. Lake Shore Drive","Chicago","IL","606052403","3123220325","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835574","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, DATANET","01/01/2019","08/27/2018","Gregory Newman","CO","Colorado State University","Standard Grant","Amy Walton","12/31/2021","$192,940.00","","Gregory.Newman@ColoState.Edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835530","Collaborative Research: Framework: Software: HDR: Building the Twenty-First Century Citizen Science Framework to Enable Scientific Discovery Across Disciplines","OAC","AISL, DATANET","01/01/2019","08/27/2018","Lucy Fortson","MN","University of Minnesota-Twin Cities","Standard Grant","Amy Walton","12/31/2021","$945,792.00","Christopher Lintott, Daniel Boley, Craig Packer","fortson@physics.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7259, 7726","062Z, 077Z, 7925","$0.00","A team of experts from five institutions (University of Minnesota, Adler Planetarium, University of Wyoming, Colorado State University, and UC San Diego) links field-based and online analysis capabilities to support citizen science, focusing on three research areas (cell biology, ecology, and astronomy).  The project builds on Zooniverse and CitSci.org, leverages the NSF Science Gateways Community Institute, and enhances the quality of citizen science and the experience of its participants.<br/><br/>This project creates an integrated Citizen Science Cyberinfrastructure (CSCI) framework that expands the capacity of research communities across several disciplines to use citizen science as a suitable and sustainable research methodology.  CSCI produces three improvements to the infrastructure for citizen science already provided by Zooniverse and CitSci.org: <br/> - Combining Modes - connecting the process of data collection and analysis; <br/> - Smart Assignment - improving the assignment of tasks during analysis; and <br/> - New Data Models - exploring the Data-as-Subject model.  By treating time series data as data, this model removes the need to create images for classification and facilitates more complex workflows.  These improvements are motivated and investigated through three distinct scientific cases:<br/> - Biomedicine (3D Morphology of Cell Nucleus).  Currently, Zooniverse 'Etch-a-Cell' volunteers provide annotations of cellular components in images from high-resolution microscopy, where a single cell provides a stack containing thousands of sliced images.  The Smart Task Assignment capability incorporates this information, so volunteers are not shown each image in a stack where machines or other volunteers have already evaluated some subset of data.<br/> - Ecology (Identifying Individual Animals).  When monitoring wide-ranging wildlife populations, identification of individual animals is needed for robust estimates of population sizes and trends.  This use case combines field collection and data analysis with deep learning to improve results.<br/> - Astronomy (Characterizing Lightcurves).  Astronomical time series data reveal a variety of behaviors, such as stellar flares or planetary transits.  The existing Zooniverse data model requires classification of individual images before aggregation of results and transformation back to refer to the original data.  By using the Data-as-Subject model and the Smart Task Assignment capability, volunteers will be able to scan through the entire time series in a machine-aided manner to determine specific light curve characteristics.<br/><br/>The team explores the use of recurrent neural networks (RNNs) to determine automated learning architectures best suited to the projects.  Of particular interest is how the degree to which neighboring subjects are coupled affects performance. The integration of existing tools, which is based on application programming interfaces (APIs), also facilitates further tool integration.  The effort creates a citizen science framework that directly advances knowledge for three science use cases in biomedicine, ecology, and astronomy, and combines field-collected data with data analysis. This has the ability to solve key problems in the individual applications, as well as benefiting the research of the dozens of projects on the Zooniverse platform. It provides benefits to researchers using citizen scientists, and to the nearly 1.6 million citizen scientists themselves.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Research on Learning in Formal and Informal Settings, within the NSF Directorate for Education and Human Resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1547360","CICI: Data Provenance: Provenance-Based Trust Management for Collaborative Data Curation","OAC","Cyber Secur - Cyberinfrastruc","09/01/2015","09/08/2015","Zachary Ives","PA","University of Pennsylvania","Standard Grant","Micah Beck","08/31/2019","$500,000.00","Susan Davidson, Val Tannen, Sampath Kannan","zives@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","8027","7434, 8089","$0.00","Data-driven science relies not only on statistics and machine learning, but also on human expertise. As data are being collected to tackle increasingly challenging scientific and medical problems, there is need to scale up the amount of expert human input (curation and, in certain cases, annotation) accordingly.  This project addresses this need by developing collaborative data curation:  instead of relying on a small number of experts, it enables annotations to be made by communities of users of varying expertise.  Since the quality of annotations by different users will vary, novel quantitative techniques are developed to assess the trustworthiness of each user, based on their actions, and to distinguish trustworthy experts from unskilled and malicious users. Algorithms are developed to combine users' annotations based on their trustworthiness.  Collaborative data curation will greatly increase the amount of human annotated data, which will, in turn, lead to better Big Data analysis and detection algorithms for the life sciences, medicine, and beyond.<br/><br/>The central problems of collaborative data curation lie in the high variability in the quality of users' annotations, and variability in the form the data takes when they annotate it.  The proposal develops techniques to take annotations made by different users over different views of data (such as an EEG display with filters and transformations applied to the signal), to use provenance to reason about how the annotations relate to the original data, and to reason about the reliability and trustworthiness of each user's annotations over this data.  To accomplish this, the research first defines data and provenance models that capture time- and space-varying data; novel reliability calculus algorithms for computing and dynamically updating the reliability and trustworthiness of individuals, based on their annotations and how these compare to annotations from recognized experts and the broader community; and a high-level language called PAL that enables the researchers to implement and compare multiple policies.  The researchers will initially develop and validate the techniques on neuroscience and time series data, within a 900+ user public data sharing portal (with 1500+ EEG and other datasets for which annotations are required). The project team later expands the techniques to other data modalities, such as imaging and genomics"
"1835877","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","DATANET","09/01/2018","08/09/2018","Barbara Minsker","TX","Southern Methodist University","Standard Grant","Amy Walton","08/31/2023","$584,151.00","Kenneth Berry","minsker@smu.edu","6425 BOAZ","Dallas","TX","752750302","2147682030","CSE","7726","062Z, 077Z, 7925","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642410","SI2-SSE: Collaborative Research: High Performance Low Rank Approximation for Scalable Data Analytics","OAC","SOFTWARE & HARDWARE FOUNDATION, Software Institutes","11/01/2016","09/08/2016","Haesun Park","GA","Georgia Tech Research Corporation","Standard Grant","Micah Beck","10/31/2019","$332,283.00","Barry Drake","hpark@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798, 8004","7433, 7942, 8004, 8005","$0.00","Big Data analytics is at the core of discovery covering vast areas such as medical informatics, business analytics, national security, and materials sciences. This project aims to model some of the key data analytics problems and design, verify, and deploy scalable methods for knowledge extraction.  The algorithms developed will be able to handle data sets of extreme sizes and will be deployable on advanced computer hardware. The goal is to realize orders-of-magnitude improvements over existing data analytics technologies, developing algorithms that are robust to incompleteness, noise, ambiguity, and high dimension in the data.  Particular focus will be parallel and distributed algorithms that can efficiently solve large problems and produce accurate solutions.  The proposed research and software development will allow domain experts to tackle Big Data sets requiring large parallel systems.  The improved performance will enable fast and scalable data analysis across applications, from social network analysis to study citizens' attitudes toward sustainability-related issues to computational marketing techniques that refine customers' shopping experiences.  The proposed work will help bridge the gap between computational science and data analytics ecosystems, two fields that stand to make great advancements from cross-fertilization.  The education and outreach plan includes graduate course creation, engagement of under-represented groups via both undergraduate and graduate research experiences, and community-building efforts by workshop and mini-symposium organization.<br/><br/>With the advent of internet-scale data, the data mining and machine learning community has adopted Nonnegative Matrix Factorization (NMF) for performing numerous tasks such as topic modeling, background separation from video data, hyper-spectral imaging, web-scale clustering, and community detection.  The goals of this proposal are to develop efficient parallel algorithms for computing nonnegative matrix and tensor factorizations (NMF and NTF) and their variants using a unified framework, and to produce a software package called Parallel Low-rank Approximation with Nonnegative Constraints (PLANCK) that delivers the high performance, flexibility, and scalability necessary to tackle the ever-growing size of today's data sets. The algorithms will be generalized to NTF problems and extend the class of algorithms we can efficiently parallelize; our software framework will allow end-users to use and extend our techniques.  Rather than developing separate software for each problem domain and mathematical technique, flexibility will be achieved by characterizing nearly all of the current NMF and NTF algorithms in the context of a block coordinate descent framework. Using this framework the shared computational kernels can be separated, which usually extend run times, from the algorithm-specific computations. Finally, the usability and practicality of the proposed software will be maintained by being application driven, establishing collaborations with early end-users, and by incrementally generalizing the framework in terms of both algorithms and problems."
"1642385","SI2-SSE: Collaborative Research: High Performance Low Rank Approximation for Scalable Data Analytics","OAC","SOFTWARE & HARDWARE FOUNDATION, Software Institutes","11/01/2016","09/08/2016","Grey Ballard","NC","Wake Forest University","Standard Grant","Micah Beck","10/31/2019","$167,713.00","","ballard@wfu.edu","1834 Wake Forest Road","Winston Salem","NC","271098758","3367585888","CSE","7798, 8004","7433, 7942, 8004, 8005","$0.00","Big Data analytics is at the core of discovery covering vast areas such as medical informatics, business analytics, national security, and materials sciences. This project aims to model some of the key data analytics problems and design, verify, and deploy scalable methods for knowledge extraction.  The algorithms developed will be able to handle data sets of extreme sizes and will be deployable on advanced computer hardware. The goal is to realize orders-of-magnitude improvements over existing data analytics technologies, developing algorithms that are robust to incompleteness, noise, ambiguity, and high dimension in the data.  Particular focus will be parallel and distributed algorithms that can efficiently solve large problems and produce accurate solutions.  The proposed research and software development will allow domain experts to tackle Big Data sets requiring large parallel systems.  The improved performance will enable fast and scalable data analysis across applications, from social network analysis to study citizens' attitudes toward sustainability-related issues to computational marketing techniques that refine customers' shopping experiences.  The proposed work will help bridge the gap between computational science and data analytics ecosystems, two fields that stand to make great advancements from cross-fertilization.  The education and outreach plan includes graduate course creation, engagement of under-represented groups via both undergraduate and graduate research experiences, and community-building efforts by workshop and mini-symposium organization.<br/><br/>With the advent of internet-scale data, the data mining and machine learning community has adopted Nonnegative Matrix Factorization (NMF) for performing numerous tasks such as topic modeling, background separation from video data, hyper-spectral imaging, web-scale clustering, and community detection.  The goals of this proposal are to develop efficient parallel algorithms for computing nonnegative matrix and tensor factorizations (NMF and NTF) and their variants using a unified framework, and to produce a software package called Parallel Low-rank Approximation with Nonnegative Constraints (PLANCK) that delivers the high performance, flexibility, and scalability necessary to tackle the ever-growing size of today's data sets. The algorithms will be generalized to NTF problems and extend the class of algorithms we can efficiently parallelize; our software framework will allow end-users to use and extend our techniques.  Rather than developing separate software for each problem domain and mathematical technique, flexibility will be achieved by characterizing nearly all of the current NMF and NTF algorithms in the context of a block coordinate descent framework. Using this framework the shared computational kernels can be separated, which usually extend run times, from the algorithm-specific computations. Finally, the usability and practicality of the proposed software will be maintained by being application driven, establishing collaborations with early end-users, and by incrementally generalizing the framework in terms of both algorithms and problems."
"1835543","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","DATANET","09/01/2018","08/09/2018","Noah Fahlgren","MO","Donald Danforth Plant Science Center","Standard Grant","Amy Walton","08/31/2023","$592,999.00","","nfahlgren@danforthcenter.org","975 N. Warson Rd.","St. Louis","MO","631322918","3145871041","CSE","7726","062Z, 077Z, 7925","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835834","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","DATANET","09/01/2018","08/09/2018","Kenton McHenry","IL","University of Illinois at Urbana-Champaign","Standard Grant","Amy Walton","08/31/2023","$3,752,045.00","Praveen Kumar, Klara Nahrstedt","kmchenry@ncsa.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7726","062Z, 077Z, 7925","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1739000","CICI: RSARC: Trustworthy Computing over Protected Datasets","OAC","DATANET, Cyber Secur - Cyberinfrastruc","09/01/2017","08/02/2018","Mayank Varia","MA","Trustees of Boston University","Standard Grant","Micah Beck","08/31/2019","$1,002,988.00","Andrei Lapets","varia@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7726, 8027","7433, 9251","$0.00","Scientists are often stymied in their research due to the inaccessibility of relevant data. Additionally, many data owners silo data away from powerful, economical cloud computing resources due to privacy and confidentiality concerns. This project enables data scientists to compute statistics over protected datasets while simultaneously empowering the owners of the underlying datasets to maintain control over how their data is used in computations and viewed by other people. The work also brings a cryptographically secure computing engine to one of the largest collections of small to medium sized research data in the world, running on a federated datacenter operated by multiple non-trusting vendors. In doing so, this project enhances the flow of information sharing to promote transparency and accountability for data sharing and processing decisions while simultaneously reducing tenants' need to trust the cloud's behavior thanks to cryptographic protections that promote confidentiality and integrity. The project enables scientific research computing on workflows involving collaborative experiments or replication and extension of existing results when the underlying data are encumbered by privacy concerns.<br/><br/>To accomplish this goal and enhance the economic potential of the cloud, the researchers and engineers on this project integrate and enhance three technologies they have previously developed. First, the Dataverse data management infrastructure houses, curates, and indexes social, physical, and life science data. Second, the Massachusetts Open Cloud (MOC) is a computing environment designed from the ground up to promote user control and &#64258;exibility over trust decisions. Third, Conclave compiles legacy code into a cryptographically secure multi-party computation program that can be executed on top of existing data processing frameworks like Hadoop and Spark. This project develops and open-sources the necessary cyberinfrastructure to integrate these technologies and provide a combined ""secure computing element"" into which data and analytics may be inserted and their resulting answers fed back. This secure computing element incorporates several designs: (i) policy-agnostic programming to ensure that legacy code may be accepted, (ii) the MOC's isolation mechanism to ensure that data owners may choose exactly which environment to entrust with their data, (iii) Conclave to hide the source data from everyone other the intended recipient (even the cloud itself), a policy engine to ensure that the data owner consents to the requested analytic, (iv) Dataverse's data classification engine to manage access control over source and derived data, and (v) a new auditing and billing mechanism to promote transparency, punish those who exceed their privileges, and provide a sustainable economic model for growth."
"1661378","Application Characterization for Adaptive Computing Platform Determination for Computational and Data-Enabled Science and Engineering","OAC","INFORMATION TECHNOLOGY RESEARC, Software Institutes, CDS&E","08/15/2016","02/21/2017","Haiying (Helen) Shen","VA","University of Virginia Main Campus","Standard Grant","Micah Beck","04/30/2019","$433,921.00","","hs6ms@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","1640, 8004, 8084","019Z, 7433, 8084, 9150, 9179, 9251","$0.00","Traditional high performance computing clusters (HPCC) and Hadoop clusters are both important platforms in computational and data-enabled science and engineering (CDS&E). Hadoop MapReduce is typically effective in large-scale data analysis while traditional HPC is more commonly employed in computational problems (petabytes vs. petaflops). HPCC can include a shim layer that allows Hadoop MapReduce to access HPC storage (denoted by Hadoop+HPCC), local storage (denoted by Hadoop), and a combination of both (denoted by Hadoop/HPCC). This project aims to identify the characteristics of various CDS&E MapReduce computational tasks, and adaptively determine the best platform (Hadoop, Hadoop+HPCC and Hadoop/HPCC) for individual applications based on their characteristics, and also optimally arrange data placement between local storage and dedicated remote storage, given performance objectives and system cost metrics. <br/><br/>Broader impacts include critical insights into the suitability of different computing platforms to different CDS&E applications, and a more advanced HPC system. Research results will be disseminated through technology transfer to industry partners, via publication in peer review journals, and in software releases. Results will also serve as catalyst for research in cyberinfrastructure, which serves the CDS&E fields. This project will provide thorough training of students and collaborative research opportunities for participating Clemson graduates, undergraduates, faculty, and K-12 students. Results will be integrated into courses taught by the PIs. The PIs will recruit new students, particularly those from underrepresented groups, to undertake the study of a STEM discipline."
"1541215","CC*DNI DIBBs: Data Analysis and Management Building Blocks for Multi-Campus Cyberinfrastructure through Cloud Federation","OAC","DATANET","10/01/2015","07/27/2018","David Lifka","NY","Cornell University","Cooperative Agreement","Amy Walton","09/30/2020","$7,149,491.00","Thomas Furlani, Richard Wolski","lifka@cac.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7726","062Z, 7433, 8048, 9251","$0.00","The ability to aggregate, share, and analyze important large data sets while optimizing time-to-science is essential to support multi-disciplinary and multi-institutional data-driven discovery. This project is deploying a federated cloud computing system in New York State and California comprised of data infrastructure building blocks designed to support scientists requiring flexible workflows and analysis tools for large-scale data sets. Data challenges from seven different communities-earth and atmospheric sciences, finance, chemistry, astronomy, civil engineering, genomics, and food science-are being addressed using a rich set of open source software, optimized frameworks, and cloud usage modalities. The federated cloud is operating at Cornell University (project lead) and at partner sites at the University at Buffalo and the University of California, Santa Barbara. The project team is supporting multi-disciplinary research groups with over forty global collaborators and documenting science use cases. The broader goal of this project is to develop a federated cloud model that encourages and rewards institutions for sharing large-scale data analysis resources that can be expanded internally with common, incremental building blocks and externally through meaningful collaborations with other institutions, public clouds, and NSF cloud resources.<br/><br/>Project documentation and webinars feature best practices and include how to create Virtual Machine instances, run at federated sites, burst to Amazon Web Services, and access, move, and store large-scale data. A new tool for cloud metrics is being built into Open XDMoD (XD Net Metrics on Demand) that features QBETS (Queue Bounds Estimation from Time Series) statistics to enable users to make online forecasts of future performance and allocation level availability as well as to predict when to burst from federation resources. A new allocations and accounting model allows institutional administrators to track utilization across federated sites and use this data as an exchange mechanism. These tools provide a better understanding of how the sharing of data infrastructure building block capacity across institutional boundaries can create wider science and engineering collaborations and increase data sharing in a scalable and sustainable way."
"1827050","CC* Network Design: Transforming Arcadia's Networking Capability, Enhancing for Innovation to Grow Research Leaders in a Technology-driven World","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","07/09/2018","Leslie Margolis","PA","Arcadia University","Standard Grant","Kevin Thompson","06/30/2020","$352,500.00","Vitaly Ford","margolisl@arcadia.edu","450 S. Easton Road","Glenside","PA","190383295","2155722887","CSE","8080","","$0.00","As a small private university, Arcadia's existing computing infrastructure constrains the productivity of faculty in Bioinformatics, Computer Science, Chemistry, and Physics who are conducting data-intensive research. Specifically, the current infrastructure impedes researchers' ability to efficiently and securely access, share, or analyze large-data sets with collaborators at other institutions. To address these research and education needs, a collaborative team representing key university faculty and technologists at Arcadia is creating a Science DMZ with a data transmission network capable of 10Gbps connectivity (more than 10 times faster than current speeds) to the Keystone Initiative for Network Based Education and Research's (KINBER) PennREN network.<br/><br/>Project's objectives are to: (1) provide high performance, secure Science DMZ network capabilities for sharing of large datasets and cloud-based education; (2) eliminate technical barriers for faculty engaged in data-intensive projects through a dedicated, friction-free path to Internet2, PennREN, and other high performance computing and data resources; (3) leverage authentication and authorization mechanisms to support our faculty through the InCommon Federation; and (4) enable future scientific possibilities and unleash innovation for students and faculty researchers. <br/><br/>Arcadia is currently considering to incorporate a data analytics requirement into its general curriculum and leverage the newly developed cyberinfrastructure to enable cloud-based opportunities, distance learning and researching on a global scale. This opportunity is supporting greater faculty and student analytical scholarship by forming a frictionless environment built to innovate and thrive in our technology-drive world.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812675","Collaborative Research: CESER: EAGER: ""FabWave"" - A Pilot Manufacturing Cyberinfrastructure for Shareable Access to Information Rich Product Manufacturing Data","OAC","CM - Cybermanufacturing System, CESER-Cyberinfrastructure for","04/01/2018","03/26/2018","Yong Chen","CA","University of Southern California","Standard Grant","William Miller","03/31/2020","$108,625.00","","yongchen@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","018Y, 7684","016Z, 067E, 152E, 7916, MANU","$0.00","Advanced manufacturing research is dependent on access to large datasets of product models to enable product designers to learn from past errors, and to discover and develop new solutions. However, such datasets are typically archived in inaccessible repositories and may be poorly described and difficult to use by others. Current manufacturing research must shift from siloed repositories of product manufacturing data to a federated, decentralized, open and inter-operable approach. This transformation can be achieved by embedding cyber-capability in every physical end-point, be it on a desktop used by a product designer or within the control systems of a manufacturing machine. The goal of this project is to develop a pilot manufacturing-focused cyberinfrastructure called FabWave. FabWave will gather together digital data on product and manufacturing processes from diverse sources, in particular, three-dimensional (3D) computer aided design (CAD) models generated by academia and public community users, into a new rich dataset that, in turn, will be fully accessible to manufacturing researchers and others. FabWave's system design will be driven by the needs of the manufacturing research community, and the project will encourage its adoption by academic users, government labs and the public. FabWave will aim to lower the barriers for users to upload, share and download 3D product model data; it will also enable manufacturing science researchers to gain access to an information rich dataset for testing and evaluation of algorithms to advance manufacturing research and offer the capability for the community to build custom apps that provide services to the community. This infrastructure development project aligns with the NSF Harnessing Data for 21st Century Science and Engineering Big Ideas vision and can serve to improve US competitiveness in advanced manufacturing.<br/><br/>The de novo cyberinfrastructure tools to be built in the FabWave project include: 1) Direct plugins within design software to allow easier capture of metadata and upload of 3D product models to the FabWave repository with limited human interaction; 2) a data-store with application programming interface (API) tools to allow users to write scripts to search through the repository for content specific to their research questions. Innovative use of the Inter-Planetary File System (IPFS) protocols along with next generation unstructured databases will serve to index and enable search services for Fabwave's users. the project will demonstrate use of FabWave for development of community facing third-party apps and libraries. Ultimately, the research community will have access to a rich source of product datasets created within research laboratories, within classes and through other open sourced projects. This data network, if expanded across universities, would accelerate advancements in cybermanufacturing, engineering systems design, and cyber-physical systems in manufacturing. In addition, FabWave will enable new areas of research in manufacturing information integration and informatics combined with reducing the barriers to exploring the science of manufacturing machine intelligence. This award is supported by the Division of Civil, Mechanical and Manufacturing Innovation and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808591","Collaborative Research: CDS&E: Theoretical Foundations and Algorithms for L1-Norm-Based Reliable Multi-Modal Data Analysis","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/27/2018","Evangelos Papalexakis","CA","University of California-Riverside","Standard Grant","Vipin Chaudhary","08/31/2021","$175,263.00","","epapalex@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","8069, 8084","026Z, 8084, 9263","$0.00","In modern applications of science and engineering, large volumes of data are collected from diverse sensor modalities, commonly stored in the form of high-order arrays (tensors), and jointly analyzed in order to extract information about underlying phenomena. This joint tensor analysis can exploit inherent dependencies across data modalities and allow for markedly enhanced inference. Standard methods for tensor analysis rely on formulations that are sensitive to heavily corrupted points among the processed data (outliers). To counteract the destructive impact of outliers in modern data analysis (and thereto relying applications), this project will investigate new theory and robust algorithmic methods. The performance benefits of the developed tools will be evaluated in applications from the fields of data analytics, machine learning and computer vision. Thus, this research aspires to increase significantly the reliability of data-enabled research across science and engineering. Combining theoretical explorations, with practical algorithmic solutions for data analysis and experimental evaluations, this project has the potential to build significant future capacity not only for U.S. academic institutions but also for the U.S. government and industry. Thus, apart from promoting the progress of science, this project could contribute to advances in the national prosperity and welfare. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities and supports diversity in STEM by involving?students from underrepresented groups.<br/><br/>In this project, the theoretical underpinnings of L1-norm tensor analysis will be investigated, with a focus on its computational hardness and exact solution. Then, based on these new foundations, efficient/practical algorithms for L1-norm tensor analysis will be explored, together with scalable and distributed software implementations. These theoretical and algorithmic investigations are expected to advance significantly the knowledge in the currently under-explored area of L1-norm tensor analysis and deliver highly impactful methodologies for outlier-resistant multimodal data processing. Next, the PIs will employ the newly developed algorithmic tools in key problems from the fields of data analytics, machine learning and computer vision. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities?and supports diversity in STEM by involving students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808582","Collaborative Research: CDS&E: Theoretical Foundations and Algorithms for L1-Norm-Based Reliable Multi-Modal Data Analysis","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/27/2018","Panagiotis Markopoulos","NY","Rochester Institute of Tech","Standard Grant","Vipin Chaudhary","08/31/2021","$323,973.00","Andreas Savakis","pxmeee@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","8069, 8084","026Z, 8084, 9263","$0.00","In modern applications of science and engineering, large volumes of data are collected from diverse sensor modalities, commonly stored in the form of high-order arrays (tensors), and jointly analyzed in order to extract information about underlying phenomena. This joint tensor analysis can exploit inherent dependencies across data modalities and allow for markedly enhanced inference. Standard methods for tensor analysis rely on formulations that are sensitive to heavily corrupted points among the processed data (outliers). To counteract the destructive impact of outliers in modern data analysis (and thereto relying applications), this project will investigate new theory and robust algorithmic methods. The performance benefits of the developed tools will be evaluated in applications from the fields of data analytics, machine learning and computer vision. Thus, this research aspires to increase significantly the reliability of data-enabled research across science and engineering. Combining theoretical explorations, with practical algorithmic solutions for data analysis and experimental evaluations, this project has the potential to build significant future capacity not only for U.S. academic institutions but also for the U.S. government and industry. Thus, apart from promoting the progress of science, this project could contribute to advances in the national prosperity and welfare. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities and supports diversity in STEM by involving students from underrepresented groups.<br/><br/>In this project, the theoretical underpinnings of L1-norm tensor analysis will be investigated, with a focus on its computational hardness and exact solution. Then, based on these new foundations, efficient/practical algorithms for L1-norm tensor analysis will be explored, together with scalable and distributed software implementations. These theoretical and algorithmic investigations are expected to advance significantly the knowledge in the currently under-explored area of L1-norm tensor analysis and deliver highly impactful methodologies for outlier-resistant multimodal data processing. Next, the PIs will employ the newly developed algorithmic tools in key problems from the fields of data analytics, machine learning and computer vision. In addition, research activities under this project will be integrated with education. Participating students, at both graduate and undergraduate levels, will gain important experience in optimization theory, machine learning, computer vision, and data mining, among other areas. Moreover, the project plan includes multiple STEM outreach activities?and supports diversity in STEM by involving?students from underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835739","Elements: Data: U-Cube: A Cyberinfrastructure for Unified and Ubiquitous Urban Canopy Parameterization","OAC","SPECIAL INITIATIVES, DATANET, LARS SPECIAL PROGRAMS, EarthCube","01/01/2019","08/18/2018","Daniel Aliaga","IN","Purdue University","Standard Grant","Amy Walton","12/31/2021","$599,999.00","Dev Niyogi, Rajesh Kalyanam","aliaga@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1642, 7726, 7790, 8074","062Z, 077Z, 1525, 4444, 7923","$0.00","Urban canopy parameters (UCPs) can be used in model simulations to study the health and behavior of a city, determine the ability to sustain a growing population, and study potential impacts of extreme weather events.  The ability to identify and compute urban canopy parameters has been a missing element in city models; this project develops that capability for use in city design and analysis, integrating weather models and remote sensing data to infer a 3D model of cities of various sizes.  The project deploys innovative science-based analysis tools within an extensible, broadly-available cyberinfrastructure portal, allowing users to ingest satellite imagery and other geographic information system (GIS) data to calculate urban canopy parameters.  The cyberinfrastructure would improve urban modeling and planning, particularly for extreme weather events.  The tools and high-performance computing and storage resources would be usable by other researchers through a portal.  Potential beneficiaries include smaller and disadvantaged cities and countries without the resources for urban characterization and modeling necessary for such urban planning.  There are also plans to transfer the results of this research to communities beyond college students -- to local teachers and secondary students and museums, and to the GIS urban planning user communities at local, state, and international levels.<br/><br/>The project develops cyberinfrastructure which would use a novel inverse modeling approach incorporating satellite images, social science and urban zonal data, to infer a 3D model of a city from which urban canopy parameters could be derived for use in simulation models.   The focus is on weather modeling, urban parameterization and a desire to better understand sustainable urbanization.  The main cyberinfrastructure products will be 3D urban models and UCP values for urban locations.  These UCP parameters will be used for fine-scale urban weather modeling, and evaluation of various classification techniques and simulation models in an integrated portal.   The approach differs from prior work that relied on simple urban canopy models, either tuned for a large metropolis or assuming that all cities are the same.  The team uses a cyberinfrastructure platform at Purdue (HubZERO) and the Geospatial Data Analysis Building Blocks (GABBs), a suite of software modules developed during a previously funded NSF Data Infrastructure project.  The resulting platform can be deployed using Amazon Web Services, extending built-in geospatial data capabilities and providing a scalable CI solution.  This platform can be used by researchers to test predictive models or deploy applications that have been developed.  The team has cultivated relationships with the research communities and stakeholders relevant to the proposed research.  Through the World Urban Database and Access Portal Tools (WUDAPT) project -- a community-based project to gather a census of cities around the world -- the team is already connected to the urban planning community globally.  The project will improve urban weather modeling accuracy and increase availability of and access to the new techniques, capabilities and dedicated cyberinfrastructure.   The results have the potential to support city officials and urban planners, especially in regions with the fastest rate of urbanization and/or those in developing countries, where access to computational resources is likely to be limited.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure will be jointly supported by the Division of Chemical, Bioengineering, Environmental, and Transport Systems, within the NSF Directorate for Engineering; and the Division of Atmospheric and Geospace Sciences and the Integrative and Collaborative Education and Research (ICER) Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812687","Collaborative Research: CESER: EAGER: ""FabWave"" - A Pilot Manufacturing Cyberinfrastructure for Shareable Access to Information Rich Product Manufacturing Data","OAC","CM - Cybermanufacturing System, CESER-Cyberinfrastructure for","04/01/2018","03/26/2018","Binil Starly","NC","North Carolina State University","Standard Grant","William Miller","03/31/2020","$191,223.00","","bstarly@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","018Y, 7684","016Z, 067E, 152E, 7916, MANU","$0.00","Advanced manufacturing research is dependent on access to large datasets of product models to enable product designers to learn from past errors, and to discover and develop new solutions. However, such datasets are typically archived in inaccessible repositories and may be poorly described and difficult to use by others. Current manufacturing research must shift from siloed repositories of product manufacturing data to a federated, decentralized, open and inter-operable approach. This transformation can be achieved by embedding cyber-capability in every physical end-point, be it on a desktop used by a product designer or within the control systems of a manufacturing machine. The goal of this project is to develop a pilot manufacturing-focused cyberinfrastructure called FabWave. FabWave will gather together digital data on product and manufacturing processes from diverse sources, in particular, three-dimensional (3D) computer aided design (CAD) models generated by academia and public community users, into a new rich dataset that, in turn, will be fully accessible to manufacturing researchers and others. FabWave's system design will be driven by the needs of the manufacturing research community, and the project will encourage its adoption by academic users, government labs and the public. FabWave will aim to lower the barriers for users to upload, share and download 3D product model data; it will also enable manufacturing science researchers to gain access to an information rich dataset for testing and evaluation of algorithms to advance manufacturing research and offer the capability for the community to build custom apps that provide services to the community. This infrastructure development project aligns with the NSF Harnessing Data for 21st Century Science and Engineering Big Ideas vision and can serve to improve US competitiveness in advanced manufacturing.<br/><br/>The de novo cyberinfrastructure tools to be built in the FabWave project include: 1) Direct plugins within design software to allow easier capture of metadata and upload of 3D product models to the FabWave repository with limited human interaction; 2) a data-store with application programming interface (API) tools to allow users to write scripts to search through the repository for content specific to their research questions. Innovative use of the Inter-Planetary File System (IPFS) protocols along with next generation unstructured databases will serve to index and enable search services for Fabwave's users. the project will demonstrate use of FabWave for development of community facing third-party apps and libraries. Ultimately, the research community will have access to a rich source of product datasets created within research laboratories, within classes and through other open sourced projects. This data network, if expanded across universities, would accelerate advancements in cybermanufacturing, engineering systems design, and cyber-physical systems in manufacturing. In addition, FabWave will enable new areas of research in manufacturing information integration and informatics combined with reducing the barriers to exploring the science of manufacturing machine intelligence. This award is supported by the Division of Civil, Mechanical and Manufacturing Innovation and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1663578","Stampede 2:  Operations and Maintenance for the Next Generation of Petascale Computing","OAC","EQUIPMENT ACQUISITIONS","10/01/2017","08/04/2018","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Robert Chadduck","09/30/2021","$30,000,000.00","William Barth, Niall Gaffney, Kelly Gaither, Tommy Minyard","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7619","","$0.00","In 2016, the National Science Foundation funded the acquisition of a large new forward-looking high performance computing (HPC) system, Stampede 2, by the Texas Advanced Computing Center (TACC) at the University of Texas at Austin. In partnership with its lead system vendor (Dell), TACC will deploy the Intel-based system in 2017, doubling the capacity of its predecessor, Stampede, by introducing new memory, processor and interconnect technologies. As Stampede 2 nears its operational deployment, a proposal for operations and maintenance (O&M) of the system was submitted by the University of Texas at Austin.  The system is expected to be used as a national resource by thousands of researchers, educators, and students annually. As a critical component of academic infrastructure, it will advance fundamental knowledge in a wide variety of science and engineering frontiers. In addition to continued partnership with Dell, subawards to Clemson University, The University of Colorado, Cornell University, Indiana University, and Ohio State University will ensure a broad national research of innovative HPC to academia and industry. <br/><br/>Stampede 2 will operate within the larger landscape of the nation's research cyberinfrastructure (CI). It joins the set of large scale computing resources that rely on and benefit from the collaborative user services model of the NSF-funded Extreme Science and Engineering Discovery Environment (XSEDE) project. These accompanying shared services provide for systems allocations, user training, technical interoperability, research and CI community engagement, and access to expertise. Stampede 2 doubles the computing, storage and networking capacity of the current system, Stampede. Delivering on the potential of this complex scientific instrument requires knowledgeable and ongoing operations, which include: robust system maintenance, reliability and availability; security; software configuration and management; efficient utilization; and research workflow optimization. Most significantly, the thousands of users who currently depend on Stampede rely on expert assistance to help in the development of new skills in order to maximize the value of the new technologies in Stampede 2. These technologies represent the future of large-scale computing. <br/><br/>The architecture of Stampede 2 reflects community consensus about HPC's exascale future; while specific technologies are in rapid flux, all paths indicate a transition to more explicit parallelism within applications. Today's applications must adapt, and Stampede 2 offers a bridge to exascale systems of tomorrow, providing capabilities for exploring new approaches to multiscale (both temporal and spatial) simulations, many forms of data intensive science, visualization, and data analysis. Stampede 2's operations will also broaden the usage base of HPC, appealing to and supporting a much greater depth and breadth of large-scale computational science for research than any other national system. <br/><br/>The Stampede 2 Operations and Maintenance project plan includes world-class operations, user support and training, application tuning and migration, education, outreach, documentation, data management, visualization, analytics-driven application support, and research collaboration. TACC and its team of partners are established CI providers. Collectively the Stampede 2 operations team will leverage a variety of other NSF-supported projects such as XSEDE, Advanced Cyberinfrastructure Research and Education Facilitators (ACI-REF), and a broad array of scientific software activities. With these complementary collaborations, the value of the O&M award is further increased."
"1829764","CyberTraining: CIU:Cross-disciplinary Training for Findable, Accessible, Interoperable, and Reusable (FAIR) science","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Venkatesh Merwade","IN","Purdue University","Standard Grant","Bogdan Mihaila","08/31/2021","$498,148.00","Wan Ju Huang, Xiaohui Carol Song, Matthew Huber","vmerwade@ecn.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","Addressing the grand challenges associated with growing population, food and water security, frequently occurring natural disasters, and changing climate requires not only geoscience domain expertise, but also computational (cyber) skills to deal with big data analytics and simulations. However, many challenges exist in providing cyber training to students, including steep learning curve for instructors, time commitment and lack of resources in terms of training material. This project creates cyber training programs for earth science students and working professional in the form of modules or labs for use in an existing course, an independent one-credit course, summer workshops and boot camps. Students receive training in running state-of-the-art numerical models, and in analyzing massive simulated and observational data sets with advanced data analysis tools. The overall goal is to create a new generation of scientists to manage data-rich and computationally intensive tasks to become globally competitive in the STEM, thus fulfilling NSF's mission to promote the progress of science.  The training component of the project will make the science openly available and transparently reproducible using the best practices in Findable, Accessible, Interoperable, and Reusable (FAIR) science. The estimated number of trainees from this project is approximately 200 students at Purdue and potentially more than 400 students, faculty and working professionals outside Purdue.<br/><br/>This project brings cyber-enabled state-of-the-art computational tools into practice by training students and working professionals through courses, workshops and boot camps at multiple institutions, including Purdue University, University of New Hampshire and University of Alabama. The project creates a cyber training curriculum that is driven by the need to acquire expertise in the following six areas: data access, geo-processing, time series analysis, computational simulation, visualization and publication. These areas form the foundation of a modular cyber training framework that supports development and implementation of training materials targeting geoscience learners. The innovation lies in enhancing geosciences curriculum to include cyber training by providing a range of flexible training options for students and working professionals. A FAIR Cyber Training (FACT) Fellowship will enhance computational thinking among graduate students and create an avenue to tap the most talented students to train the next generation of cyber savvy earth scientists. This interdisciplinary collaboration from hydrology, climate science, computer science and instruction design  expertise enables holistic training covering both domain science and computational technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1540933","IRNC: AMI: NetSage - An Open, Privacy-Aware, Network Measurement, Analysis, and Visualization Service","OAC","INTERNATIONAL RES NET CONNECT","05/01/2015","03/26/2019","Jennifer Schopf","IN","Indiana University","Cooperative Agreement","Kevin Thompson","04/30/2020","$5,450,000.00","Jason Leigh, Sean Peisert, Andrew Lake","jmschopf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7369","5913, 5921","$0.00","NetSage is an open privacy-aware network measurement, analysis, and visualization service designed to address the needs of today's international networks. Modern science is increasingly data-driven and collaborative in nature, producing petabytes of data that can be shared by tens to thousands of scientists all over the world. The NSF-supported International Research Network Connection (IRNC) links have been essential to performing these science experiments. <br/><br/>Providing near real-time monitoring and visualization of international data transfers will help ensure that scientific workflows are operating at maximum efficiency. NetSage services provide an unprecedented combination of passive measurements, including SNMP data, flow data, and Bro-based traffic analysis, as well as active measurements, mainly perfSONAR, and longitudinal network performance data visualization. User privacy is a significant concern in this project given the data flowing through the exchange points. NetSage addresses these concerns through the use of a privacy advisory board that will ensure the data gathering activities are conducted to meet all community standards. The proposed work is a partnership between Indiana University, University of California at Davis, and University of Hawaii at Manoa. This uniquely strong team combines backgrounds in production international network support, networking measurement and prediction tools, network-intensive applications and data visualization."
"1642143","Collaborative Research: CICI: Secure and Resilient Architecture: SciGuard: Building a Security Architecture for Science DMZ Based on SDN and NFV Technologies","OAC","Cyber Secur - Cyberinfrastruc","01/01/2017","08/04/2016","Hongxin Hu","SC","Clemson University","Standard Grant","Micah Beck","12/31/2019","$499,805.00","Richard Brooks, Kuang-Ching Wang, Nuyun Zhang","hongxih@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","8027","9150","$0.00","As data-intensive science becomes the norm in many fields of science, high-performance data transfer is rapidly becoming a standard cyberinfrastructure requirement. To meet this requirement, an increasingly large number of university campuses have deployed Science DMZs. A Science DMZ is a portion of the network, built at or near the edge of the campus or laboratory's network, that is designed such that the equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose computing. This project develops a secure and resilient architecture called SciGuard that addresses the security challenges and the inherent weaknesses in Science DMZs. SciGuard is based on two emerging networking paradigms, Software-Defined Networking (SDN) and Network Function Virtualization (NFV), both of which enable the granularity, flexibility and elasticity needed to secure Science DMZs. <br/><br/>Two core security functions, an SDN firewall application and a virtual Intrusion Detection System (IDS), coexist in SciGuard for protecting Science DMZs. The SDN firewall application is a software-based, in-line security function running atop the SDN controller. It can scale well without bypassing the firewall using per-flow/per-connection network traffic processing.  It is also separated from the institutional hardware-based firewalls to enforce tailored security policies for the science-only traffic sent to Science DMZs. The virtual IDS is an NFV-based, passive security function, which can be quickly instantiated and elastically scaled to deal with attack traffic variations in Science DMZs, while significantly reducing both equipment and operational costs. In addition to these functions, the researchers also design a cloud-based federation mechanism for SciGuard to support security policy automatic testing and security intelligence sharing. The new mechanisms developed in this project are robust, scalable, low cost, easily managed, and optimally provisioned, therefore substantially enhancing the security of Science DMZs. This research encourages the diversity of students involved in the project by active recruitment of women and other underrepresented groups for participation in the project. The project has substantial involvement of graduate students in research, and trains promising undergraduate students in the implementation and experiments of the proposed approach. Moreover, the project enhances academic curricula by integrating the research findings into new and existing courses."
"1642031","Collaborative Research: CICI: Secure and Resilient Architecture: SciGuard: Building a Security Architecture for Science DMZ Based on SDN and NFV Technologies","OAC","Cyber Secur - Cyberinfrastruc","01/01/2017","08/04/2016","Gail-Joon Ahn","AZ","Arizona State University","Standard Grant","Micah Beck","12/31/2019","$499,484.00","Dijiang Huang, Adam Doupe","gahn@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8027","","$0.00","As data-intensive science becomes the norm in many fields of science, high-performance data transfer is rapidly becoming a standard cyberinfrastructure requirement. To meet this requirement, an increasingly large number of university campuses have deployed Science DMZs. A Science DMZ is a portion of the network, built at or near the edge of the campus or laboratory's network, that is designed such that the equipment, configuration, and security policies are optimized for high-performance scientific applications rather than for general-purpose computing. This project develops a secure and resilient architecture called SciGuard that addresses the security challenges and the inherent weaknesses in Science DMZs. SciGuard is based on two emerging networking paradigms, Software-Defined Networking (SDN) and Network Function Virtualization (NFV), both of which enable the granularity, flexibility and elasticity needed to secure Science DMZs. <br/><br/>Two core security functions, an SDN firewall application and a virtual Intrusion Detection System (IDS), coexist in SciGuard for protecting Science DMZs. The SDN firewall application is a software-based, in-line security function running atop the SDN controller. It can scale well without bypassing the firewall using per-flow/per-connection network traffic processing.  It is also separated from the institutional hardware-based firewalls to enforce tailored security policies for the science-only traffic sent to Science DMZs. The virtual IDS is an NFV-based, passive security function, which can be quickly instantiated and elastically scaled to deal with attack traffic variations in Science DMZs, while significantly reducing both equipment and operational costs. In addition to these functions, the researchers also design a cloud-based federation mechanism for SciGuard to support security policy automatic testing and security intelligence sharing. The new mechanisms developed in this project are robust, scalable, low cost, easily managed, and optimally provisioned, therefore substantially enhancing the security of Science DMZs. This research encourages the diversity of students involved in the project by active recruitment of women and other underrepresented groups for participation in the project. The project has substantial involvement of graduate students in research, and trains promising undergraduate students in the implementation and experiments of the proposed approach. Moreover, the project enhances academic curricula by integrating the research findings into new and existing courses."
"1453430","CAREER: Enabling Distributed and In-Situ Analysis for Multidimensional Structured Data","OAC","CAREER: FACULTY EARLY CAR DEV, EPSCoR Co-Funding","07/01/2015","03/03/2015","Trilce Estrada-Piedra","NM","University of New Mexico","Standard Grant","Sushil Prasad","06/30/2020","$412,969.00","","estrada@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","1045, 9150","1045, 9150","$0.00","Advances in modern science have led to explosions of data across all science, technology, engineering, and mathematics (STEM) disciplines. Extracting meaningful knowledge from this large pool of information has become both complicated and costly. In fields like genomics and astronomy, where very large volumes of data are produced daily, it is necessary to store repositories throughout multiple, geographically distinct locations. This type of data allocation results in expensive computations and incomplete analyses. For health informatics and finances, data is typically isolated between research centers due to privacy, security, or cost issues. Again, the inability to have a global view of the data yields inaccurate outcomes at computation time. The classic centralized approach to analyzing data no longer produces optimal results; it has become a major bottleneck, hindering the advantages Big Data has to offer. Current solutions for distributed analysis still lack generality, scalability, or accuracy.  <br/><br/>This project aims to ameliorate problems in the management of distributed data while enabling scalable and accurate analyses.  The project provides a comprehensive approach to handle data-to-knowledge extraction, representation, and learning at scale. Products of this research include: (1) an algorithmic suite of semantic projections and scalable learning methods for efficient data dimensionality reduction, pattern recognition, anomaly detection, and clustering, and (2) an open source middleware for coupling distributed data acquisition processes with in-situ analytics and crowd sourcing. These products will be made available through a GitHub repository at https://github.com/distributedreasoningatunm.  Moreover, the crowd sourcing extension doubles as an educational platform, which aims to attract interest in the STEM fields.<br/>"
"1835473","Elements: Data:  Integrating Human and Machine for Post-Disaster Visual Data Analytics:  A Modern Media-Oriented Approach","OAC","DATANET","01/01/2019","08/18/2018","Shirley Dyke","IN","Purdue University","Standard Grant","Amy Walton","12/31/2021","$597,955.00","Thomas Hacker, Bedrich Benes","sdyke@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7726","062Z, 077Z, 7923","$0.00","This project creates a science-oriented visual data service that facilitates the query of datasets based on visual content. The approach allows a user to search for data based on visual similarity, even in cases where a term for the failure or observation does not yet have a scientific name.  The visual analysis data and application services will be deployed on a cloud-based platform.  The results will produce a framework enabling access to and analysis of a large amount of imagery from diverse sources.  <br/><br/>The research team creates VISER (Visual Structural Expertise Replicator), which will serve as a comprehensive cloud-based data analytics service and will facilitate the use of and integrate data and applications most needed by the user.  The framework will implement two novel concepts: data-as-a-service and applications-as-a-service, which will bring data and applications to the user without the need to configure software systems or packages.  The approach also employs artificial intelligence to interpret the contents of the images.  VISER will use convolutional neural networks (CNNs) to train custom classifiers for new categories.  Three applications will be developed and deployed within VISER:  App1 will extract relevant visual context, App2 will facilitate similarity-based visual searching (through the use of a Siamese CNN), and App3 will help perform automatic extraction of pre-event/pre-disaster images based on Google Street View.  The application of these tools would advance both the science of automated pattern recognition and of more effective construction techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839013","EAGER: Enhanced Robust Persistent Identification of Data (E-RPID)","OAC","NSF Public Access Initiative","10/01/2018","08/08/2018","Robert Quick","IN","Indiana University","Standard Grant","William Miller","09/30/2020","$299,989.00","Marlon Pierce, Laurence Lannom","rquick@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7414","7916","$0.00","An increasing challenge in modern research is the rapid growth in ability to collect and store research data; this growth is much more rapid than the ability to catalog, make accessible, and reuse these data. Major challenges facing the scientific research enterprise include making data discoverable, accessible, and re-usable at Internet scales; and making it possible to use data to replicate analyses done in published research. The importance of this issue is at the center of many recent initiatives in Open Science and Open Data. The objective of this project is to address these deficiencies and to enable more options for data interoperability and reusability in the current research data landscape. One aim is to demonstrate how technical solutions for data accessibility and management should be designed, implemented, operated, and usable by a broad range of researchers. Another aim is to ultimately increase the publication, discovery, and reuse of data, thus contributing to the greater integrity of the scientific enterprise.<br/><br/>This project aims to create a full implementation of the Digital Object Architecture (DOA) concept to improve data interoperability and reusability and potentially transform the way research in many disciplines is conducted. The approach is to integrate existing and new software components into the Robust Persistent Identification of Data (RPID) testbed which itself is a 2-year pilot project supported by NSF (Project ID 1659310). Components to be added include a Digital Object Interface Protocol (DOIP) which defines common operations performed on digital objects; and a Repository Mapping Service which would allow mapping the contents of existing repositories to the Digital Object Architecture (DOA) environment. With the new components, the ensemble ""Enhanced-RPID"" (E-RPID) testbed will allow managing of data objects by assigning individual identifiers toward making data FAIR (findable, accessible, interoperable, and reusable). This technical effort along with planned educational material are designed to lower the barrier for adoption of Persistent Identification (PID)-centric data management throughout the data lifecycle, from initial data collection to long-term use and reuse. The E-RPID testbed will be housed on the NSF-funded Jetstream computational resource, will be applied to a number of important scientific use cases, and will contribute to activities fostered by the Research Data Alliance (RDA).  The outcomes of this project are aimed to broadly benefit disciplinary and multi-disciplinary research and to facilitate better access to and utilization of existing data repositories.<br/><br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659427","CC* Networking Infrastructure: A Science DMZ to Enable Friction-Free Scientific Data Workflows","OAC","Campus Cyberinfrastrc (CC-NIE)","04/01/2017","11/15/2018","Dan Voss","KS","University of Kansas Center for Research Inc","Standard Grant","Kevin Thompson","03/31/2019","$498,961.00","Eduardo Rosa-Molinar, John Paden, Bob Lim","dan.voss@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457568","7858643441","CSE","8080","9150","$0.00","The University of Kansas (KU) is implementing a 100 Gigabit per second (Gbps) Science DMZ network for research scientists which provides a separate network for dedicated scientific research traffic workflows.  This serves many research projects at the Center for Research Computing (CRC), the Center for Remote Sensing of Ice Sheets (CReSIS), the Genome Sequencing Core (GSC), the Biodiversity Institute (BI), and the Microscopy and Analytical Imaging laboratory (MAI).  The project team is composed of domain scientists, computer scientists, and KU Information Technology administration.  The common themes uniting these diverse scientific areas are enhanced external institutional collaboration, data sharing, remote data transfer, and educational outreach.<br/><br/>The infrastructure upgrades are the foundation for a network architecture that is anchored by a dedicated 100 Gbps research network backbone devoted to prioritizing research data flows and is easily extensible for adding new connections.  This design is modeled after the successful Science DMZ network architecture from the Energy Sciences Network (ESnet), which has been replicated at more than 100 universities.  The project addresses the ?last mile? network bottleneck and the campus enterprise designed network architecture.  The current network design places barriers on research data traffic on campus enterprise networks that compete for limited network bandwidth.  This project elevates scientific research data at KU to transfer at higher speeds, which enhances interdisciplinary and external collaborations, and enables current and future scientific access to friction-free networks."
"1845799","CAREER: Scalable Approaches for Large-Scale Data-driven Bayesian Inverse Problems in High Dimensional Parameter Spaces","OAC","CAREER: FACULTY EARLY CAR DEV, COMPUTATIONAL MATHEMATICS","01/15/2019","12/17/2018","Tan Bui-Thanh","TX","University of Texas at Austin","Continuing grant","Sushil K Prasad","12/31/2023","$315,428.00","","tanbui@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1045, 1271","1045, 9263","$0.00","Inverse problems are contemporary tools in cyberinfrastructure and mathematical research, especially in inferring knowledge from observational and experimental data together with simulations and models. They are pervasive in scientific discovery and decision-making for complex, natural, engineered, and societal systems, and thus are of paramount importance across many disciplines including engineering mathematical and physical sciences. For inverse problems that serve as a basis for design, control, discovery, and decision-making, their solutions must be equipped with certain degree of confidence. Though the past decades have seen advances in both theories and computational algorithms for inverse problems, quantifying the uncertainty in their solution remains challenging and an open problem facing the computational science and engineering community. The drastic increase in the quantity of measurements and data holds promise for data-driven scientific discoveries. However, much data remains unused as inversion - a systematic tool to infer knowledge from data - is unable to scale up to the quantity of data being generated. This proposal develops computational and data scalable strategies to tackle the challenge of large-scale data-driven statistical inverse problems in order to continue the pace of scientific discoveries and to promote the progress of science, aligned with NSF's mission. The proposed integrated research and education program contributes uncertainty quantification (UQ) skills to modern education and training of future STEM workforce; provides scalable inverse/UQ mathematical algorithms/software that potentially advance the frontiers of computational science and engineering; provides inverse/UQ cutting-edge algorithms/software that can potentially improve oil/gas discovery in order to meet the ever-increasing demand in energy; constitutes the PI?s ongoing contribution to the pipeline of US scientists, engineers, and innovators to maintain the US global leadership in technology and sciences; and educates and supplies additional leaders/experts from underrepresented minorities to Big-Data/UQ research communities.<br/><br/>This project develops an integrated education and cross-disciplinary research program that tackles big-data-driven large-scale uncertainty quantification (UQ) problems in high dimensional parameter spaces. The project rigorously develops a randomized misfit approach that exploits extreme computing systems to efficiently reduce the amount of ever-growing observational data. It develops a comprehensive ensemble transform approach that has potential to solves large-scale statistical Bayesian inverse problems in a scalable manner using current and future NSF computing infrastructures. The novelty of the proposed interdisciplinary approach is to bring together advances from stochastic programming, probability theory, parallel computing, and computer vision to produce a new and rigorous data reduction method for inverse/UQ problems; justifiable efficient sampling approaches for large-scale Bayesian inverse problems; and open-source software implementing these approaches. These products can enable mathematicians, scientists, and engineers in sensing-based disciplines to address challenging inverse/UQ problems that can lead to new scientific discoveries. Inverse seismic wave propagation is chosen as the demanding testbed for the developments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1541426","CC*DNI Integration: Enhancing Science Through Custom Paths For Trusted Users","OAC","CISE RESEARCH RESOURCES","09/01/2015","02/18/2016","James Griffioen","KY","University of Kentucky Research Foundation","Standard Grant","Kevin L. Thompson","08/31/2019","$999,313.00","Zongming Fei, Kenneth Calvert","griff@netlab.uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","2890","8002, 9150","$0.00","Progress in medicine, science, and engineering -- as in essentially all areas of human endeavor -- increasingly depends on network access to very large data sets (billions to trillions of bytes) containing information such as scientific measurements, monitoring information, images, videos, simulation results, etc.  The number of researchers working with, transferring, and sharing very large data sets is growing at an alarming rate.  Although network transmission speeds are rapidly increasing, devices deployed in the middle of the network for security and management purposes -- e.g., firewalls, intrusion detectors, and the like -- create major bottlenecks that prevent researchers from being able to quickly move these large files between laboratories, high-performance computing sites, and storage facilities.<br/><br/>Historically, network designers have addressed this problem by creating and maintaining a special ""Science DMZ"" network that provided a ""fast lane"", bypassing performance-limiting security devices thanks to the trusted nature of the users and applications connected to the Science DMZ.  Today, however, the growing number and variety of users who need high-performance data transfer makes it increasingly difficult to maintain such a separate infrastructure.  This project is developing ways to provide trusted users on the normal (campus) network with dynamic access to such fast lanes by authenticating them on-the-fly, determining the nature of their data transfer, and whether it should be allowed or not.<br/><br/>This project leverages emerging Software-Defined Networking (SDN) capabilities to single out individual data flows for specialized treatment in the network.  Using the SDN infrastructure already deployed on campus, the project is developing new control software and protocols that enable trusted users to authenticate themselves to the network and set up network connections free from middlebox interference while still enforcing middlebox security functionality for normal campus traffic.  The ability of trusted users to create such fast lanes -- as opposed to (only) network administrators -- will enable future networks to handle the rapidly growing number of big data users."
"1839030","Findable Accessible Interoperable and Reusable (FAIR) Hackathon Workshop for Mathematical and Physical Sciences (MPS) Research Communities","OAC","NSF Public Access Initiative","09/01/2018","07/25/2018","Michael Hildreth","IN","University of Notre Dame","Standard Grant","Beth Plale","08/31/2019","$49,917.00","Natalie Meyers","hildreth.2@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7414","7556","$0.00","The FAIR principles (findable, accessible, interoperable, and reusable) when applied to scientific research data have the potential to both ease the burden of readying data created from NSF funded research for sharing and enhance the use and reuse of the data.  The FAIR Hackathon Workshop for Mathematics and the Physical Sciences (MPS) research communities will bring together innovative data scientists and developers from across physical sciences projects funded by NSF to solve real-world data challenges using the principles of FAIR: Findable, Accessible, Interoperable and Reusable Data. Designed to address issues of public access to data and to provide tools and relevant hands-on experience for researchers, the workshop will lay out the FAIR principles and metrics in the context of a hackathon using the successful model of The Bio-IT World FAIR Data Hackathon organized by the European Union's GOFAIR initiative of the European Open Science Cloud. The hackathon will unite MPS and research data management teams to tackle actual datasets with maximum impact potential.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1724853","CIF21 DIBBs: EI: Integrated Platform for Applied Network Data Analysis (PANDA)","OAC","DATANET, Campus Cyberinfrastrc (CC-NIE)","09/01/2017","02/12/2019","Kimberly Claffy","CA","University of California-San Diego","Standard Grant","Amy Walton","08/31/2022","$4,032,000.00","Amogh Dhamdhere, Alberto Dainotti, Alistair King","kc@caida.org","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7726, 8080","7433, 8048, 9251","$0.00","As the Internet, and society's dependence on it, have grown, the structure and dynamics of the network, and how it relates to the political economy in which it is embedded, are gathering increasing attention by researchers, operators and policy makers.  This project offers a new platform and supporting tools for collecting, analyzing, querying, and interpreting measurements of the Internet ecosystem.  It directly supports the NSF goal of providing robust, secure cyberinfrastructure to accelerate research, education, and new capabilities in data-intensive science and engineering.  Broader impacts of this project include increased public awareness of Internet structure, dynamics, performance, and evolution, which informs discussions of critical issues in current and future large-scale networking.<br/><br/>This Platform for Applied Network Data Analysis (PANDA) integrates existing research infrastructure measurement and analysis components previously developed by the Center for Applied Internet Data Analysis, and enables new scientific directions, experiments, and data products for a wide range of research activities. The design of PANDA emphasizes efficient indexing and processing of terabyte archives, advanced visualization tools to show geographic and economic aspects of Internet structure, and detailed interpretation of displayed results.The project actively engages collaborators from four targeted disciplines: networking, security, economics, and public policy.Activities include workshops to establish and stimulate multi-disciplinary collaborations, development of online video tutorials targeting non-networking experts and classroom-focused materials, an annotated bibliography and discussion forum, and a strategic advisory board.<br/> <br/>This award is supported jointly by both the Data and Networking programs within the Office of Advanced Cyberinfrastructure."
"1550404","SI2-SSI: Collaborative Research: A Robust High-Throughput Ab Initio Computation and Analysis Software Framework for Interface Materials Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes, DMREF","09/01/2016","08/31/2016","Kesong Yang","CA","University of California-San Diego","Standard Grant","Micah Beck","08/31/2019","$600,000.00","Shyue Ping Ong","kesong@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","1253, 1712, 8004, 8292","7237, 7433, 7569, 8004, 8009, 8400, 9216","$0.00","A set of techniques scientists call ""ab initio"" methods, which are derived from the fundamental laws of physics with minimal assumptions and approximations, has become a critical tool in the study and design of materials. With computing advances and software innovations, the automation of high-throughput ab initio calculations has, in particular, heralded an explosion of computed data for a large variety of materials. However, these high-throughput efforts are limited to specific properties. In contrast, materials interfaces, one of the fastest growing research areas in materials science and engineering, are showing an increasing relevance in many areas of materials applications such as catalysis and electronics. This project will develop a software framework that enables novel high-throughput interface materials investigations and design. The developed software platform will expand the genome of materials by including the computed interfacial properties of interface materials. This community-based software can potentially become a critical component of the Materials Genome Initiative and serve not just the large and diverse materials research community, but also the physics and chemistry communities. Besides featuring heavily in existing and planned courses taught by the Principal Investigators in their home institutions, the proposed framework will facilitate the training of undergraduates and graduates in the ab initio methodologies in other institutions as well. This project will also conduct public outreach activities to increase awareness of the importance of sustainable software development for data-driven interface materials science. <br/><br/>The project will develop necessary workflow management, error correction schemes, and systematic analysis tools to support ab initio studies of thermodynamics, kinetics, diffusion, and electronic property of interface materials including hetero-structures and grain boundary. It targets developmental efforts on three key focus areas of great interest to interface materials science: (i) ab initio thermodynamics of surfaces and interfaces; ii) advanced methods for materials kinetics and diffusion at materials interfaces; and iii) automated algorithms for structural construction of grain boundary and post data-processing and analysis. In doing so, this project will greatly expand the suite of interfacial materials properties that are amenable to a high-throughput ab initio treatment, paving the way for materials investigations and design in a broad spectrum of technological applications, including energy generation and storage, catalysis and electronics. In addition, by interfacing with classical-mechanics simulation codes, this framework will bridge the gap between the ab initio and classical force-field approach, which is expected to significantly advance the high-throughput simulations of materials interfaces. <br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Mathematical and Physical Sciences (Division of Materials Research and Office of Multidisciplinary Activities)."
"1626251","MRI: Acquisition of High Performance Hybrid Computing Cluster to Advance Cyber-Enabled Science and Education at Penn State","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC","10/01/2016","08/05/2016","Yuexing Li","PA","Pennsylvania State Univ University Park","Standard Grant","Stefan Robila","09/30/2019","$920,688.00","Douglas Cowen, Mahmut Kandemir, Adri van Duin, Eric Ford","YUL20@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1189, 1640","1189","$0.00","Computers are now becoming the driving forces for ground-breaking discoveries and are transforming the science and education in this new data-driven era. The Cyber-Laboratory for Astronomy, Materials and Physics (CyberLAMP) at Penn State will put together a cutting-edge supercomputer cluster that includes both traditional central processing units (CPUs) and the latest hardware accelerators, such as graphics processing units (GPUs), to advance interdisciplinary research and education in cyberscience. Astronomers and physicists will use this high-performance hybrid computer to analyze data from revolutionary surveys and experiments and to perform state-of-the-art simulations to unravel of the origin of our Universe.  Material scientists will run realistic, atomistic-scale, simulations to guide the design and development of next-generation complex materials. Computer scientists will analyze these science applications to inform the design of future computer architectures. These advances in both data analysis and simulations will enable the CyberLAMP members to shed new light on topics prioritized by national strategic plans, such as National Research Council's 2010 Decadal Survey for astronomy and astrophysics to search for habitable planets and to understand the fundamental physics of the cosmos and the White House's Materials Genome Initiative to expedite development of new materials. Furthermore, the CyberLAMP team will employ this cluster to enhance a wide range of outreach programs including: computational education to numerous students at The Pennsylvania State University, including its Commonwealth campuses; summer workshops for researchers and high-school teachers; and partnerships with industry to advance materials research and the co-design of future hardware-software systems. <br/><br/>By expediting exploratory data analysis and simulations and catalyzing cross-disciplinary collaboration in developing and prototyping algorithms, the new hybrid cluster will enable the CyberLAMP team to deliver transformative breakthroughs in a number of key research areas.  This includes state-of-the-art astrostatistics and astroinformatics for data analysis for world-leading surveys in cosmology and exoplanets, as well as sophisticated simulations to directly address the nature of dark matter and dark energy, and the formation of planetary systems; an order-of-magnitude increase of speed for reconstruction algorithms for the most ambitious astrophysical experiments probing fundamental physics, which will enlarge the discovery space for cosmic sources of neutrinos, gravity waves, and multi-messenger emitters, as well as heighten sensitivity to the neutrino mass hierarchy; dramatic advances in nanosecond-scale fully reactive molecular dynamics simulations for the development of next-generation complex materials; and novel insights for designing the next-generation of hardware accelerators in hybrid systems, highly parallel algorithms and software interfaces which could revolutionize the way hardware accelerators are used by data-intensive applications."
"1659397","CC* Network Design: The Bucknell Science DMZ Network Design and Implementation","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2017","02/14/2019","Param Bedi","PA","Bucknell University","Standard Grant","Kevin Thompson","06/30/2019","$399,200.00","Mark Yerger, Christopher Bernard","param.bedi@bucknell.edu","One Dent Drive","LEWISBURG","PA","178372111","5705773510","CSE","8080","","$0.00","Bucknell University with the support of the Pennsylvania-wide education and research network (KINBER) is building a next generation campus research network to enable secure transfer of large datasets across campus and between off-campus research partners.  Campus network infrastructure limitations emerge when such data transfers compete with other campus network traffic and further limitations prevent broad sharing of large data sets outside of the network to other potential collaborators.  Building on Bucknell's strong teacher-scholar model, new, quality, data-intensive, in-lab and in-classroom data transfer capabilities are accelerating student and faculty research, including off-campus data sharing collaborations.  <br/> <br/>This project creates a local area network optimized for high-performance scientific applications, a Science DMZ.  Key to the project's initial design is the addition of a data transfer node, combined with increased local network capacity, to improve Bucknell's ability to work with and exchange GIS, bioinformatics, and clinical data with partners within and outside of Bucknell, while preparing students for postgraduate work and study in data-intensive fields.  With special focus on improving network function to improve the throughput for larger data sets, the project will include improvements in border router technology and distribution/access nodes that support lab spaces on campus as well as the construction of a Science DMZ and incorporation of the data transfer nodes alongside the campus High Performance Computing Cluster (HPCC).  In addition, this project establishes network performance monitoring metrics, improves inter and intra-institutional data exchange capabilities, and strengthens current and future research support for faculty, students and their collaborators."
"1739419","SI2-SSE: An Ecosystem of Reusable Image Analytics Pipelines","OAC","EXTRAGALACTIC ASTRON & COSMOLO, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2017","08/24/2017","Andrew Connolly","WA","University of Washington","Standard Grant","Vipin Chaudhary","08/31/2020","$500,000.00","Mario Juric, Ariel Rokem, Alvin Cheung, Magdalena Balazinska","ajc@astro.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1217, 1253, 8004","1206, 7433, 7569, 8004, 8005, 9102","$0.00","Astronomy has entered an era of massive data streams generated by telescopes and surveys that can scan tens of thousands of square degrees of the sky across many decades of the electromagnetic spectrum. The promise of these new experiments - characterizing the nature of dark energy and the composition of dark matter, discovering the most energetic events in the universe, tracking asteroids whose orbits may intersect with that of the Earth - will only be realized if we can address the challenge of how to process and analyze the tens of petabytes of images that these astronomical surveys will generate per year. With the increasing capacity for scientists to collect ever larger sets of data, often in the form of images, our potential for scientific discovery will soon be limited not by how we collect or store data, but rather how we extract the knowledge that these data contain (e.g. how we account for noise inherent within the data, and understand when we have detected fundamentally new classes and interesting events or physical phenomena). This project is to develop an open source scalable framework for the analysis of large imaging data sets. It is designed to operate as a cloud service, incorporate seamlessly new or legacy image processing algorithms, support and optimize complex analysis workflows, and scale analyses to thousands of processors without the need for an individual user to develop custom solutions for a specific computer platforms or architecture. This framework will be integrated with state-of-the-art image analysis algorithms developed for astronomical surveys to provide an image analytics platform that can be used by future telescopes and cameras and the astronomical community as a whole. Beyond astronomy, the framework will be extended to enable scientists from the physical and life sciences that make use of imaging data (e.g. neuroscience, oceanography, biology, seismology) to focus their work on developing scientific algorithms and analyses rather than the infrastructure required to process massive data sets<br/><br/>Over the last decade, there have been many advancements in astronomical image analysis algorithms and techniques; driven by new surveys and experiments. The complexity of these techniques and the systems that run them has, however, meant that the number of users who make use of these advancements is small; typically restricted to the experiments themselves or to a small group of expert users. Because of this, the community as a whole does not benefit from the significant investment in image analytics for astronomy. In this project, the PIs address these issues by developing and deploying a scalable framework for the analysis of small and large imaging datasets. This cloud-based system will be able to incorporate new and legacy image processing algorithms, support and optimize complex analysis workflows, scale applications to thousands of processors without users needing to develop custom code for specific platforms, and support efficient sharing of algorithms and analysis results among users. It will enable state-of-the-art image analysis algorithms (e.g. those developed for surveys such as the Large Synoptic Survey Telescope; LSST) to be used by the broad astronomical community and in so doing will leverage then tens of thousands of hours that has been invested in the development of these techniques. To accomplish this the team will extract key data analysis functions from the LSST data analysis pipeline into a standalone library, independent of the LSST software stack and data access mechanisms. They will integrate this library with the Myria big data management system. Myria is an elastically scalable big data management system that operates as a service in the Amazon cloud that wedeveloped at the University of Washington. Compared with other big data systems, Myria is especially attractive because it integrates PostgreSQL database instances within its storage layer and thus provides access to PostgreSQL's rich libraries of spatial functions, which are frequently used in astronomical data analysis pipelines. At the same time, it has rich support for new and legacy Python code and for complex analytics. By integrating the library of LSST image analytics functions with Myria, new image analytics pipelines will become significantly easier to write. The skeleton of the analysis pipeline will be expressed in the MyriaL declarative query language (i.e. SQL extended with constructs such as iterations and others). The core data processing functions will directly map to Python functions, enabling the reuse of legacy code and the easy addition of new functions. The resulting code will be amenable to optimization and efficient execution using the Myria service. By doing so, they intend to reduce barriers to adoption. Users will be able to express their analysis in Python without worrying about how data and computation will be distributed in a cluster. The image analysis framework developed as part of this proposal will be made publicly available as open-source software. The PIs will utilize the use case of neuroscience to demonstrate how their system, developed for astronomy, can be deployed across multiple domains.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering, the Astronomical Sciences Division and Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences."
"1640864","CIF21 DIBBs: EI: Vizier, Streamlined Data Curation","OAC","DATANET","01/01/2017","04/26/2017","Oliver Kennedy","NY","SUNY at Buffalo","Standard Grant","Amy Walton","12/31/2019","$2,749,699.00","Juliana Freire, Boris Glavic","okennedy@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7726","7433, 8048, 9251","$0.00","Big Data promises to have a positive impact on many aspects of our lives, but assembling the data to answer questions or derive predictive models can be challenging.  Data scientists must typically go through multiple rounds of curation, or 'wrangling,' where data are organized, refined, cleaned up, and merged together before they can be analyzed.  Curation is often slow and costly, but is essential for obtaining useful and trustworthy answers.  This project develops a software tool called Vizier that aims to streamline data curation and enable domain experts who do not have computer science expertise to curate their own data.  Easier curation magnifies the value of big data by enabling a wide range of users to improve data quality, and in doing so benefits numerous types of data-driven work in government, industry, and science.<br/><br/>Vizier features an intuitive interface combining elements of notebooks and spreadsheets, allowing analysts to quickly see, edit, and revise data.  This capability is complemented by a framework for automated data cleaning steps that are seamlessly integrated with manual curation operations.  The heart of Vizier is a system for managing uncertainty and provenance of curation workflows and data, enabling the user to keep track of higher-level curation operations as well as track the lineage of data. By transparently maintaining the history of all the user's actions and their effect on the curated data, Vizier enables regret-free exploration and curation where any changes to the data and their transitive effects can be undone. By learning from past curation histories, the system will also be able to provide users with context-dependent recommendations for additional curation actions.<br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Social, Behavioral and Economic Sciences (Division of Social and Economic Sciences)."
"1550423","SI2-SSI: Collaborative Research: A Robust High-Throughput Ab Initio Computation and Analysis Software Framework for Interface Materials Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes, DMREF","09/01/2016","08/31/2016","Yifei Mo","MD","University of Maryland College Park","Standard Grant","Micah Beck","08/31/2019","$300,000.00","","yfmo@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","1253, 1712, 8004, 8292","7237, 7433, 7569, 8004, 8009, 8400, 9216","$0.00","A set of techniques scientists call ""ab initio"" methods, which are derived from the fundamental laws of physics with minimal assumptions and approximations, has become a critical tool in the study and design of materials. With computing advances and software innovations, the automation of high-throughput ab initio calculations has, in particular, heralded an explosion of computed data for a large variety of materials. However, these high-throughput efforts are limited to specific properties. In contrast, materials interfaces, one of the fastest growing research areas in materials science and engineering, are showing an increasing relevance in many areas of materials applications such as catalysis and electronics. This project will develop a software framework that enables novel high-throughput interface materials investigations and design. The developed software platform will expand the genome of materials by including the computed interfacial properties of interface materials. This community-based software can potentially become a critical component of the Materials Genome Initiative and serve not just the large and diverse materials research community, but also the physics and chemistry communities. Besides featuring heavily in existing and planned courses taught by the Principal Investigators in their home institutions, the proposed framework will facilitate the training of undergraduates and graduates in the ab initio methodologies in other institutions as well. This project will also conduct public outreach activities to increase awareness of the importance of sustainable software development for data-driven interface materials science. <br/><br/>The project will develop necessary workflow management, error correction schemes, and systematic analysis tools to support ab initio studies of thermodynamics, kinetics, diffusion, and electronic property of interface materials including hetero-structures and grain boundary. It targets developmental efforts on three key focus areas of great interest to interface materials science: (i) Ab initio thermodynamics of surfaces and interfaces; ii) Advanced methods for materials kinetics and diffusion at materials interfaces; and iii) Automated algorithms for structural construction of grain boundary and post data-processing and analysis. In doing so, this project will greatly expand the suite of interfacial materials properties that are amenable to a high-throughput ab initio treatment, paving the way for materials investigations and design in a broad spectrum of technological applications, including energy generation and storage, catalysis and electronics. In addition, by interfacing with classical-mechanics simulation codes, this framework will bridge the gap between the ab initio and classical force-field approach, which is expected to significantly advance the high-throughput simulations of materials interfaces. <br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Mathematical and Physical Sciences (Division of Materials Research and Office of Multidisciplinary Activities)."
"1757923","REU Site: Computational and Mathematical Modeling of Complex Systems","OAC","RSCH EXPER FOR UNDERGRAD SITES","03/01/2018","12/14/2017","Cristopher Moore","NM","Santa Fe Institute","Standard Grant","Sushil Prasad","02/28/2021","$323,821.00","","moore@santafe.edu","1399 HYDE PARK ROAD","SANTA FE","NM","875018943","5059462727","CSE","1139","9150, 9250","$0.00","The SFI Research Experiences for Undergraduates (REU) program is a ten-week residential research opportunity in which students develop innovative research projects in collaboration with mentors. The program asks students to discard traditional disciplinary boundaries, and learn computational modeling and data analysis techniques that can apply across the physical, natural, and social sciences. This allows students to ask big questions about real-world systems using rigorous mathematical and computational methods. Projects range from simulation to machine learning to proving theorems. The program supports the goals of science education and diversity in science by emphasizing engagement with students from non-elite institutions with limited research opportunities, women, and under-represented minorities (URMs). Early career scientists act as mentors in the program, gaining valuable experience as educators in mentoring. Research performed by SFI REUs has directly advanced the progress of science, and has focused on solving problems of direct relevance to society and the national health, including vaccination strategies for whooping cough, sustainability, economics of higher education, and social network analysis, among other topics. <br/><br/>In every STEM field, computational and mathematical modeling are rapidly becoming essential skills: translating a real-world system into a quantitative model, designing and coding computational experiments, analyzing these experiments statistically, and comparing their results with data. Projects of this kind are an ideal opportunity for undergraduate training that builds students' technical and analytical skills, connects them with the wider scientific world, and links scientific thinking with real-world contexts and applications. The SFI REU program is designed around the strengths of being a leading transdisciplinary research center. Undergraduates are recruited from multiple departments including computer science, physics, mathematics, biology, and the social sciences, and paired with mentors from many different scientific backgrounds. Recent projects include epidemiology and public health, digital humanities and topic modeling, social network structure, cell biology and proteomics, smart cities and urban data, and statistical physics. Methods utilized range from simulation to data analysis to theorem-proving, and in many cases have produced publishable work. Throughout the summer, students are offered tutorials on the basics of data analysis, algorithms, network theory, statistics, and programming in Python and C++. Tutorials are also offered on science writing, presenting research, applying for jobs in science and picking a graduate school or industry carrer, and dealing with impostor syndrome and implicit bias.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1443047","CIF21 DIBBs: Domain-Aware Management of Heterogeneous Workflows: Active Data Management for Gravitational-Wave Science Workflows","OAC","PHYSICS AT THE INFO FRONTIER, DATANET","10/01/2014","08/16/2018","Duncan Brown","NY","Syracuse University","Continuing grant","Amy Walton","09/30/2019","$1,078,712.00","Ewa Deelman, Jian Qin, Peter Couvares","dabrown@physics.syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7553, 7726","062Z, 7433, 7569, 8048, 8084","$0.00","Analysis and management of large data sets are vital for progress in the data-intensive realm of scientific research and education.  Scientists are producing, analyzing, storing and retrieving massive amounts of data.  The anticipated growth in the analysis of scientific data raises complex issues of stewardship, curation and long-term access. Scientific data is tracked and described by metadata.  This award will fund the design, development, and deployment of metadata-aware workflows to enable the management of large data sets produced by scientific analysis.  Scientific workflows for data analysis are used by a broad community of scientists including astronomy, biology, ecology, and physics. Making workflows metadata-aware is an important step towards making scientific results easier to share, to reuse, and to support reproducibility.  This project will pilot new workflow tools using data from the Laser Interferometer Gravitational-wave Observatory (LIGO), a data-intensive project at the frontiers of astrophysics. The goal of LIGO is to use gravitational waves---ripples in the fabric of spacetime---to explore the physics of black holes and understand the nature of gravity.  <br/><br/>Efficient methods for accessing and mining the large data sets generated by LIGO's diverse gravitational-wave searches are critical to the overall success of gravitational-wave physics and astronomy.  Providing these capabilities will maximize existing NSF investments in LIGO, support new modes of collaboration within the LIGO Scientific Collaboration, and better enable scientists to explain their results to a wider community, including the critical issue of data and analysis provenance for LIGO's first detections.  The interdisciplinary collaboration involved in this project brings together computational and informatics theories and methods to solve data and workflow management problems in gravitational-wave physics.  The research generated from this project will make a significant contribution to the theory and methods in identification of science requirements, metadata modeling, eScience workflow management, data provenance, reproducibility, data discovery and analysis.  The LIGO scientists participating in this project will ensure that the needs of the community are met. The cyberinfrastructure and data-management scientists will ensure that the software products are well-designed and that the work funded by this award is useful to a broader community."
"1261715","CIF21 DIBBs: Long Term Access to Large Scientific Data Sets: The SkyServer and Beyond","OAC","DATANET","10/01/2013","04/28/2017","Alexander Szalay","MD","Johns Hopkins University","Cooperative Agreement","Amy Walton","03/31/2019","$9,949,659.00","Steven Salzberg, Charles Meneveau, Aniruddha Thakar, Randal Burns, Michael Rippin","aszalay1@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7726","7433, 8048","$0.00","The Project aims to create a sustainable collaborative ecosystem built around several large scientific data sets for the broader science community. Based upon the expertise developed for the Sloan Digital Sky Survey (SDSS) SkyServer and the associated projects the Project will formalize the main system components and reengineer them to be much more reusable. <br/><br/>The Project will take full ownership of the Sloan Digital Sky Survey archive and will provide a robust environment for its continued operations, using an economy of scale enabled by common, shared building blocks derived from the existing SDSS SkyServer framework, based upon a large, scalable database system.<br/><br/>Using these building blocks, the team will build and operate open data archives from large observations and numerical simulations, including computational fluid dynamics, ocean circulation and astrophysics, reaching PB scales. The Project will further extend the tools to life sciences, like large-scale, next-generation genome sequencing experiments, as well as high-throughput neuroscience imaging data. The resulting distributed, parallel database framework will be linked to small, user-created data sets that can be used also collaboratively, in conjunction with each other and the large data collections. <br/><br/>The Project will work with selected communities to help deploying and serving data using our building blocks, demonstrating portability, generality and economies of scale; will help and encourage other institutions and communities to use the tools, while seeking collaborations that result in disruptive changes, and will build tools that accelerate the timescale to deploy new services and applications and rapidly test new ideas.<br/><br/>The Project will enable individual users to bring their ""small data"" and analyze it collaboratively in the context of the large data. <br/>Our particular goals are:<br/><br/>   (i)   Take full ownership of the SDSS Archive (database and flat files) and ensure a scalable and robust environment for its continued operation;<br/><br/>   (ii)  Build upon our decade-long effort on SDSS and its ad-hoc spinoffs, through reengineering its components into portable and general building blocks;<br/><br/>   (iii) Systematically address curation issues arising from using a service-oriented architecture (SOA), and the resulting service life-cycle;<br/><br/>   (iv) Work with projects from additional scientific domains to help deploying and serving data using our building blocks, demonstrating portability, generality and economies of scale;<br/>  <br/>   (v) Develop scalable extensions to our database cluster in order to deal with large numerical simulations scaling up to petabytes, and turn them into open numerical laboratories;<br/><br/>   (vi) Use our CasJobs Collaborative Environment to address the problem of small but complex data in the ""Long Tail"" of science."
"1553287","CAREER: Cyberinfrastructure for Realizing Predictions with Uncertainty using Computational Modeling, Data, and Bayesian Inference","OAC","CAREER: FACULTY EARLY CAR DEV","05/15/2016","05/19/2016","Paul Bauman","NY","SUNY at Buffalo","Standard Grant","Sushil K Prasad","04/30/2021","$499,306.00","","pbauman@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1045","1045","$0.00","The progress of science and engineering in the U.S. is now limited by the lack of computational infrastructure to incorporate uncertain information from experimental data into complex predictions made by computer simulation. For example, uncertainty in chemical models directly impacts the design of efficient combustion systems at the core of the U.S. energy and transportation infrastructure. The ability to incorporate uncertainty into computational models will directly advance understanding of how this uncertainty impacts subsequent model predictions and, therefore, our ability to make reliable decisions based on the predictions of these complex models. This work aims to provide such a computational infrastructure that can easily leverage the large scale computing resources deployed in the U.S. The framework developed in this work can enable improvements in the development of computational models by providing methodologies and tools for ascertaining the large sources of uncertainty. Additionally, the framework aims to enable better design of physical experiments by facilitating the identification of experimental scenarios that best inform the computational model. The cyberinfrastructure developed in this work is also aimed to provide the foundation for training the next generation of scientists and engineers to use contemporary computational and data-enabled science tools, theory, and practice. The project, thus is aligned to NSF's mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>The digital environment that this work aims to develop will support the Bayesian inference of unknown parameters in mathematical models based on partial differential equations (PDEs). Specifically, the computational environment will facilitate the creation, experimentation, and examination of all aspects of computational predictions with uncertainty: mathematical models, finite element formulations of PDEs, statistical surrogate models for complex systems, and algorithms for solving the statistical inverse problem. Significant progress in the state-of-the-art of both the solution of statistical inverse problems and the design of physical experiments will be made by easily enabling the presence and examination of uncertainty within complex simulation models. This platform will use modern high performance computing algorithms and be portable to extreme-scale computing infrastructure for the solution of those models. Finally, all elements will be deployed in portable, efficient, and easy-to-use software elements for use by the community at large."
"1839868","CICI: RDP: Open Badge Researcher Credentials for Secure Access to Restricted and Sensitive Data","OAC","Cyber Secur - Cyberinfrastruc","09/01/2018","09/27/2018","Margaret Levenstein","MI","University of Michigan Ann Arbor","Standard Grant","Micah Beck","08/31/2020","$881,342.00","Libby Hemphill, Florian Schaub, Andrea Thomer","maggiel@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8027","","$0.00","This project reduces the complexity in protecting research data by developing and piloting an open badge research credential system (OBRCS). Open badges are visual tokens that signal achievement, affiliation, authorization, or another trust relationship and are shareable across the web. The challenges of managing and protecting restricted data mean that data providers are often wary of sharing sensitive data or that data ends up in the wrong hands, and potential gains to society and science from using those data go unrealized. OBRCS allows researchers to present their evolving credentials openly and to record their achievements and credentials publicly and enables more collaboration, facilitates data re-use, and supports replication efforts. OBRCS benefits the scientific community by ensuring the integrity, resilience, and reliability of research data.<br/><br/>Combining and analyzing collections of data enable scientific breakthroughs. Efficient, secure data sharing and reuse facilitates collaboration and replication, leading to better science. However, managing different access policies, authenticating, and authorizing access to sensitive data is a challenge faced by all data management organizations. Unauthorized access threatens the integrity of data and the privacy of study participants and these threats can impact the conclusions researchers draw. The project achieves these goals through three main activities: (a) develops an open badge system for managing researcher credentials, (b) articulates levels of data sensitivity and risk that indicate criteria for access, and (c) identifies the right balance between openness and privacy for data users in a restricted data access system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829704","CyberTraining: CIC: CyberTraining for Students and Technologies from Generation Z","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Geoffrey Fox","IN","Indiana University","Standard Grant","Sushil Prasad","08/31/2021","$492,283.00","Douglas Swany, Gregor von Laszewski","gcf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","Information technology is playing a dramatically increasing role in society, industry, and research. This includes design and use of large databases, simulations and artificial intelligence applications hosted on clouds and supercomputers with convergent technologies. Correspondingly, there is an increasing need for research workforce job skills in these and related areas. This project takes freshly created Indiana University Engineering course material on this cyberinfrastructure and adapts it for training with an emphasis on the needs of under-represented communities. The techniques of the successful open source software movement are used to create sustainable communities around the course curriculum and software. The project is creating new technologies to enable this for today's generation of students. Skills in core cloud computing, big data, supercomputing and artificial intelligence are exemplified by applications in the life science and nanotechnology areas. This project enables the future research workforce to contribute effectively using advanced cyberinfrastructure, promoting the progress of science and advancing the national health, prosperity, and welfare, which serves the national interest, as stated by NSF's mission. <br/><br/>The future economic progress and research leadership of the U.S. is dependent on having a research workforce that is capable of making use of advanced cyberinfrastructure (CI) resources as articulated by the National Strategic Computing Initiative (NSCI). This requires a curriculum that changes and integrates modern concepts and practices for the new generation of students aiming at a ""data-enabled computational science and engineering"" expertise. This project takes what Indiana University has learned from a brand new four-year undergraduate engineering curriculum designed ab initio and taught so far to its first two undergraduate classes, and invests it into developing active training modules. The innovative curriculum integrates big data, simulations, clouds and high performance computing systems presented in a uniform framework. The course material is customized for communities of cyberinfrastructure researchers nucleated, built, and sustained via the dynamic use of GitHub and enhanced by innovative tools to build a novel learning management system optimized for cyberinfrastructure-intensive classes. The project modules include Cloud Computing, Big Data Applications and Analytics, Networking, High-Performance Computing, Artificial Intelligence/Machine Learning, and Information Visualization. There are residential sessions, with a call for participants, and purely online courses and these have both ""teach the student"" and ""teach the teacher"" modes; the latter enables easy spread of the classes. Hands-on learning with research projects built around the class material is fully supported. The project offers CyberTraining with all of the popular approaches used by the Apache Software Foundation, including Meetups and Hackathons. Modules for domain scientists and engineers, e.g., the cyberinfrastructure users that exploit advanced CI methods for research in nanoengineering and bioengineering are included. Both students and teachers contribute to the course improving the text, the software, including a unique set of examples and the project aims to show that one can build both learning and sustainability communities by using the proven techniques of the open source software community. The project uses proactive measures to enhance the involvement of under-represented communities in its activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1826993","CC* Networking Infrastructure: North Dakota Internet Leadership Level Infrastructure","OAC","Campus Cyberinfrastrc (CC-NIE)","08/01/2018","07/23/2018","Dana Skow","ND","North Dakota State University Fargo","Standard Grant","Kevin L. Thompson","07/31/2020","$495,524.00","Aaron Bergstrom, Marc Wallman","dane.skow@ndsu.edu","Dept 4000 - PO Box 6050","FARGO","ND","581086050","7012318045","CSE","8080","9150","$0.00","This project creates data networking and storage infrastructure dedicated to scientific research and education at North Dakota State University (NDSU) and the University of North Dakota (UND). These network upgrades prioritize science data movement, allowing both institutions to more effectively participate in collaborative research across the globe. The project also prepares both schools for much higher capacity connectivity through North Dakota's research and education (R&E) state network. The networking upgrades enhance efforts to provide a strong and stable workforce equipped with the skills and knowledge necessary to support contemporary advanced research. Examples include the current North Dakota state EPSCoR Track I project, collaborations between researchers at NDSU/UND and the other North Dakota institutions and Tribal College schools, and student internship programs using the research computing facilities on each campus. <br/><br/>100Gbps connectivity is established from campus HPC systems to the campus network border. Science drivers for this project include Precision Agriculture and Digital Agriculture initiatives studying crop and livestock data collected using drone and satellite technologies; multi-campus research collaboration on 100TB class datasets used by UND?s Center for Regional Climate Studies; and predictive design of materials based in NDSU?s Materials and Nanotechnology program. These enable collaborations between researchers in chemistry, biochemistry, computer science, coatings and polymeric materials and mechanical engineering across the globe. The networking upgrades follow the scienceDMZ model and include perfsonar based end-to-end testing capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1541468","CC*DNI CI Engineer: Adapting Research Workflows to Use Campus Cyberinfrastructure","OAC","Campus Cyberinfrastrc (CC-NIE)","01/01/2016","08/31/2016","Larry Conrad","CA","University of California-Berkeley","Continuing grant","Kevin Thompson","12/31/2019","$399,446.00","","larry_conrad@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8080","","$0.00","Research workflows across many domains from life sciences to social and behavioral sciences, are scaling to require more computational analysis of much larger data sets. Workflows that have leveraged local computation and data storage resources must be transformed to use larger shared compute facilities like the UC Berkeley Savio supercomputing cluster, and national centers like NERSC and XSEDE. However, researchers have struggled to connect their data sources (such as laboratory instruments, remote data-sets, etc.) to the shared computation and storage resources in an efficient, productive workflow. Barriers range from poor network throughput, to inexperience with data management and shared computing infrastructure and best practices.<br/><br/>The Berkeley Research Computing (BRC) program is adding cyber-infrastructure engineering support to engage research groups to understand their needs, and to design and implement effective workflows that take advantage of campus and national computation and data management resources. These research engagements transform existing workflows to advance project-specific research goals, but also provide concrete templates that other research groups can leverage, and an architectural pattern for research engagement going forward.<br/><br/>The BRC CI Engineer leverages campus resources including a Science DMZ that connects high-speed state and national network links, including California's Advanced Research and Education Network (CENIC) and the Energy Sciences Network (ESNet), to campus infrastructure, including the BRC Savio supercomputing cluster. The BRC CI Engineer draws upon campus expertise in networking, high-performance computing, and research data management, as well as the domain expertise of campus partners. As research engagements are completed and documented, we will expand faculty understanding of the new scalable workflows, and enable them to address the next generation of research questions."
"1849559","CRII: OAC: A Framework for Parallel Data-Intensive Computing on Emerging Architectures and Astroinformatics Applications","OAC","CRII CISE Research Initiation","03/15/2019","02/14/2019","Michael Gowanlock","AZ","Northern Arizona University","Standard Grant","Sushil K Prasad","02/28/2021","$174,975.00","","michael.gowanlock@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","026Y","062Z, 8228","$0.00","The amount of data that needs to be analyzed by the scientific community is increasing due to growing volumes of data collected by current and future instruments and sensors. Consequently, scientific data analysis requires a significant amount of time. To decrease the amount of time needed to analyze data, methods need to utilize more computational resources, such as more processors in a computer. While the central processing unit (CPU) in a modern computer has traditionally been used to carry out data analysis, the past decade has seen an increase in using graphics processing units (GPUs) for data analysis. The modern graphics processing unit contains thousands of processors that can be used to execute a program faster than on the CPU. However, many algorithms for data analysis do not use the GPU to its full potential within the context of the broader computer system. This project advances a framework for understanding the performance of GPUs as applied to data analysis applications. The major goal of the project is to bridge the gap between algorithms that only use the GPU, and fully integrated algorithms that exploit the strengths of both the CPU and GPU. The ensemble of algorithms that are explored in the project support the needs of the astronomy community and researchers in other scientific areas that require efficient data analysis methods. The project aims to realize a new era in CPU/GPU computing that impacts both computer science and other scientific fields.  An outcome of the project is the development of materials for educators teaching at the intersection of data analysis and parallel computing. This project includes mentoring K-12, undergraduate, and graduate students.  Consequently, the project serves the national interest, as stated by NSF's mission, by promoting the progress of science, and to advance the national health and prosperity. <br/><br/>New cyberinfrastructure, such as data analysis algorithms for emerging heterogeneous architectures, are needed to address cutting-edge scientific problems. Data analysis building blocks and algorithms have many data-dependent performance bottlenecks. New architectures have the potential to alleviate some of these key bottlenecks. However, the majority of GPU research minimally involves the CPU/host, and performs most of the computation on the GPU. This is a missed opportunity to more closely integrate both data and task parallelism between the CPU and GPU to simultaneously exploit concurrency across both architectures. This project examines a selection of key algorithms in the database, machine learning, data mining, and parallel computing communities. Using these algorithms, this project explores the continuum between GPU-only and mixed hybrid parallelism (data and task parallelism between the CPU and GPU) to identify key bottlenecks that can be reduced by exploiting underutilized resources. The selected algorithms are fundamental to scientific data processing workflows, and can advance time-domain astronomy cyberinfrastructure.  The project integrates data-intensive computing insights into courses at the undergraduate and graduate levels, and pedagogical modules are developed to be used by instructors for teaching concepts of mixed (data and task) parallelism across the CPU and GPU. This project includes mentoring students at the undergraduate, graduate, and K-12 levels, including outreach at science festivals to encourage participation and interest in science, technology, engineering, and mathematics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828380","MRI: Acquisition of Cloud Computing Infrastructure for Science and Engineering Research Innovations","OAC","MAJOR RESEARCH INSTRUMENTATION","09/01/2018","08/24/2018","Ajay Katangur","TX","Texas A&M University Corpus Christi","Standard Grant","Stefan Robila","08/31/2020","$517,657.00","Dulal Kar, Scott King, Michael Starek, Jinha Jung","Ajay.Katangur@tamucc.edu","6300 Ocean Drive, Unit 5844","Corpus Christi","TX","784125844","3618253882","CSE","1189","026Z, 1189","$0.00","This award provides for the acquisition of the Island Science and Engineering Research Cloud (ISERC). ISERC will support major innovative research at Texas A&M University-Corpus Christi (TAMUCC) and other institutions in South Texas. ISERC is an ideal base to tackle a multitude of research problems of regional, national and global significance by providing high capacity computing resources to empower science and engineering research. ISERC enabled projects include geospatial analytics, coastal resiliency, ecology, genomics, bio-informatics, environmental monitoring, health, geology, water resource management, remote sensing, unmanned systems, agriculture, and climate prediction. This research will have impact on the environmental sustainability, health and economic well-being of coastal communities across the globe. The instrument will be used by approximately 100 researchers comprising faculty, graduate and undergraduate students. It is estimated that, each year, more than 300 students enrolled in STEM related courses would also benefit from it.<br/><br/>ISERC supports heterogeneous computing environments with low-latency, high-bandwidth, high computational power, and storage for large data sets. It provides users with a dynamically scalable, robust, heterogeneous, and virtualized architecture. By allowing confidential and secure execution of guest virtual machines, it ensures the privacy and security of data. The new cloud instrument can handle a wide variety of services simultaneously, including ones for real-time, compute-bound, as well as data intensive applications. Some of the major projects that are poised to commence include: hyper-spatial 3D data streams for coastal resiliency, high throughput phenotyping for agriculture, social media data analytics for security, cloud data migration, seagrass genetics, biomimetic catalysis, climate impact on water resources, genome sequencing, and predicting environmental factors for respiratory diseases. ISERC will propel the academic community's knowledge generation and provide research opportunities that will enable many further follow-up projects, including multi-institutional and cross-disciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1559894","REU Site: REU Research Experiences and Mentoring in Data-Driven Discovery","OAC","RSCH EXPER FOR UNDERGRAD SITES","04/01/2016","02/29/2016","David Kaeli","MA","Northeastern University","Standard Grant","Sushil K Prasad","03/31/2020","$359,587.00","","kaeli@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1139","9102, 9250","$0.00","Non-Technical:<br/>This REU site will help support NSF's mission to promote the progress of science by providing multi-disciplinary research experience for undergraduates with exciting 10-week summer-based experiences in computer science/engineering laboratories, enabling work on both fundamental and applied data-driven problems, focused on applying machine learning techniques, data analytics, and parallel computing technologies, and preparing them for future scientific career.  The REU students will have the opportunity to utilize state-of-the-art high performance computing resources to tackle big data problems present in a number of problem domains, including Bioengineering, Environmental Engineering and Biomedical Engineering. PI's will leverage the Massachusetts Green High Performance Computing Center located in Holyoke, MA to provide a backdrop for this multi-disciplinary REU Site. The program will provide students with extensive cyber-infrastructure training, equipping program participants with the skillsets to explore the boundaries between math, science, engineering and parallel computing that are truly multi-disciplinary. The REU Site is located at Northeastern University in the heart of Boston.  <br/><br/>Technical:<br/>The REU projects span a wide range of technical fields in Computer Science and Engineering.  Example projects include:  applying machine learning to cluster Chronic Obstructive Pulmonary Disease (COPD) data, data mining Twitter feeds to identify trends in climate change, utilizing hardware accelerators to detect cancerous tumors in 4-D Computed Tomography images, and utilizing regression analysis on gene expressions to understand patterns in retina regeneration.  One novelty of this site is its focus on recruiting efforts, tailoring their research experiences for students that have only completed their freshmen year of their undergraduate education, engaging them early in research on their academic pathway. The REU site works to create a mentoring eco-system at both Northeastern University and the partnering institutions, developing a model that is transferrable to other institutions.  The program builds relationships with six urban community colleges/universities to build a pipeline of students, creating a diverse pool of REU candidates, showing students potential pathways to graduate education and research. The program leverages the CISE REU Toolkit for pre- and post-program surveys, as well as for recruiting. The REU-D3 Site is focused on effecting changes in mentoring attitudes and practices at the host and partnering academic institutions."
"1840218","CICI: RDP: Open Science Chain (OSC) - A Novel Distributed Ledger-Based Framework for Protecting Integrity and Provenance of Research Data","OAC","Cyber Secur - Cyberinfrastruc","09/01/2018","08/17/2018","Subhashini Sivagnanam","CA","University of California-San Diego","Standard Grant","Micah Beck","08/31/2021","$818,433.00","Viswanath Nandigam","sivagnan@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8027","","$0.00","Data sharing is an integral component of scientific research and associated publications. Researchers have the ability to extend and build upon prior research when they are able to efficiently access, validate, and verify the data referenced. Facilitating the future reuse of data in a secure and independently verifiable manner is critical to the advancement of research. Open Science Chain (OSC) allows a broad set of researchers to efficiently share metadata and easily verify authenticity of their scientific datasets in a secure manner, while preserving provenance and lineage information. <br/><br/>OSC is a web based cyberinfrastructure platform built using distributed ledger technologies that allows researchers to provide metadata and verification information about their scientific datasets and update this information as the datasets change and evolve over time in an auditable manner. The researchers are able to search, verify and validate scientific datasets and link datasets to show lineage information. OSC features a web-based portal with user-friendly interfaces for metadata registration, data search and verification capability. OSC has been designed and implemented using real world scientific datasets from a diverse set of use cases ensuring the broad applicability across scientific domains. OSC enables sharing and verification of datasets among wider research communities including large facilities, smaller labs, individual researchers and students while promoting good data documentation practices. OSC increases the confidence of the scientific results and promotes data sharing, which in turn increases productivity and promotes good science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640829","CIF21 DIBBs: PD: - Metadata Toolkits for Building Multi-Faceted Data - Relationship Models","OAC","PLASMA PHYSICS, COMPUTATIONAL PHYSICS, DATANET, EarthCube","10/01/2016","08/03/2016","Martin Greenwald","MA","Massachusetts Institute of Technology","Standard Grant","Amy Walton","09/30/2019","$500,000.00","","g@psfc.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1242, 7244, 7726, 8074","7433, 7569, 8048","$0.00","Scientific research is challenged by ever-larger, more complex data sets, stored in disparate form in complicated repositories, making it difficult to discover useful content.  One reason is the relative scarcity of 'navigational' metadata - metadata that explicitly reveals the multitude of relationships between data elements.  This project develops improved data management tools allowing data managers to create metadata schemas that reveal the multiple and complex relationships existing between data elements. The team develops these tools while collaborating directly with three different research communities:  plasma physics (with the MIT Plasma Science and Fusion Center), ocean monitoring and modeling (with the MIT Department of Earth Sciences) and uncertainty quantification (with the University of Texas Institute for Computational Engineering and Sciences). <br/><br/>The project provides tools that allow data managers to easily develop metadata schemas that represent and expose the multiple and complex relationships that exist between data elements and which are typically not well represented in data systems. Such data elements include data source, provenance, physical properties represented in the data, data versioning, annotation threads, data dictionaries, data catalogs and data shape (which typically determines which applications can consume or display the data), and larger organizational entities such as research campaigns, experimental proposals and research products (e.g., publications, presentations and public databases).  Schemas and data are manipulated through a Representational State Transfer - Application Programming Interface (RESTful API). Relationships among the data are represented as mathematical graph structures that are all built upon a common meta-schema. There is an emphasis on recording the full data lifecycle using a RESTful API and granular data object uniform resource identifier (URI) schema that facilitate instrumenting complex and varied workflows.  A modern web based exploration tool is built upon these technologies in the initial application areas of plasma physics, ocean monitoring and modeling, and uncertainty quantification.  By viewing meta-data and programs more generally as a collection of graphs whose nodes are the data files or records, the project creates a set of programs which can explore these graphs and make the system much more general and easily extensible.  Also, by allowing users to create data objects at any level of specificity, the graphs of which the data is a member can be used to label object groupings.  This ability to represent data relationships would be of use to a broad contingent of the scientific community and could be useful to the scientific enterprise in many domains.  <br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Geosciences, and the NSF Directorate for Mathematical & Physical Sciences (Division of Physics)."
"1649865","Computational Infrastructure for Brain Research: EAGER: Next-Generation Neural Data Analysis (NGNDA) Platform: Massive Parallel Analysis of Multi-Modal Brain Networks","OAC","ETF, IntgStrat Undst Neurl&Cogn Sys","09/01/2016","08/11/2016","Catherine Stamoulis","MA","Children's Hospital Corporation","Standard Grant","William Miller","08/31/2019","$299,999.00","","caterina.stamoulis@childrens.harvard.edu","300 LONGWOOD AVENUE","Boston","MA","021155737","6179192729","CSE","7476, 8624","026Z, 7916, 8089","$0.00","Unprecedented technological advances over the last decade have facilitated investigation of the brain at exquisite levels of spatial-temporal detail. Ambitious goals of large-scale efforts, including those supported by the BRAIN Initiative, include simultaneously measuring from thousands of neurons for long periods of time, and generating very high resolution images of the brain and its activity. However, enormous volumes of data will be produced by these technologies, and grand-scale analyses of these large datasets are virtually impossible to accomplish with currently available computational tools. This project is focused on addressing a number of these computational limitations by developing a novel and broadly accessible Next-Generation Neural Data Analysis (NGNDA) platform to analyze and integrate large volumes of heterogeneous brain data. This is a collaborative effort of neuroscience researchers, algorithm developers, and computing technology experts from Harvard Medical School Research Computing, the Laboratory of Cognitive Neuroscience at Boston Children's Hospital/Harvard Medical School, the Sleep and Inflammatory Systems Laboratory at Beth Israel Deaconess Medical Center/Harvard Medical School and the Epilepsy Divisions at Boston Children's Hospital and the Medical University of South Carolina.  The ultimate goal is to understand not only the healthy brain but also complex diseases and disorders that are affecting progressively larger populations resulting in enormous socioeconomic costs. This project therefore aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>This project is a pioneer effort to develop a new computational infrastructure, NGNDA, which will be specifically designed for collaborative research, to facilitate grand-scale analysis, simulation and modeling of brain connectomes across species. The overarching goal is to develop the means for intelligent parallel processing of big neural data, and efficient estimation of connectomes across scales of neural organization, via implementation of innovative algorithms and dynamic leveraging and integrating of shared institutional and national high performance computing (HPC) resources. The NGNDA platform will be implemented on the Orchestra HPC resource provided by Harvard Medical School Research Computing, and on resources of the Extreme Science and Engineering Discovery Environment (XSEDE) national computing consortium supported by the National Science Foundation. NGNDA will be validated with four very high-dimensional neural datasets, each posing a unique computational challenge. NGNDA aims to facilitate a ""convergence"" approach to Neuroscience research whereby expertise and insights from distinct disciplines are merged with cutting-edge resources and tools for a comprehensive investigation of the brain. The NGNDA computational infrastructure will be accessible to thousands of users and all its algorithms and validation data will be freely available to the community; and NGNDA may also eventually serve as a novel e-learning platform for multifaceted collaborative learning and education of next-generation neuroscientists. NGNDA will also facilitate testing the reproducibility and generalization of research findings.<br/><br/>This Early-concept Grants for Exploratory Research (EAGER) award by the CISE Division of Advanced Cyberinfrastructure is jointly supported by the SBE Division of Behavioral and Cognitive Sciences, with funds associated with the NSF Understanding the Brain activity including for developing national research infrastructure for neuroscience, and alignment with NSF objectives under the National Strategic Computing Initiative."
"1716828","SI2-SSE: GraphPack: Unified Graph Processing with Parallel Boost Graph Library, GraphBLAS, and High-Level Generic Algorithm Interfaces","OAC","Software Institutes","10/01/2016","02/07/2017","Andrew Lumsdaine","WA","University of Washington","Standard Grant","Micah Beck","09/30/2019","$499,386.00","","al75@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8004","026Z, 7433, 7942, 8004, 8005","$0.00","Modeling and simulating physical phenomena with computers is now an important tool for development and discovery in almost all fields of science and engineering.  Joining theory and experimentation, computation is now recognized as the ""third pillar"" of scientific research.  More recently, data analytics (the science of examining raw data with the purpose of drawing conclusions about that information) has emerged as an important computational tool for scientific discovery - a tool that is likely to be as important as, if not more important than, modeling and simulation.  Within the broad domain of data analytics, the use of graphs is a powerful conceptual tool that describes relationships between discrete objects.  Because of the growing importance of data analytics, many research groups have turned their attention to developing new approaches for solving large-scale graph problems.  While the research results in this area have been valuable, the software products that have been produced tend to be limited in scope and/or not of sufficient quality to be reused. This project will address this problem by creation of GraphPack, a comprehensive unified graph library with a coherent user interface and support for multiple state-of-the-art compute platforms. This work will have broad impacts in scientific and engineering application areas, larger social and economic areas depending on graph analytics, and in education. GraphPack will improve the ease of use and broaden the applicability of graph algorithms. Application areas include such diverse areas as knowledge discovery, genomics, proteomics, electronic design automation, forest management, Internet routing, power grid management, and many more.<br/><br/>GraphPack will be a reliable and comprehensive toolkit applicable across a wide variety of problems and architectures that will unleash the capabilities of the community by making the current state of the art readily available. GraphPack will develop a consistent and comprehensive set of abstractions necessary to express a wide variety of (generic) graph algorithms and data structures in the context of unimodal as well as hybrid parallelism. These abstractions will be incorporated as abstract concepts, and selected concrete efficient implementations will be provided. While genericity is an important goal, GraphPack will also provide a simplified user interface for graph algorithms for the situations where simplicity is more important than fully tuned performance. GraphPack will also provide a GraphBLAS interface based on the recent efforts to provide a standardized set of graph operations based on the concepts of linear algebra. By providing multiple interfaces with efficient parallel implementations, GraphPack will enable a wide variety of applications to take advantage of high-performance graph algorithms."
"1450088","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Lyudmila Slipchenko","IN","Purdue University","Standard Grant","Stefan Robila","07/31/2020","$600,000.00","","lslipchenko@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1253, 8004","7433, 8009","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1449723","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Charles Sherrill","GA","Georgia Tech Research Corporation","Standard Grant","Stefan Robila","07/31/2019","$600,000.00","","sherrill@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1253, 8004","7433, 8009","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1450169","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Thomas Crawford","VA","Virginia Polytechnic Institute and State University","Standard Grant","Stefan Robila","07/31/2019","$600,000.00","","crawdad@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1253, 8004","7433, 8009","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1450217","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Mark Gordon","IA","Iowa State University","Standard Grant","Stefan Robila","07/31/2020","$1,200,000.00","Theresa Windus","mark@si.msg.chem.iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","1253, 8004","7433, 8009, 9150","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.  This project is supported by programs in the Division of Chemistry in MPS and the Division of Advanced Cyberinfrastructure in CISE. <br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1450179","Collaborative Research: SI2-SSI: Removing Bottlenecks in High Performance Computational Science","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","08/01/2015","06/24/2015","Todd Martinez","CA","Stanford University","Standard Grant","Stefan Robila","07/31/2019","$600,000.00","","Todd.Martinez@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","1253, 8004","7433, 8009","$0.00","Computational chemistry has become ubiquitous throughout the fields of science and engineering. Among the many uses of computational chemistry codes are to aid in the design of new materials with specific properties, to understand protein folding dynamics related to the human body, to understand the mechanisms of enzyme catalysis to produce biofuels, and to identify and characterize gaseous species in the atmosphere. Because computational chemistry simulations can require large amounts of computer, memory and disk space, one focus of this research is to reduce the demands on computer resources by exploiting methods that can efficiently fragment large molecules into smaller pieces and at the same time take advantage of computers that have many thousands of computer cores. The principal investigators are developers of several computational chemistry programs, each of which has unique functionalities that have taken multiple person-years to develop and implement. In order to minimize further development efforts and to maximize the utility of the unique features, a second key focus of this research will be to develop the ability of each program to access the unique features and data of the other programs. A major bottleneck in the effort to construct high performance computers with more and more compute cores is the power that is required to drive such systems. One way that the research team will address the power issue is to explore the utility of low power architectures, such as graphical processing units for computational chemistry calculations. All of the newly developed codes will be made available to the user community by web download at no cost.<br/><br/>This project presents an integrated computational science approach to very high quality electronic structure and dynamics calculations that will (a) be broadly accessible to both the development and applications communities, (b) have the capability to address problems of great interest, such as the properties of liquids and solvent effects, and photochemical/photobiological dynamics with high accuracy, (c) provide interoperability and sustainability into the foreseeable future, and (d) solve bottlenecks including power consumption using accelerators. A primary goal is to provide to the broad community new, accurate approaches that may be easily used, with a clear path forward to further code improvement and development. As part of the proposed effort, in addition to the usual outlets of journal articles and public presentations, the investigators will organize workshops at prominent national meetings, so that the expertise and software developed will be available to as broad a group of users as possible.  All of the developed codes will be available on the web for easy downloads. The proposed research will develop new paradigms for interoperability, with a focus on the highly popular program suites GAMESS, NWChem, PSI4  and the AIMS dynamics code. Also included will be a cloud-based client-server model, a common quantum chemistry driver and novel data management approaches. The integration of the codes will be accomplished by making use of the combined expertise of the PIs in developing interoperable methods and data interfaces in computational science. In addition, several new methods will be developed, including novel explicit (R12) correlation methods that will be integrated with the most accurate levels of theory: multi-reference and the most accurate and novel coupled cluster methods, as well as the derivation and implementation of analytic derivatives. Applicability to large molecular systems will be made feasible by drawing upon novel fragmentation methods that scale nearly perfectly to the petascale."
"1839746","CICI: SSC: Development of a Secure and Privacy-Preserving Workflow Architecture for Dynamic Data Sharing in Scientific Infrastructures","OAC","Cyber Secur - Cyberinfrastruc","09/01/2018","08/18/2018","Xukai Zou","IN","Indiana University","Standard Grant","Micah Beck","08/31/2021","$599,998.00","Huanmei Wu, Saptarshi Purkayastha","xkzou@cs.iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8027","","$0.00","Scientific cyberinfrastructures embrace collaborative workflows where users can access and share heterogeneous data and computing resources to perform research and education tasks, which catalyze scientific discovery. One such cyberinfrastructure, JetStream, is the first production cloud funded by the NSF for general-purpose science and engineering research and education. Although Jetstream provides basic data storage security and web authentication, its security features do not satisfy the strict requirements involving sensitive data, such as healthcare data with protected health information (PHI). This project builds a secure, holistic and resilient cybersecurity architecture on JetStream so that collaborative research and education projects can share PHI securely between its users.<br/><br/>The secured infrastructure provides comprehensive multi-level protection for the PHI and its workflows through user authentication, fine-tuned data access control, confidentiality, integrity, and traceability. The project implements role-wise passwordless authentication and authorization, cryptography-based hierarchical access control, dual-level key management, and secure digital provenance integrity protection. By employing these, JetStream VMs can guarantee the security, privacy, and integrity of scientific workflows and associated data, thus protecting data and computing resources from internal and external attacks. When applied to healthcare and life-science cyberinfrastructures, it enables sensitive health data to be shared securely, which is an essential requirement for accelerating life science research. The project promotes the use of real clinical data in training to produce enormous educational impacts. The developed secure architecture is generic and applicable to other data and resource sharing environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640867","CIF21 DIBBs: EI: Data Laboratory for Materials Engineering","OAC","DMR SHORT TERM SUPPORT, DATANET","09/01/2016","09/23/2016","Venugopal Govindaraju","NY","SUNY at Buffalo","Standard Grant","Amy Walton","08/31/2020","$2,909,772.00","Krishna Rajan, Thomas Furlani, Srirangaraj Setlur","govind@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1712, 7726","026Z, 7433, 8048, 8400","$0.00","This project directly addresses the goals of the Materials Genome Initiative -- to accelerate the pace of discovery and deployment of advanced material systems.  To obtain insights for the discovery of new materials and to study existing materials, scientists and engineers rely heavily on an ever-growing number of materials research databases and scholarly research publications that date back many decades.  New materials innovation often takes years, sometimes decades, to develop a new material.  The project addresses the challenges through several steps, including automatic extraction of data from relevant electronic publications, storage of the data in formats that support comparison and analysis, development of advanced computational tools to improve the analysis, integration of the tools into a data laboratory to support the discovery of new trends and relationships among materials properties, and to predict new materials with desired properties.<br/><br/>The infrastructure building blocks developed under this project enable researchers to (i) use document processing technologies to process scientific publications and data from scientific databases in materials science to create a knowledge base; (ii) use machine learning technologies to learn from the data in this enhanced knowledge base to address a variety of use cases in materials science and engineering; and (iii) use innovative information retrieval and visualization tools for insightful analysis, facilitating faster discovery of new materials.  The tools will be hosted and disseminated through a web portal built on the HubZero platform, which will also provide users with the ability to query and visualize data, and run simulations and experiments. The data laboratory portal also provides the ability to run simulations and experiments on high-performance computing clusters using the building blocks.   The data laboratory provides a platform for materials informatics, enabling prediction of properties of metal alloys and interaction with the materials discovery and engineering user community.  While the primary target is the interdisciplinary field of materials research, the tools are designed to be domain agnostic as the core technologies can be applied to documents and databases across a broad swath of disciplines to enhance the pace of scientific discoveries.<br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Mathematical & Physical Sciences (Division of Materials Research)."
"1906052","Collaborative Research: SS2-SSI: The Agave Platform: An Open Science-As-A-Service Cloud Platform for Reproducible Science","OAC","Software Institutes","10/01/2018","02/05/2019","Rion Dooley","CA","Chapman University","Standard Grant","Bogdan Mihaila","07/31/2020","$1,240,246.00","","deardooley@gmail.com","One University Drive","Orange","CA","928661099","7146287383","CSE","8004","7433, 8004, 8009","$0.00","In today's data-driven research environment, the ability to easily and reliably access compute, storage, and derived data sources is as much a necessity as the algorithms used to make the actual discoveries. The earth is not shrinking, it is digitizing, and the ability for US researchers to stay competitive in the global research community will increasingly be determined by their ability to reduce the time from theory to discovery.  Over the last 5 years, the open source commercial sector has greatly outpaced the academic research world in its growth and adoption of programming languages, infrastructure design, and interface development. Problems that were primarily academic in nature several years ago are now common in the commercial world. Terms like big data, business intelligence, remote visualization, and streaming event processing, have moved from the classroom to the board room.  However, academic projects are largely unable to take advantage of many today's most popular and widely used open source technologies within the context of their campus and shared research infrastructure. The recently completed, NSF funded, Science Gateway Institute planning project revealed just how far behind many communities are. In a survey of over 26,000 NSF-funded PIs, science gateway developers, and leaders in higher education (i.e., CIOs, CTOs, and others), over 85% of respondents said they needed help adapting existing technologies to realize the needs of their gateway. Another 80% said they needed help simply understanding what technologies were available to them. The research community doesn't just see the gap, they live it. This project seeks to quickly close the capability gap between academic and commercial infrastructure by extending and making robust the Agave Platform, an open, Science-as-a-Service cloud platform for reproducible science. Essentially, this project will allow scientists to focus their energies on their science rather than so much on the computing technologies they use. <br/><br/><br/>This Agave Platform will build upon the success of the existing Agave Developer APIs which currently serve over 20,000 users in the plant biology community. This project includes three well-defined efforts which will synergistically evolve the current technology into a sustainable Science-as-a-Service platform for the national research community. First,it will extend the Agave Developer APIs with additional services and management interfaces to create a cohesive, self-provisioning Agave Platform which will enable Science-as-a-Service to the developer community. Second, the project team will partner with commercial and academic institutions to create a community driven Application Exchange (AX) based on Docker container technology to facilitate application transparency, portability, attribution, and reproducibility. Third, the project will consolidate existing open source contributions from projects already with the Agave ecosystem into Agave ToGo, a collection of reference science gateways in multiple languages and web frameworks. The Agave Platform will democratize access to software and infrastructure across all areas of science and engineering by modernizing the mechanisms with which the research community can utilize and access academic research infrastructure. This will bridge the gap between industrial and academic research infrastructure and allow researchers to use a new generation of open source software and technologies. The AX will enable greater interoperability and accountability in the way computational science results are published and reviewed. Through the matching investment of industrial partners, reproducibility, best practices, and rigorous scientific review will be brought to the mainstream and promoted as a fundamental aspect of the scientific process in an open, sustainable way. Agave ToGo will make custom gateways readily available to end users and developers alike. For end users, it will empower them to focus on domain science rather than computer science. For developers, it will stimulate innovation and increase the opportunity for discovery. When combined with the Agave Platform and Application Exchange, Agave ToGo will enable novice users to create scalable, reproducible, digital labs that span their office, commercial cloud, and national data centers in a matter of minutes."
"1450459","Collaborative Research: SS2-SSI: The Agave Platform: An Open Science-As-A-Service Cloud Platform for Reproducible Science","OAC","Software Institutes","08/01/2015","08/14/2015","Rion Dooley","TX","University of Texas at Austin","Standard Grant","Bogdan Mihaila","03/31/2019","$2,979,712.00","John Fonner","deardooley@gmail.com","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","7433, 8004, 8009","$0.00","In today's data-driven research environment, the ability to easily and reliably access compute, storage, and derived data sources is as much a necessity as the algorithms used to make the actual discoveries. The earth is not shrinking, it is digitizing, and the ability for US researchers to stay competitive in the global research community will increasingly be determined by their ability to reduce the time from theory to discovery.  Over the last 5 years, the open source commercial sector has greatly outpaced the academic research world in its growth and adoption of programming languages, infrastructure design, and interface development. Problems that were primarily academic in nature several years ago are now common in the commercial world. Terms like big data, business intelligence, remote visualization, and streaming event processing, have moved from the classroom to the board room.  However, academic projects are largely unable to take advantage of many today's most popular and widely used open source technologies within the context of their campus and shared research infrastructure. The recently completed, NSF funded, Science Gateway Institute planning project revealed just how far behind many communities are. In a survey of over 26,000 NSF-funded PIs, science gateway developers, and leaders in higher education (i.e., CIOs, CTOs, and others), over 85% of respondents said they needed help adapting existing technologies to realize the needs of their gateway. Another 80% said they needed help simply understanding what technologies were available to them. The research community doesn't just see the gap, they live it. This project seeks to quickly close the capability gap between academic and commercial infrastructure by extending and making robust the Agave Platform, an open, Science-as-a-Service cloud platform for reproducible science. Essentially, this project will allow scientists to focus their energies on their science rather than so much on the computing technologies they use. <br/><br/><br/>This Agave Platform will build upon the success of the existing Agave Developer APIs which currently serve over 20,000 users in the plant biology community. This project includes three well-defined efforts which will synergistically evolve the current technology into a sustainable Science-as-a-Service platform for the national research community. First,it will extend the Agave Developer APIs with additional services and management interfaces to create a cohesive, self-provisioning Agave Platform which will enable Science-as-a-Service to the developer community. Second, the project team will partner with commercial and academic institutions to create a community driven Application Exchange (AX) based on Docker container technology to facilitate application transparency, portability, attribution, and reproducibility. Third, the project will consolidate existing open source contributions from projects already with the Agave ecosystem into Agave ToGo, a collection of reference science gateways in multiple languages and web frameworks. The Agave Platform will democratize access to software and infrastructure across all areas of science and engineering by modernizing the mechanisms with which the research community can utilize and access academic research infrastructure. This will bridge the gap between industrial and academic research infrastructure and allow researchers to use a new generation of open source software and technologies. The AX will enable greater interoperability and accountability in the way computational science results are published and reviewed. Through the matching investment of industrial partners, reproducibility, best practices, and rigorous scientific review will be brought to the mainstream and promoted as a fundamental aspect of the scientific process in an open, sustainable way. Agave ToGo will make custom gateways readily available to end users and developers alike. For end users, it will empower them to focus on domain science rather than computer science. For developers, it will stimulate innovation and increase the opportunity for discovery. When combined with the Agave Platform and Application Exchange, Agave ToGo will enable novice users to create scalable, reproducible, digital labs that span their office, commercial cloud, and national data centers in a matter of minutes."
"1450437","Collaborative Research: SS2-SSI: The Agave Platform: An Open Science-As-A-Service Cloud Platform For Reproducible Science","OAC","Software Institutes","08/01/2015","06/21/2016","James Lupo","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Bogdan Mihaila","07/31/2020","$116,152.00","Kathryn Traxler","jalupo@cct.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","8004","7433, 8004, 8009, 9150","$0.00","In today's data-driven research environment, the ability to easily and reliably access compute, storage, and derived data sources is as much a necessity as the algorithms used to make the actual discoveries. The earth is not shrinking, it is digitizing, and the ability for US researchers to stay competitive in the global research community will increasingly be determined by their ability to reduce the time from theory to discovery.  Over the last 5 years, the open source commercial sector has greatly outpaced the academic research world in its growth and adoption of programming languages, infrastructure design, and interface development. Problems that were primarily academic in nature several years ago are now common in the commercial world. Terms like big data, business intelligence, remote visualization, and streaming event processing, have moved from the classroom to the board room.  However, academic projects are largely unable to take advantage of many today's most popular and widely used open source technologies within the context of their campus and shared research infrastructure. The recently completed, NSF funded, Science Gateway Institute planning project revealed just how far behind many communities are. In a survey of over 26,000 NSF-funded PIs, science gateway developers, and leaders in higher education (i.e., CIOs, CTOs, and others), over 85% of respondents said they needed help adapting existing technologies to realize the needs of their gateway. Another 80% said they needed help simply understanding what technologies were available to them. The research community doesn't just see the gap, they live it. This project seeks to quickly close the capability gap between academic and commercial infrastructure by extending and making robust the Agave Platform, an open, Science-as-a-Service cloud platform for reproducible science. Essentially, this project will allow scientists to focus their energies on their science rather than so much on the computing technologies they use. <br/><br/><br/>This Agave Platform will build upon the success of the existing Agave Developer APIs which currently serve over 20,000 users in the plant biology community. This project includes three well-defined efforts which will synergistically evolve the current technology into a sustainable Science-as-a-Service platform for the national research community. First,it will extend the Agave Developer APIs with additional services and management interfaces to create a cohesive, self-provisioning Agave Platform which will enable Science-as-a-Service to the developer community. Second, the project team will partner with commercial and academic institutions to create a community driven Application Exchange (AX) based on Docker container technology to facilitate application transparency, portability, attribution, and reproducibility. Third, the project will consolidate existing open source contributions from projects already with the Agave ecosystem into Agave ToGo, a collection of reference science gateways in multiple languages and web frameworks. The Agave Platform will democratize access to software and infrastructure across all areas of science and engineering by modernizing the mechanisms with which the research community can utilize and access academic research infrastructure. This will bridge the gap between industrial and academic research infrastructure and allow researchers to use a new generation of open source software and technologies. The AX will enable greater interoperability and accountability in the way computational science results are published and reviewed. Through the matching investment of industrial partners, reproducibility, best practices, and rigorous scientific review will be brought to the mainstream and promoted as a fundamental aspect of the scientific process in an open, sustainable way. Agave ToGo will make custom gateways readily available to end users and developers alike. For end users, it will empower them to focus on domain science rather than computer science. For developers, it will stimulate innovation and increase the opportunity for discovery. When combined with the Agave Platform and Application Exchange, Agave ToGo will enable novice users to create scalable, reproducible, digital labs that span their office, commercial cloud, and national data centers in a matter of minutes."
"1659403","CC* Integration: SANDIE: SDN-Assisted NDN for Data Intensive Experiments","OAC","CISE RESEARCH RESOURCES","07/01/2017","06/19/2017","Edmund Yeh","MA","Northeastern University","Standard Grant","Deepankar Medhi","06/30/2019","$1,000,000.00","Harvey Newman, Christos Papadopoulos","eyeh@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","2890","","$0.00","Advancing discovery in many scientific fields depends crucially on our ability<br/>to extract the wealth of knowledge buried in massive datasets whose scale and complexity continue<br/>to grow exponentially with time. In order to address this fundamental challenge, this project will<br/>develop and deploy SANDIE, a Named Data Networking (NDN) architecture supported by advanced<br/>Software Defined Network services for Data Intensive Science, with the Large Hadron Collider (LHC) high energy<br/>physics program as the leading use case. <br/><br/>The implementation of SANDIE will leverage two state of the art testbeds: the NDN testbed hosted and serving the climate<br/>science community at Colorado State, and the SDN testbed hosted at Caltech. Building on these facilities, and the support for SDN services<br/>From multiple advanced Research & Education network partners,<br/>we will deploy a set of ten high performance, relatively low cost NDN edge caches with SSDs<br/>and 40G or 100G network interfaces at six participating sites: Caltech, Northeastern, UCSD,<br/>University of Florida, MIT and CERN, together with an existing cache at CSU."
"1450413","Collaborative Research: SS2-SSI: The Agave Platform: An Open Science-As-A-Service Cloud Platform for Reproducible Science","OAC","Software Institutes","08/01/2015","08/14/2015","Gwen Jacobs","HI","University of Hawaii System","Standard Grant","Bogdan Mihaila","07/31/2019","$804,045.00","","gwenj@hawaii.edu","2440 Campus Road, Box 368","Honolulu","HI","968222234","8089567800","CSE","8004","7433, 8004, 8009, 9150","$0.00","In today's data-driven research environment, the ability to easily and reliably access compute, storage, and derived data sources is as much a necessity as the algorithms used to make the actual discoveries. The earth is not shrinking, it is digitizing, and the ability for US researchers to stay competitive in the global research community will increasingly be determined by their ability to reduce the time from theory to discovery.  Over the last 5 years, the open source commercial sector has greatly outpaced the academic research world in its growth and adoption of programming languages, infrastructure design, and interface development. Problems that were primarily academic in nature several years ago are now common in the commercial world. Terms like big data, business intelligence, remote visualization, and streaming event processing, have moved from the classroom to the board room.  However, academic projects are largely unable to take advantage of many today's most popular and widely used open source technologies within the context of their campus and shared research infrastructure. The recently completed, NSF funded, Science Gateway Institute planning project revealed just how far behind many communities are. In a survey of over 26,000 NSF-funded PIs, science gateway developers, and leaders in higher education (i.e., CIOs, CTOs, and others), over 85% of respondents said they needed help adapting existing technologies to realize the needs of their gateway. Another 80% said they needed help simply understanding what technologies were available to them. The research community doesn't just see the gap, they live it. This project seeks to quickly close the capability gap between academic and commercial infrastructure by extending and making robust the Agave Platform, an open, Science-as-a-Service cloud platform for reproducible science. Essentially, this project will allow scientists to focus their energies on their science rather than so much on the computing technologies they use. <br/><br/><br/>This Agave Platform will build upon the success of the existing Agave Developer APIs which currently serve over 20,000 users in the plant biology community. This project includes three well-defined efforts which will synergistically evolve the current technology into a sustainable Science-as-a-Service platform for the national research community. First,it will extend the Agave Developer APIs with additional services and management interfaces to create a cohesive, self-provisioning Agave Platform which will enable Science-as-a-Service to the developer community. Second, the project team will partner with commercial and academic institutions to create a community driven Application Exchange (AX) based on Docker container technology to facilitate application transparency, portability, attribution, and reproducibility. Third, the project will consolidate existing open source contributions from projects already with the Agave ecosystem into Agave ToGo, a collection of reference science gateways in multiple languages and web frameworks. The Agave Platform will democratize access to software and infrastructure across all areas of science and engineering by modernizing the mechanisms with which the research community can utilize and access academic research infrastructure. This will bridge the gap between industrial and academic research infrastructure and allow researchers to use a new generation of open source software and technologies. The AX will enable greater interoperability and accountability in the way computational science results are published and reviewed. Through the matching investment of industrial partners, reproducibility, best practices, and rigorous scientific review will be brought to the mainstream and promoted as a fundamental aspect of the scientific process in an open, sustainable way. Agave ToGo will make custom gateways readily available to end users and developers alike. For end users, it will empower them to focus on domain science rather than computer science. For developers, it will stimulate innovation and increase the opportunity for discovery. When combined with the Agave Platform and Application Exchange, Agave ToGo will enable novice users to create scalable, reproducible, digital labs that span their office, commercial cloud, and national data centers in a matter of minutes."
"1827210","CC* Network Design and Implementation for Small Institutions: Rural Campus Connectivity for Research and Teaching on the Prairie","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/25/2018","Channon Visscher","IA","Dordt College","Standard Grant","Kevin Thompson","06/30/2020","$362,589.00","Nathan Tintle, Nick Breems, Kari Sandouka, Brian Van Donselaar","cvisscher@dordt.edu","498 4th Avenue NE","Sioux Center","IA","512501606","7127226264","CSE","8080","","$0.00","This project provides cyberinfrastructure upgrades at Dordt College for an Internet2 connection and enhanced network connectivity between campus locations, data sources, and computing resources. Through the application of data-intensive multi-omics and biostatistics research (such as high-throughput genotyping and phenotyping) at agricultural and prairie sites, these upgrades are designed to equip ongoing research to better understand, first hand, land-use impacts on local ecology and microbiome, prairie genetics, soil erosion, nutrient uptake and loss, carbon sequestration, drought susceptibility, and water quality. The Rural Campus Connectivity for Research and Teaching on the Prairie project also connects Dordt College to national research and education resources and enhances its STEM education impact by providing cutting-edge research, teaching, and outreach opportunities through collaborative, interdisciplinary, and data-driven science for undergraduate students and STEM teachers-in-training.<br/> <br/>The primary goal of this cyberinfrastructure project is the installation and connection of a fiber optic line from the rurally-situated Dordt College campus to an Internet2 connection point. Campus network upgrades also include the development of a Science DMZ and associated services such as Federated ID Management, data transfer management, performance monitoring through the implementation of a PerfSONAR measurement node, and wireless connectivity to a rural satellite campus. A new network switch will be used as the hub of the 10Gbps DMZ to connect local storage, compute clusters and instrumentation to the existing campus network and Internet2.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1451018","IRNC: Backbone: AmLight Express and Protect (ExP)","OAC","INTERNATIONAL RES NET CONNECT","04/01/2015","03/06/2019","Julio Ibarra","FL","Florida International University","Cooperative Agreement","Kevin Thompson","03/31/2020","$5,739,800.00","Heidi L. Morgan, Donald Cox","julio@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7369","116E, 5913, 5974, 9251","$0.00","Science research and education activities between the U.S. and nations in South America have been evolving, benefiting from the shared investments between the U.S., Brazil and other nations in Latin America.   Astronomy, High-Energy Physics, Comparative research in Biodiversity, Ecology and Genomics, Materials Science, Seismology, Future Internet Research, Ultra-High Definition (UHD) video  and streaming research collectively represent a suite of collaborative science research communities, with resources and cyberinfrastructure geographically dispersed across the U.S. and the nations of Latin America with diverse network services requirements.  As data production of science applications increases, so to is the demand for high-throughput production network services to transport data from where it is collected, processed, then archived.<br/><br/>In response to the network requirements of these U.S.-Latin America collaborative science research communities, the AmLight Express and Protect (ExP) implements a hybrid network strategy that combines optical spectrum (Express) and leased capacity (Protect) that builds a reliable, leading-edge diverse network infrastructure for research and education. Researchers will be able to leverage the resources of AmLight ExP to foster network innovation and to address increasing network services requirements between the U.S. and the nations in South America.<br/><br/>AmLight ExP is a reliable, leading-edge infrastructure for research and education. With significant investments from the Academic Network of So Paulo (ANSP), and Rede Nacional de Ensino e Pesquisa (RNP) and the Association of Universities for Research in Astronomy (AURA), the total bandwidth provided by AmLight ExP between the U.S. and South America is expected to grow to more than 680 Gibabits per second in aggregate capacity between 2015 and 2020.  This serves as a flexible inter-regional infrastructure, enabling communities of scientists to expand their research, education, and learning activities uniquely empowered through access to unlit optical spectrum on submarine cables, and through AmLight ExP's use of dynamic circuits in a production environment. AmLight ExP increases the rate of discovery in the U.S. and across the Western Hemisphere. Faster discovery means quicker focus on the greatest benefit for society. AmLight ExP acts as a catalyst for new communities of researchers and learners with a bridge linking Latin Americans of the Western Hemisphere, benefiting U.S Hispanic students, teachers and researchers. FIU is committed to serving the needs graduate and undergraduate education through models that bring together students and the networking community with scientists from all domains."
"1827126","CC* Integration: SENSELET: Sensory Network Infrastructure for Scientific Laboratory Environments","OAC","CISE RESEARCH RESOURCES","10/01/2018","06/29/2018","Klara Nahrstedt","IL","University of Illinois at Urbana-Champaign","Standard Grant","Deepankar Medhi","09/30/2020","$500,000.00","Roy Campbell, Kenton McHenry, John Dallesasse, Tracy Smith","klara@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","2890","9102","$0.00","Scientific instruments (e.g., scanning electron microscopes) are extensively used to<br/>discover new materials, develop novel semiconductor device fabrication recipes, and perform new biological processes.<br/>One way to speed up scientific discoveries is to provide scientists with advanced cyber-infrastructures to capture, transmit,<br/>store, share, analyze, and correlate as much environmental metadata (e.g., humidity, temperature)<br/>from scientific lab environments as possible. Current network infrastructure does not capture<br/>any external wireless sensory data around the instruments.<br/>The recent advent of low-cost, cloud-based sensors and the introduction of diverse wireless network<br/>technologies, low-cost mobile and personal devices, and Internet of Things (IoT) solutions provide a<br/>novel and viable path for automating sensory data collection in diverse science laboratory environments.<br/><br/>SENSELET, a SEnsory Network infrastructure for SciEntific Lab EnvironmenTs, has the goals of (a)<br/>deploying a diverse wireless and scalable sensory infrastructure close to scientific instruments, and (b)<br/>correlating and synchronizing sensory data with cloud-based instrument data and metadata in real-time<br/>and on-demand. The SENSELET infrastructure will provide additional measurements that will increase<br/>accuracy of scientific results, and enable better environmental monitoring and control of labs for lab<br/>managers. The SENSELET infrastructure will include (a) wireless sensors such as humidity, temperature,<br/>and vibration sensors; (b) an edge computing device with multiple wireless communication interfaces<br/>residing in the lab; and (c) private cloud computing service to store<br/>and correlate sensory data with instrument data in real-time or on-demand. SENSELET will<br/>provide trusted and real-time instrument data uploading, curation, search, and coordination services.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854312","CIF21 DIBBs: PD: Cyberinfrastructure Tools for Precision Agriculture in the 21st Century","OAC","HYDROLOGIC SCIENCES, DATANET","06/01/2018","09/18/2018","Michela Taufer","TN","University of Tennessee Knoxville","Standard Grant","Amy Walton","06/30/2020","$438,105.00","","taufer@utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","1579, 7726","7433, 8048, 9150","$0.00","This interdisciplinary project applies computer science approaches and computational resources to large multidimensional environmental datasets, and synthesizes this information into finer resolution, spatially explicit products that can be systematically analyzed with other variables.  The main emphasis is ecoinformatics, a branch of informatics that analyzes ecological and environmental science variables such as information on landscapes, soils, climate, organisms, and ecosystems.  The project focuses on synthesis/computational approaches for producing high-resolution soil moisture datasets, and the pilot application is precision agriculture. The effort combines analytical geospatial approaches, machine learning methods, and high performance computing (HPC) techniques to build cyberinfrastructure tools that can transform how ecoinformatics data is analyzed.<br/><br/>The investigators build upon publicly available data collections (soil moisture datasets, soil properties datasets, and topography datasets) to develop: (1) tools based on machine-learning techniques to downscale coarse-grained data to fine-grained datasets of soil moisture information; (2) tools based on HPC techniques to estimate the degree of confidence and the probabilities associated with the temporal intervals within which soil-moisture-base changes, trends, and patterns occur; and (3) data- and user- interfaces integrating data preprocessing to deal with data heterogeneity and inaccuracy, containerized environments to assure portability, and modeling techniques to represent temporal and spatial patterns of soil moisture dynamics. The tools will inform precision agriculture through the generation and use of unique information on soil moisture for the coterminous United States.  Accessibility for field practitioners (e.g., local soil moisture information) is made possible through lightweight virtualization, mobile devices, and web applications.<br/> <br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Earth Sciences within the NSF Directorate for Geosciences."
"1663954","Collaborative Research: SI2-SSI: Inquiry-Focused Volumetric Data Analysis Across Scientific Domains: Sustaining and Expanding the yt Community","OAC","PETASCALE - TRACK 1, Software Institutes","10/01/2017","03/26/2019","Leigh Orf","WI","University of Wisconsin-Madison","Standard Grant","Stefan Robila","09/30/2023","$279,866.00","","leigh.orf@ssec.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7781, 8004","7433, 7781, 8004, 8009","$0.00","Scientific discovery across the physical sciences is increasingly dependent on the analysis of volumetric - or three-dimensional - data, that may come from a supercomputer simulation, direct measurement, or mathematical models. Researchers typically seek to extract meaningful insights from this data by visualizing and analyzing it in various ways. The ways in which scientists process volumetric data are actually quite similar across domains, but cross-disciplinary knowledge transfer and tool development is blocked by barriers of terminology. This project seeks to enhance an analysis and visualization toolkit named yt that is currently primarily used for astrophysical simulations. yt allows scientists to access and analyze data at several different levels by providing an interface that is designed to answer questions motivated by the underlying scientific problem, while worrying less about details such data formats, specific analysis techniques etc. yt's utilization in computational astrophysics has dramatically increased access to advanced algorithms for both visualization and analysis, and fostered the growth of a community of researchers sharing techniques and results. This project seeks to make yt available and adopted by scientists in other domains of science thus reproducing its success in astrophysics in these other science domains. This project will expand the yt community beyond theoretical astrophysics and enable and promote collaboration and advanced data analysis in the fields of meteorology, seismology and global tomography, observational astronomy, hydrology and oceanography, and plasma physics. <br/><br/>Improvements to the yt project will proceed along four principal technical avenues. The first is to develop a system that adapts the way yt presents data via a set of domain contexts that encode the ontology, domain-specific vocabulary, and common analysis tasks for a given field of study. This will include creating a domain context system as well as a set of five pilot domain contexts developed in collaboration with domain practitioners. The second is to overhaul the yt field system, adding more versatility and enabling significant optimizations. Thirdly, the project team will implement non-spatial indexing schemes, providing methods for accessing and analyzing data that is not organized according to the standard spatial axes. The final improvement will be the development of a non-local analysis system, allowing generalized path traversal as well as domain convolutions. To ensure wide dissemination and use of these improved capabilities, the team will design domain-specific documentation and training materials, and organize outreach and training events for early-career researchers. This will consist of both hands-on technical workshops and curricula developed in collaboration with Data Carpentry for utilization at other institutions. This combination of technical developments and social investments has been designed to ensure both readiness of the software and engagement of the targeted research communities."
"1663914","Collaborative Research: SI2-SSI: Inquiry-Focused Volumetric Data Analysis Across Scientific Domains: Sustaining and Expanding the yt Community","OAC","Software Institutes","10/01/2017","07/28/2017","Matthew Turk","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","09/30/2022","$1,061,721.00","Nathan Goldbaum","mjturk@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004","7433, 8004, 8009","$0.00","Scientific discovery across the physical sciences is increasingly dependent on the analysis of volumetric - or three-dimensional - data, that may come from a supercomputer simulation, direct measurement, or mathematical models. Researchers typically seek to extract meaningful insights from this data by visualizing and analyzing it in various ways. The ways in which scientists process volumetric data are actually quite similar across domains, but cross-disciplinary knowledge transfer and tool development is blocked by barriers of terminology. This project seeks to enhance an analysis and visualization toolkit named yt that is currently primarily used for astrophysical simulations. yt allows scientists to access and analyze data at several different levels by providing an interface that is designed to answer questions motivated by the underlying scientific problem, while worrying less about details such data formats, specific analysis techniques etc. yt's utilization in computational astrophysics has dramatically increased access to advanced algorithms for both visualization and analysis, and fostered the growth of a community of researchers sharing techniques and results. This project seeks to make yt available and adopted by scientists in other domains of science thus reproducing its success in astrophysics in these other science domains. This project will expand the yt community beyond theoretical astrophysics and enable and promote collaboration and advanced data analysis in the fields of meteorology, seismology and global tomography, observational astronomy, hydrology and oceanography, and plasma physics. <br/><br/>Improvements to the yt project will proceed along four principal technical avenues. The first is to develop a system that adapts the way yt presents data via a set of domain contexts that encode the ontology, domain-specific vocabulary, and common analysis tasks for a given field of study. This will include creating a domain context system as well as a set of five pilot domain contexts developed in collaboration with domain practitioners. The second is to overhaul the yt field system, adding more versatility and enabling significant optimizations. Thirdly, the project team will implement non-spatial indexing schemes, providing methods for accessing and analyzing data that is not organized according to the standard spatial axes. The final improvement will be the development of a non-local analysis system, allowing generalized path traversal as well as domain convolutions. To ensure wide dissemination and use of these improved capabilities, the team will design domain-specific documentation and training materials, and organize outreach and training events for early-career researchers. This will consist of both hands-on technical workshops and curricula developed in collaboration with Data Carpentry for utilization at other institutions. This combination of technical developments and social investments has been designed to ensure both readiness of the software and engagement of the targeted research communities."
"1550514","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/26/2018","Gabrielle Allen","IL","University of Illinois at Urbana-Champaign","Continuing grant","Micah Beck","06/30/2020","$375,000.00","Matthew Turk","gdallen@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7244, 8004","7433, 7569, 8009, 8084","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1550405","INSPIRE: Quantitative Estimation of Space-Time Processes in Volumetric Data (QUEST)","OAC","OFFICE OF MULTIDISCIPLINARY AC, PHYSICAL & DYNAMIC METEOROLOGY, INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE, PHYSICS OF LIVING SYSTEMS, INSPIRE","01/01/2016","08/24/2015","Lawrence Frank","CA","University of California-San Diego","Standard Grant","William Miller","12/31/2019","$999,589.00","Joshua Wurman, Vitaly Galinsky, Leigh Orf","lfrank@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","1253, 1525, 1640, 7231, 7246, 8078","4444, 8091, 8653","$0.00","This INSPIRE project is jointly funded by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science, Physics of Living Systems in the Division of Physics in the Directorate for Math and Physical Science, Physical and Dynamic Meteorology in the Division of Atmospheric and Geospace Sciences in the Directorate for Geoscience, and the INSPIRE program in the Office of Integrative Activities.<br/><br/>Advances in scientific instrumentation and computational hardware and software have resulted in an unprecedented ability to acquire, simulate, and visualize time resolved three-dimensional (3D) volumes of data, offering the promise of a greater understanding of complex systems previously beyond our technical grasp.  However, as the size and complexity of these data increase, analyzing them becomes increasingly problematic, inhibiting scientific discovery and limiting the utility of the data acquired at great expense and effort.  Two particularly cogent examples come from two seemingly disparate scientific fields: neuroscience and meteorology.  Magnetic resonance imaging (MRI) scanners can now acquire functional MRI (FMRI) volumes of brain activity in almost real time, while mobile Doppler radar (MDR) systems are capable of acquiring time-dependent volumetric images of thunderstorms during tornado formation.   In this project, entitled QUantitative Estimation of Space-Time processes in volumetric data (QUEST), the University of California, San Diego, Center for Scientific Computation in Imaging (CSCI), in partnership with the Center for Severe Weather Research (CSWR) and the Cooperative Institute for Meteorological Satellite Studies (CIMSS) will develop a novel framework for the analysis of time-varying 3D volumes, guided by large scale numerical simulations, to investigate two of the outstanding scientific questions of our age: What is the relationship between brain structure and function?, and How do strong, long-track tornadoes form?  The resulting computational platform will be disseminated to the NSF community through the open source analysis and visualization platform (STK) to improve the ability of researchers to quantitatively analyze, visualize, and explore complex time varying volumetric datasets. <br/><br/>This INSPIRE project develops advanced methods for automated quantitative characterization of subtle space-time patterns embedded within spatio-temporal data from 3D voxel-based digital imaging modalities based upon the team's recently formulated entropy field decomposition (EFD) theory, a probabilistic method efficiently that employs the information field theoretic approach with prior information supplied using the team's entropy spectrum pathways theory, in conjunction with numerical simulations designed both to constrain results to physically realizable solutions.  The cross-disciplinary approach focuses on two outstanding problems in the respective fields of neuroscience and severe weather meteorology: 1) The identification of structural and functional modes of the human brain from high resolution anatomical MRI, diffusion tensor MRI, functional MRI data from the Human Connectome Project combined with numerical simulations of diffusion and functionally weighted MRI signals, and 2) The identification of signatures of tornado genesis and maintenance from MDR data from the Doppler-On-Wheels network in conjunction with tornado simulations using the CM1 model.  Significant social impact would result from the ability to categorize states of brain activity in normal and diseased populations and the ability to reduce the lead time between tornado formation and warning to threatened populations.  More generally, this novel methodology has the potential to transform the way analysis is conducted in a wide range of disciplines by enabling automated, quantitative detection of important, though perhaps subtle, variations in large, complex datasets undetectable by current traditional techniques."
"1659300","CC*Data: National Cyberinfrastructure for Scientific Data Analysis at Scale (SciDAS)","OAC","DATANET, EPSCoR Co-Funding","02/15/2017","02/07/2017","Frank Feltus","SC","Clemson University","Standard Grant","Amy Walton","01/31/2020","$2,952,217.00","Melissa Smith, Stephen Ficklin, Ray Idaszak, Claris Castillo","ffeltus@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","7726, 9150","7433, 9150","$0.00","Scientific discovery is increasingly dependent on huge datasets that require computing at unprecedented scale.  Laboratory computers and spreadsheets simply cannot handle the data flowing from modern measurement devices, such as DNA sequencers.   Scientific experiments now require understanding of both the underlying science and the cyberinfrastructure (CI) ecosystem to design and execute necessary computations.  Fortunately, significant and strategic support from the public and private sectors is creating a distributed computational ecosystem at the national level to help meet the computational demands of large datasets.  This project, the Scientific Data Analysis at Scale (SciDAS) is designed to improve flexibility and accessibility to national resources, helping researchers more effectively use a broader array of these resources.  SciDAS is developed using large-scale systems biology and hydrology datasets, but is extensible to many other domains.<br/><br/>On a technical level, SciDAS federates access to multiple national CI resources including NSF Cloud, Open Science Grid, the Extreme Science and Engineering Discovery Environment (XSEDE v2.0), petascale supercomputers such as COMET, and campus resources.  Central to SciDAS is the use of ExoGENI dynamic networked infrastructure to enable Layer-2 connectivity and data movement between these resources and data repositories.  SciDAS relies on the integrated-Rule-Oriented-Data-System (iRODS), enhanced with software-defined-networking (SDN) capabilities, to support network-aware data management decisions and efficient use of network resources. The distributed and scalable nature of both the data-sharing and the compute infrastructure are exploited to optimize for computer and data locality, boosting the performance of workflows and scientific productivity.  Scientific discovery use cases in systems biology and hydrology will drive cyberinfrastructure development at the petascale level while simultaneously generating useful results for domain scientists."
"1841480","Scalable Cyberinfrastructure for Early Warning Gravitational Wave Detections","OAC","CESER-Cyberinfrastructure for","10/01/2018","12/21/2018","Chad Hanna","PA","Pennsylvania State Univ University Park","Standard Grant","William Miller","09/30/2020","$983,911.00","","crh184@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7684","020Z, 062Z","$0.00","Recent advances in astronomical facilities have opened new windows on the universe that extend observing capabilities beyond conventional telescopes. Joint observations that combine telescopes, neutrino detectors, and new gravitational wave detectors including the NSF-supported LIGO Observatory are revealing aspects of the universe that are presently a mystery. Just recently, signals from the collision of two extremely dense neutron stars - a merger known as GW170817 - were detected by both conventional telescopes and gravitational wave detectors.  The event was detected first in gravitational waves, two seconds later in gamma rays, after 10 hours in optical, ultraviolet, infrared, and much later in x-ray and radio waves.  From this single event, the world learned that some short-hard gamma ray bursts indicate neutron star mergers, that these mergers might be the origin of many elements in the periodic table such as gold and platinum, that gravity and light travel at the same speed, and that gravitational waves really could measure how fast the universe is expanding.  Despite what was learned, GW170817 left the world with many questions. What object was formed afterward? Was it another neutron star? Was it a black hole? Why was the gamma ray burst associated with GW170817 unlike anything else that had been observed? Answering these questions, and conducting these kinds of joint observations on a regular basis, requires significant computing and software infrastructure (cyberinfrastructure).<br/><br/>The project proposes to develop the cyberinfrastructure necessary to give earlier gravitational event alerts to other astronomical facilities than is currently possible, allowing researchers to collect as much data as possible about these new types of celestial events. This project will fortify the streaming data delivery of LIGO by producing sub-second data delivery to a streaming early warning search for neutron star mergers. Substantial automated monitoring and feedback will ensure the entire system operates without manual intervention. The project will capitalize upon existing NSF investments in cyber-infrastructure for real-time gravitational wave analysis and will significantly augment the data delivery and automation layer for detections which is presently a bottleneck and failure mode. Using gravitational waves to provide an early warning for robotic telescopes will significantly enhance the scientific utility of LIGO data and significantly facilitate multi-messenger astrophysics.  This proposal will promote the progress of science in two of the National Science Foundation's 10 Big Ideas: ""Harnessing the Data Revolution"" ""Windows on the Universe: The Era of Multi-Messenger Astrophysics"". <br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845208","CAREER: Scalable Sparse Linear Algebra for Extreme-Scale Data Analytics and Scientific Computing","OAC","CAREER: FACULTY EARLY CAR DEV","02/15/2019","02/04/2019","Hasan Metin Aktulga","MI","Michigan State University","Continuing grant","Sushil K Prasad","01/31/2024","$321,497.00","","hma@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1045","026Z, 1045","$0.00","This project addresses several technical challenges and develops a computing infrastructure to enable solving very large scientific problems that require high end computing such as for physics and material sciences (""scientific computing"") and for analyzing patterns within huge amounts of data such as those generated by social media (""big data analytics"").  A unifying computational motif in the seemingly disparate fields of big data analytics and scientific computing is that the models currently used to solve the relevant problems often result in large amount of data with significant, irregular gaps (technically known as ""sparse matrices""). The scale of solving such problems typically require execution on massively parallel computers. Due to the unique characteristics associated with sparse matrix computations, achieving high performance and scalability is challenging. This project aims to develop an extensive set of scalable sparse matrix algorithms and software to address such challenges. By significantly improving the productivity of domain scientists working on big data analytics and scientific computing, this project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense. Research plans are tightly integrated with educational and outreach objectives at various levels. The centerpiece of the outreach efforts is a Computer Science summer school and mentorship plans for high school students. <br/><br/>To tackle the challenges presented by the increasingly deep memory hierarchies of modern computer architectures that include cache, high-bandwidth device memories (HBM), DRAM, and non-volatile random access memory (NVRAM) and facilitate high performance execution of sparse matrix computations, a comprehensive research plan is explored. The centerpiece of this project is a data-flow middleware with a simple application programming interface, called DeepSparse, that aims to support a wide variety of sparse solvers, while ensuring architecture and performance portability. DeepSparse converts a given sparse solver code into a directed acyclic graph (DAG) where nodes represent computational tasks and edges represent the data-flow between tasks. Novel DAG partitioning and scheduling algorithms, which are also extended to their hypergraph counterparts, are developed to ensure that data movement between memory layers is minimized during execution of the task graph. Performance models based on the extended Roofline model and innovative memory management schemes that draw upon ideas from disk storage systems are explored to ensure high bandwidth and low latency access to sparse solver data on NVRAM devices. All software and tools developed in this research are distributed as open source projects for a broad impact. Overall, goals of this project are well aligned with the National Strategic Computing Initiative, which aims to foster innovations that can bring the fields of big data analytics and scientific computing closer.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1739772","Collaborative Research: SI2: SSE: Extending the Physics Reach of LHCb in Run 3 Using Machine Learning in the Real-Time Data Ingestion and Reduction System","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/24/2017","Mike Williams","MA","Massachusetts Institute of Technology","Standard Grant","Stefan Robila","08/31/2020","$275,000.00","","mwill@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1253, 7244, 8004","7433, 8004, 8005","$0.00","In the past 200 years, physicists have discovered the basic constituents of ordinary matter and the developed a very successful theory to describe the interactions (forces) between them.  All atoms, and the molecules from which they are built, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions.  Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories.  The predictions of these theories can be very, very precise, and they have been validated with equally precise experimental measurements.  Most recently, a new fundamental particle required to unify the weak and electromagnetic interactions, the Higgs boson, was discovered at the Large Hadron Collider (LHC), located at the CERN laboratory in Switzerland. Despite the vast amount of knowledge acquired over the past century about the fundamental particles and forces of nature, many important questions still remain unanswered. For example, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions.  As it has only been observed via its gravitation interactions, it is called dark matter.  What is it?  Equally interesting, why is there so little anti-matter in the universe when the fundamental interactions we know describe matter and anti-matter as almost perfect mirror images of each other? The LHC was built to discover and study the Higgs boson and to search for answers to these questions. The first data-taking run (Run 1, 2010-2012) of the LHC was a huge success, producing over 1000 journal articles, highlighted by the discovery of the Higgs boson. The current LHC run (Run 2, 2015-present) has already produced many world-leading results; however, the most interesting questions remained unanswered. The LHCb experiment, located on the LHC at CERN, has unique potential to answer some of these questions. LHCb is searching for signals of dark matter produced in high-energy particle collisions at the LHC, and performing high-precision studies of rare processes that could reveal the existence of the as-yet-unknown forces that caused the matter/anti-matter imbalance observed in our universe. The primary goal of this project - supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences - is developing and deploying software utilizing Machine Learning (ML) that will enable the LHCb experiment to significantly improve its discovery potential in Run 3 (2021-2023). Specifically, the ML developed will greatly increase the sensitivity to many proposed types of dark matter and new forces by making it possible to much more efficiently identify and study potential signals -- using the finite computing resources available.  <br/><br/>The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, on which both PIs work, produce about 100 terabytes of data per second, close to a zettabyte of data per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger-system designs are dictated by the rate at which the sensors can be read out, the computational power of the data-ingestion system, and the available storage space for the data. The LHCb detector is being upgraded for Run 3 (2021-2023), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger are analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To process all the data on CPU farms, ML will be used to develop and deploy new trigger algorithms. The specific objectives of this proposal are to more fully characterize LHCb data using ML and build algorithms using these characterizations: to replace the most computationally expensive parts of the event pattern recognition; to increase the performance of the event-classification algorithms; and to reduce the number of bytes persisted per event without degrading physics performance. Many potential explanations for dark matter and the matter/anti-matter asymmetry of our universe are currently inaccessible due to trigger-system limitations. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential. This redesign must go beyond scalable technical upgrades; radical new strategies are needed."
"1663893","Collaborative Research:  SI2-SSI: Inquiry-Focused Volumetric Data Analysis Across Scientific Domains: Sustaining and Expanding the yt Community","OAC","Software Institutes","10/01/2017","07/28/2017","Benjamin Holtzman","NY","Columbia University","Standard Grant","Stefan Robila","09/30/2022","$261,310.00","","benh@ldeo.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8004","7433, 8004, 8009","$0.00","Scientific discovery across the physical sciences is increasingly dependent on the analysis of volumetric - or three-dimensional - data, that may come from a supercomputer simulation, direct measurement, or mathematical models. Researchers typically seek to extract meaningful insights from this data by visualizing and analyzing it in various ways. The ways in which scientists process volumetric data are actually quite similar across domains, but cross-disciplinary knowledge transfer and tool development is blocked by barriers of terminology. This project seeks to enhance an analysis and visualization toolkit named yt that is currently primarily used for astrophysical simulations. yt allows scientists to access and analyze data at several different levels by providing an interface that is designed to answer questions motivated by the underlying scientific problem, while worrying less about details such data formats, specific analysis techniques etc. yt's utilization in computational astrophysics has dramatically increased access to advanced algorithms for both visualization and analysis, and fostered the growth of a community of researchers sharing techniques and results. This project seeks to make yt available and adopted by scientists in other domains of science thus reproducing its success in astrophysics in these other science domains. This project will expand the yt community beyond theoretical astrophysics and enable and promote collaboration and advanced data analysis in the fields of meteorology, seismology and global tomography, observational astronomy, hydrology and oceanography, and plasma physics. <br/><br/>Improvements to the yt project will proceed along four principal technical avenues. The first is to develop a system that adapts the way yt presents data via a set of domain contexts that encode the ontology, domain-specific vocabulary, and common analysis tasks for a given field of study. This will include creating a domain context system as well as a set of five pilot domain contexts developed in collaboration with domain practitioners. The second is to overhaul the yt field system, adding more versatility and enabling significant optimizations. Thirdly, the project team will implement non-spatial indexing schemes, providing methods for accessing and analyzing data that is not organized according to the standard spatial axes. The final improvement will be the development of a non-local analysis system, allowing generalized path traversal as well as domain convolutions. To ensure wide dissemination and use of these improved capabilities, the team will design domain-specific documentation and training materials, and organize outreach and training events for early-career researchers. This will consist of both hands-on technical workshops and curricula developed in collaboration with Data Carpentry for utilization at other institutions. This combination of technical developments and social investments has been designed to ensure both readiness of the software and engagement of the targeted research communities."
"1640899","CIF21 DIBBS: EI: The Local Spectroscopy Data Infrastructure (LSDI)","OAC","DMR SHORT TERM SUPPORT, PROJECTS, DATANET, CDS&E","10/01/2016","07/27/2016","Kristin Persson","CA","University of California-Berkeley","Standard Grant","Amy Walton","09/30/2021","$3,940,400.00","Mark Asta, Sophia Hayes, Shyue Ping Ong","kristinpersson@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","1712, 1978, 7726, 8084","7237, 7433, 7569, 8048, 8400, 9102, 9263","$0.00","Traditional empirical and 'one-at-a-time' materials testing is unlikely to meet the innovation needs in chemical and materials research, to enable emerging industries to address challenges in energy, national security, healthcare, and other areas in a timely manner.  Historically, novel materials exploration has been slow and expensive, taking on average 18 years from concept to commercialization.  This project has identified a major scientific challenge - characterization of materials and chemical systems via spectroscopy - that can greatly enhance and expand materials research through accumulation, organization, and automation of both experimental and computational resources and data.  Currently, a large amount of time is invested in the interpretation and understanding of spectroscopic data, since no resource for efficiently accomplishing these tasks is available. This project allows materials researchers and chemists working in the spectroscopic field to access a searchable database of existing parameters and spectra for comparative, automated identification, and to address the full range of data elements -- production, curation, analysis, dissemination and sharing.  The resulting data resource contributes to the cyberinfrastructure of the broader materials, chemistry, and engineering community, and has the potential to catalyze the discovery of new materials and the innovative use of materials and chemical systems in science and industry. <br/><br/>The goal of the Local Spectroscopy Data Infrastructure (LSDI) project is to establish the first computational local atomic environment spectroscopy database, based on well-benchmarked computational spectra, to enable a publicly available, online resource for rapid material characterization, to accelerate materials development and optimization. Through novel technological advancements involving nanoscale engineering of defects, interfaces and surfaces, it has become increasingly important to determine the local atomic environments in materials. Spectroscopic techniques - including X-Ray Absorption Near Edge Spectroscopy (XANES), Extended X-Ray Absorption Fine Structure (EXAFS), Electron Energy Loss Spectroscopy (EELS), and Nuclear Magnetic Resonance (NMR) - have become essential characterization tools in elucidating atomic-scale chemical structure, electronic properties, and quantum phenomena in materials. There is a growing need for a general-use resource to help make spectral assignments for all researchers, including non- specialists, by capitalizing on recent advances in computational methods to populate an interactive database consisting of solid-state X-ray absorption and NMR spectra and associated parameters. This project includes: (i) creation of robust, benchmarked workflows for first-principles calculation of XAS/NMR spectra; (ii) data generation, curation and storage; (iii) development of automated spectral analysis algorithms; (iv) dissemination through the Materials Project; and (v) dynamic interaction with the community through the new Materials Data Cloud (MDCloud) environment. The data infrastructure developed by this project will allow a researcher who has recorded an experimental spectrum, such as by NMR, XANES or XAFS, of a solid-state material - even one with a disordered or non-crystalline structure - to access through the internet a searchable database of existing parameters and spectra for comparative, automated identification, along with a computational resource for simulating the spectra associated with various structural and chemical hypotheses.  The LSDI contributes to the cyberinfrastructure of the materials, chemistry, and engineering communities, and supports advances in the fundamental understanding of spectroscopic methods, materials and chemical systems. The system catalyzes the discovery of new materials, and supports innovative use of materials and chemical systems in science and industry, consistent with the goals of the Materials Genome Initiative.<br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Engineering Directorate (Division of Civil, Mechanical & Manufacturing Innovation) and the NSF Directorate for Mathematical & Physical Sciences (Division of Chemistry and Division of Materials Research)."
"1740102","Collaborative Research: SI2:SSE: Extending the Physics Reach of LHCb in Run 3 Using Machine Learning in the Real-Time Data Ingestion and Reduction System","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/24/2017","Michael Sokoloff","OH","University of Cincinnati Main Campus","Standard Grant","Stefan Robila","08/31/2020","$224,621.00","","mike.sokoloff@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","1253, 7244, 8004","7433, 8004, 8005","$0.00","In the past 200 years, physicists have discovered the basic constituents of ordinary matter and the developed a very successful theory to describe the interactions (forces) between them.  All atoms, and the molecules from which they are built, can be described in terms of these constituents.  The nuclei of atoms are bound together by strong nuclear interactions.  Their decays result from strong and weak nuclear interactions. Electromagnetic forces bind atoms together, and bind atoms into molecules. The electromagnetic, weak nuclear, and strong nuclear forces are described in terms of quantum field theories.  The predictions of these theories can be very, very precise, and they have been validated with equally precise experimental measurements.  Most recently, a new fundamental particle required to unify the weak and electromagnetic interactions, the Higgs boson, was discovered at the Large Hadron Collider (LHC), located at the CERN laboratory in Switzerland. Despite the vast amount of knowledge acquired over the past century about the fundamental particles and forces of nature, many important questions still remain unanswered. For example, most of the matter in the universe that interacts gravitationally does not have ordinary electromagnetic or nuclear interactions.  As it has only been observed via its gravitation interactions, it is called dark matter.  What is it?  Equally interesting, why is there so little anti-matter in the universe when the fundamental interactions we know describe matter and anti-matter as almost perfect mirror images of each other? The LHC was built to discover and study the Higgs boson and to search for answers to these questions. The first data-taking run (Run 1, 2010-2012) of the LHC was a huge success, producing over 1000 journal articles, highlighted by the discovery of the Higgs boson. The current LHC run (Run 2, 2015-present) has already produced many world-leading results; however, the most interesting questions remained unanswered. The LHCb experiment, located on the LHC at CERN, has unique potential to answer some of these questions. LHCb is searching for signals of dark matter produced in high-energy particle collisions at the LHC, and performing high-precision studies of rare processes that could reveal the existence of the as-yet-unknown forces that caused the matter/anti-matter imbalance observed in our universe. The primary goal of this project - supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences - is developing and deploying software utilizing Machine Learning (ML) that will enable the LHCb experiment to significantly improve its discovery potential in Run 3 (2021-2023). Specifically, the ML developed will greatly increase the sensitivity to many proposed types of dark matter and new forces by making it possible to much more efficiently identify and study potential signals -- using the finite computing resources available.  <br/><br/>The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, on which both PIs work, produce about 100 terabytes of data per second, close to a zettabyte of data per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year, comparable to the largest-scale industrial data sets. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger-system designs are dictated by the rate at which the sensors can be read out, the computational power of the data-ingestion system, and the available storage space for the data. The LHCb detector is being upgraded for Run 3 (2021-2023), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger are analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To process all the data on CPU farms, ML will be used to develop and deploy new trigger algorithms. The specific objectives of this proposal are to more fully characterize LHCb data using ML and build algorithms using these characterizations: to replace the most computationally expensive parts of the event pattern recognition; to increase the performance of the event-classification algorithms; and to reduce the number of bytes persisted per event without degrading physics performance. Many potential explanations for dark matter and the matter/anti-matter asymmetry of our universe are currently inaccessible due to trigger-system limitations. As HEP computing budgets are projected to be approximately flat moving forward, the LHCb trigger system must be redesigned for the experiment to realize its full potential. This redesign must go beyond scalable technical upgrades; radical new strategies are needed."
"1550551","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/26/2018","Frank Loffler","LA","Louisiana State University & Agricultural and Mechanical College","Continuing grant","Micah Beck","06/30/2020","$374,557.00","Peter Diener, Steven Brandt","knarf@cct.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7244, 8004","7433, 7569, 8009, 8084, 9150","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1550461","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/25/2018","Pablo Laguna","GA","Georgia Tech Research Corporation","Continuing grant","Micah Beck","06/30/2020","$385,278.00","David Bader","pablo.laguna@physics.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7244, 8004","7433, 7569, 8009, 8084","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1550436","SI2-SSI: Collaborative Research: Einstein Toolkit Community Integration and Data Exploration","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/26/2018","Manuela Campanelli","NY","Rochester Institute of Tech","Continuing grant","Micah Beck","06/30/2020","$364,601.00","Joshua Faber, Yosef Zlochower","manuela@astro.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7244, 8004","7433, 7569, 8009, 8084","$0.00","A new astronomy has arrived with the recent detection of gravitational waves. Modeling of sources of gravitational radiation is more than ever a critical necessity in order to interpret the observations. The project Einstein Toolkit has as overarching mission to provide the scientific community with a sustainable software platform of core computational tools for research focused on astrophysical systems endowed with complex multi-scale/multi-physics properties which are governed by Einstein's equations of General Relativity. The central premise of the project Einstein Toolkit is to create a broad and vibrant community of users, a community where interdisciplinary collaborations are the norm and not the exception, a community driving advances in the next generation of high-performance computing cyberinfrastructure. The main objectives of the project Einstein Toolkit are: developing software tools for a radical increase in scientific productivity, achieving sustainability of the software ecosystem, addressing software engineering challenges, and the curation of data from general relativistic numerical simulations. <br/><br/>This project will achieve its goals through two major activity areas. Regarding the software ecosystem and its sustainability, the scheduler that handles the flow of tasks in a problem will be redesigned to be more versatile and to improve its performance. In addition new software modules will be developed to broaden the choices of  initial data and matter sources, as well as modules for problems requiring a high degree of experimentation with equations and numerical methodologies.  A new general relativistic magneto-hydrodynamics code will also be integrated in the Einstein Toolkit. The second activity area involves building  a simulation data repository. The repository will allows user to compare results, contribute data,  test  innovative ideas and algorithms for gravitational wave data analysis, and to explore or discover new phenomena in sources of gravitational radiation. The broader impact effort in the project Einstein Toolkit will be organized in two major activity areas. The first involves community integration. The project will support a program of ease-of-use on-line tutorials and a workshops/tutorial series. The program will  help small groups or individual investigators familiarizing with the codes and modules in the toolkit as well as pathways to become a developer. Regarding outreach and education, the project Einstein Toolkit will enable interdisciplinary training of students and postdocs in numerical relativity, computational astrophysics and computer science. The effort will includes developing a teaching resources bank for educational activities involving computational topics applied to gravitational physics and astrophysics. The educational resources will be suitable for computational courses in general relativity and astrophysics at both the graduate and undergraduate level.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1753840","CIF21 DIBBs: EI: North East Storage Exchange","OAC","ADVANCES IN BIO INFORMATICS, ETF, DATANET","07/01/2017","10/10/2017","James Cuff","MA","Trustees of Boston University","Standard Grant","Amy Walton","10/31/2021","$3,846,298.00","","james_cuff@harvard.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","1165, 7476, 7726","7433, 8048, 8089, 8091","$0.00","Research progress is increasingly dependent upon the available capacity of storage to flexibly exploit large volumes of digital information.  The North East Storage Exchange (NESE) project creates a next-generation storage infrastructure specifically targeted at enabling new levels of collaborative research in projects regularly involving petabytes of information.  This storage exchange will integrate with a computational and network infrastructure that links Harvard University, Boston University, the Massachusetts Institute of Technology (MIT), Northeastern University and the University of Massachusetts system.  This project contributes to building a national data infrastructure to support advanced research in such priority topics as health care, epidemiology, physics, and earth science, among others.<br/><br/>NESE will provide a high capacity, highly networked, secure, cost effective, scalable, and accessible data store that lowers barriers to research, collaboration, and information sharing within and beyond the participating multi-university community. Some examples of NESE projects that will be early users of NESE include one of the four US Tier 2 centers that store and process ATLAS data from the Large Hadron Collider; the Center for Brain Science at Harvard University, which is generating 300 million micron-resolution images to map the billion neurons and synapses that make up a cubic millimeter of the human brain; and MIT collaborations with NASA and DARPA in next generation global ocean modeling and monitoring systems.  NESE addresses several critical infrastructural challenges: the creation of a sustainable multi-institutional resource; advancement of methods for data retention, management, and access to sensitive research data; implementation of controls that simplify protection of sensitive data; and building a sustainable, collaborative operating infrastructure to support future research.<br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Biological Sciences (Division of Biological Infrastructure), and by NSF's Understanding the Brain and BRAIN initiative activities."
"1829721","Collaborative Research: CyberTraining: CIU: Hour of Cyberinfrastructure: Developing Cyber Literacy for Geographic Information Science","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Anand Padmanabhan","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sushil Prasad","07/31/2021","$22,375.00","","apadmana@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","044Y","026Z, 062Z, 7361","$0.00","Cyberinfrastructure empowers the growing knowledge economy in the United States, and plays a role in defense, homeland security, agriculture, and commerce by providing powerful computational resources to support data analytics and modeling. However, many scientific disciplines currently face the question of how to seamlessly integrate cyberinfrastructure training in their educational programs. Students and researchers in these disciplines thus often lack experience in using the most advanced tools and techniques to grapple with the crucial global challenges they are being trained to investigate. This project addresses this challenging problem by creating a clear curriculum model for educators - an Hour of Cyberinfrastructure (Hour of CI) - that integrates cyberinfrastructure skill building into domain-specific curriculum, with a clear learning goal for students: try cyberinfrastructure for one hour. Geospatially-based lessons in this project draw on real-world problems from social sciences, environmental sciences, and geosciences to make them accessible and meaningful to students in many scientific disciplines. Hour of CI lessons are available via an easy-to-use science gateway for broad-scale educational use. The project broadens access and enable community adoption of cyberinfrastructure for the nation's future scientific research workforce thus serving the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and secure the national defense.<br/><br/>The Hour of CI project is a nationwide campaign introducing hundreds of diverse undergraduate and graduate students to cyberinfrastructure. Modeled on the ""Hour of Code, the Hour of CI project is building a sustainable learning community and scalable training environment to train almost two hundred educators and over five hundred graduate and undergraduate students at institutions ranging from R1 universities to two-year teaching colleges in the short-term and potentially thousands more in the long-term. The project is developing 17 interactive, online lessons for students and creating supplementary curriculum materials for instructors. Hour of CI lessons are being developed using a learning outcome centered Backward Design Process in which students are exposed to cyberinfrastructure, establish conceptual foundations, and build a core set of skills to help them achieve Cyber Literacy for Geographic Information Science, which requires learners to be knowledgeable in eight core areas: cyberinfrastructure, parallel computing, big data, computational thinking, interdisciplinary communication, spatial thinking, geospatial data, and spatial modeling and analytics. The project lowers the barrier to entry for educators and students by building on a science gateway called the GISandbox to provide a cyberinfrastructure-enabled training environment accessible through a web browser for all Hour of CI lessons. The sustainable learning community built during the course of this project will continue to expand adoption of the Hour of CI beyond the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829718","Collaborative Research: CyberTraining: CIU: Hour of Cyberinfrastructure: Developing Cyber Literacy for Geographic Information Science","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Forrest Bowlick","MA","University of Massachusetts Amherst","Standard Grant","Sushil Prasad","07/31/2021","$40,515.00","","fbowlick@umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","044Y","026Z, 062Z, 7361","$0.00","Cyberinfrastructure empowers the growing knowledge economy in the United States, and plays a role in defense, homeland security, agriculture, and commerce by providing powerful computational resources to support data analytics and modeling. However, many scientific disciplines currently face the question of how to seamlessly integrate cyberinfrastructure training in their educational programs. Students and researchers in these disciplines thus often lack experience in using the most advanced tools and techniques to grapple with the crucial global challenges they are being trained to investigate. This project addresses this challenging problem by creating a clear curriculum model for educators - an Hour of Cyberinfrastructure (Hour of CI) - that integrates cyberinfrastructure skill building into domain-specific curriculum, with a clear learning goal for students: try cyberinfrastructure for one hour. Geospatially-based lessons in this project draw on real-world problems from social sciences, environmental sciences, and geosciences to make them accessible and meaningful to students in many scientific disciplines. Hour of CI lessons are available via an easy-to-use science gateway for broad-scale educational use. The project broadens access and enable community adoption of cyberinfrastructure for the nation's future scientific research workforce thus serving the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and secure the national defense.<br/><br/>The Hour of CI project is a nationwide campaign introducing hundreds of diverse undergraduate and graduate students to cyberinfrastructure. Modeled on the ""Hour of Code, the Hour of CI project is building a sustainable learning community and scalable training environment to train almost two hundred educators and over five hundred graduate and undergraduate students at institutions ranging from R1 universities to two-year teaching colleges in the short-term and potentially thousands more in the long-term. The project is developing 17 interactive, online lessons for students and creating supplementary curriculum materials for instructors. Hour of CI lessons are being developed using a learning outcome centered Backward Design Process in which students are exposed to cyberinfrastructure, establish conceptual foundations, and build a core set of skills to help them achieve Cyber Literacy for Geographic Information Science, which requires learners to be knowledgeable in eight core areas: cyberinfrastructure, parallel computing, big data, computational thinking, interdisciplinary communication, spatial thinking, geospatial data, and spatial modeling and analytics. The project lowers the barrier to entry for educators and students by building on a science gateway called the GISandbox to provide a cyberinfrastructure-enabled training environment accessible through a web browser for all Hour of CI lessons. The sustainable learning community built during the course of this project will continue to expand adoption of the Hour of CI beyond the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829708","Collaborative Research: CyberTraining: CIU: Hour of Cyberinfrastructure: Developing Cyber Literacy for Geographic Information Science","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Eric Shook","MN","University of Minnesota-Twin Cities","Standard Grant","Sushil Prasad","07/31/2021","$373,990.00","","eshook@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","044Y","026Z, 062Z, 7361","$0.00","Cyberinfrastructure empowers the growing knowledge economy in the United States, and plays a role in defense, homeland security, agriculture, and commerce by providing powerful computational resources to support data analytics and modeling. However, many scientific disciplines currently face the question of how to seamlessly integrate cyberinfrastructure training in their educational programs. Students and researchers in these disciplines thus often lack experience in using the most advanced tools and techniques to grapple with the crucial global challenges they are being trained to investigate. This project addresses this challenging problem by creating a clear curriculum model for educators - an Hour of Cyberinfrastructure (Hour of CI) - that integrates cyberinfrastructure skill building into domain-specific curriculum, with a clear learning goal for students: try cyberinfrastructure for one hour. Geospatially-based lessons in this project draw on real-world problems from social sciences, environmental sciences, and geosciences to make them accessible and meaningful to students in many scientific disciplines. Hour of CI lessons are available via an easy-to-use science gateway for broad-scale educational use. The project broadens access and enable community adoption of cyberinfrastructure for the nation's future scientific research workforce thus serving the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and secure the national defense.<br/><br/>The Hour of CI project is a nationwide campaign introducing hundreds of diverse undergraduate and graduate students to cyberinfrastructure. Modeled on the ""Hour of Code, the Hour of CI project is building a sustainable learning community and scalable training environment to train almost two hundred educators and over five hundred graduate and undergraduate students at institutions ranging from R1 universities to two-year teaching colleges in the short-term and potentially thousands more in the long-term. The project is developing 17 interactive, online lessons for students and creating supplementary curriculum materials for instructors. Hour of CI lessons are being developed using a learning outcome centered Backward Design Process in which students are exposed to cyberinfrastructure, establish conceptual foundations, and build a core set of skills to help them achieve Cyber Literacy for Geographic Information Science, which requires learners to be knowledgeable in eight core areas: cyberinfrastructure, parallel computing, big data, computational thinking, interdisciplinary communication, spatial thinking, geospatial data, and spatial modeling and analytics. The project lowers the barrier to entry for educators and students by building on a science gateway called the GISandbox to provide a cyberinfrastructure-enabled training environment accessible through a web browser for all Hour of CI lessons. The sustainable learning community built during the course of this project will continue to expand adoption of the Hour of CI beyond the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829804","Collaborative Research: CyberTraining: CIU: Hour of Cyberinfrastructure: Developing Cyber Literacy for Geographic Information Science","OAC","CyberTraining - Training-based","08/01/2018","07/02/2018","Karen Kemp","CA","University of Southern California","Standard Grant","Sushil Prasad","07/31/2021","$44,090.00","","kakemp@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","044Y","026Z, 062Z, 7361","$0.00","Cyberinfrastructure empowers the growing knowledge economy in the United States, and plays a role in defense, homeland security, agriculture, and commerce by providing powerful computational resources to support data analytics and modeling. However, many scientific disciplines currently face the question of how to seamlessly integrate cyberinfrastructure training in their educational programs. Students and researchers in these disciplines thus often lack experience in using the most advanced tools and techniques to grapple with the crucial global challenges they are being trained to investigate. This project addresses this challenging problem by creating a clear curriculum model for educators - an Hour of Cyberinfrastructure (Hour of CI) - that integrates cyberinfrastructure skill building into domain-specific curriculum, with a clear learning goal for students: try cyberinfrastructure for one hour. Geospatially-based lessons in this project draw on real-world problems from social sciences, environmental sciences, and geosciences to make them accessible and meaningful to students in many scientific disciplines. Hour of CI lessons are available via an easy-to-use science gateway for broad-scale educational use. The project broadens access and enable community adoption of cyberinfrastructure for the nation's future scientific research workforce thus serving the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and secure the national defense.<br/><br/>The Hour of CI project is a nationwide campaign introducing hundreds of diverse undergraduate and graduate students to cyberinfrastructure. Modeled on the ""Hour of Code, the Hour of CI project is building a sustainable learning community and scalable training environment to train almost two hundred educators and over five hundred graduate and undergraduate students at institutions ranging from R1 universities to two-year teaching colleges in the short-term and potentially thousands more in the long-term. The project is developing 17 interactive, online lessons for students and creating supplementary curriculum materials for instructors. Hour of CI lessons are being developed using a learning outcome centered Backward Design Process in which students are exposed to cyberinfrastructure, establish conceptual foundations, and build a core set of skills to help them achieve Cyber Literacy for Geographic Information Science, which requires learners to be knowledgeable in eight core areas: cyberinfrastructure, parallel computing, big data, computational thinking, interdisciplinary communication, spatial thinking, geospatial data, and spatial modeling and analytics. The project lowers the barrier to entry for educators and students by building on a science gateway called the GISandbox to provide a cyberinfrastructure-enabled training environment accessible through a web browser for all Hour of CI lessons. The sustainable learning community built during the course of this project will continue to expand adoption of the Hour of CI beyond the project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1547467","CICI: Data Provenance: Protecting Provenance Integrity and Privacy","OAC","Cyber Secur - Cyberinfrastruc","12/01/2015","09/15/2015","Ashish Gehani","CA","SRI International","Standard Grant","Micah Beck","11/30/2019","$499,728.00","","ashish.gehani@sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","6508592651","CSE","8027","7434","$0.00","The amount of data associated with scientific and medical analysis has grown, and in parallel, the desire to share resulting data has increased.  Similarly, as more science and medical analysis is performed by exploring large volumes of data, with numerous transformations along the way, tracking the history of a result is of increasing utility. Despite these incentives, concerns such as respecting human subject and patient privacy, maintaining intellectual property, and compliance with copyright laws remain as barriers to sharing data with its detailed provenance. By simplifying the process of safely releasing provenance metadata, the project will facilitate the sharing of data with a concomitant benefit to science and medicine.<br/><br/>The project creates software to tightly couple data and its accompanying provenance metadata, and sanitize the resulting provenance when it is shared. A storage overlay is developed to perform partial computation over the data as it enters the system. This allows the application of pre-configured functions to incoming data without affecting extant workload performance, by leveraging idle processing resources. The paradigm facilitates efficient content-based provenance that describes the state of the data in the system. This allows references in the provenance metadata to be strongly linked to integrity claims about the specific data inputs and outputs, even when these correspond to large data sets. When such scientific and medical data sets are combined, new privacy-violating inferences may be drawn.<br/><br/>The project investigates techniques that operate on sets of provenance graphs, descriptions of unsafe inferences, and ontological or probabilistic characterizations of the relationships of provenance elements. A tool is developed to transform the provenance into graphs with reduced leakage of sensitive information that are still usable for pre-defined provenance queries, and to identify which elements need auxiliary protection."
"1445604","High Performance Computing System Acquisition: Jetstream - A Self-Provisioned, Scalable Science and Engineering Cloud Environment","OAC","ETF, EQUIPMENT ACQUISITIONS, DATANET","12/01/2014","03/13/2019","David Hancock","IN","Indiana University","Cooperative Agreement","Robert Chadduck","11/30/2020","$13,751,929.00","Ian Foster, Matthew Vaughn, Nirav Merchant, James Taylor","dyhancoc@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7476, 7619, 7726","116E, 7433, 7619, 9251","$0.00","High Performance Computing System Acquisition: Jetstream - a self-provisioned, scalable science and engineering cloud environment<br/><br/>Jetstream will be a new type of computational research resource open for the national (nonclassified) research community - a data analysis and computational resource that US scientists and engineers will use interactively to conduct their research anytime, anywhere. Jetstream will complement current NSF-funded computational resources and bring a cloud-based system to the NSF computational resources incorporating the best elements of commercial cloud computing resources with some of the best software in existence for solving important scientific problems. This system will enable many US researchers and engineers to make new discoveries that are important to understanding the world around us and will help researchers make new discoveries that improve the quality of life of American citizens.<br/><br/>In terms of technical details, Jetstream will be a configurable large-scale computing resource that leverages both on-demand and persistent virtual machine technology to support a much wider array of software environments and services than current NSF resources can accommodate. As a fully configurable ""cloud"" resource, Jetstream bridges the obvious major gap in the current ecosystem, which has machines targeted at large-scale High-Performance Computing, high memory, large data, high-throughput, and visualization resources. As the open cloud for science, Jetstream will:<br/> <br/>*Provide ""self-serve"" academic cloud services, enabling researchers or students to select a VM image from a published library, or alternatively to create or customize their own virtual environment for discipline- or task-specific personalized research computing.<br/><br/>*Host persistent VMs to provide services beyond the command line interface for science gateways and other science services. For example, Jetstream will become a primary host of the popular Galaxy scientific workbench and its main datasets, bringing many Galaxy users to the NSF ecosystem from day one.<br/> <br/>*Enable new modes of sharing computations, data, and reproducibility.<br/> <br/>*Expand access to the NSF XSEDE ecosystem by making virtual desktop services accessible from institutions with limited resources"
"1657895","CC* Networking Infrastructure: Science DMZ and Research Network Upgrade for Louisiana State University Health Sciences Center New Orleans","OAC","Campus Cyberinfrastrc (CC-NIE)","03/01/2017","12/12/2016","Mohamad Qayoom","LA","Louisiana State University Health Sciences Center","Standard Grant","Kevin Thompson","02/29/2020","$499,640.00","Judy Crabtree, Bettina Owens, Christopher Taylor, Jovanny Zabaleta","mqayoo@lsuhsc.edu","433 Bolivar St.","New Orleans","LA","701122223","5045684804","CSE","8080","9150","$0.00","This CC* Networking Infrastructure project at LSU Health Sciences Center New Orleans (LSUHSC-NO) is constructing a high-speed science network to connect LSUHSC-NO's research areas to facilitate unimpeded movement of large data sets and to provide a pathway to national and global high-performance computing resources. The project enables researchers, scientists, and students to exchange and store large data sets, to expand their opportunities for remote collaboration, and to facilitate leadership in research and education within multiple disciplines of Science at LSUHSC-NO: Anatomy, Biochemistry, Cell Biology, Genetics, Hematology, Immunology, Microbiology, Molecular Biology, Oncology, Ophthalmology, Parasitology, Pathology, Pharmacology, and Physiology. This project has broader implications for the scientific community including the next generation of researchers and scientists. Whether it is a high school student pondering one's future by participating in a summer research program or internship, a budding student enrolled in curricula at LSUHSC-NO, or a collaborative researcher somewhere in cyberspace, this project is key to the future of research at LSUHSC-NO and this project unlocks the door to many exciting discoveries.<br/><br/>This project consists of three objectives: (i) Upgrade the distribution and access layer networking infrastructure to provide 10Gbps and/or 1Gbps ports in the research areas; (ii) Re-architect the campus network to support large data flows by designing and building a Science DMZ; and (iii) Connect to LSUHSC-NO's regional optical exchange's Science DMZ via 20Gbps connections."
"1642369","Collaborative Research: SI2-SSE: WRENCH: A Simulation Workbench for Scientific Worflow Users, Developers, and Researchers","OAC","Software Institutes","01/01/2017","09/12/2016","Henri Casanova","HI","University of Hawaii","Standard Grant","Stefan Robila","12/31/2019","$257,956.00","","henric@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Many scientific breakthroughs can only be achieved by performing complex processing of vast amounts of data efficiently.  In domains as crucial to our society as climate modeling, oceanography, particle physics, seismology, or computational biology (and in fact in most fields of physics, chemistry, and biology today), scientists nowadays routinely define ""scientific workflows"". These workflows are complex descriptions of scientific processes as data and inter-dependent computations on these data. When executed, typically with great expenses of computing, storage, and networking hardware, these workflows can produce groundbreaking results. A famous and recent example is the workflow that was used as part of the LIGO project to confirm the first detection of gravitational waves from colliding black holes. Scientific workflows are mainstays in today's science. Their efficient  execution (in terms of speed, reliability, and cost) is thus crucial. This project seeks to provide a software framework, called WRENCH (Workflow Simulation Workbench), that will make it possible to simulate large-scale hypothetical scenarios quickly and accurately on a single computer, obviating the need for expensive and time-consuming trial and error experiments. WRENCH potentially enables scientists to make quick and informed choices when executing their workflows, software developers to implement more efficient software infrastructures to support workflows, and researchers to develop novel efficient algorithms to be embedded within these software infrastructures.  In addition, WRENCH makes it possible to bring scientific workflow content into undergraduate and graduate computer science curricula. This is because meaningful knowledge can be gained by students using a single computer and the WRENCH software stack, making such learning possible even at institutions without access to high-end computing infrastructures, such as many non-Ph.D.-granting and minority-serving institutions. As a result, this work will contribute to producing computer science graduates better equipped to take an active role in the advancing of science.  Due to its potentially transformative impact on scientific workflow usage, development, research, and education, this project promises to promote the progress of science across virtually all its fields, ultimately resulting in broad and numerous benefits to our society.<br/><br/>Scientific workflows have become mainstream for conducting large-scale scientific research.  As a result, many workflow applications and Workflow Management Systems (WMSs) have been developed as part of the cyberinfrastructure to allow scientists to execute their applications seamlessly on a range of distributed platforms.  In spite of many success stories, building large-scale workflows and orchestrating their executions efficiently (in terms of performance, reliability, and cost) remains a challenge given the complexity of the workflows themselves and the complexity of the underlying execution platforms.  A fundamental necessary next step is the establishment of a solid ""experimental science"" approach for future workflow technology development. Such an approach is useful for scientists who need to design workflows and pick execution platforms, for WMS developers who need to compare alternate design and implementation options, and for researchers who need to develop novel decision-making algorithms to be implemented as part of WMSs.  The broad objective of this work is to provide foundational software, the Workflow Simulation Workbench (WRENCH), upon which to develop the above experimental science approach.  Capitalizing on recent advances in distributed application and platform simulation technology, WRENCH makes it possible to (i) quickly prototype workflow, WMS implementations, and decision-making algorithms; and (ii) evaluate/compare alternative options scalably and accurately for arbitrary, and often hypothetical, experimental scenarios.  This project will define a generic and foundational software architecture, that is informed by current state-of-the-art WMS designs and planned future designs.  The implementation of the components in this architecture when taken together form a generic ""scientific instrument"" that can be used by workflow users, developers, and researchers.  This scientific instrument will be instantiated for several real-world WMSs and used for a range of real-world workflow applications. In a particular case-study, it will be used with a popular WMS (Pegasus) to revisit published results and scheduling algorithms in the area of workflow planning optimizations. The objective is to demonstrate the benefit of using an experimental science approach for WMS research.  Another impact of this project is that it  makes it possible to include scientific workflow content pervasively in undergraduate and graduate computer science curricula, even for students without any access to computing infrastructure, by defining meaningful pedagogic activities that only require a computer and the WRENCH software stack. This educational impact will be demonstrated in the classroom in both undergraduate and graduate courses at our institutions."
"1642335","Collaborative Research: SI2-SSE: WRENCH: A Simulation Workbench for Scientific Workflow Users, Developers, and Researchers","OAC","Software Institutes","01/01/2017","09/12/2016","Rafael Ferreira da Silva","CA","University of Southern California","Standard Grant","Stefan Robila","12/31/2019","$240,000.00","","rafsilva@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Many scientific breakthroughs can only be achieved by performing complex processing of vast amounts of data efficiently.  In domains as crucial to our society as climate modeling, oceanography, particle physics, seismology, or computational biology (and in fact in most fields of physics, chemistry, and biology today), scientists nowadays routinely define ""scientific workflows"". These workflows are complex descriptions of scientific processes as data and inter-dependent computations on these data. When executed, typically with great expenses of computing, storage, and networking hardware, these workflows can produce groundbreaking results. A famous and recent example is the workflow that was used as part of the LIGO project to confirm the first detection of gravitational waves from colliding black holes. Scientific workflows are mainstays in today's science. Their efficient  execution (in terms of speed, reliability, and cost) is thus crucial. This project seeks to provide a software framework, called WRENCH (Workflow Simulation Workbench), that will make it possible to simulate large-scale hypothetical scenarios quickly and accurately on a single computer, obviating the need for expensive and time-consuming trial and error experiments. WRENCH potentially enables scientists to make quick and informed choices when executing their workflows, software developers to implement more efficient software infrastructures to support workflows, and researchers to develop novel efficient algorithms to be embedded within these software infrastructures.  In addition, WRENCH makes it possible to bring scientific workflow content into undergraduate and graduate computer science curricula. This is because meaningful knowledge can be gained by students using a single computer and the WRENCH software stack, making such learning possible even at institutions without access to high-end computing infrastructures, such as many non-Ph.D.-granting and minority-serving institutions. As a result, this work will contribute to producing computer science graduates better equipped to take an active role in the advancing of science.  Due to its potentially transformative impact on scientific workflow usage, development, research, and education, this project promises to promote the progress of science across virtually all its fields, ultimately resulting in broad and numerous benefits to our society.<br/><br/>Scientific workflows have become mainstream for conducting large-scale scientific research.  As a result, many workflow applications and Workflow Management Systems (WMSs) have been developed as part of the cyberinfrastructure to allow scientists to execute their applications seamlessly on a range of distributed platforms.  In spite of many success stories, building large-scale workflows and orchestrating their executions efficiently (in terms of performance, reliability, and cost) remains a challenge given the complexity of the workflows themselves and the complexity of the underlying execution platforms.  A fundamental necessary next step is the establishment of a solid ""experimental science"" approach for future workflow technology development. Such an approach is useful for scientists who need to design workflows and pick execution platforms, for WMS developers who need to compare alternate design and implementation options, and for researchers who need to develop novel decision-making algorithms to be implemented as part of WMSs.  The broad objective of this work is to provide foundational software, the Workflow Simulation Workbench (WRENCH), upon which to develop the above experimental science approach.  Capitalizing on recent advances in distributed application and platform simulation technology, WRENCH makes it possible to (i) quickly prototype workflow, WMS implementations, and decision-making algorithms; and (ii) evaluate/compare alternative options scalably and accurately for arbitrary, and often hypothetical, experimental scenarios.  This project will define a generic and foundational software architecture, that is informed by current state-of-the-art WMS designs and planned future designs.  The implementation of the components in this architecture when taken together form a generic ""scientific instrument"" that can be used by workflow users, developers, and researchers.  This scientific instrument will be instantiated for several real-world WMSs and used for a range of real-world workflow applications. In a particular case-study, it will be used with a popular WMS (Pegasus) to revisit published results and scheduling algorithms in the area of workflow planning optimizations. The objective is to demonstrate the benefit of using an experimental science approach for WMS research.  Another impact of this project is that it  makes it possible to include scientific workflow content pervasively in undergraduate and graduate computer science curricula, even for students without any access to computing infrastructure, by defining meaningful pedagogic activities that only require a computer and the WRENCH software stack. This educational impact will be demonstrated in the classroom in both undergraduate and graduate courses at our institutions."
"1626645","MRI: Acquisition of Hybrid CPU/GPU High Performance Computing and Storage for STEM Research and Education at San Jose State University","OAC","MAJOR RESEARCH INSTRUMENTATION, ETF","08/01/2016","08/03/2016","Sen Chiao","CA","San Jose State University Foundation","Standard Grant","Stefan Robila","07/31/2019","$900,798.00","Brooke Lustig, Aaron Romanowsky, Kamran Turkoglu, Ehsan Khatami","sen.chiao@sjsu.edu","210 North Fourth Street","San Jose","CA","951125569","4089241400","CSE","1189, 7476","1189","$0.00","This award is to acquire a state-of-the-art high-performance computing (HPC) facility at San Jose State University (SJSU). The HPC system will provide faculty and students regular access to a modern, on-campus computing facility for computational science and engineering research. As a key hub for STEM fields in the San Francisco Bay Area, this facility will promote the progress of science and engineering, as well as offer a wide diversity of experiences for our students through required laboratory courses and research opportunities. This multidisciplinary and collaborative project involves faculty and students from biological science, chemistry, computer science, aerospace engineering, computer engineering, meteorology and climate science, physics, astronomy, mathematics, and statistics. The HPC will further enhance SJSU?s capability as a focal point in training members of the biotechnology/ pharmaceutical and information technology workforce in the Bay Area. This facility will also contribute to attracting students from underrepresented groups as well as local community colleges into STEM fields at SJSU. It is anticipated that more than 200 students from SJSU each year in STEM related courses and activities will benefit from such an instrument.<br/><br/>Comprising a hybrid central processing unit (CPU)/graphics processing unit (GPU) HPC built using 1696 compute cores and a 1.0-petabyte High Performance Storage System (HPSS). This system will be used for computational analysis, data-intensive research, rich media, three-dimensional computer modeling, data mining, and large-scale simulation. The projects that are poised to commence include: on-demand numerical weather prediction, assimilation, and analysis (Atmospheric Science); dynamical modeling of orbits and dark matter in gas-poor galaxies (Physics and Astronomy); computational modeling of Tat peptide mutants binding to BIV TAR RNA and protein-protein interfaces (Biochemistry); quantum mechanical properties of materials in the atomic scale (Physics and Astronomy); guidance and trajectory optimization strategies in presence of wind, and spacecraft and orbital trajectory optimization (Aerospace Engineering); genomic assessment of adaptation, and pharmacological and evolutionary perspective on bioactive compounds in marine invertebrates (Biological Science); high-resolution simulations of weather phenomena, dust transport, and climate on Mars (Planetary Science); and efficient algorithms for modeling large amount of data in high dimensions (Mathematics and Statistics). The HPC infrastructure and associated user group will enable many further follow-up projects, including cross-disciplinary collaborations as well as participation from the wider SJSU community."
"1709727","CDS&E: Space-Time Parallel Algorithms for Solving PDE-Constrained Optimization Problems","OAC","CDS&E-MSS, CDS&E","09/01/2017","08/24/2017","Adrian Sandu","VA","Virginia Polytechnic Institute and State University","Standard Grant","Micah Beck","08/31/2020","$500,000.00","","sandu@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8069, 8084","026Z, 7433, 8084, 9263","$0.00","Many fields of science and engineering, from atmospheric science to aeronautics, and from material science to cosmology, rely on complex models built from ""first principles"" in order to study the phenomena of interest; the fundamental physical laws are often described by time-dependent partial differential equations (PDEs). These models are implemented in complex software that performs vast amount of computations and processes large data sets in order to simulate the physical reality. PDE-based models typically run long times on parallel computers, using large numbers of cores.  A central problem in these fields of science and engineering is that of optimizing the system of interest according to specific design criteria. For example, in aeronautics, one wants not only to simulate the flight of an airplane, but also to design the best aircraft using shape optimization. In numerical weather prediction, one needs not only simulate the evolution of the atmosphere, but also to optimally utilize the information coming from satellite, aircraft, and ground based measurements in order to keep the forecasts accurate. All these applications seek to optimize systems governed by PDEs. This is an extremely challenging quest, since solving a PDE-constrained optimization problem is one-two orders of magnitude costlier than the underlying forward PDE simulation. There is considerable need for novel highly-parallel solution methodologies. This project develops the algorithmic infrastructure to support large-scale optimization of systems governed by time-dependent PDEs. New ideas will be used to unravel and exploit the inherent parallelism. First, we seek to parallelize the computations in both space and time. The space is divided in subdomains, the time in subintervals, and sub-models on each time subinterval and on each spatial subdomain are run concurrently on different sets of processors. Next, to further increase computational effectiveness, we will build surrogate models, i.e., inexpensive approximate models that capture the main dynamical characteristics of the full PDE-based models. Parallel construction of new surrogates is proposed using local-in-space-and-time information. The main idea is to perform optimization using the inexpensive surrogate models, transferring the improved design to the full PDE-model, re-computing a surrogate for the new configuration, and iterating. Enormous computational savings can be realized this way. Lastly, the new algorithms will be laid on solid theoretical foundations, and will be applied to speed up the incorporation of measurement data in a numerical weather prediction model. The tools developed in this project will enable leap developments in many fields in science and engineering where time-dependent PDE-constrained optimization problems are central. Important examples include aircraft shape optimization, seismic imaging, medical imaging, optimal control of fabrication processes, and inverse problems. The project will directly train one doctoral student and one postdoctoral researcher, will involve undergraduates in research, will develop graduate level educational materials, and will attract students from under-represented groups in parallel computing and large-scale simulations of the physical world.<br/><br/><br/>This project develops the algorithmic infrastructure to support large-scale optimization of systems governed by time-dependent partial differential equations (PDEs). PDE optimization problems are central to many fields in science and engineering.  They are considerably more complex, and costlier to solve, than the underlying PDE simulations. There is considerable need for highly-parallel solution methodologies. In order to address this, the project proposes a space-time parallel formalism, and new reduced order modeling techniques, that have the potential to speed up the PDE-constrained optimization solution process by several orders of magnitude. (1) Intellectual merit: This work develops a space-time parallel formalism for the solution of large scale PDE-constrained optimization problems. The space is divided in subdomains, the time in subintervals, and the forward and adjoint models are run in parallel on each time subinterval and on each spatial subdomain. Solution continuity equations are imposed more stringently as the optimization process advances. This work formulates reduced-order PDE-constrained optimization problems using local-in-space-and-time reduced order models. Such models can represent the system dynamics much better than traditional global approaches. Moreover, both the off-line construction of local reduced order models and the on-line reduced order simulations can be carried out concurrently on each time subinterval and on each spatial subdomain, resulting in considerable speed-ups. A trust region framework is employed for provably convergent reduced order optimization algorithms. The new methodologies are demonstrated on large atmospheric data assimilation applications. (2) Broader impact: The tools developed in this project will enable leap developments in many fields in science and engineering where time-dependent PDE-constrained optimization problems are central. Important examples include aircraft shape optimization, seismic imaging, medical imaging, optimal control of fabrication processes, and inverse problems. One doctoral student and one postdoctoral researcher are directly trained in PDE-constrained optimization, reduced order modeling, high performance computing, and science applications. Graduate level educational materials are developed.<br/><br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1547272","CICI: Center of Excellence: Center for Trustworthy Scientific Cyberinfrastructure","OAC","Software Institutes, Cyber Secur - Cyberinfrastruc, Secure &Trustworthy Cyberspace","01/01/2016","02/26/2019","Von Welch","IN","Indiana University","Standard Grant","Kevin Thompson","06/30/2020","$7,829,993.00","Barton Miller, Randal Butler, James Basney, James Marsteller, Craig Jackson","vwelch@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8004, 8027, 8060","7433, 7434, 8004, 8027, 8211, 9251","$0.00","The National Science Foundation funds over seven billion dollars of research annually, nearly all of which relies heavily on information technology. The digital data produced and computing systems used by that research are subject to the same risks as other data and computing systems on the Internet. Appropriate cybersecurity is necessary both to make today's scientific discoveries possible and to ensure that the science is trustworthy. However, NSF science is often necessarily performed in open, collaborative environments that span organizational and national boundaries. These constraints limit the use of traditional cybersecurity paradigms and technologies. Moreover, maintaining the usability of computer systems while providing cybersecurity is critical for many scientists who are not information technology experts. Different science domains also have varying requirements for data confidentiality and integrity. As the NSF Cybersecurity Center of Excellence, the Center for Trustworthy Scientific Cyberinfrastructure (CTSC) brings together experts in cybersecurity, knowledgeable and experienced in the scientific endeavor, who will provide the NSF community with leadership and support necessary to tackle the unique cybersecurity challenges in maximizing the production of trustworthy NSF science.<br/><br/>CTSC will directly support individual NSF cyberinfrastructure projects and Large Facilities through collaborative engagements that address specific project needs. CTSC engagement activities include (but are not limited to) security reviews, security architecture design, identity and access management, and software assurance. CTSC will provide cybersecurity situational awareness to the NSF cyberinfrastructure community through timely advisories and notices. In collaboration with the Department of Energy's Energy Science Network, CTSC will develop and publish an information security threat model scoped to the particular assets and interests of the open science community. CTSC will continue to organize the annual NSF Cybersecurity Summit for Large Facilities and Cyberinfrastructure, providing the community with the opportunity to share best practices, attend practical training sessions, and collaborate on solving common challenges. CTSC will perform outreach and dissemination of best practices via the CTSC website (http://trustedci.org), blog posts, email lists, and online chats, as well as providing cybersecurity training in person and via online courses."
"1660241","CC* Network Design: ARCHES (Advanced Research Computing in the Humanities Engineering and Sciences) Network at the University of New Orleans","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2017","04/19/2018","Dhruva Chakravorty","LA","University of New Orleans","Standard Grant","Kevin Thompson","06/30/2019","$333,000.00","Steven Rick, Vassil Roussev, Christopher Summa, Dhruva Chakravorty, Stephen Ware","dchakrav@uno.edu","2000 Lakeshore Drive","New Orleans","LA","701480001","5042806836","CSE","8080","9150","$0.00","Increased computing capacity coupled with the availability of large data sets have emphasized the need for improved network infrastructure to support research in varied data intensive sciences. While researchers have domain-specific needs for storing, accessing, managing and curating their data, the underlying networking needs remain the same. The Advanced Research Computing in the Humanities Engineering and Sciences (ARCHES) network facilitates research in fields of computational inquiry by helping address the continuously evolving challenges of managing scientific data. The main features of the project include (a) deploying an aggregation router to improve connectivity to Internet 2;  (b) enhancing the current data transport capabilities around campus laboratories, computational clusters and the digital humanities media laboratory; (c) deploying perfSONAR to monitor network traffic; and (d) assess the impact of cyberinfrastructure in building novel collaborations between scientific research and educational groups. <br/><br/>The ARCHES network design improves on successful implementations of similar networks and finds an appropriate balance to simultaneously address network security, research, and educational needs. The network enables educators to adopt new pedagogical approaches and technologies, and to provide students with an enhanced educational experience. With its emphasis on workforce development, ARCHES provides pathways for development of new curricular material and supports research experiences for undergraduate students and schoolteachers. The network provides new opportunities for researchers, students, and educators alike, thus having a significant impact on a number of research domains."
"1827177","CC* Integration: End-to-End Performance and Security Driven Federated Data-Intensive Workflow Management","OAC","CISE RESEARCH RESOURCES","10/01/2018","06/29/2018","Prasad Calyam","MO","University of Missouri-Columbia","Standard Grant","Deepankar Medhi","09/30/2020","$500,000.00","Timothy Middelkoop, Trupti Joshi, Isa Jahnke, Saptarshi Debroy","calyamp@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","2890","","$0.00","Data-intensive science applications in research fields such as bioinformatics, chemistry,<br/>and material science are increasingly becoming multi-domain in nature. To augment local<br/>campus CyberInfrastructure (CI) resources, these applications rely on multi-institutional resources<br/>that are remotely accessible (e.g., scientific instruments, supercomputers, public clouds).<br/>Provisioning of such federated CI resources has been traditionally based on applications'<br/>performance and quality of service (QoS) requirements. This project aims to augment<br/>traditional resource provisioning schemes through novel schemes for<br/>formalizing end-to-end security requirements to align security posture<br/>across multi-domain resources with heterogeneous policies.<br/><br/>This project addresses the end-to-end multi-domain security design for<br/>scientific applications by defining and formalizing security specifications along an application's workflow lifecycle stages.<br/>The research work will advance the current knowledge for a CI engineer in the following areas:<br/>(i) how to intelligently perform resource allocations among private and public cloud locations;<br/>(ii) by streamlining end-to-end security posture across domains that are constructed via dynamic network services; and<br/>(iii) how to ""bring your own compute"" programs in large facilities to reduce turnaround times in a secured and policy-compliant manner.<br/>The resulting security formalization and alignment schemes will be implemented as a security middleware coupled within a unified resource broker framework that: (a) operationally integrates various software tools and systems such as perfSONAR, OpenStack, iRODS and Shibboleth; and (b) supports prototypes of web-portals and actual users (e.g., researchers and educators) within usability evaluation and validation experiments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642161","CICI: Secure and Resilient Architecture: Effective and Economical Protection for High-Performance Research and Education Networks","OAC","Cyber Secur - Cyberinfrastruc","10/01/2016","08/08/2016","Johanna Amann","CA","International Computer Science Institute","Standard Grant","Micah Beck","09/30/2019","$999,513.00","Robin Sommer, Michael Dopheide","johanna@icir.org","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","8027","9102","$0.00","Scientific research requires the free exchange of information and ideas among collaborators worldwide. For this, scientists depend critically on full and open access to the Internet. Yet in today's world, such open access also exposes sites to incessant network attacks like theft of information, parasitic resource consumption, or suffering from (or inadvertently participating in) denial-of-service (DOS) attacks. Some of the most powerful networks today remain particularly hard to defend: the 100G environments and backbones that facilitate modern data-intensive sciences - physics, astronomy, medicine, climate research - prove extremely sensitive to the slightest disturbances. For these networks, traditional enterprise solutions such as firewalls and intrusion detection systems (IDS), remain infeasible as they cannot operate reliably at such high speeds. This project develops a novel, comprehensive framework that integrates software and hardware for the economical protection of critical high-performance science infrastructure.<br/><br/>The project increases the performance of network monitoring by offloading low-level operations from software into hardware, such as switches and computer network interface cards. The project enables network monitoring systems to tie into the hardware offloading being developed. Furthermore, the project expands the capabilities of network monitoring systems to create visibility into science networks, for example, by adding support for the protocols used for high-speed scientific data transfers. It also extends support for responding actively to malicious activity like denial-of-service attacks. This project implements these capabilities in the open-source Bro network security monitor utilized by many NSF-supported organizations nationwide to protect their scientific cyberinfrastructure."
"1541407","CC*DNI Networking Infrastructure: Campus Research Network - High Bandwidth Private Network Path for Research Data from Experiment to Analysis and Back Again at USF","OAC","Campus Cyberinfrastrc (CC-NIE)","01/01/2016","12/16/2015","Timothy Fawcett","FL","University of South Florida","Standard Grant","Kevin Thompson","06/30/2019","$495,645.00","Kenneth Christensen, Ann Eddins, Jeffrey Krischer, Joseph Walton","tfawcett@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","8080","","$0.00","The University of South Florida (USF) is building a Campus Research Network (CRN) to provide campus research laboratories high-speed connectivity to centralized USF High Performance Computing (HPC) facilities to facilitate scientific data-driven experimental work and to enable the analysis of extremely large data sets. The CRN is a dedicated 100 Gb/s Science DMZ connecting the USF Health Informatics Institute, the Neurophysiological Laboratory, and the Auditory & Speech Sciences Laboratory to central USF HPC resources. High performance data transfer nodes (capable of sustaining over 100 Gb/s read/write from/to disk throughput) are installed near the HPC resources as well as the laboratories to facilitate high-speed data transfers, allowing for data transferred to the cluster for analysis to be immediately available to all compute nodes. <br/><br/>The new CRN promises to accelerate the development of innovative, data-driven experimental techniques using near real-time feedback based on the analysis of large experimental data sets to alter experimental and treatment conditions and/or medical device settings to provide a better understanding of age-related effects on brain function as well as enabling the development of brain-controlled sharing devices. The CRN also advances the understanding of diabetes and rare diseases by providing the bandwidth required to transfer the extremely large data sets curated by the Health Informatics Institute to the central USF HPC cluster. This project lays a foundation for connecting multiple Tampa-area USF campuses to the Florida Lambda Rail (and Internet2) at 100 Gb/s, allowing for state-wide, national, and international collaborations."
"1445606","Bridges: From Communities and Data to Workflows and Insight","OAC","ETF, EQUIPMENT ACQUISITIONS, DATANET","12/01/2014","08/28/2018","Nicholas Nystrom","PA","Carnegie-Mellon University","Cooperative Agreement","Robert Chadduck","11/30/2019","$20,895,167.00","Michael Levine, Ralph Roskies, JRay Scott, John Urbanic, Paola Buitrago, Jason Sommerfield","nystrom@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7476, 7619, 7726","7433","$0.00","1.  Abstract: Nontechnical Description<br/><br/>The Pittsburgh Supercomputing Center (PSC) will provide an innovative and groundbreaking high-performance computing (HPC) and data-analytic system, Bridges, which will integrate advanced memory technologies to empower new communities, bring desktop convenience to HPC, connect to campuses, and intuitively express data-intensive workflows.<br/><br/>To meet the requirements of nontraditional HPC communities, Bridges will emphasize memory, usability, Title and effective data management, leveraging innovative new technologies to transparently benefit applications and lower the barrier to entry.<br/>Three tiers of processing nodes with shared memory ranging from 128GB to 12TB will address an extremely broad range of user needs including interactivity, workflows, long-running jobs, virtualization, and high capacity. Flexible node allocation will enable interactive use for debugging, analytics, and visualization. Bridges will also include a shared flash memory device to accelerate Hadoop and databases. <br/><br/>Bridges will host a variety of popular gateways and portals through which users will easily be able to access its resources. Its many nodes will allow long-running jobs, flexible access to interactive use (for example, for debugging, analytics, and visualization, and access to nodes with more memory. Bridges will host a broad spectrum of application software, and its familiar operating system and programming environment will support high-productivity programming languages and development tools.<br/><br/>Bridges will address data management at all levels. Its shared Project File System, connected to processing nodes by a very capable, appropriately scaled fabric, will provide high-bandwidth, low-latency access to large datasets. Storage on each node will provide local filesystem space that is frequently requested by users and will prevent congestion to the shared filesystem. A set of nodes will be optimized for and dedicated to running databases to support gateways, workflows, and applications. Dedicated web server nodes will enable distributed workflows.<br/><br/>Bridges will introduce powerful new CPUs and GPUs, and a new interconnection fabric to connect them. These new technologies will be supported by extremely broad set of applications, libraries, and easy-to-use programming languages and tools.<br/>Bridges will interoperate with and complement other NSF Advanced Cyberinfrastructure resources and large scientific instruments such as telescopes and high-throughput genome sequencers, and it will provide convenient bridging to campuses.<br/><br/>Bridges will enable important advances for science and society. By supporting pioneers who set examples in fields not traditionally users of HPC, and by lowering the barrier of entry, this project will spur others to recognize the power of the technology and transform their fields, as has happened in traditional HPC fields such as physics and chemistry. The project will engage students in research and systems internships, develop and offer training to novices and experts, extend the impact of the new system to minority schools and EPSCoR states, impact the undergraduate and graduate curriculum at many universities, raise the level of computational awareness at four-year colleges, and support the introduction of computational thinking into high schools.<br/><br/>2. Abstract: Technical Description<br/><br/>The Pittsburgh Supercomputing Center will substantially increase the scientific output of a large community of scientific and engineering researchers who have not traditionally used high-performance computing (HPC) resources. This will be accomplished by the acquisition, deployment, and management of Bridges, a HPC system designed for extreme flexibility, functionality, and usability. Bridges will be supported by operations, user service, and networking staff attuned to the needs of these new user communities, and it will offer a wide range of software appropriate for nontraditional HPC research communities. Users will be able to access Bridges through a variety of popular gateways and portals, and the system will provide development tools for gateway building.<br/> <br/>Innovative capabilities to be introduced by Bridges are:<br/><br/>  1.  Three tiers of processing nodes will offer 128GB, 3TB, and 12TB of hardware-supported, coherent shared memory per node to address an extremely broad range of user needs including interactivity, workflows, long-running jobs, virtualization, and high capacity. The 12TB nodes, featuring a proprietary, high-bandwidth internal communication fabric, will be particularly valuable for genome sequence assembly, graph analytics, and machine learning. Bridges will also include a shared flash memory device to accelerate Hadoop and databases. Flexible node allocation will enable interactive use for debugging, analytics, and visualization.<br/><br/>  2.  Bridges will provide integrated, full-time relational and NoSQL databases to support metadata, data management and efficient organization, gateways, and workflows. Database nodes will include SSDs for high IOPs and will be allocated through an extension to the XRAC process. Dedicated web server nodes with high-bandwidth connections to the national cyberinfrastructure will enable distributed workflows. The system topology will provide balanced bandwidth for nontraditional HPC workloads and data-intensive computing.<br/><br/>  3.  Bridges will introduce powerful new CPUs (Intel Haswell and Broadwell), GPUs (NVIDIA GK210 and GP100), the innovative, high-performance Intel Omni Scale Fabric to support increasingly productive development of advanced applications, supported by an extremely broad set of applications, libraries, and easy-to-use programming languages and tools such as OpenACC, parallel MATLAB, Python, and R.<br/><br/>  4.  A shared Project File System (PFS) will provide high-bandwidth, low-latency access to large datasets. Each node will also provide distributed, high-performance storage to support many emerging applications, intermediate and temporary storage, and reduce congestion on the shared PFS.<br/><br/>Bridges will enable important advances for science and society. By supporting pioneers who set examples in fields not traditionally users of HPC, and by lowering the barrier of entry, this project will spur others to recognize the power of the technology and transform their fields, as has happened in traditional HPC fields such as physics and chemistry. The project will engage students in research and systems internships, develop and offer training to novices and experts, extend the impact of the new system to minority schools and EPSCoR states, impact the undergraduate and graduate curriculum at many universities, raise the level of computational awareness at four-year colleges, and support the introduction of computational thinking into high schools."
"1659367","CC* Data: ImPACT - Infrastructure for Privacy-Assured compuTations","OAC","DATANET","05/01/2017","04/19/2018","Ilya Baldin","NC","University of North Carolina at Chapel Hill","Standard Grant","Amy Walton","04/30/2020","$2,983,303.00","Thomas Nechyba, Jeffrey Chase, Jonathan Crabtree, Charles Schmitt, Thomas Carsey","ibaldin@renci.org","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7726","7433","$0.00","This project addresses one of the fundamental problems in distributed computing: the ability to compute on sensitive or private data in a secure manner.  This is critically important for multi-institution and cross-disciplinary sharing and analysis of data, particularly in health-related fields and the social sciences.  This project builds upon and enhances several cyberinfrastructure elements which have been developed through other NSF awards and are already part of the existing research base.  By developing methods for securely sharing private data in different collaborative settings, ImPACT (Infrastructure for Privacy-Assured CompuTations) lowers the barriers to using privacy-restricted data in scientific collaborations. It reduces the time to discovery and opens new avenues for research.<br/><br/>ImPACT enables a variety of research scenarios that require integration of data across subdomains where relevant data sets are held by different stakeholders. The goal of ImPACT is to enable cooperative processing across the stakeholder-owned datasets, while respecting the privacy policies of the individual owners, and to provide a model for collaboration that could be readily used by other institutions.  The project brings together a team that includes social science researchers, cyber-infrastructure experts, and distributed systems and security experts from the University of North Carolina, Duke University, and Indiana University. A non-profit organization formed by the city of Durham, NC provides some of the data sets that are used in the project. To enable end-to-end data flows, ImPACT builds upon prior investments by the NSF Campus Cyberinfrastructure program at the Duke and UNC campuses, as well as the Global Environment for Network Innovations (GENI) program to support straightforward transfers of data within the respective virtual infrastructures.  By supporting three different trust models between collaborators, the project develops diverse methodologies with best practices in networking, data management, security, and privacy preservation to accommodate a variety of collaborative scenarios. Data access, sharing, processing, and publishing are integrated into privacy-aware systems that allow scientists to use their own tools and that build upon enabling cyberinfrastructure technologies like Dataverse, CyVerse, and the Open Resource Control Architecture (ORCA) control software."
"1841598","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Derek Fox","PA","Pennsylvania State Univ University Park","Standard Grant","William Miller","09/30/2019","$33,712.00","Chad Hanna","dfox@astro.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841590","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","03/15/2019","Claudio Kopper","MI","Michigan State University","Standard Grant","William Miller","09/30/2019","$16,002.00","","koppercl@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841594","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","David Hogg","NY","New York University","Standard Grant","William Miller","09/30/2019","$36,469.00","Federica Bianco","david.hogg@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1443068","CIF21 DIBBs: Building a Scalable Infrastructure for Data-Driven Discovery and Innovation in Education","OAC","STEM + Computing (STEM+C) Part, PROGRAM EVALUATION, REAL, DATANET","01/01/2015","02/25/2019","Ken Koedinger","PA","Carnegie-Mellon University","Standard Grant","Amy Walton","12/31/2019","$5,720,819.00","Carolyn Rose, John Stamper","Koedinger@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","005Y, 7261, 7625, 7726","7433, 7726, 8048, 9251","$0.00","This project is creating a community software infrastructure, called LearnSphere, that supports sharing, analysis, and collaboration across the wide variety of educational data.  LearnSphere supports researchers as they improve their understanding of human learning.  It also helps course developers and instructors improve teaching and learning through data-driven course redesign.  The goal is to transform learning science and engineering through a large, distributed data infrastructure, and develop the capacity for course developers, instructors, and learning engineers to make use of it.<br/><br/>LearnSphere maintains a central store of metadata about what datasets exist, but also has distributed features allowing contributors control over access to their own data. It provides a hub to link many communities of educational researchers, provides a repository for researchers to store their data, and provides an open analytic method library and workflow-authoring environment for researchers to build models and run them across datasets.<br/><br/>The research team has extensive experience not only in using educational data mining to make discoveries and improve student outcomes, but also in the creation of educational data infrastructures. They have developed the DataShop infrastructure, which is currently the largest open repository of educational technology data including over 550 datasets. A newer data infrastructure, MOOCdb, is being developed to store and analyze Massively Open Online Course (MOOC) data. The Open Learning Initiative has produced data stored in DataShop for many years and is expanding into the MOOC space. Dialogue-based tutoring systems and student affect sensors are producing new kinds of data that are being added to LearnSphere. The researchers are further improving data collection infrastructure in MOOCs especially by adding platform components for massive multi-factor online experiments. The project is also creating new methods for data integration, discourse data storage and analytics, and new algorithms for automated discovery, as well as new learning science discoveries that result from these algorithms. <br/><br/>By integrating these building blocks in LearnSphere, the project will facilitate cross-modality and cross-domain educational data analysis that is not possible today."
"1841586","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Adam Brazier","NY","Cornell University","Standard Grant","William Miller","09/30/2019","$23,301.00","","abrazier@astro.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845840","CAREER: Scalable Software and Algorithmic Infrastructure for Probabilistic Graphical Modeling","OAC","CAREER: FACULTY EARLY CAR DEV","02/15/2019","02/04/2019","Jaroslaw Zola","NY","SUNY at Buffalo","Continuing grant","Sushil K Prasad","01/31/2024","$307,735.00","","jzola@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1045","1045","$0.00","The data-driven reasoning is one of the major factors propelling progress in science and engineering. In many practical applications, especially in biology and medicine, data-driven reasoning has been based on probabilistic graphical models, that are preferred because of the accuracy in data representation and ease of interpretation. In probabilistic graphical modeling, the modeled objects, for example, the attributes of a patient stored in electronic health records, are represented as random variables, and the goal is to learn dependencies between these variables. However, the methods to learn high-quality probabilistic graphical models from the data are computationally challenging, and do not scale to datasets emerging in modern applications. Therefore, this project aims to enable high-quality probabilistic graphical modeling of large datasets by using high performance computing techniques. To this end, the project introduces a framework of fundamental operations underlying probabilistic graphical modeling, including for managing data and coordinating computations, together with their software fulfillment, that can efficiently leverage large-scale parallel computers. The framework is designed to benefit both practitioners interested in the analysis of large-scale data, and researchers interested in the development of new learning algorithms. The validation and evaluation of the framework is based on the analysis of electronic health records with the goal of early prognosis and diagnosis of patients with chronic obstructive pulmonary disease - problems vital for improving quality and reducing the cost of healthcare. Furthermore, the framework provides the foundation to train the next generation of biomedical professionals in the use of data analytics on advanced cyberinfrastructure. Thus, the project is aligned with NSF's mission to promote the progress of science, and to advance the national health, prosperity and welfare.<br/><br/>This project responds to the recognized and growing demand for scalable machine learning methods that could capitalize on parallel architectures such as large clusters of multi-core processors. The research focus is on exact structure learning of probabilistic graphical models, for example Bayesian networks and Markov random fields, in the context of biomedical data analytics. The project is based on the two main components: a new high performance abstraction for managing data in machine learning applications, including memory efficient strategies for answering counting queries on multi-core processors, and a new programming model for distributed memory systems to facilitate efficient exploration of large-scale combinatorial search spaces, such as those described by tress, lattices or graphs. These abstractions are used to realize a set of new parallel, exact algorithms for structure search, and the related problems, for example Markov blankets identification, that accelerate learning by exploiting various properties of the input data and the underlying search spaces. The research component is driven by the timely and socially relevant application in personalized and preventive medicine, enabled by a massive collection of the actual electronic health records. The project aims to delivers multiple artifacts, including MPI and OpenMP-based software, benchmark data and educational materials, all released as open source for use, further development, enhancement, and incorporation by the community. The research activities are tightly coupled with multiple educational efforts, spanning development of an interdisciplinary course for medical professionals to train them in the use of advanced cyberinfrastructure, engagement of undergraduate students and underrepresented minorities in research, and outreach to middle and high school students to attract them to STEM.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838807","EAGER: Managing our expectations: quantifying and characterizing misleading trajectories in ecological processes","OAC","NSF Public Access Initiative","10/01/2018","08/09/2018","Christine Bahlai","OH","Kent State University","Standard Grant","Beth Plale","09/30/2020","$175,624.00","Kaitlin Whitney","cbahlai@kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","442420001","3306722070","CSE","7414","7916","$0.00","A fundamental problem in ecology is understanding how to scale discoveries: from patterns observed in the lab or the plot to the field or the region, or bridging between short term observations to long term trends and trajectories. The PIs propose a method to directly address the temporal aspects of scaling ecological observations, which involves reusing data from the two dozen Long Term Ecological Research (LTER) sites, an NSF program in place since the early 1980s.  The PIs intend to bridge the gap between short-term observations and the long-term trends using an automated approach of repeatedly sampling moving windows of data from existing long-term time series, and analyzing these sampled data as if they represented the entire dataset.  By compiling typical statistics used to describe the relationship in the sampled data and through repeated samplings, the results will provide insights to the questions, how often are the trends observed in short term data misleading, and can we use characteristics of these trends to predict our likelihood of being misled?  The experiences in reusing the LTER data will be captured and shared with the ecology and open science community.<br/> <br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835573","Elements: Software: The Integrated Geoscience Observatory (InGeO)","OAC","Software Institutes, EarthCube, Space Weather Research","01/01/2019","09/11/2018","Asti Bhatt","CA","SRI International","Standard Grant","Stefan Robila","12/31/2021","$599,750.00","Russell Cosgrove","asti.bhatt@sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","6508592651","CSE","8004, 8074, 8089","026Z, 062Z, 077Z, 4444, 7923, 8004","$0.00","This award will support the development of the Integrated Geoscience Observatory (InGeO). InGeO is an online platform that integrates data and associated software tools contributed by researchers into a unified toolset to enable studying the convergent, systems science. Complex processes in the Sun-Earth system affect the habitability of earth. As technological progress continues, our society also becomes increasingly vulnerable to the disruptions introduced by these complex processes. Study of the Sun-Earth system is traditionally broken-up into separate geoscience disciplines, typically focusing on a specific region of the system. However, often the interaction between several regions end up significantly affecting terrestrial systems. For example, a chain of processes occurring between the Sun, magnetosphere, and ionosphere often result in loss of GPS signal that severely affects all the communication/navigation and infrastructure that depends on this technology. To broach the larger question of the interaction of the subsystems studied by the separate communities, it is necessary to overcome the barriers of communication posed by different observational instruments, software tools for interpreting data, and modeling methods.<br/><br/>To promote systems science, InGeO creates an integrated package of software tools specifically designed to help researchers find and integrate diverse observational data and distributes this toolkit to the community in the most flexible way possible. Building on the pilot project supported by the NSF EarthCube program, the specific features of the toolkit include: (a) linking diverse data sets from multiple observational data repositories; (b) using these data within a shareable computing environment, complete with a full selection of standard numerical and data-processing tools; and  (c) enabling an ever growing assortment of targeted, user-contributed tools, such as assimilative models or interpolation methods. The toolkit can be accessed and used either through a web-based computing environment or by downloading Docker containers for local installation, with a nearly seamless transition between the two. The toolkit is open-source and ensures credit attribution to data and software contributors, and the InGeO team works with members of scientific community (especially students and early career researchers) to improve the toolkit and build a sustainable, more connected user-base.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Activities Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642453","SI2-SSE: The Next Generation of The Montage Image Mosaic Engine: Beyond Mosaics","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","10/01/2016","09/08/2016","Graham Berriman","CA","California Institute of Technology","Standard Grant","Micah Beck","09/30/2019","$499,795.00","","gbb@ipac.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1253, 1798, 8004","1206, 7433, 8004, 8005","$0.00","Data sets produced by modern space missions and ground-based telescopes are so large and complex that they threaten to overwhelm astronomers' ability to analyze them.  In response, the astronomy and IT communities are actively engaged in developing powerful new tools that can process these data.  The Montage Image Mosaic Engine is a participant in this community of next generation tool development.  Up to now, Montage has been used for high-performance processing of images, so that astronomers can take small images and create a mosaic that reveals the structure of a large region of the sky.  With this new funding, Montage will be extended from a processing engine into a ""one-stop shopping"" toolkit that will enable astronomers to manage their image data as well as process them.  They will be able to analyze their data and repair defects found in them, discover and access datasets released worldwide that will make analysis of their own data more valuable, compare and integrate their data sets with other data in web browsers, create their own data archives that can be accessed, as required, by team members or the public, and process large scale image mosaics through an on-demand service that will be hosted by a new astronomy data sharing platform called SciServer.  All these features will dramatically reduce the overhead needed in managing data and will lead to a corresponding increase in productivity.  Montage will at the same time retain all its existing capabilities.  Montage is written as a toolkit rather than as an application, so astronomers can use each of its functions independently of each other and embed them in their processing environments.  It is written in a language, C, that optimizes processing, runs on all Unix based operating systems used by all astronomers, and functions just as well on single machines as on massive computational facilities.  These features enable Montage to be used by astronomers analyzing data on their desktops as well as by teams of scientists creating complex, massive new data sets for distribution to the astronomy community.  One example is its use in the analysis of images to detect new near-Earth asteroids.  It has also been taken up by the computer science community, who use it as an example of a real-world application in driving the development of powerful new cyberinfrastructure platforms for processing massive scientific data sets.  One of the important current topics in scientific software is how to make powerful new tools sustainable so that they can be updated and used over many years.  Montage is a potential model of sustainable software, and is active in national debates on approaches to sustainability.  A version of Montage will be developed that will operate on Windows platforms, used by educators and students.  This brings Montage to high-school students, youth and amateur astronomers.  Partnerships have been built with two groups of educators who will work to integrate Montage into tools used by high-school students.  For example, Montage will be integrated into tools used by students and youth to process images through a network of educational telescopes called SkyNet.<br/><br/>This project will deploy new functionality currently existing as proof-of-concept prototypes, developed as a result of community input, that will transform Montage into a ""data and metadata access, data management and curation toolkit in a box,"" while retaining all its current image processing functionality.  The new functions are: discovery and acquisition of data at scale in space and in time, using R-tree based indexing schemes; tools for analyzing and repairing metadata; tools for visualizing sky coverage of multiple data sets; migration of an on-demand image mosaic service from IPAC to operate at scale on the SciServer platform; investigation of processing images in browsers with Javascript-enabled tools, and the development of custom image processing environments. Finally, a Windows-enabled version of Montage will be delivered.  Montage will continue to have all the benefits of its architecture.  It is written in C for portability and sustainability, is highly scalable and delivered as a toolkit that is easy to incorporate into processing environments.  Montage is the only mosaic engine with all these characteristics.  It is delivered with an Open Source BSD 3-clause license and accessible on GitHub.  Development uses rigorous software engineering methods, and exploits a hybrid of the evolutionary prototyping and staged delivery development models.  Astronomy data sets are becoming much larger and at the same time more complex.  Montage will provide the tools to access, discover and manage data sets and their metadata.  Many more datasets will be made accessible in this way, and astronomers will be able to develop their own data access services without the overhead of maintaining a database.  Because it is written in ANSI-C and has structured inputs and outputs, astronomers will be able to integrate it into any *nix-based platform or environment to support data management and integration.  The capability to search in space and time will be of immense value in querying upcoming time-domain data sets such as ZTF and LSST.  The on-demand service will be an exemplar of deploying image processing services at scale.  Thus, Montage will be of value to large missions and projects, as well as the long-tail of scientists working in small groups.  The impact of Montage will be measured by tracking growth in citations to Montage, and to the creation of data sets that use the software, and by tracking usage of services and environments that use Montage as part of their underpinnings.  Montage is having considerable impact on broad intellectual and educational areas outside astronomical research: (1) Advancing Learning and Discovery- It is used in developing Education and Public Outreach products such as a 5-color Galactic Plane image of Herschel data.  It has found applicability in undergraduate research projects.  The Windows version will bring Montage to high-school students, youth and amateur astronomers.  Browser processing of images may lead to more sophisticated Citizen Science projects; (2) Dissemination to Enhance Scientific and Technological Understanding- It is recognized as an example of sustainable software, and the development team is active in national debates on the issues of software transparency and sustainability; (3) Enhancing Infrastructure for Research and Education- Montage is an exemplar application used to develop national cyber-infrastructure."
"1841617","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Erotokritos Katsavounidis","MA","Massachusetts Institute of Technology","Standard Grant","William Miller","09/30/2019","$32,362.00","","kats@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1443013","CIF21 DIBBs: T2-C2: Timely and Trusted Curator and Coordinator Data Building Blocks","OAC","DATANET, Cyber Secur - Cyberinfrastruc, DMREF","10/01/2014","12/23/2016","Klara Nahrstedt","IL","University of Illinois at Urbana-Champaign","Standard Grant","Amy Walton","09/30/2019","$1,500,000.00","David Nicol, John Rogers, N Aluru, Paul Braun, Brian Cunningham","klara@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7726, 8027, 8292","024E, 7433, 7434, 8027, 8048, 9102","$0.00","A 2008 National Academy report on Integrated Computational Materials Engineering noted that in a rapidly changing and increasingly competitive global market, integrated innovative design and rapid product development must be supported by computationally based designs, fast engineering analysis, and efficient data management tools.  This project addresses the National Materials Review Board recommendations of reducing the cycle from the design of new materials to the fabrication of new devices using new materials. To address the materials-to-devices cycle challenge, the project  focuses on the potential of capturing, curating, correlating and coordinating materials-to-devices digital data in a real-time and trusted manner before fully archiving and publishing the data for wide access and sharing.  The software developed in this project is useful throughout the materials science and device fabrication fields, by automatically collecting, archiving, and providing collected information on all phases of materials and device fabrication development.<br/> <br/>The project develops the Timely and Trusted Curation and Coordination (T2-C2) Data Framework, consisting of two data blocks: <br/>1) T2-C2 Curator, providing  real-time acquisition and curation of digital data from selected materials-making / characterization and device-fabrication instruments in the collaborative research units at the university, the Material Research Lab (MRL) and the Micro-and-Nanotechnology Lab (MNTL) , and<br/>2) T2-C2 Coordinator, where collected data are filtered, correlations among data and dependency relations are identified, and the results are connected to other data processing capabilities. <br/>The goal of the T2-C2 framework is to enable reduction of the development time and cost of materials-making /characterization to device-making processes.<br/><br/>Through open-source software licenses and training programs, the project impacts material science,  device fabrication and other fields within the university, and other interdisciplinary research institutions and their materials design and manufacturing processes. Through courses, tutorials, workshops, and outreach, the project develops interdisciplinary scientists,  teaches the next generation of students, and informs broader audiences about the potential of timely and trusted data collection, curation, spatio-temporal analytics, and correlations between material-making/characterization and device-fabrication processes."
"1755464","CRII: OAC: Scalable Cyberinfrastructure for Big Graph and Matrix/Tensor Analytics","OAC","CRII CISE Research Initiation, EPSCoR Co-Funding","06/01/2018","05/23/2018","Da Yan","AL","University of Alabama at Birmingham","Standard Grant","Sushil Prasad","05/31/2020","$170,941.00","","yanda@uab.edu","AB 1170","Birmingham","AL","352940001","2059345266","CSE","026Y, 9150","026Z, 8228, 9150","$0.00","The existing distributed graph and matrix analytics frameworks are designed with data-intensive workloads in mind, rendering them inefficient for compute-intensive applications such as graph mining and scientific computing. The goal of this project is to develop novel big data frameworks for two compute-intensive tasks, graph mining and matrix/tensor computations, respectively. The two frameworks advance the field of big data analytics by motivating future systems for compute-intensive analytics, and promoting their application in various scientific areas to improve research productivity. The two systems will be available for public use, and can serve several cross-disciplinary projects in computer forensics, computational physics, and bioinformatics. The project includes mentoring graduate students and training K-12 students through summer internships, as well as related new course materials and outreach activities to help the public learn big data technologies. Thus, the project aligns with the NSF's mission to promote the progress of science and to advance the national health and prosperity.<br/><br/>The graph mining system and the matrix/tensor platform share the design of (i) a tailor-made storage subsystem providing efficient and flexible data access, and (ii) a computation subsystem with fine-grained task control for data-reuse-aware task assignment and load balancing. The graph mining system, called G-thinker, aims to facilitate the writing of distributed programs which mine from a big graph those subgraphs that satisfy certain requirements. Such mining problems are useful in many applications like community detection and subgraph matching. These problems usually have a high computational complexity, and existing serial algorithms tackle these problems by backtracking in a duplication-free vertex-set numeration tree, which recursively partitions the search space. G-thinker adopts an intuitive programming interface that minimizes the effort of adapting an existing serial subgraph mining algorithm for distributed execution. The subgraphs to mine are spawned from individual vertices and they grow their frontiers as needed, and memory overflow is avoided by spilling subgraphs to disks when needed. In each machine, vertices and edges shared by multiple subgraphs need only be transmitted and cached once, which minimizes communication (and hence data waiting) so that CPU cores are better utilized. To address the load-balancing problem of power-law graphs, G-thinker explores recursive decomposition and work stealing to allow idle machines to steal subgraphs for mining from heavily-loaded machines. The project also explores a distributed matrix/tensor storage and computing framework, where matrix/tensor partitions are stored in multiple replicas using different storage schemes to efficiently support all kinds of submatrix access operations. This flexible storage scheme offers the upper-layer computations much more opportunities for fine-grained optimizations, including smarter task scheduling and in-situ updates. The use of this framework is exemplified by matrix multiplication and LU factorization. Both of the proposed frameworks can help build a cyberinfrastructure for collaborations with scientists in science, medicine, and industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1535081","SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data","OAC","Software Institutes","09/01/2015","08/05/2015","Jing Yang","NC","University of North Carolina at Charlotte","Standard Grant","Bogdan Mihaila","08/31/2019","$200,950.00","","Jing.Yang@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","8004","7433, 8005","$0.00","Advanced technologies in sensing and computing have created urban trajectory datasets of humans and vehicles travelling over urban networks. Understanding and analyzing the large-scale, complex data reflecting city dynamics is of great importance to enhance both human lives and urban environments. Domain practitioners, researchers, and decision-makers need to manage, query and visualize such big and dynamics data to conduct both exploratory and analytical tasks. To support these tasks, this project develops a open source software tool, named TrajAnalytics, which integrates computational techniques of scalable trajectory database, intuitive and interactive visualization, and high-end computers, to explicitly facilitate interactive data analytics of the emerging trajectory data.<br/><br/>The software provides a new platform for researchers in transportation assessment and planning to directly study moving populations in urban spaces. Researchers in social, economic and behavior sciences can use the software to understand the complicated mechanisms of urban security, economic activities, behavior trends, and so on. Specifically, this software advances the research activities in the community of transportation studies. The software also acts as an outreach platform allowing government agencies to communicate more effectively with the public with the real-world dynamics of city traffic, vehicles, and networks. The integration of research and education prepares the next generation workforce, where students across multi-disciplines can benefit through their participation in the development and use of the software.<br/><br/>In order to lay the foundations for effective analysis of urban trajectory datasets, this software (1) facilitates easy access and unprecedented capability for researchers and analysts with a cloud-based storage and computing infrastructure; (2) develops a parallel graph based data model, named TrajGraph, where massive trajectories are managed on a large-scale graph created from urban networks which is naturally amenable for studying urban dynamics; and (3) provides a visualization interface, named TrajVis, which supports interactive visual analytics tasks with a set of visualization tools and interaction functions. A variety of transportation-related researchers from geography, civil engineering, computer science participate in the design, evaluation, and use of the software. This robust and easy-to-use software enables the users to conduct iterative, evolving information foraging and sense making."
"1535031","SI2-SSE: Collaborative Research: TrajAnalytics: A Cloud-Based Visual Analytics Software System to Advance Transportation Studies Using Emerging Urban Trajectory Data","OAC","Software Institutes","09/01/2015","08/05/2015","Ye Zhao","OH","Kent State University","Standard Grant","Bogdan Mihaila","08/31/2019","$300,000.00","Xinyue Ye","zhao@cs.kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","442420001","3306722070","CSE","8004","7433, 8005","$0.00","Advanced technologies in sensing and computing have created urban trajectory datasets of humans and vehicles travelling over urban networks. Understanding and analyzing the large-scale, complex data reflecting city dynamics is of great importance to enhance both human lives and urban environments. Domain practitioners, researchers, and decision-makers need to manage, query and visualize such big and dynamics data to conduct both exploratory and analytical tasks. To support these tasks, this project develops a open source software tool, named TrajAnalytics, which integrates computational techniques of scalable trajectory database, intuitive and interactive visualization, and high-end computers, to explicitly facilitate interactive data analytics of the emerging trajectory data.<br/><br/>The software provides a new platform for researchers in transportation assessment and planning to directly study moving populations in urban spaces. Researchers in social, economic and behavior sciences can use the software to understand the complicated mechanisms of urban security, economic activities, behavior trends, and so on. Specifically, this software advances the research activities in the community of transportation studies. The software also acts as an outreach platform allowing government agencies to communicate more effectively with the public with the real-world dynamics of city traffic, vehicles, and networks. The integration of research and education prepares the next generation workforce, where students across multi-disciplines can benefit through their participation in the development and use of the software.<br/><br/>In order to lay the foundations for effective analysis of urban trajectory datasets, this software (1) facilitates easy access and unprecedented capability for researchers and analysts with a cloud-based storage and computing infrastructure; (2) develops a parallel graph based data model, named TrajGraph, where massive trajectories are managed on a large-scale graph created from urban networks which is naturally amenable for studying urban dynamics; and (3) provides a visualization interface, named TrajVis, which supports interactive visual analytics tasks with a set of visualization tools and interaction functions. A variety of transportation-related researchers from geography, civil engineering, computer science participate in the design, evaluation, and use of the software. This robust and easy-to-use software enables the users to conduct iterative, evolving information foraging and sense making."
"1339856","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)","OAC","CROSS-EF ACTIVITIES, Software Institutes","10/01/2013","08/29/2013","Mark Miller","CA","University of California-San Diego","Standard Grant","Bogdan Mihaila","09/30/2019","$1,742,099.00","Amitava Majumdar","mmiller@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7275, 8004","7433, 8009","$0.00","Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources' complexities. Given Science Gateways' demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.<br/><br/>SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP's adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development."
"1339774","Collaborative Research: SI2-SSI: Open Gateway Computing Environments Science Gateways Platform as a Service (OGCE SciGaP)","OAC","INFORMATION TECHNOLOGY RESEARC, CROSS-EF ACTIVITIES, Software Institutes","10/01/2013","09/14/2016","Marlon Pierce","IN","Indiana University","Standard Grant","Bogdan Mihaila","09/30/2019","$2,519,986.00","Suresh Marru","marpierc@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","1640, 7275, 8004","019Z, 7433, 8009, 9179","$0.00","Science Gateways are virtual environments that dramatically accelerate scientific discovery by enabling scientific communities to utilize distributed computational and data resources (that is, cyberinfrastructure). Successful Science Gateways provide access to sophisticated and powerful resources, while shielding their users from the resources' complexities. Given Science Gateways' demonstrated impact on progress in many scientific fields, it is important to remove barriers to the creation of new gateways and make it easier to sustain them. The Science Gateway Platform (SciGaP) project will create a set of hosted infrastructure services that can be easily adopted by gateway providers to build new gateways based on robust and reliable open source tools. The proposed work will transform the way Science Gateways are constructed by significantly lowering the development overhead for communities requiring access to cyberinfrastructure, and support the efficient utilization of shared resources.<br/><br/>SciGaP will transform access to large scale computing and data resources by reducing development time of new gateways and by accelerating scientific research for communities in need of access to large-scale resources. SciGaP's adherence to open community and open governance principles of the Apache Software Foundation will assure open source software access and open operation of its services. This will give all project stakeholders a voice in the software and will clear the proprietary fog that surrounds cyberinfrastructure services. The benefits of SciGaP services are not restricted to scientific fields, but can be used to accelerate progress in any field of endeavor that is limited by access to computational resources. SciGaP services will be usable by a community of any size, whether it is an individual, a lab group, a department, an institution, or an international community. SciGaP will help train a new generation of cyberinfrastructure developers in open source development, providing these early career developers with the ability to make publicly documented contributions to gateway software and to bridge the gap between academic and non-academic development."
"1443083","CIF21 DIBBs: Ubiquitous Access to Transient Data and Preliminary Results via the SeedMe Platform","OAC","POLAR CYBERINFRASTRUCTURE, CRCNS, DATANET","10/01/2014","09/08/2014","Amit Chourasia","CA","University of California-San Diego","Standard Grant","Amy Walton","09/30/2019","$1,329,379.00","Michael Norman","amit@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","5407, 7327, 7726","5407, 7433, 7726, 8048, 8089, 8091","$0.00","Computational simulations have become an indispensible tool in a wide variety of science and engineering investigations.  Quick and effective assessments of the resulting data are necessary for efficient use of researcher time and computation resources, but this process is complicated when a large collaborating team is geographically dispersed and/or some team members do not have direct access to the computation resource and output data.  Current methods for sharing and assessing transient data and preliminary results are cumbersome and labor intensive; each research team must create their own scripts and ad hoc procedures to push data from system to system and user to user.  Better tools and cyberinfrastructure are needed to support preliminary results sharing for collaborating computational science teams. <br/><br/>This project develops web-based building blocks and cyberinfrastructure to enable easy sharing and streaming of transient data and preliminary results from computing resources to a variety of platforms, from mobile devices to workstations, making it possible to quickly and conveniently view and assess results and provide an essential missing component in High Performance Computing and cloud computing infrastructure."
"1841527","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Kelly Holley-Bockelmann","TN","Vanderbilt University","Standard Grant","William Miller","09/30/2019","$20,075.00","","k.holley@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841625","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","OFFICE OF MULTIDISCIPLINARY AC, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Patrick Brady","WI","University of Wisconsin-Milwaukee","Standard Grant","William Miller","09/30/2019","$262,279.00","David Kaplan, Philip Chang","prbrady@uwm.edu","P O BOX 340","Milwaukee","WI","532010340","4142294853","CSE","1253, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841588","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Zsuzsanna Marka","NY","Columbia University","Standard Grant","William Miller","09/30/2019","$27,600.00","","zsuzsa@astro.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841370","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Ignacio Taboada","GA","Georgia Tech Research Corporation","Standard Grant","William Miller","09/30/2019","$26,589.00","","itaboada@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841612","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Joshua Bloom","CA","University of California-Berkeley","Standard Grant","William Miller","09/30/2019","$29,542.00","","joshbloom@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841591","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Peter Couvares","CA","California Institute of Technology","Standard Grant","William Miller","09/30/2019","$23,291.00","","peter.couvares@ligo.org","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841544","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Dale Howell","CA","Las Cumbres Observatory Global Telescope Network","Standard Grant","William Miller","09/30/2019","$28,356.00","","ahowell@lcogt.net","6740 Cortona Dr Suite 102","Goleta","CA","931175575","8058801608","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841636","Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics","OAC","COMPUTATIONAL PHYSICS, CESER-Cyberinfrastructure for","10/01/2018","09/10/2018","Gabrielle Allen","IL","University of Illinois at Urbana-Champaign","Standard Grant","William Miller","09/30/2019","$38,210.00","","gdallen@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1798, 7244, 7684","020Z, 026Z, 062Z, 069Z","$0.00","Multi-Messenger Astrophysics (MMA) is an exciting new field of science that combines traditional astronomy with the brand new ability to measure phenomena such as gravitational waves and high-energy neutrino particles that originate from celestial objects. MMA was galvanized when the collision of two neutron stars (labeled GW/GRB 170817A) was detected last year by multiple large science instruments including the NSF-funded Laser Interferometer Gravitational-wave Observatory (LIGO), its sister gravitational wave observatory VIRGO in Italy, NASA's Fermi gamma-ray telescope in space, and many other optical and radio telescopes world-wide. These multiple measurements of just one event improved our estimate of the speed of gravity to phenomenal precision, confirmed that neutron star mergers are the origin of the heavier atomic elements, and gave us a measurement of the radii of neutron stars to about one kilometer accuracy. Over the next decade, LIGO will identify dozens of such events per year. In parallel, the NSF-supported IceCube neutrino observatory at the U.S. South Pole Station and other neutrino and cosmic ray observatories are detecting high-energy cosmic rays from distant cosmological sources; and the new NSF-led Large Synoptic Survey Telescope and upcoming radio astronomy facilities will come online to survey the skies with unprecedented speed and depth throughout the Universe. While each kind of astronomical observing system will bring fascinating new discoveries, it is in their combination - through MMA - that transformative new insights into some of the most fundamental questions about the Universe can be realized: What is the nature of the highest-energy cosmic particle accelerators? What are the properties of cold and hot bulk matter at supra-nuclear densities? How do black holes form and evolve, across their full range of masses, and throughout cosmic time?<br/> <br/>The promise of Multi-Messenger Astrophysics can be realized only if sufficient cyberinfrastructure is available to rapidly handle, combine, and analyze the very large-scale distributed data from all the types of astronomical measurements. This project seeks to carry out community planning for scalable cyberinfrastructure to support MMA. The primary goal is to identify the key questions and cyberinfrastructure projects required by the community to take full advantage of the substantial investments in current facilities, and to realize the enormous potential of the multiple imminent next-generation projects over the decade to come. Two products of the project will be: 1) a community white paper that presents an in-depth analysis of the cyberinfrastructure needs and the opportunities for collaborations among astronomers, computer scientists, and data scientists; and 2) a strategic plan for a scalable cyberinfrastructure institute for multi-messenger astrophysics laying out its proposed mission, identifying the highest priority areas for cyberinfrastructure research and development for the US-based multi-messenger astrophysics community, and presenting a strategy for managing and evolving a set of services that benefits and engages the entire community.<br/> <br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI) and the objectives of ""Harnessing the Data Revolution"" and ""Windows on the Universe, two of the 10 Big Ideas for Future NSF Investments.<br/> <br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences and the Division of Physics in the Directorate of Mathematical & Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835784","Elements: Data: HDR: Collaborative Research: Developing an On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery","OAC","POLAR CYBERINFRASTRUCTURE, DATANET","01/01/2019","08/08/2018","Hongjie Xie","TX","University of Texas at San Antonio","Standard Grant","Amy Walton","12/31/2021","$219,505.00","Alberto Mestas-Nunez","hongjie.xie@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","5407, 7726","062Z, 077Z, 7923","$0.00","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of  sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne).  By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community.  This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products.  The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.<br/><br/>The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information.  This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835512","Collaborative Research:  Elements: Data: HDR: Developing On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery","OAC","DATANET","01/01/2019","08/08/2018","Xin Miao","MO","Missouri State University","Standard Grant","Amy Walton","12/31/2021","$127,966.00","","XinMiao@missouristate.edu","901 South National","Springfield","MO","658970027","4178365972","CSE","7726","062Z, 077Z, 7923","$0.00","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of  sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne).  By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community.  This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products.  The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.<br/><br/>The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information.  This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659449","CC* Networking Infrastructure: University of California Santa Barbara Network Upgrade to 100 Gigabit","OAC","Campus Cyberinfrastrc (CC-NIE)","08/01/2017","07/20/2017","Kelly Caylor","CA","University of California-Santa Barbara","Standard Grant","Kevin L. Thompson","07/31/2019","$481,730.00","Greg Husak, Leila Carvalho, Matthew Hall, David Siegel","caylor@ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","8080","","$0.00","This project provides cyberinfrastructure upgrades that will allow UC Santa Barbara to continue to scale and expand the capability of both internal and external research computing while simultaneously supporting increased cybersecurity and threat detection measures. Upgraded services will enable rapid, unimpeded movement of the diverse and distributed scientific data sets utilized by 120 research teams at UC Santa Barbara. Networking infrastructure upgrades will facilitate learning, science and research, and establish greater accessibility of scientific and research data within the UC Santa Barbara campus as well as with research partners across the nation and the world. <br/><br/>Proposed network enhancements address urgent network saturation challenges across campus by facilitating efficient movement of large data sets between three critical groups of data producers, users, and repositories: (1) Earth and Environmental research groups on campus who affiliate within the Earth Research Institute; (2) the Alexandria Digital Research Library, which is UC Santa Barbara Library's home for collections of digital research materials hosted in the UCSB Library; and (3) the North Hall Data Center on the UC Santa Barbara campus, which is a central repository of data archives, storage, and backup. Additionally, the proposal supports UC Santa Barbara's national and world-wide collaborative computing efforts by increasing networking bandwidth at the edge of campus. These collaborations include involvement in the NSF-funded Pacific Research Platform, the Aristotle federated cloud initiative, the Large Hadron Collider Open Network Environment, and the Extreme Science and Engineering Discovery Environment (XSEDE)."
"1835507","Collaborative Research: Elements: Data: HDR: Developing On-Demand Service Module for Mining Geophysical Properties of Sea Ice from High Spatial Resolution Imagery","OAC","DATANET, EarthCube","01/01/2019","08/08/2018","Chaowei Yang","VA","George Mason University","Standard Grant","Amy Walton","12/31/2021","$249,270.00","","cyang3@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7726, 8074","062Z, 077Z, 7923","$0.00","Sea ice acts as both an indicator and an amplifier of climate change. At present, there are multiple sources of  sea ice observations which are obtained from a variety of networks of sensors (in situ, airborne, and space-borne).  By developing a smart cyberinfrastructure element for the analysis of high spatial resolution (HSR) remote sensing images over sea ice, the science community is better able to extract important geophysical parameters for climate modeling. The project contributes new domain knowledge to the sea ice community.  This is accomplished by integrating HSR images that are spatiotemporally discrete to produce a more rapid and reliable identification of ice types, and by a standardized image processing that allows creating compatible sea ice products.  The cyberinfrastructure module is a value-added on-demand web service that can be naturally integrated with existing infrastructure.<br/><br/>The key objective is to develop a reliable and efficient on-demand Open Geospatial Consortium-compliant web service, which is capable of extracting accurate geographic knowledge of water, submerged ice, bare ice, melt ponds, deformed 'ridging' ice, ridge shadows, and other information from HSR images with limited human intervention. The embedded spatial-temporal analysis framework provides functions to search, explore, visualize, organize, and analyze the discrete HSR images and other related remote sensing data and field data. The project creates a data and knowledge web service for the Arctic sea ice community by integrating computer vision and machine learning algorithms, computing resources, and HSR image data and other useful datasets. The conceptual model improves data flow, so users would query data, download value-added data, and have more consistent results across various sources of information.  This creates new opportunities for scientific analysis that minimizes the investment of time in processing complex and spatiotemporally-discrete HSR imagery. The project includes a strong emphasis on teaching and development of the next-generation workforce through course curricula development, involvement of graduate and undergraduate students in research, and the offering of summer workshops for K-12 teachers (funded by other agencies). The collected images and results of the image analyses will be shared with the public in a timely manner through the NSF Arctic Data Center.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by EarthCube and the Office of the Polar Programs Arctic Natural Sciences Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1724845","CIF21 DIBBs: PD: Building High-Availability Data Capabilities in Data-Centric Cyberinfrastructure","OAC","DATANET","08/01/2017","04/13/2018","Haiying (Helen) Shen","VA","University of Virginia Main Campus","Standard Grant","Amy Walton","07/31/2020","$515,994.00","","hs6ms@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7726","7433, 8048, 9251","$0.00","This project supports data-related analysis in a wide range of science and engineering applications. It will contribute to the development of scalable data-centric cyberinfrastructure capabilities, to accelerate interdisciplinary and collaborative research.  The exascale file system will serve as catalyst for research in data storage architectures, and will enable new data-focused services and capabilities that advance scientific discoveries, collaborations, and innovations.  The project addresses a major data challenge common to a range of communities such as social science, economics, and bioengineering, and will provide thorough training and collaborative research opportunities for project participants.<br/> <br/>Both high performance computing (HPC) clusters and Hadoop clusters use file systems. A Hadoop cluster uses the Hadoop Distributed File System (HDFS) that resides on compute nodes, while an HPC cluster usually uses a remote storage system. Despite years of efforts on research and application development on HPC and Hadoop clusters, the file systems in both types of clusters still face a formidable challenge, that of achieving exascale computing capabilities. The centralized data indexing in HDFS and HPC storage architectures cannot provide high scalability and reliability, and both HDFS and HPC storage architectures have shortcomings such as single point of failure and insufficiently efficient data access. This project builds scalable high-availability data capabilities in data-centric cyberinfrastructure to overcome the shortcomings and create a highly scalable file system with new techniques for distributed load balancing, data replication and consistency maintenance."
"1553436","CAREER: Low Latency, Parallel, and Context Aware Vision in Computed Tomography","OAC","CAREER: FACULTY EARLY CAR DEV","05/15/2016","05/17/2016","James Shackleford","PA","Drexel University","Standard Grant","Sushil K Prasad","04/30/2021","$472,513.00","","shack@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","1045","1045","$0.00","The increasing availability and maturity of non-invasive volumetric imaging techniques has provided the scientific, engineering, and medical communities with powerful methods of acquiring dense quantitative representations of both animate and inanimate objects---providing us with powerful methods of information capture.  This research effort concentrates on approaches for identifying and labeling representations within the captured data that integrate a priori structural and spatial configuration knowledge.  The automatic identification and labeling of volumetric pixels (voxels) produced by volumetric scanners presents challenging ill-posed problems that presently remain unsolved.  Even for well trained clinicians, determining organ boundaries when applying anatomical labels to low contrast medical images can be highly subjective, despite possessing strong prior anatomical knowledge of structure and shape.  Consequently, an anatomical delineation performed by different clinicians, even for the same patient image, fails to provide consistency.  More difficult ill-posed problems, such as matching corresponding voxels between images of a patient taken at different times---or, more difficult yet, between two entirely different patients---are so subjective that humans rarely provide consistent answers.  This research effort aims to develop algorithmic solutions to these problems in order to provide consistent quantitative analysis across volumes; thereby removing subjectivity attributable to inattentional blindness or unintentional bias.  With further advancement, such algorithms will be adequately fast and robust to extract anatomic structural information and perform patient correspondence autonomously at massive scales; thereby enabling powerful data analytics paving the way for the future of data driven medicine.  Through classroom integration and curriculum development, the PI will train science and engineering students to work with this next generation of image processing algorithms.  The PI will recruit and mentor researchers at both the undergraduate and graduate levels with emphasis on the recruitment of underrepresented groups within STEM related fields.  Therefore, this research aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>This research project consists of three primary efforts: first, the simultaneous segmentation of multiple structures using spatial relationship priors (i.e. situationally aware segmentation); second, the development of structurally aware registration algorithms (leveraging segmentation results); and third, the development of these algorithms specifically targeted to data parallel computer architectures.  Situationally aware algorithms developed by the PI will aim to perform simultaneous multi-target segmentations as well as anatomically specialized inference of organ deformation and motion that are robust to imaging inconsistencies, setup variations, and low-dose imaging.  The PI will investigate algorithmic methods possessing robustness to noisy, incomplete, or otherwise challenging image acquisitions by solving multiple inverse problems simultaneously and achieving higher accuracy through the coupling of solutions and exploitation of signal sparsity under certain basises.  The algorithms produced by this research will have broad societal impacts on fields employing computed tomography including archeology, soil sciences, the timber industry, biological sciences, industrial X-ray based inspection, and the aviation security industry.  In addition to the dissemination of the novel algorithms, the PI will develop high-performance parallel implementations as a library released under a permissive open-source license.  Development will occur in the open using Git; thereby enabling agile decentralized development that encourages increased utilization by and contributions from scientists and developers extramural to the project.  Algorithmic facilities initially selected for inclusion are areas of principal investigator's expertise and cover a wide spectrum of applications including motion estimation, image stitching, segmentation, 3D volume reconstruction (computed tomography), and registration/image fusion.  Through consortia and workshops, domain experts will be encouraged to contribute their expertise in established and emerging fields (e.g.  digital image forensics); enabling scientific cross-fertilization and collaboration across domain specific fields."
"1659421","CC* Networking Infrastructure: Expanding STEM Research on Chicago's South Side","OAC","Campus Cyberinfrastrc (CC-NIE)","06/01/2017","12/21/2016","Ophir Trigalo","IL","Illinois Institute of Technology","Standard Grant","Kevin L. Thompson","05/31/2019","$342,798.00","Ibukun Oyewole, Petre Bucurica","trigalo@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","8080","","$0.00","To drive innovation, improve research capabilities and productivity, enhance faculty competiveness, and foster remote collaborations of resources and people, the Illinois Institute of Technology (IIT or Illinois Tech) is building a Science DMZ (Demilitarized Zone) ecosystem to provide access to a secured, high-throughput infrastructure for the IIT research community. The proposed Science DMZ bridges the needs of many diverse data-intensive research projects and address common issues and limitations with real-time data analysis that results from the current campus network. The overall goal for the Science DMZ is to deploy a scalable research network infrastructure with a minimum bandwidth of 10Gbps that can isolate and secure research network traffic from other segments of the campus network without impacting performance. This goal is buttressed by several broad objectives including increasing the bandwidth between the research data center and Internet2 exchange node; and connecting with other world-class universities and research center at throughput speeds sufficient for practical end-to-end research collaborations.<br/><br/>The deployment of the Science DMZ has far-reaching broader impacts. For example, the Science DMZ makes it feasible for IIT researchers analyzing urban metropolitan transportation congestion relief, safety and capital investment to improve the efficiency of intersection utilization across the United States. The Institute of Food Safety and Health (IFSH) is working on a whole genome sequencing project that aims to create a global catalogue of bacteria-causing food poisoning. This advanced high performance infrastructure provides greater collaboration opportunities with Tier-1 research universities and by default, introduces greater opportunities for IIT students to get hands-on experience working with faculty on various research projects, thereby improving the quality of their STEM education. Collaborative activities are also planned with several of IIT's K-12 outreach initiatives including the Global Leaders Program, which seeks to increase access to STEM fields."
"1440745","CC*IIE Integration: Dynamically Optimizing Research Data Workflow with a Software Defined Science Network","OAC","INFORMATION TECHNOLOGY RESEARC","10/01/2014","08/22/2018","Yang Yang","CT","Yale University","Standard Grant","Kevin Thompson","09/30/2019","$788,605.00","Andrew Sherman, David Galassi, Robert Bjornson, Robert Starr","yry@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","1640","1640, 8002","$0.00","Intelligent management of campus research networks has become a major challenge for many institutions, as their networks grow rapidly in size and complexity in order to meet the demands of on-campus scientists who are conducting research, collaborating with peers, and fulfilling their mission of scientific education. Traditional, static network management approaches are no longer adequate, since they often result in low efficiency, poor usability, and unpredictable network application performance.<br/> <br/>The goal of this project is to design and deploy a novel intelligent network cyberinfrastructure that greatly expands the ability of scientists to rapidly and efficiently move the large quantities of data required for computation- and data-intensive scientific workflows. To ensure a broad impact, the project includes specific focus on a range of science drivers in diverse fields such as astronomy, climatology, and genomics.<br/> <br/>The project achieves its goal by leveraging and validating several prior networking research and development efforts. These include Maple, a novel Software Defined Networking (SDN) programming framework developed at Yale, and an Application Layer Traffic Optimization (ALTO) protocol and framework pioneered at Yale and now incorporated in a proposed standard for the Internet by the Internet Engineering Task Force. Maple simplifies network programming for end-to-end, complex, dynamically constructed network services, while ALTO enables network applications to adapt dynamically, according to network states, to deliver network efficiency and application quality of service. In addition, the project builds on prior Yale and NSF investments in high-speed physical network cyberinfrastructure, the widely-adopted InCommon authentication framework, and IPv6 technology."
"1827227","CC* Network Design: The Western South Dakota Research and Education Network","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/29/2018","Douglas Jennewein","SD","University of South Dakota Main Campus","Standard Grant","Kevin Thompson","06/30/2020","$697,922.00","Kara Keeter, Cynthia Anderson, Kenneth Benjamin, Paul Hinker","Doug.Jennewein@usd.edu","414 E CLARK ST","Vermillion","SD","570692307","6056775370","CSE","8080","9150","$0.00","High-speed connectivity to remote state and national cyberinfrastructure resources such as advanced computing systems, data stores, and scientific instrumentation is crucial in states like South Dakota. For this project, the South Dakota School of Mines and Technology (SDSM&T) and Black Hills State University (BHSU) are collaborating with the University of South Dakota (USD) to transform the way campus and state networks are designed and operated by addressing the unique challenges of deploying accessible cyberinfrastructure in a rural state. Specifically, SDSM&T and BHSU are building out campus network infrastructure in support of multidisciplinary research and education including: advanced imaging and microscopy, underground science, weather prediction, and high throughput genetic sequencing.<br/><br/>With a system architecture conceived and designed in year 1 and deployed in year 2 this project aims to increase STEM research and education productivity in western South Dakota by re-architecting campus networks and regional aggregation points so that they are optimized for current and future research and education applications. Architectural characteristics include aspects of the Science DMZ approach to streamline academic data flows; improved wired and wireless connections within and between science buildings and the network core; network measurement leveraging existing campus edge PerfSONAR nodes as well as new nodes at each campus core; data transfer nodes based on the Flash IO Network Appliance; and Globus middleware for data movement. With guidance from state, regional, and national advisors this collaboration is establishing an aggregation model for accessible cyberinfrastructure in rural regions where under-resourced institutions are the norm.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640575","CIF21 DIBBs: EI: Continuous Capture of Metadata for Statistical Data","OAC","ADVANCES IN BIO INFORMATICS, DATANET","10/01/2016","08/02/2016","George Alter","MI","University of Michigan Ann Arbor","Standard Grant","Amy Walton","09/30/2019","$2,565,643.00","Jared Lyle","altergc@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1165, 7726","7433, 8048","$0.00","As the research community responds to increasing demands for public access to scientific data, the need for improvement in data documentation has become critical.  Accurate and complete metadata is essential for data sharing and for interoperability across different data types.  However, the process of describing and documenting scientific data has remained a tedious, manual process even when data collection is fully automated.  General purpose statistical packages (SPSS, SAS, Stata, R) are fundamental to research in the social and behavioral sciences, environmental sciences, biomedical research, and many other fields, but these packages lack tools for documenting how data are modified and new variables created.  By creating tools to capture data transformations from statistical analysis packages, this project creates efficiencies and reduces the costs of data collection, preparation, and re-use.  Two research communities with strong metadata standards and heavy reliance on statistical analysis software (social and behavioral sciences and earth observation sciences) are targeted, but the approach is generalizable to other scientific domains.<br/><br/>Automating documentation of data transformations involves three main steps. First, the most common data transformation operators are standardized and mapped to the Validation and Transformation Language (VTL), an emerging independent standard for describing operations on data in detail.  Second, software parses command scripts for the most widely used statistics packages and translates data transformation operations into VTL.  Third, software tools modify metadata files adhering to existing standards to reflect changes to the data.  This approach embeds detailed variable-level provenance information into standard metadata, and makes it available for data discovery services and automated data analysis tools.   <br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Biological Sciences (Division of Biological Infrastructure), and the NSF Directorate for Social, Behavioral and Economic Sciences (Division of Social and Economic Sciences)."
"1912444","SI2-SSE:   GenApp - A Transformative Generalized Application Cyberinfrastructure","OAC","Software Institutes","08/31/2018","02/11/2019","Emre Brookes","MT","University of Montana","Standard Grant","Stefan Robila","09/30/2020","$197,151.00","","emre.brookes@umontana.edu","32 CAMPUS DRIVE","Missoula","MT","598120001","4062436670","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Scientific computing and computational analysis are becoming integral aspects of virtually any field of science, be it exact sciences like Physics, Chemistry and Biology, or social sciences. Efforts of many research laboratories are focused on creation of scientific codes for data generation, analysis and interpretation. However, publicly funded and often hard won scientific codes developed in a typical research laboratory too frequently become unsustainable  beyond the lifetime of funding or shortly after staff rotation. Projects that are funded to afford expensive computer science expertise simply to maintain and update existing software divert scarce resources from the lab's primary goals and often translates the problem without resolving it.  Only a select number of researchers receive sufficient funding to maintain and update software, limiting the dissemination of new ideas and techniques. The diversity and continually changing nature of software environments compounds the issues.  Enabling user utilization presents hurdles in deployment, access and training.  These issues also create barriers to the implementation of new ideas embodied in new codes.  The GenApp project's goals are to address these issues. To begin with, GenApp enables the rapid dissemination of scientific codes to researchers with minimal software expertise. As more researchers use these codes, more of them become vested in the codes, which helps their sustainability. <br/><br/>The fundamental goal of this project is to advance the GenApp framework into a transformative tool to broadly benefit the scientific software developer community. GenApp is a generalized application generation framework intended for rapid deployment of scientific codes, which can generate both science gateways and stand-alone applications. Among the main unique features of GenApp are the minimal technical expertise requirement for the end user and an open-end design ensuring sustainability of generated applications. To produce fully functional applications, GenApp weaves libraries of fragments and user defined modules as directed by simple definition files, created from a uniform, logical, and simple-to-encode general interface definition file provided by GenApp.  This general definition file and the underlying software can be reused indefinitely to produce applications in a variety of existing and yet-to-be defined software environments. Preserving such simplicity with GenApp's maturation is one of the main developmental strategies. To achieve the goal of GenApp four focus Aims have been proposed. The first is infrastructure development, which includes general enhancements to the capabilities of GenApp. The second is documentation, training, dissemination, outreach and sustainability - all important aspects to produce a software product that is useful to the community. The third is simply feedback, since user and developer feedback will help drive the first two Aims.  The final Aim includes two structural biology domain science applications that will adopt and drive GenApp development.  GenApp will see its primary practical utilization in making highly demanding novel computational and analysis tools accessible to experimentalists and theoreticians working in the nuclear magnetic resonance (NMR) and small-angle scattering (SAS) domains of structural biology. The GenApp framework will serve as a platform for applications utilizing advanced tools requiring efficient use of HPC resources, tools for modeling SAS data with molecular simulations, and a large software suite for a combined analysis of NMR and SAS measurements coupled to computational modeling. Easy access to these powerful tools will enable hitherto impossible studies of a number of fundamental biological problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1664022","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","05/15/2017","04/16/2018","Lucas Wilson","TX","University of Texas at Austin","Standard Grant","Stefan Robila","04/30/2020","$500,000.00","","jnet@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1739549","SI2-SSE: GenApp - A Transformative Generalized Application Cyberinfrastructure","OAC","Software Institutes","10/01/2017","08/29/2017","David Fushman","MD","University of Maryland College Park","Standard Grant","Stefan Robila","09/30/2020","$130,394.00","","fushman@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Scientific computing and computational analysis are becoming integral aspects of virtually any field of science, be it exact sciences like Physics, Chemistry and Biology, or social sciences. Efforts of many research laboratories are focused on creation of scientific codes for data generation, analysis and interpretation. However, publicly funded and often hard won scientific codes developed in a typical research laboratory too frequently become unsustainable  beyond the lifetime of funding or shortly after staff rotation. Projects that are funded to afford expensive computer science expertise simply to maintain and update existing software divert scarce resources from the lab's primary goals and often translates the problem without resolving it.  Only a select number of researchers receive sufficient funding to maintain and update software, limiting the dissemination of new ideas and techniques. The diversity and continually changing nature of software environments compounds the issues.  Enabling user utilization presents hurdles in deployment, access and training.  These issues also create barriers to the implementation of new ideas embodied in new codes.  The GenApp project's goals are to address these issues. To begin with, GenApp enables the rapid dissemination of scientific codes to researchers with minimal software expertise. As more researchers use these codes, more of them become vested in the codes, which helps their sustainability. <br/><br/>The fundamental goal of this project is to advance the GenApp framework into a transformative tool to broadly benefit the scientific software developer community. GenApp is a generalized application generation framework intended for rapid deployment of scientific codes, which can generate both science gateways and stand-alone applications. Among the main unique features of GenApp are the minimal technical expertise requirement for the end user and an open-end design ensuring sustainability of generated applications. To produce fully functional applications, GenApp weaves libraries of fragments and user defined modules as directed by simple definition files, created from a uniform, logical, and simple-to-encode general interface definition file provided by GenApp.  This general definition file and the underlying software can be reused indefinitely to produce applications in a variety of existing and yet-to-be defined software environments. Preserving such simplicity with GenApp's maturation is one of the main developmental strategies. To achieve the goal of GenApp four focus Aims have been proposed. The first is infrastructure development, which includes general enhancements to the capabilities of GenApp. The second is documentation, training, dissemination, outreach and sustainability - all important aspects to produce a software product that is useful to the community. The third is simply feedback, since user and developer feedback will help drive the first two Aims.  The final Aim includes two structural biology domain science applications that will adopt and drive GenApp development.  GenApp will see its primary practical utilization in making highly demanding novel computational and analysis tools accessible to experimentalists and theoreticians working in the nuclear magnetic resonance (NMR) and small-angle scattering (SAS) domains of structural biology. The GenApp framework will serve as a platform for applications utilizing advanced tools requiring efficient use of HPC resources, tools for modeling SAS data with molecular simulations, and a large software suite for a combined analysis of NMR and SAS measurements coupled to computational modeling. Easy access to these powerful tools will enable hitherto impossible studies of a number of fundamental biological problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1664190","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","05/15/2017","05/08/2017","David Anderson","CA","University of California-Berkeley","Standard Grant","Stefan Robila","04/30/2020","$999,999.00","","davea@ssl.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8004","7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1659502","REU Site : Research Experiences in Computational Science, Engineering, and Mathematics (RECSEM)","OAC","RSCH EXPER FOR UNDERGRAD SITES","05/15/2017","05/09/2017","Kwai Wong","TN","University of Tennessee Knoxville","Standard Grant","Sushil Prasad","04/30/2020","$359,426.00","Stanimire Tomov","kwong@utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","1139","9102, 9250","$0.00","The Research Experiences in Computational Science, Engineering, and Mathematics (RECSEM) REU site program at the University of Tennessee (UTK) directs a group of ten undergraduate students to explore the emergent interdisciplinary computational science models and techniques via a number of cohesive compute and data intensive applications. The RECSEM program complements the growing importance of computational sciences in many advanced degree programs and provides scientific understanding and discovery to undergraduates with an intellectual focus on research projects using high performance computing (HPC).  This program aims to deliver a real-world research experience to the students by partnering with teams of scientists engaged in scientific computing research at the National Institutes of Computational Sciences (NICS), the Innovative Computing Laboratory (ICL), and the Joint Institute for Computational Sciences (JICS) at UTK and Oak Ridge National Laboratory.   Additional international students supported by PI's partner universities in Hong Kong also participate in this program. Together the students of RECSEM work collaboratively to achieve their research tasks, and at the same time share a unique opportunity for trading academic experiences, scientific ideas, and cultural social activities in this ten week long program.  Thus, this REU site program supports the NSF's mission to promote the progress of science and to advance the national prosperity.<br/><br/>This program is organized around a synergetic theme of ideas and practices those are common to many scientific applications.  The research projects are categorized in three interrelated areas of research interests: engineering applications, numerical mathematics, and linear algebraic software and tools.  The scope of work for these projects put emphasis on conducting software development, model implementation, and design and evaluation of numerical experiments under the guidance of a team of experts in each scientific domain.  Projects include computation in multi-scale materials science and biomechanics applications, simulation of traffic flow phenomena, implementation of high order parallel numerical schemes, and processing of images with different techniques and algorithms of machine learning and data analytics.  The students would have opportunities to perform large-scale scientific simulations on HPC clusters as well as world-class state-of-the-art supercomputers equipped with the latest hardware technologies provided by the Extreme Science and Engineering Discovery Environment (XSEDE) organization.  These latest computing units include graphical processing units (GPUs), multicore processors and the Intel Xeon Phi processors (MIC). This program is organized in four major stages: HPC training, research formulation, project action, and scientific reporting.  These stages aim to gradually assist the students towards finishing their research projects in time with appropriate level of motivation and guidance."
"1659702","REU Site: INCLUSION - Incubating a New Community of Leaders Using Software, Inclusion, Innovation, Interdisciplinary and OpeN-Science","OAC","RSCH EXPER FOR UNDERGRAD SITES","03/01/2017","12/23/2016","Daniel Katz","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sushil K Prasad","02/29/2020","$359,997.00","Olena Kindratenko","dskatz@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1139","9102, 9250","$0.00","Research laboratories are no longer rooms full of test tubes or microscopes; computers have taken center stage and research across all fields is becoming increasingly digital. This research, including modeling, simulation, data collection, and data analysis is dependent on software, some of which is written or adopted from existing code. However, this software is mostly developed and used by researchers who are not trained in software development, leading to software that is functional but not well-structured, well-documented, robust, or reusable.  To promote the progress of science, the Incubating a New Community of Leaders Using Software, Inclusion, Innovation, Interdisciplinary and Open Science (INCLUSION) project immerses a highly motivated and diverse cohort of students in a guided research process that enables them to experience the thrill of discovery and to adopt the role of scientists as one which is authentically theirs. It aims to inspire students to pursue research careers using open source software and equips them with the skills and knowledge to prepare them for graduate school and employment opportunities, where they will help advance the national health, prosperity, and welfare, and secure the national defense.  INCLUSION trains pairs of students in software skills leveraging and building upon state-of-art lessons. The students work with pairs of mentors on interdisciplinary socially-impactful research projects that develop and use open source software across a wide variety of STEM fields. Their work can lead to research advances and the project contributes open source tools to the larger scientific community, leading to additional advances. By doing so, INCLUSION provides interdisciplinary training for undergraduate researchers, facilitates students' professional growth, and prepares them for the STEM workplace, increasing diversity in the STEM pipeline through engagement in research.<br/><br/>INCLUSION is a 10-week summer program comprising general software-in-research training and focused research projects. Its goals are to: 1) Stimulate enrollment in graduate studies of a diverse student body through involvement of pairs of students in scientific investigation under the mentorship of at least two University of Illinois faculty members from different disciplines; 2) Develop students' ability to apply open source software (OSS) technologies to real-world problems; 3) Promote collaborative ties between the student's institution and University of Illinois faculty, staff and students and provide a basis for future collaborative studies; 4) Develop well documented and tested software for the science and engineering research community; and 5) Build intercultural competences that make minority and underrepresented students competitive in the global marketplace.  INCLUSION students are undergraduates from underrepresented groups from all types of institutions, focused on Minority Serving Institutions.  Mentors develop skills for creating both formal and informal mentoring relationships. The students participate in research and professional development activities that are designed to achieve the goal of retaining and graduating undergraduate students in science and engineering who might not have similar opportunities for research experiences at their own institutions. Challenging research problems prompt students' guided development of research skills: investigation, presentation, and publication: one-on-one introductions by mentors transition to student-led talks and culminate with publications and conference experiences during or after the summer."
"1740097","SI2-SSE:   GenApp - A Transformative Generalized Application Cyberinfrastructure","OAC","Software Institutes","10/01/2017","08/29/2017","Emre Brookes","TX","University of Texas Health Science Center San Antonio","Standard Grant","Stefan Robila","03/31/2019","$269,601.00","","emre.brookes@umontana.edu","7703 FLOYD CURL DR","San Antonio","TX","782293901","2105672340","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Scientific computing and computational analysis are becoming integral aspects of virtually any field of science, be it exact sciences like Physics, Chemistry and Biology, or social sciences. Efforts of many research laboratories are focused on creation of scientific codes for data generation, analysis and interpretation. However, publicly funded and often hard won scientific codes developed in a typical research laboratory too frequently become unsustainable  beyond the lifetime of funding or shortly after staff rotation. Projects that are funded to afford expensive computer science expertise simply to maintain and update existing software divert scarce resources from the lab's primary goals and often translates the problem without resolving it.  Only a select number of researchers receive sufficient funding to maintain and update software, limiting the dissemination of new ideas and techniques. The diversity and continually changing nature of software environments compounds the issues.  Enabling user utilization presents hurdles in deployment, access and training.  These issues also create barriers to the implementation of new ideas embodied in new codes.  The GenApp project's goals are to address these issues. To begin with, GenApp enables the rapid dissemination of scientific codes to researchers with minimal software expertise. As more researchers use these codes, more of them become vested in the codes, which helps their sustainability. <br/><br/>The fundamental goal of this project is to advance the GenApp framework into a transformative tool to broadly benefit the scientific software developer community. GenApp is a generalized application generation framework intended for rapid deployment of scientific codes, which can generate both science gateways and stand-alone applications. Among the main unique features of GenApp are the minimal technical expertise requirement for the end user and an open-end design ensuring sustainability of generated applications. To produce fully functional applications, GenApp weaves libraries of fragments and user defined modules as directed by simple definition files, created from a uniform, logical, and simple-to-encode general interface definition file provided by GenApp.  This general definition file and the underlying software can be reused indefinitely to produce applications in a variety of existing and yet-to-be defined software environments. Preserving such simplicity with GenApp's maturation is one of the main developmental strategies. To achieve the goal of GenApp four focus Aims have been proposed. The first is infrastructure development, which includes general enhancements to the capabilities of GenApp. The second is documentation, training, dissemination, outreach and sustainability - all important aspects to produce a software product that is useful to the community. The third is simply feedback, since user and developer feedback will help drive the first two Aims.  The final Aim includes two structural biology domain science applications that will adopt and drive GenApp development.  GenApp will see its primary practical utilization in making highly demanding novel computational and analysis tools accessible to experimentalists and theoreticians working in the nuclear magnetic resonance (NMR) and small-angle scattering (SAS) domains of structural biology. The GenApp framework will serve as a platform for applications utilizing advanced tools requiring efficient use of HPC resources, tools for modeling SAS data with molecular simulations, and a large software suite for a combined analysis of NMR and SAS measurements coupled to computational modeling. Easy access to these powerful tools will enable hitherto impossible studies of a number of fundamental biological problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1664084","Collaborative Research: SI2-SSI: Expanding Volunteer Computing","OAC","Software Institutes","05/15/2017","05/08/2017","Michael Zentner","IN","Purdue University","Standard Grant","Stefan Robila","04/30/2020","$499,996.00","","mzentner@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8004","7433, 8004, 8009","$0.00","Volunteer computing (VC) uses donated computing time consumer devices such as home computers and smartphones to do scientific computing. It has been shown that VC can provide greater computing power, at lower cost, than conventional approaches such as organizational computing centers and commercial clouds. BOINC is the most common software framework for VC. Essentially, donors of computing time simply have to load BOINC on their computer or smartphone, and then register to donate at the BOINC web site. VC provides ""high throughput computing"": handling lots of independent jobs, with performance goals based on the rate of job completion rather than completion time for individual jobs. This type of computing (all known as high-throughput computing) is in great demand in most areas of science. Until now, the adoption of VC has been limited by its structure. For example, VC projects (such as Einstein@home and Rosetta@home) are operated by individual research groups, and volunteers must browse and choose from among many such projects. As a result, there are relatively few VC projects, and volunteers are mostly tech-savvy computer enthusiasts.  This project aims to solve these problems using two complementary development efforts: First, it will add BOINC-based VC conduits to two major high-performance computing providers: (a) the Texas Advanced Computing Center, a supercomputer center, and (b) nanoHUB, a web portal for nano science that provides computing capabilities.Also, a unified control interface to VC will be developed, tentatively called Science United, where donors can register.  The project  will benefit thousands of scientists who use these facilities, and it will create technology that makes it easy for other HPC providers to add their own VC back ends. Also, Science United will provide a simpler interface to BOINC volunteers where they will register to support scientific areas, rather than specific projects. Science United will also serve as an allocator of computing power among projects. Thus, new projects will no longer have to do their own marketing and publicity to recruit volunteers. Finally, the creation of a single VC ""brand"" (i.e Science United) will allow coherent marketing of VC to the public. By creating a huge pool of low-cost computing power that will benefit thousands of scientists, and increasing public awareness of and interest in science, the project plans to establish VC as a central and long-term part of the U.S. scientific cyber infrastructure.<br/><br/>Adding VC to an existing HPC facility involves several technical issues, which will be addressed as follows: (1) Packaging science applications (which typically run on Linux cluster nodes) to run on home computers (mostly Windows, some Mac and Linux): the team is developing an approach using VirtualBox and Docker, in which the application and its environment (Linux distribution, libraries, executables) are represented as a set of layers comprising a Docker image, which is then run as a container within a Linux virtual machine on the volunteer device. This has numerous advantages: it reduces the work of packaging applications to near zero; it minimizes network traffic because a given Docker layer is downloaded to a host only once; and it provides a strong security sandbox so that volunteer computers are protected from buggy or malicious applications, (2) File management: Input and output files must be moved between existing private servers and public-facing servers that are accessible to the outside Internet. A file management system will be developed, based on Web RPCs, for this purpose. This system will use content-based naming so that a given file is transferred and stored only once. It also maintains job/file associations so that files can be automatically deleted from the public server when they are no longer needed. (3) Submitting and monitoring jobs:  BOINC provides a web interface for efficiently submitting and monitoring large batches of jobs. These were originally developed as part of a system to migrate HTCondor jobs to BOINC. This project is extending it to support the additional requirements of TACC and nanoHUB. Note that these new capabilities are not specific to TACC or nanoHUB: they provide the glue needed to easily add BOINC-based VC to any existing HTC facility. The team is also developing RPC bindings in several languages (Python, C++, PHP). The other component of the project, Science United, is a database-driven web site and an associated web service for the BOINC clients. Science United will control volunteer hosts (i.e. tell them which projects to work for) using BOINC's ""Account Manager"" mechanism, in which the BOINC client on each host periodically contacts Science United and is told what projects to run. Project servers, not Science United, will distribute jobs and files. Science United will define a set of ""keywords"" for science areas (physics, biomedicine, environment, etc.) and for location (country, institution). Projects will be labelled with appropriate keywords. Volunteers will have a yes/no/maybe interface for specifying the types of jobs they want to run. Science United will thus provide a mechanism in which a fraction of total computing capacity can be allocated to a project for a given period. Because total capacity changes slowly over time, this allows near-certain guaranteed allocations. Science United will embody a scheduling system that attempts to enforce allocations, honor volunteer preferences, and maximize throughput. Finally, Science United will do detailed accounting of computing. Volunteer hosts will tell Science United how much work (measured by CPU time and FLOPs, GPU time and FLOPs, and number of jobs) they have done for each project. Science United will maintain historical records of this data for volunteers and projects, and current totals with finer granularity (e.g. for each host/project combination). Finally, Science United will provide web interfaces letting volunteers see their contribution status and history, and letting administrators add projects, control allocations, and view accounting data."
"1713684","Mapping Proton Quark Structure using Petabytes of COMPASS Data","OAC","PETASCALE - TRACK 1","06/01/2017","04/26/2017","Caroline Riedl","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","05/31/2020","$38,892.00","Matthias Perdekamp, Naomi Makins","criedl@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","","$0.00","More than 99% of the mass of visible matter in the universe is nuclear matter. Protons and neutrons are the building blocks of atomic nuclei. Nuclear fusion processes at the core of our sun are the source of the vast energy flow that sustains life on earth. Fission of nuclei provides about 20% of the electricity consumed in the United States and propels many naval vessels. The knowledge of nuclear forces and instrumentation developed for the atomic nuclei and its constituents have important applications, such as x-ray and magnetic resonance imaging (MRI), radiation therapies for cancer treatment, materials science, x-ray lithography, as well as propulsion and power generation.<br/><br/>This project will use Blue Waters to understand the strong force governing the fundamental structure of nuclear matter in nature.  Specifically, the project will use Blue Waters to analyze data from the COMPASS experiment at CERN.  The COMPASS experiment at CERN uses high-energy particle beams to explore the quark substructure of the proton. The experiment constitutes a powerful microscope that can look deep inside the proton. A precise measurement of the dynamics and arrangement of quarks inside the proton will provide experimental input needed to improve the quantitative understanding of the strong nuclear force. COMPASS produces enormous amounts of experimental and Monte-Carlo simulation data.  With its massive data storage capabilities and petascale processing capabilities, Blue Waters will turn the COMPASS data into unique images of quark position and motion inside the proton and will thus refine the theory, Quantum Chromo Dynamics, describing the strong nuclear force."
"1850353","CRII: OAC: Online Optimization of End-to-End Data Transfers in High Performance Networks","OAC","CRII CISE Research Initiation, EPSCoR Co-Funding","03/15/2019","02/20/2019","Engin Arslan","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Sushil K Prasad","02/28/2021","$174,299.00","","earslan@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","026Y, 9150","8228, 9150","$0.00","With the advancement of computing and sensing technology, the amount of data generated by scientific applications is growing rapidly. To accommodate this growth, high speed networks with up to 400 Gbps capacities have been established. Despite the increasing availability of high-speed wide-area networks and the use of modern data transfer protocols designed for high performance, file transfers in practice attain only a fraction of theoretical maximum throughput, leaving networks underutilized and users unsatisfied. This project aims to develop a real-time transfer tuning algorithm to optimize file transfer throughput in high speed networks. Improved data transfer performance does not only enable efficient execution of distributed scientific applications but also fosters collaboration between scientists at geographically separated institutions by reducing time it takes to share data. This project complements the efforts to build next generation networking infrastructure by offering a novel solution to maximize utilization. The project also facilitates the development of a graduate level high-performance networking course at University of Nevada, Reno, and contribute to the education of undergraduate, female, and under-representative students. Therefore, this research aligns with the NSF's mission to promote the progress of science and to advance national prosperity, and welfare. <br/><br/>It is critical to fully utilize available network bandwidth to meet stringent end-to-end performance requirements of distributed scientific workflows. Yet, existing data transfer applications (e.g., scp, bbcp, and ftp) fail to saturate the available network bandwidth due to several factors, such as end system limitations, ill-designed transfer protocols, and poor storage performances. Application-layer transfer tuning offers a comprehensive solution to enhance transfer throughput significantly and can be applied with only client-side modifications. However, finding optimal configuration for application-layer parameters is challenging due to large search space and complex dynamics of network and storage subsystems. This project applies state-of-the-art online convex optimization to application-layer parameter tuning problem as it offers performance and convergence guarantees even under complete uncertainty. In addition to being fast and optimal, online learning algorithms can guarantee the fair distribution of resources among users when combined with game-theory inspired utility functions. The project aims to improve the performance of large streaming applications under dynamic network conditions through anomaly detection and mitigation. It has three unique and innovative aspects: (i) It uses state-of-the art online learning algorithm to fine tune application-layer parameters in real-time. (ii) It improves accuracy and efficiency of sample transfers to minimize the overhead of real-time tuning. (iii) It offers quality of service for delay-sensitive transfers (e.g., high-speed streaming applications) through continuous performance monitoring and adaptive tuning.<br/><br/>This project is jointly funded by Office of Advanced Cyberinfrastructure (OAC) and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839014","Challenges and Opportunities in Scientific Data Discovery and Reuse in the Data Revolution: Harnessing the Power of AI","OAC","NSF Public Access Initiative","10/01/2018","08/17/2018","Nicholas Nystrom","PA","Carnegie-Mellon University","Standard Grant","Beth Plale","09/30/2019","$50,000.00","Paola Buitrago, Huajin Wang, Keith Webster","nystrom@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7414","7556","$0.00","The volume and heterogeneity of scientific data goes beyond a researcher?s ability to find relevant data, make different formats interoperable, and deal with difficult ontological and language issues. Progress requires bringing together experts in data curation with experts in Artificial Intelligence (AI) to begin a dialog and collaboration on AI tools to span the gap between data and its reuse.  The PIs will organize the conference ""Challenges and Opportunities in Scientific Data Discovery and Reuse in the Data Revolution: Harnessing the Power of AI""; the objective is to bring together diverse stakeholders including research librarians and data managers, the AI community, and users and consumers of scientific data to dialog around critical places in the data lifecycle where AI could be groundbreaking.  <br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659255","CC* Networking Infrastructure: Building HPRNet (High-Performance Research Network) for Advancement of Data Intensive Research and Collaboration","OAC","Campus Cyberinfrastrc (CC-NIE)","03/01/2017","12/22/2016","Farzad Mashayek","IL","University of Illinois at Chicago","Standard Grant","Kevin Thompson","02/29/2020","$499,745.00","Maxine Brown, Simon Alford, Richard Cavanaugh, Cynthia Herrera Lindstrom","mashayek@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","8080","","$0.00","The sizes of scientific datasets are growing exponentially across all scientific disciplines due to several factors such as improved scientific instrumentation, social media and decreasing costs of storage. To extract real value from these geographically distant datasets, researchers need to have access to these datasets at high speeds which is typically not possible with traditional campus networks. The University of Illinois at Chicago (UIC) is building ""HPRNet"", a high performance research network providing last mile connectivity for over 31 research projects. HPRNet not only improves the ongoing research productivity, but also sets the stage for future innovations and collaborations. UIC is a public university and minority serving institution (MSI) in the heart of Chicago area where HPRNet significantly impacts the research training of underrepresented groups. The project team is working with other NSF and institutionally funded minority training programs on campus to ensure access to HPRNet resources.<br/><br/>For HPRNet's deployment, 13 locations are identified at UIC where 10 to 40 Gigabit uplinks to regional, national and international R&E networks are established. HPRNet builds on the Science DMZ model that works in concert with the current campus research network (CRN) and a special data storage system known as Data Transfer Node (DTN) to deliver high-performance and reliable network paths for data-intensive applications, including high-volume bulk data transfer, remote experiment and/or instrumentation control, cloud computing, data-mining and advanced visualization."
"1835892","Elements:Software:NSCI: Empowering Data-driven Discovery with a Provenance Collection, Management, and Analysis Software Infrastructure","OAC","Software Institutes","10/01/2018","09/06/2018","Yong Chen","TX","Texas Tech University","Standard Grant","Vipin Chaudhary","09/30/2021","$599,982.00","William Hase, Brian Ancell, Dong Dai","yong.chen@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Scientific breakthroughs are increasingly powered by advanced computing and data analysis capabilities delivered by high performance computing (HPC) systems. In the meantime, many scientific problems have moved to a level of complexity that the ability of understanding the results, auditing how a result is generated, and reproducing the important experiments or simulation results, is critical to scientists. Enabling such a capability in HPC systems requires a holistic collection, management, and analysis software infrastructure for ""provenance"" data, the metadata that describes the history of a piece of data. Such a software infrastructure does not exist yet, which motivates the proposed software development of a lightweight provenance service. With such a software element, many advanced data management functionalities such as identifying the data sources, parameters, or assumptions behind a given result, auditing data history and usage, or understanding the detailed process that how different input data are transformed into outputs can be possible. Responding to the National Strategic Computing Initiative, this project will provide an attractive software infrastructure to future national HPC systems to improve the productivity of science in complex HPC simulation and analysis cycles. The project team will also recruit underrepresented students, mentor graduate and undergraduate students, integrate results into curriculum, and publish and disseminate results.<br/><br/>The lightweight provenance service software on HPC systems will provide: 1) an always-on, background service that automatically and transparently collects and manages provenance for scientific applications, 2) captures comprehensive provenance with accurate causality to support a wide range of use cases, and 3) provides easy-to-use analysis tools for scientists to quickly explore and utilize the provenance. This project will integrate the development, education, and outreach efforts tightly together.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1450195","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","08/01/2015","08/04/2015","Robert Fovell","NY","SUNY at Albany","Standard Grant","Bogdan Mihaila","07/31/2019","$246,111.00","","rfovell@albany.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1828083","MRI: Acquisition of a Next Generation, Data-Centric Supercomputer","OAC","MAJOR RESEARCH INSTRUMENTATION, CYBERINFRASTRUCTURE","09/01/2018","08/24/2018","Christopher Carothers","NY","Rensselaer Polytechnic Institute","Standard Grant","Stefan Robila","08/31/2021","$999,000.00","Mark Shephard, Kristin Bennett, Mohammed Zaki, George Slota","chrisc@cs.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1189, 7231","1189","$0.00","The project will support the acquisition of a data centric supercomputer at Rensselaer Polytechnic Institute (RPI). This instrument will lead to significant advancements in science and engineering problems currently being tackled at RPI's Center for Computational Innovations (CCI) for applications including: the definition of new designed materials, applying active flows control for energy savings and microbiological systems modeling for medical treatment planning. The research will also include the development of new extreme-scale simulation technologies, graph analysis algorithms and the construction of entirely new simulation workflows. Hundreds of researchers and students from over 20 universities, 5 DOE national laboratories, 3 major industrial research centers (Corning, GE and IBM), 50 faculty, 4 start-ups across 11 U.S. states will take advantage of this proposed cyberinstrument to continue making a deep impact on their research. Student participation has been key to CCI's current success and national interest is anticipated not only due to the instrument's ability to advance current research but also due to its potential as a prototype model for future exascale systems.  Students engaged in projects supported by the instrument will become the next generation of compute and data intensive experts.<br/><br/>The new instrument integrates IBM POWER9 CPUs with next generation NVIDIA Volta GPUs into a hardware accelerated unified memory system (e.g., cache coherent). Additionally, all compute nodes are augmented with non-volatile memory storage, and a subset of the nodes include FPGA acceleration. The system will be used by faculty, students and CCI collaborators to address current barriers caused by the need to interact with massive data volumes that are used in and produced by next generation simulation tools. The cyberinstrument and algorithmic developments to be carried out will enable a new level of understanding and enhance our ability to solve many key challenges including: the accurate diagnosis of breast cancer directly from large-scale image datasets; semantic integration of the abundance of heterogeneous, multimodal, and multiscale data to improve personal health; modeling plasmas in fusion reactors; modeling active flow control devices that will greatly increase the weather conditions under which wind turbines will produce electricity; and combined biological data and model integration on molecular, cellular, and organ levels to understand organism-level phenomena and gain predictive understanding in systems biology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740229","SI2-SSE: Collaborative Research: A Sustainable Future for the Glue Multi-Dimensional Linked Data Visualization Package","OAC","EXTRAGALACTIC ASTRON & COSMOLO, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2017","08/29/2017","Michelle Borkin","MA","Northeastern University","Standard Grant","Stefan Robila","09/30/2020","$164,142.00","","m.borkin@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1217, 1253, 8004","1206, 7433, 7569, 8004","$0.00","Glue is a free and open-source application that allows scientists and data scientists to explore relationships within and across related datasets. Glue makes it easy create a wide variety of visualizations (such as scatter plots, bar charts, images) of data, including three dimensional views. What makes Glue unique is its ability to connect datasets together, without merging them into one. Thus, for example, two Earth-based mapping data sets may be connected and jointly visualized by using the coordinates (e.g. latitude and longitude) to glue the maps together, so that when a  user selects (e.g. with a lasso tool) regions in one data set, the corresponding selected subset of data will highlight in all related visualizations simultaneously. These ?linked views"" are especially powerful across wide varieties of plot types. For example, if a user interested in air traffic control glues a data set with information about the 3D locations of all airplanes to a second data set giving weather information, that user could make a combination of selections that would highlight (on maps, in 3D views, or any other display) planes at particular altitudes where thunderstorms might be likely to occur within a specific period of time. In particular, Glue makes it easy for users to create their own kinds of visualizations, which is important because different disciplines often need very specialized ways of looking at data. The software is already being used widely across several disciplines, in particular, astronomy and medicine, for which has been specially optimized. This project will add new features to make Glue more useful in more fields of science (e.g. bioinformatics, epidemiology) where there is demand for linked-view visualization, as well as making it more accessible as an educational tool. In addition, this project will train new users and developers, who will expand Glue into a much more sustainable community effort. <br/><br/>Glue is an open-source package that allows scientists to explore relationships within and across related datasets, by making it easy for them to make multi-dimensional linked visualizations of datasets, select subsets of data interactively or programmatically in 1, 2, or 3 dimensions, and see those selections propagate live across all open visualizations of the data (e.g. graphs, maps, diagnostics charts). A unique feature of glue is that datasets from different sources can be linked to each other, using user-defined mathematical relationships between sets of data components, which makes it possible to carry out selections across datasets. Glue, written in Python, is designed from the ground-up for multidisciplinary work, and it is currently helping researchers make discoveries in geoscience, genomics, astronomy, and medicine. It is also giving insights into data from outside academia, including open data provided by governments and cities. To become sustainable in the long term, glue development needs to become a community-driven effort. Through tutorial and developer workshops, coding sprints, and strategic collaborations with researchers in several disciplines and experienced open source developers, the glue team will help user communities extend glue by developing new functionality useful within particular fields of research. The team will help users contribute the most widely-needed functionality back to glue, and will recruit active contributors to participate in core glue development. As the community grows, glue development will be guided to focus on several major features useful to the broad research community, including: support for very large datasets, support for running glue fully in the browser (inside Jupyter notebooks and Jupyter Lab), and improved interoperability with third-party tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria"
"1739657","SI2-SSE: Collaborative Research: A Sustainable Future for the Glue Multi-Dimensional Linked Data Visualization Package","OAC","EXTRAGALACTIC ASTRON & COSMOLO, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","10/01/2017","08/29/2017","Alyssa Goodman","MA","Harvard University","Standard Grant","Stefan Robila","09/30/2020","$332,369.00","","agoodman@cfa.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","1217, 1253, 8004","1206, 7433, 7569, 7659, 8004, 8005, 9102","$0.00","Glue is a free and open-source application that allows scientists and data scientists to explore relationships within and across related datasets. Glue makes it easy create a wide variety of visualizations (such as scatter plots, bar charts, images) of data, including three dimensional views. What makes Glue unique is its ability to connect datasets together, without merging them into one. Thus, for example, two Earth-based mapping data sets may be connected and jointly visualized by using the coordinates (e.g. latitude and longitude) to glue the maps together, so that when a  user selects (e.g. with a lasso tool) regions in one data set, the corresponding selected subset of data will highlight in all related visualizations simultaneously. These ?linked views"" are especially powerful across wide varieties of plot types. For example, if a user interested in air traffic control glues a data set with information about the 3D locations of all airplanes to a second data set giving weather information, that user could make a combination of selections that would highlight (on maps, in 3D views, or any other display) planes at particular altitudes where thunderstorms might be likely to occur within a specific period of time. In particular, Glue makes it easy for users to create their own kinds of visualizations, which is important because different disciplines often need very specialized ways of looking at data. The software is already being used widely across several disciplines, in particular, astronomy and medicine, for which has been specially optimized. This project will add new features to make Glue more useful in more fields of science (e.g. bioinformatics, epidemiology) where there is demand for linked-view visualization, as well as making it more accessible as an educational tool. In addition, this project will train new users and developers, who will expand Glue into a much more sustainable community effort. <br/><br/>Glue is an open-source package that allows scientists to explore relationships within and across related datasets, by making it easy for them to make multi-dimensional linked visualizations of datasets, select subsets of data interactively or programmatically in 1, 2, or 3 dimensions, and see those selections propagate live across all open visualizations of the data (e.g. graphs, maps, diagnostics charts). A unique feature of glue is that datasets from different sources can be linked to each other, using user-defined mathematical relationships between sets of data components, which makes it possible to carry out selections across datasets. Glue, written in Python, is designed from the ground-up for multidisciplinary work, and it is currently helping researchers make discoveries in geoscience, genomics, astronomy, and medicine. It is also giving insights into data from outside academia, including open data provided by governments and cities. To become sustainable in the long term, glue development needs to become a community-driven effort. Through tutorial and developer workshops, coding sprints, and strategic collaborations with researchers in several disciplines and experienced open source developers, the glue team will help user communities extend glue by developing new functionality useful within particular fields of research. The team will help users contribute the most widely-needed functionality back to glue, and will recruit active contributors to participate in core glue development. As the community grows, glue development will be guided to focus on several major features useful to the broad research community, including: support for very large datasets, support for running glue fully in the browser (inside Jupyter notebooks and Jupyter Lab), and improved interoperability with third-party tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria"
"1626338","MRI: Development of iSNARLD (Instrument for Situational Network Awareness for Real-time and Long-term Data)","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC","09/01/2016","08/03/2016","Gregory Peterson","TN","University of Tennessee Knoxville","Standard Grant","Stefan Robila","08/31/2019","$822,120.00","Hairong Qi, Victor Hazlewood, Garrett Rose","gdp@utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","1189, 1640","1189, 9150","$0.00","In partnership with Alliance Technology Group (Alliance), The University of Tennessee (UT) proposes to develop an instrument for Situational Network Awareness for Real-time and Long-term Data (iSNARLD). Developing this unique instrument enables network packet capture coupled with a powerful analytics engine capable of providing insights into the characteristics of network traffic, breadth of traffic flows, detection of potential attacks or unauthorized data exfiltration, sources of inefficiencies or incorrect behaviors, and the behaviors of user or automated traffic. This instrument will enable, for the first time, analysis that can focus on a specific point in time (such as for real-time analysis), across a range of time spanning months to years, or both. Leveraging the respective strengths in the current the partnership will greatly enhance the prospects for developing an effective instrument with unprecedented capabilities and versatility. Furthermore, the collected network traffic data and associated analytics capabilities will provide information enabling revolutionary new insight into a spectrum of exciting new research thrusts, such as cyber security in forensics and intrusion detection, real-time network situational awareness, operations research into network performance and bottleneck identification, as well as a limitless panoply of other possibilities that heretofore have been impossible. Ergo, the project has extraordinary potential impact on the broad research community as well as to society as a whole. Moreover, the system will impact national cyberinfrastructure and local research as well as the educational mission at UT. Finally, the proposed system has excellent prospects for wide commercial adoption through the partnership with Alliance Technology.<br/><br/>The proposed instrument for Situational Network Awareness for Real-time and Long-term Data (iSNARLD) will fundamentally transform networking and security research through its packet capture and impressive analytics capability. The instrument is the next major step in the evolution of networking infrastructure that provides high bandwidth coupled with security capabilities to detect intrusion and protect computational and data assets. The instrument will build upon a 20Gb/sec SentryWire Sentry 250 system from Alliance with an effective capacity of up to 4 PB (sufficient for months of network data, depending on specific traffic patterns). The proposed system will provide: (1) high-bandwidth, lossless packet capture; (2) metadata extraction to summarize and aggregate packet data to enable efficient analytics; (3) (near) real-time data analytics to support situational awareness of flows and performance along with security applications such as intrusion detection and prevention of data exfiltration; (4) batch or off-line analytics for machine-learning-based discovery; (5) visualization support for enhanced situational awareness and network usage insight; and (6) unprecedented scale of network data (months to years) for analysis. The proposed system presents dramatically improved capabilities for providing situational awareness and understanding network usage in real-time or across periods spanning months to years, providing revolutionary opportunities across a broad spectrum of networking research, operations, and even social science."
"1553685","CAREER: A Framework for Ad Hoc Model Construction in Data Streaming Environments","OAC","CAREER: FACULTY EARLY CAR DEV","05/15/2016","05/12/2016","Sangmi Pallickara","CO","Colorado State University","Standard Grant","Sushil K Prasad","04/30/2021","$491,243.00","","sangmi@cs.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","1045","1045, 9102","$0.00","Over the past decade there has been an exponential growth in data volumes driven in part by data streams generated by computer programs and observational equipment such as satellites, radars, and ecological sensors. Given the data volumes, it can be difficult to harness the data to understand phenomena and/or to make forecasts. Fitting models to the observational data is one way to accomplish this. A precursor to building such models is extracting features from the data. Models constructed using such features can then be used to predict what the outcome will be and when it is likely to happen.  This research will provide scientists and researchers the tools needed to make sense of data streams generated in streaming environments. Domains where this research is broadly applicable include smart cities, traffic planning, homeland security, and ecological monitoring. The project includes an educational component focused on increasing female student participation in college STEM majors. Therefore, this project aligns with the NSF mission to promote the progress of science, to advance the national health, prosperity and welfare, and to secure the national defense.<br/><br/>The project aims to carry out research to create an enabling infrastructure to support the generation, assessment, and refinement of ad hoc models from voluminous, multidimensional, time-series observational data at scale. Challenges in ad hoc model creation stem from the combinatorially explosive number of ways in which models can be realized. The framework, Synapse, aims to support and simplify the naturally iterative and interactive model building process over voluminous streaming data. Modelers will only need to specify a basic set of bootstrap parameters; the framework will manage complexities relating to: (1) how streams are dispersed, (2) how data accesses are managed, (3) coping with I/O and memory contentions, and (4) dispersion of model generation workloads. The research involves scalable techniques for data dispersion employing distributed hash table data structures, map-reduce-based workflows and orchestration of model creation workloads, training data management, and interactive visual assessment of model performance. A visualization component will allow modelers to quickly and effectively assess the quality of a multiplicity of models each possibly covering a different portion of the input feature space and to use these assessments to guide decisions about selection, updates or replacements of models. If successful, the framework will scale with increases in data volumes, the number of available data streams, model generation workloads, and live model evaluations."
"1441963","SI2-SSI: SAGEnext: Next Generation Integrated Persistent Visualization and Collaboration Services for Global Cyberinfrastructure","OAC","Software Institutes, Cyber Secur - Cyberinfrastruc","05/16/2014","09/07/2018","Jason Leigh","HI","University of Hawaii","Continuing grant","Bogdan Mihaila","09/30/2019","$5,204,441.00","","leighj@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","CSE","8004, 8027","7433, 8004, 8009, 8211, 9150, 9251","$0.00","Cyberinfrastructure runs the gamut - from computing, networks, data stores, instruments, observatories, and sensors, to software and application codes - and promises to enable research at unprecedented scales, complexity, resolution, and accuracy. However, it is the research community that must make sense of all the data being amassed, so the SAGEnext (Scalable Adaptive Graphics Environment) framework is an innovative user-centered platform that connects people to their data and colleagues, locally and remotely, via tiled display walls, creating a portal, or wide-aperture ""digital lens,"" with which to view their data and one another. It enables Cyber-Mashups, or the ability to juxtapose and integrate information from multiple sources in a variety of resolutions, as easily as the Web makes access to lower-resolution images today. <br/><br/>SAGEnext expands on a vibrant partnership among national and international universities, supercomputer centers, government laboratories and industry; 100 institutions worldwide use the current version of SAGE. For computational scientists, from such diverse fields as biology, earth science, genomics, or physics, SAGEnext will transform the way they manage the scale and complexity of their data. For computer scientists, SAGEnext is a platform for conducting research in human-computer interaction, cloud computing, and advanced networking. SAGEnext capabilities, integrating visualization application codes, cloud documents, stereo 3D, and new user-interaction paradigms, is unprecedented and heretofore not available, and will have a transformative effect on data exploration and collaboration, making cyberinfrastructure more accessible to end users, in both the laboratory and in the classroom. SAGEnext, integrated with advanced cyberinfrastructure tools, will transform the way today's scientists and future scientists manage the scale and complexity of their data, enabling them to more rapidly address problems of national priority - such as global climate change or homeland security - which benefits all mankind. These same tools can better communicate scientific concepts to public policy and government officials, and via museum exhibitions, to the general public."
"1450170","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","08/01/2015","08/04/2015","William Capehart","SD","South Dakota School of Mines and Technology","Standard Grant","Bogdan Mihaila","07/31/2019","$183,956.00","","William.Capehart@sdsmt.edu","501 East Saint Joseph Street","Rapid City","SD","577013995","6053941218","CSE","1525, 8004, 8074","4444, 7433, 8009, 9150","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1443080","CIF21 DIBBs: Scalable Capabilities for Spatial Data Synthesis","OAC","GEOGRAPHY AND SPATIAL SCIENCES, DATANET","10/01/2014","09/04/2014","Shaowen Wang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Amy Walton","09/30/2019","$1,499,998.00","Anand Padmanabhan, Katarzyna Keahey","shaowen@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1352, 7726","7433, 8048","$0.00","This project will develop a set of tools for spatial data synthesis through scalable data aggregation and integration based on cloud computing, CyberGIS, and other existing tools.  Many scientific problems require the aggregation and integration of large and varied spatial data from a multitude of sources, yet existing approaches and software cannot effectively synthesize the enormous amounts of spatial data that often are available.  This project will resolve problems associated with the use of massive spatial data, thus facilitating work dependent on this type of data for scientific problem solving, such as research on population dynamics and urban sustainability.  Learning materials derived from the research activities will be openly accessible through the CyberGIS Science Gateway.  Targeted massive open online course development will provide inexpensive and efficient ways to teaching students about the capabilities and underlying scientific principles of spatial data synthesis.  A summer school will be offered during the second half of the project to provide a more focused and in-depth training event.<br/><br/>This research project will create scalable capabilities for spatial data synthesis enabled by cloud computing and CyberGIS.  The project will begin by developing the capabilities for solving specific scientific problems and then move on to engage a broader community for validating and improving the core capabilities.  The research will incorporate two interrelated themes:  (1) measuring urban sustainability based on a number of social, environmental, and physical factors and processes; and (2) examining population dynamics by synthesizing multiple population data sources with social media data.  Spatial data synthesis capabilities that the project will provide include extracting metadata and dealing with problems of spatial references and units.  The project also will develop a fundamental capability to characterize uncertainty in data and its propagation."
"1261727","CIF21 DIBBs: Integrating Geospatial Capabilities into HUBzero","OAC","DATANET","10/01/2013","08/30/2017","Xiaohui Carol Song","IN","Purdue University","Cooperative Agreement","Amy Walton","09/30/2019","$5,173,883.00","Larry Biehl, Venkatesh Merwade, Nelson Villoria","carolxsong@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7726","7433, 8048","$0.00","This project will develop geospatial data analysis building blocks as core part of the HUBzero Scientific Collaboration platform to enable researchers and educators to create and share geospatial data sets and modeling tools.  The project will build upon geospatial capabilities that have been developed by IT experts and validated by the science community and bring such capabilities to the masses, hence any domain scientist can develop and deploy geospatial applications with graphical user interfaces on the web.  The deliverables include (1) tools and web service interfaces of ""data space"" for managing and sharing data, including geospatial data, and (2) Extended RAPPTURE Toolkit APIs to support geospatial mapping, image processing, visualization, and access to shared data in the data space. The building blocks software developed in this project will be open source as part of the HUBzero open source releases."
"1838994","EAGER: Crowdsourcing Metadata Enhancements to Improve the Discoverability and Reusability of Scientific Data: Experimental Evaluations","OAC","NSF Public Access Initiative","10/01/2018","08/15/2018","Margaret Levenstein","MI","University of Michigan Ann Arbor","Standard Grant","Beth Plale","09/30/2020","$298,234.00","","maggiel@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7414","7916","$0.00","This exploratory pilot project will undertake two experiments to determine what motivates people to contribute metadata enhancements to data that have been archived, but that are not sufficiently findable, accessible, interoperable, and reusable.  Current practice in data curation relies on the efforts of data producers and professional data curators to produce and provide metadata, including variable level data descriptors, study key words, and bibliographic citations to data-related publications. These efforts are expensive and, as a result, are often undersupplied, leaving data that has been archived and shared with the scientific community of limited value for reuse. The experiments in this project will directly inform potentially transformative efforts to engage the broader community in crowdsourcing enhancements to metadata so that tools and interfaces can be designed that will induce others to participate in this valuable activity, tapping into their knowledge of and interest in data in particular domains to increase data FAIRness. <br/><br/>This project is supported by the National Science Foundation Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838958","FAIR Publishing Guidelines for Spectral Data and Chemical Structures in Support of Chemistry and Related Disciplinary Communities","OAC","NSF Public Access Initiative","09/01/2018","08/17/2018","Leah McEwen","NY","Cornell University","Standard Grant","Beth Plale","08/31/2019","$24,611.00","","lrm1@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7414","7556","$0.00","In this project the PIs will convene a community workshop to establish publishing guidelines for sharing machine-readable files of commonly reported chemical data classes.  The workshop organizers seek involvement from researchers, publishers, funding agencies, libraries, database and repository managers, tool developers, chemical societies, and standards organizations. Topics to discuss include formulating value propositions for data (that is, what data should be kept and for how long), and formulating criteria and stakeholder roles to better support data that are Findable, Accessible, Interoperable, and Reusable (FAIR).  Several outcomes are expected, including best-practice publishing guidelines specifying FAIR criteria for spectral and chemical structure data; and workflow models for capturing, managing and sharing these data in domain repositories. This workshop will build on momentum from the Research Data Alliance (RDA), the International Union of Pure and Applied Chemistry (IUPAC), the American Chemical Society (ACS) and other related scientific societies. <br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659282","CC* Storage: The South Dakota Data Store, a Modular, Affordable Platform to Enable Data-Intensive Research and Education","OAC","DATANET, EPSCoR Co-Funding","03/01/2017","02/07/2017","Douglas Jennewein","SD","University of South Dakota Main Campus","Standard Grant","Amy Walton","02/29/2020","$198,184.00","Brian Burrell, Cynthia Anderson, Dongming Mei, Cheryl Tiahrt","Doug.Jennewein@usd.edu","414 E CLARK ST","Vermillion","SD","570692307","6056775370","CSE","7726, 9150","7433, 9150","$0.00","Expanding opportunities for data-driven research and increasing requirements for data management in sponsored research have resulted in a growing need for retention of both long-term archival data sets that are infrequently accessed, as well as 'active archives' of data that are accessed periodically to revisit, revise, and share experimental results.   For this project, the University of South Dakota will acquire, deploy, and maintain the South Dakota Data Store (SDDS), a network-accessible, sharable, multi-campus storage resource integrated with existing campus cyberinfrastructure.  Initially, SDDS will support twelve STEM projects across eight departments at four institutions in South Dakota, including 30 faculty, 43 postdocs, and 303 students.  SDDS will provide South Dakota researchers with a centralized, efficient, high-performance platform for both archival of and shared access to large quantities of electronic data. <br/><br/>SDDS includes two major services. The Sharing Tier provides high-reliability, high-availability, network-accessible storage for research requiring persistent access to large quantities of data. The Archival Tier provides long-term offsite archival-grade storage.   SDDS will serve all faculty, staff, postdocs, students, and graduate students at the University of South Dakota. Through a partnership of multiple institutions, SDDS will also serve researchers at South Dakota State University, Black Hills State University, and the South Dakota School of Mines and Technology, with access open to further community adoption by all educational and not-for-profit noncommercial researchers.  The project leverages prior investments at the University of South Dakota and South Dakota State University in dedicated Science DMZ networks.  All participants will be able to access SDDS via existing 10Gb connectivity, and end-to-end performance measurement is ensured using existing PerfSONAR deployments at all involved institutions."
"1659425","CC* Cyber Team: Creating a Community of Regional Data and Workflow Cyberinfrastructure Facilitators","OAC","Campus Cyberinfrastrc (CC-NIE)","04/01/2017","02/11/2019","Thomas Hauser","CO","University of Colorado at Boulder","Standard Grant","Kevin Thompson","03/31/2020","$1,497,820.00","Howard Siegel, Patrick Burns, James Williams, Thomas Cheatham, Robert McDonald","thomas.hauser@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8080","","$0.00","A distributed team of data and workflow facilitators (""cyberteam"") from the University of Colorado Boulder, the University of Utah, and Colorado State University support experimental and observational science (EOS) research as part of the Rocky Mountain Advanced Computing Consortium (RMACC). Advances in the number/diversity of data sets require enhanced capabilities to access, reuse, process, analyze, understand, curate, share, and preserve data. A critical aspect of these efforts is to provide expert support for efficient and effective workflows involving data generation, data analysis, visualization, and preservation. Typically, these activities have been the responsibilities of individual researchers, and as a result, data can be difficult to reuse by others. Likewise, computational and data generation workflows are often cobbled together, hard-coded, and not readily amenable for sharing.<br/><br/>These problems are addressed by the distributed RMACC cyberteam who will provide support for researchers in the region by assisting them with data and workflow reuse and management. The facilitators have complementary skills and expertise, and are fully integrated into campus and regional efforts. Their focus is on data curation and metadata, and data and compute workflows, including protected information. These facilitators, in collaboration with others in the region, provide and develop regional, shared resources to support data management for small research groups and under-resourced communities, including working with regional RMACC partners."
"1740095","SI2-SSE: TLDS: Transactional Lock-Free Data Structures","OAC","CI REUSE, Software Institutes","09/01/2017","02/11/2019","Damian Dechev","FL","University of Central Florida","Standard Grant","Stefan Robila","08/31/2020","$427,472.00","","dechev@cs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","6892, 8004","7433, 8004, 8005","$0.00","Exploiting the parallelism in multiprocessor systems is a major challenge in computer science. Multicore programming demands a change in the way we design and use fundamental data structures. Non-blocking data structures allow un-constrained access to shared data; however, care must be taken so that data is not overwritten incorrectly. A main obstacle to the design and use of non-blocking data structures is the lack of a generic methodology for allowing efficient transactional and composable operations. This project, supported by the Office of Advanced Cyberinfrastructure, and the Division of Computing and Communication Foundations, will explore and define the fundamental techniques for the design and use of transactional multiprocessor algorithms. The software elements produced in this work will significantly improve the functionality and reuse of the existing concurrent containers and will accelerate the performance of multiprocessor applications beyond what is possible with current programming libraries. The programming elements created in this project will be disseminated through the open-source release of a software library. The project will also help contribute to a workforce trained in systems programming.<br/><br/>To achieve the project's goals, the PI will explore solutions to overcome two key challenges for supporting high-performance data structure transactions: 1) how to efficiently buffer write operations so that their modifications are invisible to operations outside the transaction's scope, and 2) how to minimize the penalty of rollbacks when aborting partially executed transactions. A representative collection of six transactional lock-free containers will be created including: a linked-list, a set, a skiplist, a multi-dimensional list, a priority queue, and a dictionary. This work will substantially advance the knowledge and practice in multiprocessor software design. To the best of the PI's knowledge, lock-free transactional transformation is the first methodology that provides both lock-free progress and semantic conflict detection for data structure transactions. To achieve this, the PI will create new techniques for conflict detection and recovery in the execution of transactions. A node-based conflict detection scheme that does not rely on Software Transactional Memory nor require the use of an additional data structure will be introduced. A new and efficient conflict recovery strategy will be designed, based on the interpretation of the logical status of nodes instead of explicitly revoking executed operations in an aborted transaction. This research will introduce the first approach for the efficient execution of composable non- blocking transactions across multiple data structures. The software components and data structures engineered as part of this work will be significant research artifacts themselves."
"1450356","SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists","OAC","SPECIAL PROJECTS - CCF, Software Institutes","06/01/2015","06/01/2015","Barbara Lerner","MA","Mount Holyoke College","Standard Grant","Bogdan Mihaila","05/31/2020","$422,997.00","","blerner@mtholyoke.edu","50 College Street","South Hadley","MA","010756456","4135382000","CSE","2878, 8004","7433, 8009","$0.00","Reproducability is the cornerstone of scientific progress. Historically, scientists make their work reproducible by including a formulaic description of the experimental methodology used in an experiment. In an age of computational science, such descriptions no longer adequately describe scientific methodology. Instead, scientific reproducibility relies on a precise and actionable description of the data and programs used to conduct the research. Provenance is the name given to the description of how a digital artifact came to be in its present state. Provenance includes a precise specification of an experiment's input data and the programs or procedures applied to that data. Most computational platforms do not record such data provenance, making it difficult to ensure reproducability. This project addresses this problem through the development of tools that transparently and automatically capture data provenance as part of a scientist's normal computational workflow.<br/><br/>An interdisciplinary team of computer scientists and ecologists have come together to develop tools to facilitate the capture, management, and query of data provenance -- the history of how a digital artifact came to be in its present state.  Such data provenance improves the transparency, reliability, and reproducibility of scientific results. Most existing provenance systems require users to learn specialized tools and jargon and are unable to integrate provenance from different sources; these are serious obstacles to adoption by domain scientists. This project includes the design, development, deployment, and evaluation of an end-to-end system (eeProv) that encompasses the range of activity from original data analysis by domain scientists to management and analysis of the resulting provenance in a common framework with common tools. This project leverages and integrates development efforts on (1) an emerging system for generating provenance from a computing environment that scientists actually use (the R statistical language) with (2) an emerging system that utilizes a library of language and database adapters to store and manage provenance from virtually any source. Accomplishing the goals of this proposal requires fundamental research in resolving the semantic gap between provenance collected in different environments, capturing detailed provenance at the level of a programming language, defining precisely aspects of provenance required for different use cases, and making provenance accessible to scientists."
"1849821","CRII: OAC: Real-time Computational Modeling of Crop Phenological Progress towards Scalable Satellite Precision Farming","OAC","CRII CISE Research Initiation","03/15/2019","02/14/2019","Chunyuan Diao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sushil K Prasad","02/28/2021","$174,988.00","","chunyuan@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","026Y","8228","$0.00","Precision agriculture aims to leverage advanced data-intensive technology to maximize agricultural productivity and to reduce environmental footprints. With considerable technological advancements, it is a pivotal time to harness the intensive data collected in space and time to benchmark precision agriculture systems worldwide. The recent launch of groups of satellites designed to work together -- known as ""satellite constellations"" -- opens up unprecedented opportunities to revolutionize precision agriculture, through monitoring the crop phenological progress at fine spatial and temporal scales. However, the gigantic amount of data brings significant challenges to conventional remote sensing software and tools. The overarching goal of this project is to prototype advanced remote sensing cyberinfrastructure in support of both data- and compute-intensive satellite-based precision agriculture systems. This advanced cyberinfrastructure is applicable to a diverse range of agricultural systems, especially for resource poor and vulnerable smallholder farming systems. With its potential to improve global farming practices, the cyberinfrastructure helps optimize the trajectory of agricultural development to meet future crop demands as well as lower environmental impacts. The integrated educational and training activities of the project offer unique learning opportunities to students of various academic levels and backgrounds, and enhance the broader engagement of diverse scientific communities, especially minority and underrepresented groups. Therefore, this research aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>The advanced remote sensing cyberinfrastructure focuses on the development of an innovative real-time phenological computational (RTPC) model and a high-performance system to harness massive parallelism in modeling crop phenological progress towards scalable satellite-based precision farming. The RTPC model integrates dynamic complex networks with time series remote sensing, and is unique to predict the real-time crop phenological progress at both fine spatial and temporal scales. The high-performance system enhances the parallelism of the RTPC using a hybrid computation model, including a node-level computation model and a system-wide data distribution model. The node-level computation model takes advantage of multi-core architecture of computing nodes to parallelize the compute-intensive RTPC in predicting dynamic network characteristics of crop phenology. The system-wide data distribution model devises a novel Space-and-Time parallel decomposition strategy in distributing massive remote sensing time series data to reduce memory requirements and to achieve high scalability. An open-source toolkit is designed to facilitate the open development and adoption of the remote sensing cyberinfrastructure across a broad range of disciplines. Through leveraging the power of high performance computing and this hybrid computation model, the cyberinfrastructure can analyze PB-level remotely sensed data in a highly scalable manner to conduct real-time monitoring of earth system dynamics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755179","CRII: OAC: Inferring, Attributing, Mitigating and Analyzing the Malicious Orchestration of Internet-scale Exploited IoT Devices: A Network Telescope Approach","OAC","CRII CISE Research Initiation","03/01/2018","02/12/2018","Elias Bou-Harb","FL","Florida Atlantic University","Standard Grant","Sushil K Prasad","02/29/2020","$175,000.00","","ebouharb@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","026Y","026Z, 8228, 9102","$0.00","Despite the benefits provided by the widespread adoption and deployment of diverse Internet-enabled devices such as phones and smart home components in consumer markets and critical infrastructure - the so called Internet of Things (IoT) devices, security concerns are rising as such devices also introduce new vulnerabilities that could be leveraged by attackers to launch disrupting cyber-attacks.  The objective of this project is to enable exploration of the inherent insecurity of the IoT paradigm by exploring innovative data analytics as applied to raw cyber security data.  Insights gained will allow detection, characterization and attribution of Internet-scale compromised IoT devices, coupled with their malicious activities, in near real-time.  Several technical challenges impede addressing IoT security at large, including, the excessive diversity of IoT devices in addition to their Internet-wide deployment, the lack of IoT-relevant data and the shortage of IoT-specific actionable attack signatures.  In this context, this project serves NSF's mission to promote the progress of science by aiming to generate a first-of-a-kind, large-scale analysis of the magnitude of compromised IoT devices.  The project also promotes cyber security research and training for minorities, given that it will be executed within the boundaries of a designated Hispanic-serving institution.  Moreover, the project will contribute to operational cyber security by developing a real-time capability for storing and sharing IoT-relevant threat information.<br/><br/>The project will draw-upon macroscopic, large-scale passive measurement data collected in real-time from a network telescope to highlight the severity of the insecurity of the IoT paradigm.  Network telescopes, most commonly known as darknets, constitute a set of routable, allocated yet unused IP addresses.  The project will design and develop real-time algorithms that are capable of inferring Internet-scale exploited IoT devices by exploring darknet data.  Furthermore, the project will investigate formal correlation approaches rooted in stochastic data structures between IoT-relevant passive measurements and malware samples to aid in the attribution and thus the remediation objective.  The project will further explore the orchestration behavior of seemingly independent IoT activities, which operate within well-coordinated IoT botnets.  To this end, the project will innovate time series analytics based upon trigonometric interpolation techniques, recursive optimal stochastic estimators, and bitmap matching algorithms to infer such IoT botnets by employing passive measurements.The project will also (1) develop a unique cyberinfrastructure for IoT cyber threat indexing by automating the proposed algorithms, techniques and methods, (2) generate IoT-specific signatures by employing piecewise hashing techniques, and (3) create access methods based on an API mechanism and a front-end service facilitated by Elasticsearch to allow the sharing of IoT-centric empirical data, threat intelligence and signatures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1450277","SI2-SSI: Collaborative Research: Bringing End-to-End Provenance to Scientists","OAC","SPECIAL PROJECTS - CCF, Software Institutes","06/01/2015","08/13/2015","Margo Seltzer","MA","Harvard University","Standard Grant","Bogdan Mihaila","05/31/2020","$1,422,728.00","Aaron Ellison, Emery Boose","margo@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","2878, 8004","7433, 8009","$0.00","Reproducability is the cornerstone of scientific progress. Historically, scientists make their work reproducible by including a formulaic description of the experimental methodology used in an experiment. In an age of computational science, such descriptions no longer adequately describe scientific methodology. Instead, scientific reproducibility relies on a precise and actionable description of the data and programs used to conduct the research. Provenance is the name given to the description of how a digital artifact came to be in its present state. Provenance includes a precise specification of an experiment's input data and the programs or procedures applied to that data. Most computational platforms do not record such data provenance, making it difficult to ensure reproducability. This project addresses this problem through the development of tools that transparently and automatically capture data provenance as part of a scientist's normal computational workflow.<br/><br/>An interdisciplinary team of computer scientists and ecologists have come together to develop tools to facilitate the capture, management, and query of data provenance -- the history of how a digital artifact came to be in its present state.  Such data provenance improves the transparency, reliability, and reproducibility of scientific results. Most existing provenance systems require users to learn specialized tools and jargon and are unable to integrate provenance from different sources; these are serious obstacles to adoption by domain scientists. This project includes the design, development, deployment, and evaluation of an end-to-end system (eeProv) that encompasses the range of activity from original data analysis by domain scientists to management and analysis of the resulting provenance in a common framework with common tools. This project leverages and integrates development efforts on (1) an emerging system for generating provenance from a computing environment that scientists actually use (the R statistical language) with (2) an emerging system that utilizes a library of language and database adapters to store and manage provenance from virtually any source. Accomplishing the goals of this proposal requires fundamental research in resolving the semantic gap between provenance collected in different environments, capturing detailed provenance at the level of a programming language, defining precisely aspects of provenance required for different use cases, and making provenance accessible to scientists."
"1640813","CIF21 DIBBs: EI: mProv: Provence-Based Data Analytics Cyberinfrastructure for High-frequency Mobile Sensor Data","OAC","INFORMATION TECHNOLOGY RESEARC, Computer Systems Research (CSR, Networking Technology and Syst, DATANET, Smart and Connected Health","09/01/2016","09/12/2016","Santosh Kumar","TN","University of Memphis","Standard Grant","Amy Walton","08/31/2021","$4,000,000.00","Mani Srivastava, Zachary Ives, Ida Sim","skumar4@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","1640, 7354, 7363, 7726, 8018","7364, 7433, 8048","$0.00","This project addresses a rapidly growing opportunity: the ability of the research community to use high-frequency mobile sensor data.  Mobile sensors (embedded in phones, vehicles, wearables, and the environment) continuously capture data in great detail, and have the potential to address problems in a range of scientific and engineering domains.  This effort focuses upon a specific case -- health data -- that builds upon several capabilities developed in National Institutes of Health (NIH) sponsored projects for assembling and analyzing health data collected through mobile sensors and apps.  Improvements to the usefulness of extremely noisy, distributed data can serve many communities, and the components are extensible outside the human health domain.  <br/><br/>Mobile sensors present a distinct set of data challenges: the data quantity and quality fluctuate, and uncertainty can be high.  Establishing provenance on such noisy data is a challenge, and there are limitations on access to data from human subjects.  This project addresses several of the distinctive challenges associated with mobile sensor data.   Variability is addressed by providing detailed annotation with metadata (such as provenance and quality), and by providing facilities for context-specific reasoning about the metadata.  The system captures provenance metadata along with data in a stream, and propagates this information alongside derived data from one stage to the next.  This creates cyberinfrastructure that makes it possible to 'replay' mobile device data with different configurations, to comparatively benchmark two algorithms or to diagnose erroneous output.  The project builds upon the capabilities and success of the NIH-funded Center of Excellence in Mobile Sensor Data to Knowledge (MD2K), which provides an open-source cyberinfrastructure enabling the collection, curation, analysis, visualization, and interpretation of high-frequency mobile sensor data.  Conducting research with mobile sensor data collected by others continues to be challenging; this project develops a companion open-source provenance cyberinfrastructure, facilitating the sharing of the mobile sensor data itself.  Results include metadata standards, interfaces, and runtime support for annotating data streams with the source (sensor, location, sampling rate, continuous or episodic), semantics of output (number, probability, class), provenance (features, rules for decision), and validation (specificity, sensitivity, benchmark used).  The infrastructure accommodates a wide variety of data types and enables data discovery, analytics, visualization, integration, and validation by third party researchers. The project improves the ability of the wider scientific and engineering community to use mobile sensing systems and metadata, and it also has immediate, tangible societal benefits in health and wellness.  <br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Computer & Information Science & Engineering (Division of Computer and Network Systems, and Division of Information and Intelligent Systems)."
"1535086","SI2-SSE: Human- and Machine-Intelligent Software Elements for Cost-Effective Scientific Data Digitization","OAC","ADVANCES IN BIO INFORMATICS, Software Institutes","08/01/2015","04/22/2016","Andrea Matsunaga","FL","University of Florida","Standard Grant","Bogdan Mihaila","07/31/2019","$488,048.00","Mauricio Tsugawa","ammatsun@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1165, 8004","7433, 8005","$0.00","In the era of data-intensive scientific discovery, Big Data scientists in all communities spend the majority of their time and effort collecting, integrating, curating, transforming, and assessing quality before actually performing discovery analysis. Some endeavors may even start from information not being available and accessible in digital form, and when it is available, it is often in non-structured form, not compatible with analytics tools that require structured and uniformly-formatted data. Two main methods to deal with the volume and variety of data as well as to accelerate the rate of digitization have been to apply crowdsourcing or machine-learning solutions. However, very little has been done to simultaneously take advantage of both types of solutions, and to make it easier for different efforts to share and reuse developed software elements. The vision of the Human- and Machine-Intelligent Network (HuMaIN) project is to accelerate scientific data digitization through fundamental advances in the integration and mutual cooperation between human and machine processing in order to handle practical hurdles and bottlenecks present in scientific data digitization. Even though HuMaIN concentrates on digitization tasks faced by the biodiversity community, the software elements being developed are generic in nature, and expected to be applicable to other scientific domains (e.g., exploring the surface of the moon for craters require the same type of crowdsourcing tool as finding words in text, and the same questions of whether machine-learning tools could provide similar results can be tested).<br/><br/>The HuMaIN project proposes to conduct research and develop the following software elements: (a) configurable Machine-Learning  applications for scientific data digitization (e.g., Optical Character Recognition and Natural Language Processing), which will be made automatically available as RESTful services for increasing the ability of HuMaIN software elements to interoperate with other elements while decreasing the software development time via a new application specification language; (b) workflows leading to a cyber-human coordination system that will take advantage of feedback loops (e.g., based on consensus of crowdsourced data and its quality) for self-adaptation to changes  and increased sustainability of the overall system, (c) new crowdsourcing micro-tasks with ability of being reusable for a variety of scenarios and containing user activity sensors for studying time-effective user interfaces, and (d) services to support automated creation and configuration of crowdsourcing workflows on demand to fit the needs of individual groups. A cloud-based system will be deployed to provide the necessary execution environment with traceability of service executions involved in cyber-human workflows, and cost-effectiveness analysis of all the software elements developed in this project will provide assessment and evaluation of long standing what-if scenarios pertaining human- and machine-intelligent tasks. Crowdsourcing activities will attract a wide range of users with tasks that require low expertise, and at the same time it will expose volunteers to applied science and engineering, potentially attracting interest of K-12 teachers and students."
"1450177","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","08/01/2015","08/04/2015","Brian Ancell","TX","Texas Tech University","Standard Grant","Bogdan Mihaila","07/31/2019","$166,428.00","","brian.ancell@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1642397","SI2 - SSE: A Next-Generation Decision Diagram Library","OAC","Software Institutes","01/01/2017","09/08/2016","Andrew Miner","IA","Iowa State University","Standard Grant","Stefan Robila","12/31/2019","$498,672.00","Gianfranco Ciardo","asminer@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","8004","026Z, 7433, 7942, 8004, 8005","$0.00","There are a variety of scientific problems whose solution is made difficult because of the extremely large number of possibilities that must be considered and evaluated. Often, the difficulty is caused by a large number of combinations of interacting components, even though the individual components are relatively simple. Relevant practical problems are measuring the reliability of a communication network where links may fail (made difficult by the number of different communication paths in the network), or determining that an automobile's brakes will always work (made difficult by the number of combinations of the interacting software and hardware components in an automobile), or determining that the failure of one power generator will not cause a cascading failure that affects a large portion of the nation's power grid. This class of problem is conceptually similar to finding an optimal solution for Rubik's cube (which is made difficult by the large number of different possible configurations) or, in chess, determining if there is a sequence of moves in chess such that white can always force a win (made difficult by the huge number of different possible chess games). The goal of this project is to develop a software library called Meddly that various applications can use to build and represent solutions to these types of combinatoric problems. The underlying technology of Meddly is decision diagrams, a mechanism for organizing data in such a way that repeated patterns or subpatterns are automatically discovered and exploited during computation. The project will add to the capabilities of decision diagram technology, help to advance the understanding of this technology as well as apply it to more types of problems. Several researchers from around the world have expressed interest in Meddly, and as part of this project, developers will assist those researchers to integrate Meddly into existing tools, which will then be applied to real problems. The project also has educational goals, through the engagement of students in the project, and the incorporation of this work in existing courses.<br/><br/>Many computer-based scientific or engineering applications need to store, analyze, and manipulate large data. Often, this data has enough structure that specialized data structures and algorithms can have dramatically smaller memory and time requirements than explicit approaches. An important such case is symbolic verification of hardware and software, where, traditionally, binary decision diagrams (BDDs) have been successfully employed to study systems with enormous state spaces. Several software libraries for BDDs have arisen to support these operations, and BDDs have been applied to diverse applications as a means to exploit structure that is often hidden. However, in the past decade, decision diagram theory has continued to advance, by generalizing BDDs to variants with multi-way decisions (MDDs), multi-way or numerical outcomes, or edge values to encode real-valued data, and by proposing a variety of reduction rules to change (and often shrink) the decision diagram shape, as well as many important algorithmic improvements. Unfortunately, decision diagram libraries have not kept up with these theoretical advances. The proposed work seeks to fill this gap, by merging and expanding two existing prototype libraries developed by the two invesigators, Meddly and TEDDY, into a powerful, next-generation decision diagram library that supports a more general theory of decision diagrams. The new library will encompass (1) non-binary variables, including a-priori unbounded discrete domains and even infinite domains under certain restrictions, (2) non-boolean function values, attached either to terminal nodes or to the edges of the decision diagram, and (3) a more general definition of canonicitythat includes a wide spectrum of reduction rules. Several proposed activities will help smooth the learning curve for users adopting this library, from proven methods such as user manuals, tutorials, examples, wikis, and user groups, to novel ones such as the development of visualization techniques to aid the debugging and understanding of decision diagrams. The proposed software will push decision diagram library support far beyond the capabilities of today's typical BDD libraries, allowing exciting new applications to emerge in diverse fields well beyond classic ones such as symbolic model checking. Additionally, the proposed research activities will improve our understanding of decision diagram technology, providing deep new insights into the nature of structured functions and their representations.  This has the potential to advance the state of the art both in fields that currently utilize decision diagrams, as improved library support can lead to the ability to tackle problem instances of unprecedented size, and in fields where the availability of a library implementing the proposed decision diagram variants will allow researchers to tackle classic problems with novel approaches based on decision diagrams. A next-generation decision diagram library will positively impact disciplines ranging from engineering to computer science theory to biology, via improved software applications that manage large and structured data. Letters of support attest to the many research groups worldwide eager to include more general and powerful decision diagram capabilities in their tools. The anticipated educational impact includes development of publicly available online tutorials; research, implementation, and experimentation opportunities for both undergraduate and graduate students; and integration of the developed techniques and software into existing courses via lectures, assignments, and projects. The underlying theory and developed software resulting from the proposed activities will reinforce concepts that students will retain and apply during their careers."
"1626552","MRI: Acquisition of a Data Lifecycle Instrument (DaLI) for Management and Sharing of Data from Instruments and Observations","OAC","MAJOR RESEARCH INSTRUMENTATION","08/01/2016","08/03/2016","Hakizumwami B. Runesha","IL","University of Chicago","Standard Grant","Stefan Robila","07/31/2019","$725,558.00","Callum Ross, Gordon Kindlmann","runesha@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1189","1189","$0.00","Data from instruments and observations are being generated at increasing rates which leads to increased challenges in data management and computation. It is critical that we address these challenges because these large datasets enable massive breakthroughs across many scientific disciplines. Instruments and observations can generate terabytes of data per day, which often must be transferred from remote locations, field stations, or core facilities to the user's home system for storage and analysis. To address these challenges, The University of Chicago (UChicago) will acquire and operate a Data Lifecycle Instrument (DaLI) to manage and share data from instruments and observations at UChicago and the Marine Biological Laboratory (MBL). DaLI will simplify data management for researchers, allowing them to acquire, transfer, process, and share data from instruments and observations in a single workflow as well as share their data with a larger community of users. In partnership with UChicago, MBL, and collaborating Minority Serving Institutions, DaLI will be used as a training instrument to prepare students to meet the data challenges of the 21st century and will support several outreach programs. <br/><br/>The Data Lifecycle instrument (DaLI) for management and sharing of data from instruments and observations will enable researchers to (a) acquire, transfer, process, and store data from experiments and observations in a unified workflow, (b) manage data collections over their entire life cycle, and (c) share and publish data. DaLI will also (d) enhance outreach and education opportunities and (e) provide a replicable model for other institutions. DaLI will create a scalable, seamless, and replicable infrastructure for data management and sharing to enable new transformative science and enable researchers to implement best practices in data management. The DaLI platform will consist of four pools of resources: a high-performance compute resource for pre- and post-processing of data, a high-performance storage pool, a low cost storage pool, and a tape backup pool. In addition to hardware, DaLI is designed to have software tools that create intuitive interfaces for data lifecycle management and integration with the campus and national cyberinfrastructures."
"1535108","SI2-SSE: Analyze Visual Data from Worldwide Network Cameras","OAC","INFORMATION TECHNOLOGY RESEARC, Software Institutes","08/01/2015","01/11/2017","Yung-Hsiang Lu","IN","Purdue University","Standard Grant","Stefan Robila","07/31/2019","$599,987.00","","yunglu@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1640, 8004","019Z, 1504, 7433, 8004, 8005, 9179, 9251","$0.00","Many network cameras have been deployed for a wide range of purposes, such as monitoring traffic, evaluating air pollution, observing wildlife, and watching landmarks. The data from these cameras can provide rich information about the natural environment and human activities. To extract valuable information from this network of cameras, complex computer programs are needed to retrieve data from the geographically distributed cameras and to analyze the data. This project creates a open source software infrastructure by solving many problems common to different types of analysis programs.  By using this infrastructure, researchers can focus on scientific discovery, not writing computer programs. This project can improve efficiency and thus reduce the cost for running programs analyzing large amounts of data. This infrastructure promotes education because students can obtain an instantaneous view of the network cameras and use the visual information to understand the world. Better understanding of the world may encourage innovative solutions for many pressing issues, such as better urban planning and lower air pollution. This project can enhance diversity through multiple established programs that encourage underrepresented minorities to pursue careers in science and engineering.   <br/><br/>This project will combine: (1) the ability to retrieve data from many heterogeneous and distributed cameras, (2) the management of computational and storage resources using cloud computing, and (3) improved performance by reducing data movement, balancing loads among multiple cloud instances, and enhancing data-level parallelism. The project provides an application programming interface (API) that hides the underlying sophisticated infrastructure. This infrastructure will handle both real-time streaming data and archival data in a uniform way, so that the same analysis programs can be reused.  This project has four major components: (1) a web-based user interface, (2) a database that stores the details about the network cameras, (3) a resource manager that allocates cloud instances, and (4) a computational engine that execute the programs written by users. The service-oriented architecture will allow new functions to be integrated more easily by the research community."
"1828567","MRI: Acquisition of a Regional Resource for Long-Term Archiving of Large Scale Research Data Collections","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC, DATANET, LARS SPECIAL PROGRAMS","09/01/2018","08/23/2018","Henry Neeman","OK","University of Oklahoma Norman Campus","Standard Grant","Stefan Robila","08/31/2021","$967,755.00","Horst Severini, Amy McGovern, Kendra Dresback, Laura Bartley","hneeman@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","1189, 1640, 7726, 7790","062Z, 1189, 9150","$0.00","The project will support University of Oklahoma (OU)'s acquisition, deployment and maintenance of a large-scale storage instrument -- the OU & Regional Research Store (OURRstore). This instrument will enable faculty, staff, postdocs, graduate students and undergraduates to pursue data-intensive research (by building large and growing data collections), and to share and publish these datasets (which they can make discoverable and searchable). Science, Technology, Engineering and Mathematics (STEM) research is increasingly data-intensive, with massive growth of research data collections. Yet a substantial fraction of universities and colleges have been underprepared not only for the volume, velocity and variety of data, but especially for long term stewardship of rapidly growing data collections. In addition, many STEM research projects require substantial storage during their experiments, so much so that holding it all on disk is too expensive to be practical. This challenge is quickly becoming more acute, because disk prices are now improving much more slowly than in the past.<br/><br/>OURRstore is a large-scale storage instrument, consisting of a substantial tape library, as well as crucial support subsystems such as servers, disk, network components and software. OURRstore allows the massive growth of storage capacity needed to address the requirements of the large and broad research community that it serves. Via an innovative, low cost business model, OURRstore offers a cost-effective data storage and access and solution. The research topics supported by the instrument include: weather forecasting, weather radar and weather data mining, including for severe storms such as tornadoes and hurricanes; earthquake triggers and seismology; data and modeling for agriculture, forestry, water and earth ecosystems; coastal simulation; molecular systems that control growth and development of vegetation; microbial contamination of infrastructure; effects of repeated stress on organisms; RNA processing in mitochondria; nanotechnology; visual neuroscience; physiological adaptation to extreme environments; astrophysical objects and stellar atmospheres; malware cybersecurity; influence of microblogs on social media. OURRstore maximizes its impact across the US in the following ways. First, it serves as a national model for affordable, large scale, long term, multi-institutional storage. Second, the OURRstore community includes a substantial number of people from underrepresented populations: over 200 research team members are one or more of the following: women, African Americans, Hispanics, Native Americans, disabled, and US veterans. Third, the instrument will enhance OU's current ""Supercomputing in Plain English"" webinar series, a popular outreach program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1738929","CICI: RSARC: Infrastructure Support for Securing Large-Scale Scientific Workflows","OAC","Cyber Secur - Cyberinfrastruc","09/01/2017","07/14/2017","Ping Yang","NY","SUNY at Binghamton","Standard Grant","Micah Beck","08/31/2020","$999,999.00","Shiyong Lu, Guanhua Yan, Fengwei Zhang","pyang@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","8027","","$0.00","The scientific workflow is an important paradigm for automating and accelerating data processing and sharing in the scientific community. The correctness of scientific discoveries relies on the trustworthiness and reliability of the data processed by scientific workflows and the underlying cyberinfrastructure. Unfortunately, modern scientific workflow systems lack robust infrastructure support for the trustworthy execution of scientific workflows and for the protection of the data processed by such workflows. A scientist or student may forge or alter datasets or computation simply to get papers accepted for publication. A malicious user may also publish forged workflow data on websites, misleading other scientists into investigating and publishing invalid results. This project aims to support a community of engineers and scientists to collaboratively and securely collect, analyze, and share data using scientific workflows.  The success of this project contributes significantly to the national cyberinfrastructure vision of securing the scientific discovery process for a wide range of science and engineering disciplines.<br/><br/>This project develops infrastructure support for secure execution of scientific workflows, detection of anomalous execution flows, and protection of scientific data.  In particular, this project: (1) develops a trusted execution environment for scientific workflows leveraging the Intel Software Guard Extension (SGX) to protect the execution of scientific workflows as well as the data processed by scientific workflows; (2) produces encrypted, tamper-proof, and non-repudiable block-graphs that enable scientists to verify the origin of scientific data and examine how a piece of data was modified and distributed; and (3) develops a machine-learning based anomaly detection technique to detect anomalous execution flows based on logs collected by the underlying cyberinfrastructure."
"1649923","Computational Infrastructure for Brain Research: EAGER: A Scalable Solution for Processing High Resolution Brain Connectomics Data","OAC","ETF, IntgStrat Undst Neurl&Cogn Sys","09/01/2016","08/04/2016","Valerio Pascucci","UT","University of Utah","Standard Grant","William Miller","08/31/2019","$300,000.00","Alessandra Angelucci","pascucci@acm.org","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7476, 8624","026Z, 7916, 8089, 8091","$0.00","Obtaining a ""connectome"" or map of the wiring of the brain is crucial to understanding brain structure and function, and has been set as long-term goals of several international government-funded initiatives due to the potential benefits for improving health, treating brain diseases, and understanding development. As technologies for sample preparation and microscopy advance, it is becoming feasible to image large sections of brain tissue. However, the vast quantities of data produced with these techniques is far outpacing the ability of neuroscientists to analyze the data. This project will address the data analysis challenge by developing new computational software tools that facilitate use of advanced computing for connectomics studies, in alignment with NSF's mission to promote the progress of science and advance national health, prosperity and welfare.<br/><br/>Understanding the microarchitecture and neuronal morphologies that comprise neural circuitry in the brain is crucial to understanding brain function. This EAGER project aims to build the computational and data infrastructure that is necessary to manage and process large microscopy imaging data sets for connectomics studies, bringing High Performance Computing (HPC) resources into the neuroscience workflow. The project will employ a data model that enables scientists to visualize, interact with, and process data of any size that is stored in any remote location, from USB drives to high-performance parallel file systems. The software infrastructure will furthermore enable automatic mapping of analysis procedures designed by neuroscientists to remote HPC systems. The system will leverage state-of-the-art tools and practices developed in the HPC community, and aims to result in greatly accelerated studies of connectivity in the brain at scale.<br/><br/>This Early-concept Grants for Exploratory Research (EAGER) award by the CISE Division of Advanced Cyberinfrastructure is jointly supported by the CISE Division of Information and Intelligent Systems, with funds associated with the NSF Understanding the Brain, BRAIN Initiative activities, and for developing national research infrastructure for neuroscience. This project also aligns with NSF objectives under the National Strategic Computing Initiative."
"1442997","CIF21 DIBBs: An Infrastructure for Computer Aided Discovery in Geoscience","OAC","AERONOMY, DATANET, EarthCube","11/01/2014","08/14/2014","Victor Pankratius","MA","Massachusetts Institute of Technology","Standard Grant","Amy Walton","10/31/2019","$1,424,765.00","Frank Lind, Philip Erickson","pankrat@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1521, 7726, 8074","7433, 8048","$0.00","Next-generation Geoscience needs to handle rapidly growing data volumes from ground-based and space-based sensor networks. As real-world phenomena are mapped to data, the scientific discovery process essentially becomes a search process across multidimensional data sets. The extraction of meaningful discoveries from this sea of data therefore requires highly efficient and scalable machine assistance to enhance human contextual understanding. This is necessary both for testing new hypotheses as well as for the detection of novel events and monitoring for natural hazards.<br/><br/>This project develops a computer-aided discovery approach that provides scientists with better support to answer questions such as: What inferences can be drawn from an identified feature?  What does a finding mean and how does it fit into the big theoretical picture? Does it contradict or confirm previously established models and findings? How can  concepts and ideas be tested effectively? To achieve this, scientists can programmatically express hypothesized Geoscience scenarios, constraints, and model variations. This approach helps delegate the automatic exploration of the combinatorial search space of possible explanations in parallel on a variety of data sets. Furthermore, programmable crawlers can scale the search and discovery of interesting phenomena on cloud-based infrastructures. The computer-aided discovery prototype is evaluated in case studies from Geospace science, including the exploration of structures in space and time using combined GPS, optics, and Geospace radar data."
"1547301","CICI: Data Provenance: Data Quality and Security Evaluation Framework for Mobile Devices Platform","OAC","Cyber Secur - Cyberinfrastruc","09/01/2016","09/08/2015","Leon Reznik","NY","Rochester Institute of Tech","Standard Grant","Micah Beck","08/31/2019","$200,042.00","","lr@cs.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","8027","7434","$0.00","Cyberinfrastructure advancements over the last decade laid a strong foundation for data generation and their communication on a staggering scale, opening up exciting opportunities for setting up a new collaboration between data providers and users, ordinary citizens and government or commercial agencies.  Nowadays, one can easily collect data via a mobile device's sensors and use a smartphone to report dangerous (for example, road slickness or construction hazard) conditions to a government agency. But not all data have equal quality, fidelity, and value. The data may originate from a poor quality camera. The high quality sensor data may be maliciously altered during its transfer over a network with low security. This project develops a framework to calculate integral data quality and security (DQS) indicators and provide them to the user along with data itself. This innovation has a high potential to significantly improve a wide spectrum of science and technology applications as it fuses different types of data based on a new quality and security information application. <br/><br/>The project builds a proof-of-concept design platform to develop, verify and promote a comprehensive methodology for DQS evaluation. It focuses on the integration of cybersecurity metrics with other diverse metrics, such as accuracy, reliability, timeliness and safety, into a single methodological and technological framework. The framework includes generic data structures and algorithms implementing a DQS evaluation. While the developed evaluation techniques cover a wide range of data sources, from cloud based data systems to embedded sensors, the framework's implementation concentrates on using an ordinary user's Android smartphone. The developed methodologies will be verified by their implementation on the eNeighborhoodWatch platform, an application that not only enables interaction between users and government but also, with the user's permission, perform data collection for scientific studies (e.g. noise pollution, earthquake prediction), facilitating collaboration between thousands of ordinary users with the research community."
"1547290","CICI: Data Provenance: Data Quality and Security Evaluation Framework for Mobile Devices Platform","OAC","Cyber Secur - Cyberinfrastruc","09/01/2016","07/12/2018","Justin Cappos","NY","New York University","Standard Grant","Micah Beck","08/31/2019","$315,807.00","","jcappos@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","8027","7434, 9251","$0.00","Cyberinfrastructure advancements over the last decade laid a strong foundation for data generation and their communication on a staggering scale, opening up exciting opportunities for setting up a new collaboration between data providers and users, ordinary citizens and government or commercial agencies.  Nowadays, one can easily collect data via a mobile device's sensors and use a smartphone to report dangerous (for example, road slickness or construction hazard) conditions to a government agency. But not all data have equal quality, fidelity, and value. The data may originate from a poor quality camera. The high quality sensor data may be maliciously altered during its transfer over a network with low security. This project develops a framework to calculate integral data quality and security (DQS) indicators and provide them to the user along with data itself. This innovation has a high potential to significantly improve a wide spectrum of science and technology applications as it fuses different types of data based on a new quality and security information application. <br/><br/>The project builds a proof-of-concept design platform to develop, verify and promote a comprehensive methodology for DQS evaluation. It focuses on the integration of cybersecurity metrics with other diverse metrics, such as accuracy, reliability, timeliness and safety, into a single methodological and technological framework. The framework includes generic data structures and algorithms implementing a DQS evaluation. While the developed evaluation techniques cover a wide range of data sources, from cloud based data systems to embedded sensors, the framework's implementation concentrates on using an ordinary user's Android smartphone. The developed methodologies will be verified by their implementation on the eNeighborhoodWatch platform, an application that not only enables interaction between users and government but also, with the user's permission, perform data collection for scientific studies (e.g. noise pollution, earthquake prediction), facilitating collaboration between thousands of ordinary users with the research community."
"1642446","SI2-SSE: Visualizing Astronomy Repository Data using WorldWide Telescope Software Systems","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","10/01/2016","08/18/2017","Patrick Heidorn","AZ","University of Arizona","Standard Grant","Micah Beck","09/30/2019","$499,997.00","Douglas Roberts, Julie Steffen","heidorn@u.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","1253, 1798, 8004","1206, 7433, 8004, 8005","$0.00","WorldWide Telescope (WWT) provides a powerful data-visualization interface for data exploration and presentation. Through the open source WWT visualization software systems, this project enables the broader use of institutional and community-based, researcher-oriented astronomy data repositories and computational tools. WWT will be integrated with Astrolabe, a University of Arizona (UA) data repository targeted at researchers with legacy data, mostly supporting scholarly articles, and being built to provide dataset access using the (NSF-funded) CyVerse cyberinfrastructure. By creating a full-featured WWT web-based front-end, modularized for ease of connection to diverse astronomy data archives and computational tools, this project incorporates previously un-curated data. The UA and American Astronomical Society (AAS) project team will engage the astronomy research community to participate in these developments to make WWT a community-defined interface for data archives with linkages provided between data, software tools, and journal publications. By expanding the functionality and enabling the integration of WWT into repositories through a web-based version, WWT will become a common tool in the astronomers' workflow including supporting citizen science activities carried out by volunteers. This enables strong connections between research, education, and outreach so that an astronomer could use WWT as part of their research activities, and then share the work with educational communities and the public to benefit society.<br/><br/>The astronomy community faces many challenges related to the large scale of big data, specifically: (1) the visualization of specific datasets in the context of other observations; and (2) organizing and sharing underlying data to enable reuse, citation, and verification of results. Data archives often end up reinventing visualization systems, in turn duplicating previous efforts, and resulting in visualization interfaces built around the data rather than the needs of researchers. The astronomy researcher workflow incorporates depositing data to make it discoverable through search and browsing, accessible through open access, actionable through connections to existing tools as well as community-developed tools running on CyVerse, and finally visualizing or citing data. Through this project, WWT will provide a powerful interface for browsing, interacting with high performance and high throughput processing, and displaying data retrieved from searches of the archives. Effective searching will be enabled by integration with community-managed taxonomy, in the form of the Unified Astronomy Thesaurus (UAT) in both the Astrolabe functions and the WWT interface. In creating a full-featured, web-based client modular version of WWT as a front-end to archives, starting with Astrolabe, and integrating the UAT into search and browsing functions, this project will both serve the broad community of astronomy researchers as well as mitigating costs for archives to develop this visualization capacity."
"1565338","CRII: ACI: Accelerating In-Situ Scientific Data Analysis Using Software-Defined Storage Resource Enclaves","OAC","CRII CISE Research Initiation, RSCH EXPER FOR UNDERGRAD SITES","06/01/2016","04/30/2017","Xuechen Zhang","WA","Washington State University","Standard Grant","Sushil Prasad","05/31/2019","$189,999.00","","xuechen.zhang@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","026Y, 1139","8228, 9251","$0.00","Data intensive knowledge discovery requires scientific applications to run concurrently with analytics and visualization codes, executing in situ for timely output inspection and knowledge extraction. Consequently, the Input/Output (I/O) pipelines for large scientific data analysis can be long and complex because they comprise many ""stages"" of analytics across different layers of the I/O stack of high-performance computing systems. Performance limitations at any I/O layer can cause an I/O bottleneck resulting in longer than expected end-to-end I/O latency. In this project, PI aims to implement a novel data management infrastructure called Software-defined Storage Resource Enclaves (SIREN) at system levels to enforce end-to-end policies that dictate an I/O pipeline's performance. The cross-cutting nature of the technologies developed in the project can help large scientific data analytics leverage the full capability of memory and storage devices on supercomputers. The project will facilitate the development of a graduate level data-intensive computing course at Washington State University Vancouver, and contribute to the education of undergraduate, female, and under-representative students.  Therefore, this research aligns with the NSF mission to promote the progress of science and to advance the national prosperity and welfare.<br/><br/>The technical objectives of the project are three-fold. First, SIREN aims to allow administrators to set allocations for enclaves to manage a group of applications that belong to the same I/O pipeline. Second, it intends to enforce I/O policies (e.g., proportional sharing) at more than one layer of I/O stacks simultaneously considering characteristics of storage devices (e.g., disparity of read/write capacity for SSDs and performance sensitivity to data locality for disks) to achieve optimal performance. Third, PI aims to solve storage-specific implementation issues, including design of user-friendly interfaces, enclave naming and resolution, metadata management, failure handling, and admission control. The introduction of SIREN can fundamentally change the execution model of data staging services widely used on supercomputers. It will also contribute to the understanding of performance characteristics of I/O pipelines under external I/O interference during data staging."
"1450089","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","08/01/2015","08/04/2015","Russ Schumacher","CO","Colorado State University","Standard Grant","Bogdan Mihaila","07/31/2019","$177,173.00","","russ.schumacher@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1450488","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","08/01/2015","10/04/2017","Carl Maltzahn","CA","University of California-Santa Cruz","Standard Grant","Bogdan Mihaila","07/31/2019","$695,525.00","","carlosm@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","1525, 8004, 8074","7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1642441","SI2-SSE: BONSAI: An Open Software Infrastructure for Parallel Autotuning of Computational Kernels","OAC","Software Institutes","11/01/2016","09/12/2016","Jakub Kurzak","TN","University of Tennessee Knoxville","Standard Grant","Vipin Chaudhary","10/31/2019","$499,977.00","Piotr Luszczek","kurzak@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","8004","7433, 8004, 8005","$0.00","Most supercomputers today accelerate the computations by using processors with many cores to solve important problems in science and engineering. Although this reduces the cost of the hardware system, it greatly increases the complexity of writing and optimizing (""tuning"") software. This project extends a previously funded NSF project:  Benchtesting Environment for Automated Software Tuning (BEAST) program to create a software toolkit that allows for semi-automatic optimization of software, thereby reducing the programming overhead. This project, BEAST OpeN Software Autotuning Infrastructure (BONSAI) will greatly increase the efficiency of scientists and engineers to develop fast and efficient programs to solve their problems. BONSAI has tremendous support from various computer processor manufacturing companies and academic institutions. The BONSAI system will be available as open-source software for academic and commercial use and many students will be trained in using the software.<br/><br/>The emergence and growing dominance of hybrid systems that incorporate accelerator processors, such as GPUs and coprocessors, have made it far more difficult to optimize the performance of the different computational kernels that do the majority of the work in most research applications. The BONSAI project aims to create a transformative solution to this problem by developing a software infrastructure that uses parallel hybrid machines to enable large autotuning sweeps on computational kernels for GPU accelerators and many-core coprocessors. The system will go beyond just measuring runtimes, allowing for collection and analysis of non-trivial amount of data from hardware performance counters and power meters. The system will have a modular architecture and rely on standard data formats and interfaces to easily connect with mainstream tools for data analytics and visualization. The BONSAI project will leverage the experiences of the BEAST project, which established a successful autotuning methodology and validated an autotuning workflow. BONSAI will equip the community with a software environment for applying parallel resources to the tuning and performance analysis of computational kernels. Specifically, the work will be organized around the following objectives: (1) Harden and extend the programming language called BeastLang, which was created during prior research as a way of defining the search space that the autotuning infrastructure generates and explores. BeastLang enables users to create parameterized kernel specifications that encode the interplay between the kernels themselves, the compilation tools, and the target hardware. It will be integrated with the other components of BONSAI, have its Python syntax enhanced and extended, its compiler improved, and be supplemented with a runtime that supports it with multi-way parallelism for the autotuning process. (2) Develop and test a benchtesting engine for making large scale parallel tuning sweeps, using large numbers of GPU accelerators or many-core coprocessors. This engine will support both parallel compilation and parallel tests of the resulting kernels, using many distributed memory nodes and multithreading within each node, with dynamic load balancing. It will produce an extensive collection of performance information from hardware counters, and possibly energy meters, as well as collection of information about the saturation of the compiled code with different classes of instructions. (3) Develop and test a software infrastructure for collecting, preprocessing, and analyzing BONSAI performance data. The system will a) simplify the task of instrumenting the kernel and provide a simple interface for selecting the metrics to be collected with sensible defaults; b) simplify the process of collecting hardware counters and performance data from various open source and vendor specific libraries; and c) provide tools that allow the user to quickly and efficiently transform output data to a format that can be easily read and analyzed using mainstream tools such as R and Python. (4) Document and illustrate the process of using BONSAI to tune various different types of kernels. These model case studies will include discussions of how BeastLang was applied to create the parameterized kernel stencil, how the parallel benchtesting engine is invoked to generate and explore the search space, and how the data collected from the operation of the engine can be analyzed and visualized to gain insights that can correct or refine the process for another iteration. The BONSAI project has the potential to fundamentally transform autotuning research by: 1) Making autotuning accessible to a broad audience of developers from a broad range of computing disciplines, as opposed to a few selected individuals with the wizardry to set up a successful experiment within the confines of serial execution. 2) Changing the general perception of autotuning as not just the means of producing fast code, but as a general technique for performance analysis and reasoning about the complex software and hardware interactions, and positioning the technique as one of primary tools for hardware-software co-design. 3) Boosting interest in exploring neglected avenues of computing, such as exploration of unorthodox data layouts, and challenge the status quo of legacy software interfaces. BONSAI has the potential to bring autotuning to the forefront of software development and to help position autotuning as a pillar of software engineering."
"1450439","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","08/01/2015","08/04/2015","Allen Evans","WI","University of Wisconsin-Milwaukee","Standard Grant","Bogdan Mihaila","07/31/2019","$164,381.00","","evans36@uwm.edu","P O BOX 340","Milwaukee","WI","532010340","4142294853","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1450180","Collaborative Research: SI2-SSI: Big Weather Web: A Common and Sustainable Big Data Infrastructure in Support of Weather Prediction Research and Education in Universities","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","08/01/2015","08/04/2015","Mohan Ramamurthy","CO","University Corporation For Atmospheric Res","Standard Grant","Bogdan Mihaila","07/31/2019","$98,702.00","","mohan@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","1525, 8004, 8074","4444, 7433, 8009","$0.00","Earth science communities need to rely on access to large and growing amounts of curated data to make progress in research and provide adequate education. Existing infrastructures pose significant barriers to this access, especially for small to mid-size research groups and primarily undergraduate institutions: cloud services disappear when funding runs out to pay for them and therefore do not provide the long-term availability required for curated data. Similarly, in-house IT infrastructure is maintenance-intensive and requires dedicated resources for which long-term funding is often unavailable. The goal of the Big Weather Web is to make in-house IT infrastructure affordable by combining the application of three recent technologies: virtualization, federated smart storage, and big data management. Virtualization allows push-button deployment and maintenance of complex systems, smart storage provides automatic, community-wide data availability guarantees, and big data management allows for easy curation of data and its products. The combination of these three technologies allows communities to create a standard community-specific computational environment and efficiently refine it with minimal repetition of work, introducing a high degree of reproducibility in research and education. This reproducibility accelerates learning and amplifies everyone?s contribution. Due to virtualization, it can easily take advantage of cloud services whenever they become available, and it can run on in-house IT infrastructure using significantly reduced maintenance resources. The Big Weather Web will be developed in the context of numerical weather prediction with the expectation that the resulting infrastructure can be easily adapted to other data-intensive scientific communities.<br/><br/>The volume, variety, and velocity of scientific data generated is growing exponentially. Small to mid-size research groups and especially primarily undergraduate institutions (PUIs) do not have the resources to manage large amounts of data locally and share their data products globally at high availability. This lack of resources has a number of consequences in education and research that have been well-documented in recent EarthCube workshops: (1) data-intensive scientific results are not easily reproducible, whether in the context of research or education, (2) limited or non-existent availability of intermediate results causes a lot of unnecessary duplication of work and makes learning curves unnecessarily steep, and consequently (3) scientific communities of practice are falling behind technological innovations. This Big Weather Web project focuses on the numerical weather prediction community. Numerical weather models produce terabytes of output per day, comprising a wealth of information that can be used for research and education, but this amount of data is difficult to transfer, store, or analyze for most universities. The Big Weather Web addresses this situation with the design, implementation, and deployment of ""nuclei,"" which are shared artifacts that enable reliable and efficient access and sharing of data, encode best practices, and are sustainably maintained and improved by the community. These nuclei use existing and well-established technologies, but the integration of these technologies will significantly reduce the resource burden mentioned above. Nucleus 1 is a large ensemble distributed over seven universities. Nucleus 2 is a common storage, linking, and cataloging methodology implemented as an appliance-like Data Investigation and Sharing Environment (DISE) that is extremely easy to maintain and that automatically ensures data availability and safety. Nucleus 3 is a versioned virtualization and container technology for easy deployment and reproducibility of computational environments. Together, these nuclei will advance discovery and understanding through sharing of data products and methods to replicate scientific results while promoting teaching, training, and learning by creating a shared environment for scientific communities of practice. These shared environments are particularly important for underrepresented groups who otherwise have limited access to knowledge that is primarily propagated by social means. Our approach is a significant step towards improving reproducibility in the complex computational environments found in many scientific communities."
"1838960","FAIR Publishing Guidelines for Spectral Data and Chemical Structures in Support of Chemistry and Related Disciplinary Communities","OAC","NSF Public Access Initiative","09/01/2018","08/17/2018","Vincent Scalfani","AL","University of Alabama Tuscaloosa","Standard Grant","Beth Plale","08/31/2019","$24,671.00","","vfscalfani@ua.edu","801 University Blvd.","Tuscaloosa","AL","354870005","2053485152","CSE","7414","7556, 9150","$0.00","In this project the PIs will convene a meeting to establish publishing guidelines for sharing machine-readable files of commonly reported chemical data classes.  The workshop organizers seek involvement from researchers, publishers, funding agencies, libraries, database and repository managers, tool developers, chemical societies, and standards organizations. Topics to discuss include formulating value propositions for data (that is, what data should be kept and for how long), and formulating criteria and stakeholder roles to better support data that are Findable, Accessible Interoperable, and Reusable (FAIR).  Several outcomes are expected, including best-practice publishing guidelines specifying FAIR criteria for spectral and chemical structure data; and workflow models for capturing, managing and sharing these data in domain repositories. This workshop will build on momentum from the Research Data Alliance (RDA), the International Union of Pure and Applied Chemistry (IUPAC), the American Chemical Society (ACS) and other related scientific societies. <br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1730655","CyberTraining: DSE: Self-Service Training Modules for Data-Intensive Neuroscience Learning and Research","OAC","CyberTraining - Training-based","09/01/2017","07/12/2017","Satish Nair","MO","University of Missouri-Columbia","Standard Grant","Sushil Prasad","08/31/2020","$494,651.00","Amitava Majumdar, Prasad Calyam, David Bergin","nairs@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","CSE","044Y","026Z, 7361, 9150","$0.00","This project will develop cyberinfrastructure-based training modules that advance the existing training methods used for learning and research in data-intensive neuroscience communities.  The project outcomes will enhance research into our understanding of both normal and abnormal brains, contributing to NSF's mission of advancing progress in both science and health.  The project activities will address important gaps in existing training methods that arise because neuroscience research and education activities are increasingly becoming data-intensive.  There is a growing need to integrate and analyze voluminous data being generated at multiple levels to explore the functioning of normal and abnormal brains.  Consequently, research and training in the area now necessitates access to distributed resources, including multiple software packages, high-performance computing with large numbers of cores, virtual desktops with data sharing/collaboration capabilities, neuro-data archives, and also requires multi-disciplinary expertise (e.g., engineering, biology, psychology).  Computational neuroscience researchers, undergraduate and graduate students and teachers (three targeted communities in this project) face challenges in accessing such resources and expertise in a scalable and extensive manner.  Further, they lack the necessary training in the use of advanced cyberinfrastructure (CI) technologies and distributed resources to improve their scientific productivity and to pursue large-scale data-enabled investigations.<br/>    <br/>The transformative nature of project's training modules is in the ""self-service"" nature planned for the modules that make them accessible to neuroscience users in an ""on-demand"" and ""personalized"" manner.  The training modules development will be based on survey of training needs, and will be focused on having students/teachers/multi-disciplinary researchers use, apply and create hands-on laboratory exercises and tools that can be deployed locally (i.e., within institutional CI) and be supplemented with publicly accessible national resources such as the NSF-funded Neuroscience Gateway (NSG).  The training modules will considerably enhance existing traditional neuroscience courses covering foundational concepts at undergraduate, graduate and teacher-training levels with hands-on laboratory exercises related to managing scientific workflows, CI middleware and application programming interfaces (APIs) to integrate geographically distributed resources.  The proposed activities will leverage existing active training programs in cloud computing and in neuroscience, and will use NSF-supported advanced CI resources that are available locally at University of Missouri and at NSG.  Project outcomes will be integrated into on-going courses (with its 50+ neuroscience faculty spanning 10 departments, and 5 colleges), into on-going NSF and NIH summer training programs, which recruit diverse participants including under-served and under-represented students, and into an on-going K-12 outreach program in neuro-robotics. The summer trainees that are being recruited in this project include over 50 students, neuroscience faculty and cyberinfrastructure engineers interested in advanced cyberinfrastructure capabilities for diverse research and education efforts.  In addition, over 80 students will benefit from the training modules within formal classroom courses in existing neuroscience and cyberinfrastructure courses at the University of Missouri, and over 150 students will benefit from outreach activities that include webinars and tutorials at conferences."
"1839032","Community Meeting on Scalable Data Publication Infrastructure","OAC","NSF Public Access Initiative","11/01/2018","08/31/2018","Guenter Waibel","CA","University of California, Office of the President, Oakland","Standard Grant","Beth Plale","10/31/2019","$49,888.00","","guenter.waibel@ucop.edu","1111 Franklin Street","Oakland","CA","946075200","5109879850","CSE","7414","7556","$0.00","Disciplinary (non-institutional) data repositories have much higher rates of adoption than Institutional Repositories (IRs) in part because they are directly integrated with key points in researcher workflows, particularly during the publishing of scientific articles. But community repositories lack partnerships with institutional expertise found in librarians and data curators.  This project addresses researcher need through taking the first steps in an open, community-supported initiative in research data curation and publishing.  To effectively leverage institutional knowledge and serve researchers as end users, more and varied types of institutions need to have a say in the values and necessities. To build broad community support, this invitational workshop will explore community requirements, identify impediments, study sustainable business models, and make recommendations regarding the widespread promotion and adoption of effective, scalable, and sustainable institutional data publication infrastructure.<br/><br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1808576","CDS&E: Collaborative Research: Strategies for Managing Data in Uncertainty Quantification at Extreme Scales","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/28/2018","Tan Bui-Thanh","TX","University of Texas at Austin","Standard Grant","Vipin Chaudhary","08/31/2022","$409,829.00","","tanbui@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8069, 8084","026Z, 8084, 9263","$0.00","The exponential increase in the quantity of measurements and data holds tremendous promise for data-driven scientific discovery and decision making.  In many cases, data-driven scientific discovery is mathematically formulated as an inverse problem.  For inverse problems that serve as a basis for discovery and decision-making for complex problems, the uncertainty in its solutions must be quantified.  Though the past decades have seen tremendous advances in both theories and computational algorithms for inverse problems, quantifying the uncertainty (UQ) in their solutions taking big-data issues into account remains challenging.  This is largely due to computationally demanding nature of existing mathematical techniques that are unable to scale up to the amount of data being generated.  Consequently, much of the available data remains unused.  This project develops UQ algorithms that are both computationally scalable as well as datascalable for making scientific progresses in geosciences and medical imaging. In particular, the proposed methods are evaluated in the context of two challenging data-driven applications: (1) from large amount of seismograms (records of the ground motion) perform geophysical imaging to infer earth's interior structure to better understand earthquakes, and (2) from magnetic resonance (MR) cine images of patients estimate the heart's function (e.g. motion, contraction) to detect early onset of heart disease (cardiomyopathy).<br/><br/><br/>The goal of this collaborative research project is to develop an integrated research program that addresses the data management and data analytics arising from both observations and scientific simulations, with applications from diverse domains at extreme scales. The project develops innovative statistical, mathematical, and parallel computational methods to manage the large amounts of simulation data as well as the ever increasing amounts of observation data required for extreme-scale UQ problems in general and Bayesian inverse problems in particular. These methods will be of immediate practical utility to scientists  and engineers dealing with big data and large-scale UQ problems in sensing-based disciplines, geosciences, climatology, medical imaging, etc. The successful completion of the project would  provide a first step towards the development of mathematical and computational methods for a wide range of data-driven  large-scale inverse and UQ challenges that can lead to original scientific discoveries and promote the progress of science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1651724","CAREER: Towards Fast and Scalable Algorithms for Big Proteogenomics Data Analytics","OAC","CAREER: FACULTY EARLY CAR DEV, SOFTWARE & HARDWARE FOUNDATION, COMPUTATIONAL BIOLOGY","04/01/2017","04/06/2017","Fahad Saeed","MI","Western Michigan University","Standard Grant","Sushil Prasad","03/31/2022","$499,999.00","","FSAEED@FIU.EDU","1903 West Michigan Avenue","Kalamazoo","MI","490085200","2693878298","CSE","1045, 7798, 7931","1045, 7931, 7942","$0.00","Proteogenomics studies require combination and integration of mass spectrometry data (MS) for proteomics and next generation sequencing (NGS) data for genomics.  This integration drastically increases the size of the data sets that need to be analyzed to make biological conclusions.  However, existing tools yield low accuracy and exhibit poor scalability for big proteogenomics data.  This CAREER grant is expected to lay a foundation for fast algorithmic and high performance computing solutions suitable for analyzing big proteogenomics data sets.  Design of accurate computational algorithms suitable for peta-scale data sets will be pursued and the software implementation will run on massively parallel supercomputers and graphical processing units.  The direction in this CAREER proposal is towards designing and building infrastructure, which would be useful for the broadest biological and ecological community.  A comprehensive interdisciplinary education will be executed for K12, undergraduate and graduate students to ensure that US retains its global leadership position in STEM fields.  This project thus serves the national interest, as stated by NSF's mission: to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>The goal of the proposed CAREER grant is to design and develop algorithmic and high performance computing (HPC) foundations for practical sublinear and parallel algorithms for big proteogenomics data - especially for non-model organisms with previously unsequenced or partially sequenced genomes.  Integration of MS and NGS data sets required for proteogenomics studies exhibit enormous volume and velocity of data: NGS technologies such as Chip-Seq can generate tera-bytes of DNA/RNA data and mass spectrometers can generate millions of spectra (with thousand of peak per spectra).  The current systems for analyzing MS data are mainly driven by heuristic practices and do not scale well.  This CAREER proposal will explore a new class of reductive algorithms for analysis of MS data that can allow peptide deductions in sublinear time, compression algorithms that operate in sub-linear space, and denovo algorithms that operate on lossy reduced-form of the MS data.  Novel low-complexity sampling and reductive algorithms that can exploit the sparsity of MS data such as non-uniform FFT based convolution kernels can lead to superior similarity metrics not prone to spurious correlations.  The bottleneck in large system-biology studies is the low-scalability of coarse-grained parallel algorithms that do not exploit MS-specific data characteristics and lead to unbalanced loads due to non-uniform compute time required for peptide deductions.  This project aims to explore design and implementation of scalable algorithms for both NGS and MS data on multicore and GPU platforms using domain decomposition techniques based on spectral clustering, MS-specific hybrid load-balancing based on work-load estimate, and HPC dimensionality reduction strategies and novel out-of-core sketching & streaming fine-grained parallel algorithms.  These HPC solutions can enable previously impractical proteogenomics projects and allow biologists to perform computational experiments without needing expensive hardware.  All of the implemented algorithms will be made available as open-source code interfaced with Galaxy framework to ensure maximum impact in systems biology labs.  These designed techniques will then be integrated so that matching of spectra to RNA-Seq data can be accomplished without a reconstructed transcriptome.  The proposed tools aim to reveal new biological insight such as novel genes, proteins and PTM's and are crucial steps towards understanding the genomic, proteomic and evolutionary aspects of species in the tree of life."
"1808652","CDS&E: Collaborative Research: Strategies for Managing Data in Uncertainty Quantification at Extreme Scales","OAC","CDS&E-MSS, CDS&E","09/01/2018","08/28/2018","Hari Sundar","UT","University of Utah","Standard Grant","Vipin Chaudhary","08/31/2022","$396,066.00","","hari@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8069, 8084","026Z, 8084, 9263","$0.00","The exponential increase in the quantity of measurements and data holds tremendous promise for data-driven scientific discovery and decision making.  In many cases, data-driven scientific discovery is mathematically formulated as an inverse problem.  For inverse problems that serve as a basis for discovery and decision-making for complex problems, the uncertainty in its solutions must be quantified.  Though the past decades have seen tremendous advances in both theories and computational algorithms for inverse problems, quantifying the uncertainty (UQ) in their solutions taking big-data issues into account remains challenging.  This is largely due to computationally demanding nature of existing mathematical techniques that are unable to scale up to the amount of data being generated.  Consequently, much of the available data remains unused.  This project develops UQ algorithms that are both computationally scalable as well as datascalable for making scientific progresses in geosciences and medical imaging. In particular, the proposed methods are evaluated in the context of two challenging data-driven applications: (1) from large amount of seismograms (records of the ground motion) perform geophysical imaging to infer earth's interior structure to better understand earthquakes, and (2) from magnetic resonance (MR) cine images of patients estimate the heart's function (e.g. motion, contraction) to detect early onset of heart disease (cardiomyopathy).<br/><br/><br/>The goal of this collaborative research project is to develop an integrated research program that addresses the data management and data analytics arising from both observations and scientific simulations, with applications from diverse domains at extreme scales. The project develops innovative statistical, mathematical, and parallel computational methods to manage the large amounts of simulation data as well as the ever increasing amounts of observation data required for extreme-scale UQ problems in general and Bayesian inverse problems in particular. These methods will be of immediate practical utility to scientists  and engineers dealing with big data and large-scale UQ problems in sensing-based disciplines, geosciences, climatology, medical imaging, etc. The successful completion of the project would  provide a first step towards the development of mathematical and computational methods for a wide range of data-driven  large-scale inverse and UQ challenges that can lead to original scientific discoveries and promote the progress of science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839018","Optimizing  Openness in Human Participants Research: Harmonizing Standards for Consent Agreements and Data Management Plans to Empower the Reuse of Sensitive Scientific Data","OAC","NSF Public Access Initiative","10/01/2018","08/15/2018","Colin Elman","NY","Syracuse University","Standard Grant","Beth Plale","09/30/2020","$299,787.00","Diana Kapiszewski, Lynette Hoelter, Margaret Levenstein","celman@maxwell.syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7414","7916","$0.00","Recent technological advances allow for an increase in the amount of sensitive human participants data that can be safely shared. Institutional Review Boards' (IRBs) traditional reluctance to allow the sharing of such data unnecessarily constrains their long-term reuse. This project seeks to close the gap between modern options for safely sharing sensitive data and IRB practices by establishing socio-technical infrastructure to support a sustained dialogue and productive partnerships between data repositories and IRBs with the objective to pilot new consensus guidance, protocols, and templates on the sharing and long-term re-use of sensitive data generated through research with human participants.<br/><br/>This project is supported by the National Science Foundation Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838981","Type-Based Automation of Scientific Data Management","OAC","NSF Public Access Initiative","10/01/2018","08/13/2018","Giridhar Manepalli","VA","Corporation for National Research Initiatives (NRI)","Standard Grant","Beth Plale","09/30/2020","$297,796.00","","gmanepalli@cnri.reston.va.us","1895 Preston White Drive","Reston","VA","201915434","7036208990","CSE","7414","7916","$0.00","An approach to scientific data interoperability and reuse is through global, persistent, and uniquely identified data types that can be assembled to characterize research data sets.  This project proposes to identify data types using persistent identifiers (PIDs). The PIDs resolve to records that specify the way in which metadata, such as the provenance of the data, is structured and recorded.  The basic premise is that machine interpretable data is a critical goal to achieving FAIRness (findability, accessibility, interoperability, and reuse) of data as data discovery at a global scale depends on automated processing of the information in digital form.  A type based approach to data interpretability that utilizes persistent IDs at the granularity of data types can overturn the Internet and stimulate an ecosystem of new tools for FAIR data. This pilot effort involves evaluating the approach through, in part, by constructing a critical mass of use cases.<br/><br/>This project is supported by the National Science Foundation Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1839010","EAGER: Preserve/Destroy Decisions for Simulation Data in Computational Physics and Beyond","OAC","NSF Public Access Initiative","08/15/2018","08/08/2018","Victoria Stodden","IL","University of Illinois at Urbana-Champaign","Standard Grant","William Miller","07/31/2020","$300,000.00","Darko Marinov","vcs@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7414","7916","$0.00","The scientific research community has been increasingly developing ways to share and re-use research data, thereby allowing more discoveries to be made from previous research investments. Much of the focus on sharing and reusability has been on experimental and observational data. This project addresses the equally vexing challenge of how to make best use and re-use of the massive data produced in computational simulations. Important research questions guiding this project include: the degree to which simulation results can be replicated; the advantages of storing the simulation data itself for others to reuse as compared to providing the computational software so that others can re-run the simulations; and understanding which software testing practices can facilitate the replication/reuse of simulation data and the simulation software that produces those data. The principal investigators will address these questions by performing extensive replication and software code testing on a set of computational physics simulation datasets and software code that they had gathered through a previous study. The project will produce publicly available, fully reproducible computational physics works as examples for publishing results in a way that the data and code are effectively reusable.<br/><br/>The principal investigators aim to improve understanding of, and increase, the reusability of the code and data associated with simulation-based research.  This project specifically aims to better inform data destroy/preservation decisions in the simulation context, toward improving the reusability and interoperability of simulation data and code. The project will also consider important questions such as how software engineering testing practices relate to computational physics practices, and how changes in computational environments affect code execution and the regeneration of simulation data. Ultimately, the results of this work are intended to guide the research community on how to best produce and disseminate research code. It is anticipated that the results for computational physics can be extended to develop general guidelines for simulation data and code sharing for other communities, the appropriate code testing to do so, and best practices for development of associated cyberinfrastructure and tools. <br/><br/>This project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838628","Integrative Structural Determination Federation (iSDF) Workshop","OAC","NSF Public Access Initiative","09/01/2018","08/04/2018","Helen Berman","NJ","Rutgers University New Brunswick","Standard Grant","Beth Plale","08/31/2019","$50,000.00","","berman@rcsb.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7414","7556","$0.00","The PIs will organize a workshop that brings together members and stakeholders of the integrative Structure Determination Federation (iSDF), a new entity that is envisioned as a network of model and data resources that contribute to structural biology. The proposed workshop is aimed at initiating engagement of diverse experimental data and structural model communities in order to plan for an interoperating network of model and data repositories.  The workshop builds upon three years? effort on building blocks required for developing  coordinated network of structural biology resources.   This earlier effort stems from an action in 2014 by wwPDB to create an Integrative/Hybrid Methods task force which had, as one of its key recommendations, to create a federated system of model and data archives.  The outcome of the workshop is a set of requirements for creating well-aligned data standards and protocols for efficient data exchange among participating repositories.  <br/><br/>This project is supported by the National Science Foundation Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659356","CC* Integration: Regional Embedded Cloud for As-a-Service Transformation (RECAST)","OAC","CISE RESEARCH RESOURCES","04/15/2017","02/26/2019","Xi Yang","MD","University of Maryland College Park","Standard Grant","Deepankar Medhi","03/31/2019","$500,000.00","Xi Yang, Frederick McCall, Bertrand Sobesto","maxyang@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","2890","","$0.00","Science DMZs (SDMZ) have been widely deployed in the Research and Education networking community and recognized<br/>as a global best practice for supporting cyberinfrastructure and big data applications. Despite innovations<br/>in performance, security, and management, the SDMZs remain focused on the<br/>support of big data projects and cyberinfrastructure. It remains difficult to create physical ""disk<br/>to disk"" or virtual ""scientist to scientist"" connections. Researchers are increasingly interested<br/>in an emerging class of hybrid services focused on turnkey, on-demand integration of private/public<br/>clouds, cyberinfrastructure, scientific resources, and high performance networks.<br/><br/>The RECAST project will develop and deploy a regional based Software Defined Science<br/>DMZ (SD-SDMZ) as a mechanism to provide flexible edge services that include traditional data<br/>transfer functions. RECAST leverages state-of-the-art Cloud and Software-defined networking technologies to transform<br/>an SDMZ --  that was historically focused on high-speed data transfer -- into a Software Defined Infrastructure that offers<br/>compute and network services in a fully virtualized, programmable and orchestrated fashion. Built upon technologies similar to those<br/>used in modern data centers, this ""cloudified SDMZ"" will  provide a scalable, multi-tenant environment, where<br/>each user or user group can allocate dedicated resources and use the resources in an isolated,<br/>Independent, and elastic fashion. Embedding this resource in the University of Maryland Mid-Atlantic<br/>Crossroads regional network will allow the SD-SDMZ to leverage rich connectivity to advance<br/>cyberinfrastructure to provide these new and advanced services."
"1450468","SI2-SSI Integration of Synchrotron X-Ray Analysis Software Methods into the Larch Framework","OAC","OFFICE OF MULTIDISCIPLINARY AC, GEOPHYSICS, DMR SHORT TERM SUPPORT, Software Institutes, EarthCube","10/01/2015","09/16/2015","Matthew Newville","IL","University of Chicago","Standard Grant","Stefan Robila","09/30/2019","$540,969.00","","newville@cars.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1253, 1574, 1712, 8004, 8074","7433, 7574, 8004, 8009, 9216","$0.00","The solutions to many of the outstanding problems in geology, environmental science, material science, and biology require understanding the chemical state and detailed atomic structure of the molecules and solids that make up our world.  Such problems range from understanding the molecular forms of arsenic in rice, determining the chemical composition of the earths interior, and improving the performance and reducing the environmental impact of batteries that are in our laptops, cellphones, and cars.  The nation's synchrotron facilities provide powerful X-ray facilities that allow researchers to study these questions by investigating the chemical makeup and crystal structure of complex, real-world materials such as plant seeds, contaminated soils, human and animal tissue, minerals and meteorites, and working batteries and catalysts.  Synchrotron measurement techniques have developed very rapidly over the past few decades, and are being used by many more researchers.  The ability to handle and interpret the large and complex datasets now being routinely generated at these facilities is often a significant challenge, even for experts.  The work here will develop the Larch X-ray analysis framework to provide open-source software that is easy to use and specific enough to correctly interpret several categories of synchrotron X-ray data.  The approach will provide tools that are flexible enough to enable researchers to explore and interpret new combinations of data easily enough to make new connections and discoveries in a wide variety of scientific areas.<br/><br/><br/>This project will integrate visualization and analysis software for multiple synchrotron X-ray techniques into the open source and extensible Larch X-ray Analysis framework.  The immediate focus of the work is to support visualization and quantitative analysis of the rich and complex data from X-ray microprobes, including X-ray fluorescence imaging, fluorescence and absorption spectroscopies, and X-ray diffraction.  The Larch framework already provides a suite of analysis procedures for X-ray absorption spectroscopy and fluorescence imaging, and has been designed to be readily extensible by adding plug-ins in Python, widely used in scientific computing and being embraced in the synchrotron user communities.  Existing state-of-the-art analysis procedures for X-ray fluorescence, X-ray absorption, and X-ray diffraction have been identified to be integrated into the Larch framework, adapting and translating the software as needed to be compatible with the open-source Python framework.  With the combination of state-of-the-art analysis methods for multiple data types, Larch will provide a single well-supported and -documented analysis package with robust, easy to use analytic methods for a range of synchrotron X-ray data. By being easily extensible, the Larch package can also accommodate methods for other synchrotron X-ray techniques."
"1740151","SI2-SSE: Deep Forge: a Machine Learning Gateway for Scientific Workflow Design","OAC","Software Institutes","09/01/2017","08/24/2017","Akos Ledeczi","TN","Vanderbilt University","Standard Grant","Stefan Robila","08/31/2020","$400,000.00","Peter Volgyesi","akos.ledeczi@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","8004","7433, 8004, 8005","$0.00","Recent advances in machine learning have already had a transformative impact on our lives. However, astonishing successes in diverse domains, such as image classification, speech recognition, self-driving cars and natural language processing, have mostly been driven by commercial forces, and these techniques have not yet been widely transitioned into various science domains. The field is ripe for innovation since many science fields have readily available large-scale datasets, as well as access to public or private compute infrastructure capable of executing computationally expensive artificial neural network (ANN) training workflows. The main roadblocks seem to be the steep learning curve of the ANN tools, the accidental complexities of setting up and executing machine learning workflows, and the fact that finding the right deep neural network architecture requires significant experience and lots of experimentation. DeepForge overcomes these obstacles by providing an intuitive visual interface, a large library of reusable components and architectures as well as automatic software generation enabling domain scientist to experiment with ANNs in their own field. There is unmet high demand of talent in machine learning, exactly because it has so much potential in a wide variety of application areas. Therefore, any tool that helps scientists apply machine learning in their own domains will have a broad impact. The promise of DeepForge is to flatten the learning curve, hide low level unimportant details and provide components that are reusable within and across disciplines. Therefore, DeepForge will have transformative impact on a number of fields.<br/><br/>DeepForge, a web- and cloud-based software infrastructure raises the abstraction of creating ANN workflows via an intuitive visual interface and by managing training artifacts. Hence, it enables domain scientists to leverage recent advances in machine learning. DeepForge will also integrate with existing cyberinfrastructure, including private and commercial compute clusters, cloud services (e.g. Amazon EC2), public supercomputing resources, and online repositories of scientific datasets. The DeepForge visual language for designing ANN architectures and workflows is powerful enough to capture the concepts related to common deep learning tasks, yet it provides a high level of abstraction that shields the users from the underlying complexity at the same time. DeepForge will provide a facility that allows for sharing design artifacts across a wide interdisciplinary user community. Curating a rich library of reusable components, integrating with a wide variety of existing cyberinfrastructure resources from data sources to compute platform and providing data provenance in a seamless manner are other advantages of the project. DeepForge will promote ""data as product,"" ""model as product,"" and ""service as product"" concepts through integration with the Digital Object Identifier (DOI) infrastructure. DeepForge will enable scientist to assign DOIs to their shared assets providing data provenance enabling citing and publicly reproducing research results by executing the referenced ANN workflows with the linked data artifacts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750970","CAREER: Computational Optics and Photonics for Deep Imaging of Live Tissue","OAC","CAREER: FACULTY EARLY CAR DEV, EPSCoR Co-Funding","05/01/2018","04/30/2018","Heidy Sierra","PR","University of Puerto Rico Mayaguez","Standard Grant","Sushil Prasad","04/30/2023","$498,905.00","","heidy.sierra1@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","1045, 9150","026Z, 1045, 9102, 9150","$0.00","Innovation in non-invasive imaging techniques is part of the effort to provide rapid screening, diagnosis as well as to guide treatment in numerous settings that aspire to offer affordable and efficient healthcare.  Most of the existing high-resolution methods are effective primarily on thin and nearly homogeneous transparent samples or over tissue surface.  In most realistic scenarios, it is important to acquire information at depth within tissue.  High-resolution volumetric imaging approaches may require expensive computational tools for data analysis and complex hardware configurations.  Computational optics grounded on signal processing and image reconstruction concepts offers promising alternatives.  This research contributes to advance the related state-of-the-art in translational cyberinfrastructure and biomedical technology.  Results from this research can improve non-invasive imaging systems for research and patient care while supporting the NSF mission to promote the progress of science and advance the national health.  The development of this project involves multidisciplinary efforts from computer science, bioengineering and electrical engineering as well as educational activities with the participation of students from underrepresented groups.   <br/><br/>This project focuses on providing a framework to support advances on optical imaging techniques that can perform at the needed resolution and speed for various scenarios such as healthcare and biomedical research.  The research plan is geared to creating an advanced cyberinfrastructure with simulation and analysis tools to build a computational optical system for deep imaging of live tissues.  The components of the framework include three-dimensional optical imaging models employing nonlinear scattering theory that integrate tissue optical properties to characterize their effect into the imaging resolution performance.  Additionally, it includes the integration of light-tissue-interaction modeling parameters with compressive sensing concepts and machine learning algorithms for advanced data management.  This project targets realistic challenges in biomedical research, including (i) a gap between complex physics of light propagation in tissues and the design of efficient high-resolution imaging systems, (ii) computational optics and photonics for deep imaging of live tissues, and (iii) integration with reliable and state-of-the-art data analytics and visualization environments.  The simulations and computational optics tools focus on confocal imaging of skin tissue, which is widely used in biomedical research, and is potentially adoptable in the clinic to guide diagnosis of skin conditions.  The education plan addresses three major areas: i) research training and experiences for graduate and undergraduate students, i) course development in topics related with computational optics and data analytics, and iii) outreach to K-12 students and professionals to introduce research issues and opportunities in computational imaging and data analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827225","CC* Network Design: Network Upgrades to Improve Engagement in Science Discovery and Education","OAC","Campus Cyberinfrastrc (CC-NIE)","08/01/2018","07/25/2018","Donna Liss","MO","Truman State University","Standard Grant","Kevin L. Thompson","07/31/2020","$400,000.00","Jon Beck, Jim McNabb","dliss@truman.edu","100 E. Normal","Kirksville","MO","635014200","6607857245","CSE","8080","9150","$0.00","Truman State University, with the support of the Missouri Research and Education Network (MOREnet) and the University of Missouri, is upgrading the campus network and infrastructure to better enable access to, and use of, scientific data through improved data transfer capabilities for large datasets.  The enhanced infrastructure supports faculty and undergraduate research in understanding star spots, quantifying light pollution in geographical areas, understanding the inhibition mechanisms of drugs to treat global health issues, natural language processing to improve approaches in artificial intelligence, and building low-cost, real-time, soil sensors.  An expanded curriculum that includes hands-on training in cybersecurity and IPv6 technologies is also enabled by utilizing these network resources.  <br/><br/>The improvements include upgrades to the network switch and distribution technologies that result in a ten-fold increase in data transfer and access rates for faculty in STEM-related disciplines, as well as increases in the data transfer bandwidth to the campus observatory, deployment of IPv6 in support of the computer science curriculum, establishment of network performance metrics to inform continued growth, and robust and secure access (through federated identity management) to off-campus research tools and intra-institutional collaboration opportunities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740288","SI2-SSE: Abaco - Flexible, Scalable, and Usable Functions-As-A-Service via the Actor Model","OAC","Software Institutes","10/01/2017","08/30/2017","Joseph Stubbs","TX","University of Texas at Austin","Standard Grant","Stefan Robila","09/30/2020","$418,593.00","Matthew Vaughn","jstubbs@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","026Z, 7433, 8004, 8005, 9102","$0.00","Researchers in virtually every field of science rely heavily on computing to perform their research.  In addition, increased availability of data has enabled entirely new kinds of analyses which have yielded answers to many important questions. However, these analyses are complex, and  require advanced computer science expertise.  This project seeks to simplify the manner in which researchers can create analysis tools that also scale better and more reliable. This project, Abaco, adopts and adapts the ""Actor"" model, which is a technique for designing software systems as a collection of simple functions, which can then be provided as a cloud-based capability on high performance computing environments. Doing this will significantly simplify the way scientific software is developed and used. Scientific software developers will find it much easier to design and implement a system. Further, scientists and researchers that use software will be able to easily compose together collections of actors with pre-determined functionality in order to get the computation and data they need. <br/><br/>The project will combine technologies and techniques from cloud computing, including Linux Containers and the ""functions-as-a-service"" paradigm with the proven Actor theoretical model for computing. Each Actor is implemented as a Linux container, and provides a single function on a cloud. The resulting system allows for small, lightweight programs to be run on virtually any system. This project will also extend Abaco's ability to implement data capabilities, such as data federation and discoverability. Abaco programs can be used to be used, for example, to build federated datasets consisting of separate datasets from all over the internet. By reducing the barriers to developing and using such services, this project will boost the productivity of scientists and engineers working on the problems of today, and better prepare them to tackle the new problems of tomorrow. Abaco has broad applicability across science domains, from biology to engineering to environmental studies. Further, the Abaco team will conduct substantial training and support activities aimed at empowering researchers to benefit from this approach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1440733","SAVI: GECAT-Global Initiative to Enhance Collaborative Computing and Analysis Tools","OAC","INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE, DATANET, Science Across Virtual Instits","09/01/2014","09/18/2018","William Kramer","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","08/31/2019","$2,000,000.00","John Towns","wtkramer@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1640, 7231, 7726, 8077","5918, 5921, 5936, 5942, 5952, 7433, 7726, 8058, 9200","$0.00","Major scientific research initiatives and enterprises are global in nature, requiring national cyberinfrastructure capabilities to not only be interoperable worldwide but to be designed to support collaborations spanning national boundaries. Collective knowledge and skills of teams is increasingly required to solve the key technical and scientific issues facing us today.  In order to realize continued scientific discovery in the United States, international collaboration in interoperable forms of large-scale distributed computing, access to remote data, development of collaboration technologies and skill sets are required.   Many scientific projects are international in scope, necessitating collaboration across borders, including the coordination of resources . International collaborations to construct telescopes with associated virtual observatories (such as LSST, DES and SKA), particle detectors (such as ATLAS and CMS for the Large Hadron Collider), climate (IPCC) and biological and environmental data repositories are already underway. <br/><br/>This project supports international virtual research organizations (IVROs) that promote and enable efficient collaboration and co-operation of researchers and developers in multiple countries. It aims to seed the development of new, innovative cyberinfrastructure features enabling international scientific collaboration for scientific advances. It builds on existing relationships and activities previously established by the PIs and institution.  The project directly supports the participation of multiple U.S .participants (senior and junior faculty, postdoctoral fellows, graduate students) in workshops, and to interact, communicate, and collaborate in small research and development ""seed"" projects with international partners from multiple countries in Europe and Asia. The project provides an annual series of forums for publicly sharing cutting-edge research, education and training experiences covering all aspects of cyberinfrastructure, including distributed data management, system resilience, performance issues with scaling, efficient mechanisms for training and support.  The project focuses on the development and study of strong International Virtual Research Organizations promoting interoperability of national cyberinfrastructures for international research.  These IVROs are intended to facilitate defining and developing strategies and action plans for collaborative projects, and activities leading to the availability of high end middleware tools, and interoperability of national cyberinfrastructures for addressing and resolving grand science challenges through advancing cyberinfrastructure and computational science.<br/><br/>This is designated as a Science Across Virtual Institutes (SAVI) award and is co-funded by NSF's Office of International and Integrative Activities and the Directorate for Computer and Information Science and Engieering."
"1812786","CYBER-INSIGHT: Evaluating Cyberinfrastructure Total Cost of Ownership","OAC","ETF","02/01/2018","12/21/2017","Daniel Reed","IA","University of Iowa","Standard Grant","Edward Walker","01/31/2020","$299,588.00","Steven Fleagle","dan-reed@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7476","7916","$0.00","Academic research computing technologies is rapidly changing.   In addition, new methods in gleaning insight from the deluge of data that is becoming available in science and society are fueling demand for computing infrastructure, and motivating commercial companies to provision infrastructure similar to those found at academic environments.  These factors, as well as others, are putting pressure on academic institutions to find cost-effective approaches to providing computing infrastructure to maximize scientific benefit while minimizing cost.  This project seeks to understand the cost-effectiveness of different methods of delivering High Performance Computing capabilities to the academic research community.   Specifically, the project proposes to build, publish, and regularly update comparisons of the evolving total cost of ownership (TCO) of on premise HPC clusters and data storage systems relative to NSF-supported facilities and commercial cloud services such as Amazon AWS, Microsoft Azure, and Google Cloud Platform.  The results of the project will have broad applicability to the national research community, institutional resource providers, and funding agencies. In addition, the project is collaborating with many organizations to promote long-term maintenance and data sharing of the project outcomes, with the goal of creating a sustainable infrastructure for long-term analysis.<br/><br/>Concretely, the team proposes to build and host a Jupyter notebook that will enable the cyberinfrastructure (CI) community to add, modify, and explore total cost of ownership (TOC) models based on a variety of usage patterns and performance expectations. This will allow the project to explore questions such as when is it cost-effective to use commercial clouds compared to university clusters to service researcher needs? How do staff costs, hardware utilization, capital depreciation, replacement costs, the value of money, return on investment, power and cooling costs, and bandwidth charges affect TCO?  As science gateways hide resources behind web interfaces, when and where should such queries execute?  In addition to investigating these important questions, the project is also expected to facilitate community building, shared experimentation and comparisons.  The ultimate goal of the project is to develop comparative models that allow funding agencies, institutional resource providers, and individual users to evaluate the costs and benefits of specific choices and subsidies. Finally, the project will collaborate with the Coalition for Academic Scientific Computation (CASC), the Association of American Universities (AAU) and the Association of Public and Land Grant Universities (APLU) senior research officers (SROs) and CIOs to promote long-term maintenance and data sharing of the project outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835690","Elements: Software: Autonomous, Robust, and Optimal In-Silico Experimental Design Platform for Accelerating Innovations in Materials Discovery","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Byung-Jun Yoon","TX","Texas A&M Engineering Experiment Station","Standard Grant","Bogdan Mihaila","09/30/2021","$600,000.00","Xiaofeng Qian, Xiaoning Qian, Raymundo Arroyave","bjyoon@ece.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","1712, 8004","026Z, 054Z, 077Z, 7923, 7926, 8004, 9216","$0.00","Accelerating the development of novel materials that have desirable properties is a critical challenge as it can facilitate advances in diverse fields across science, engineering, and medicine with significant contributions to economic growth. For example, the US Materials Genome Initiative calls for cutting the time for bringing new materials from discovery to deployment by half at a fraction of the cost, by integrating experiments, computer simulations, and data analytics. However, the current prevailing practice in materials discovery relies on trial-and-error experimental campaigns and/or high-throughput screening approaches, which cannot efficiently explore the huge design space to develop materials with the targeted properties. Furthermore, measurements of material composition, structure, and properties often contain considerable errors due to technical limitations in materials synthesis and characterization, making this exploration even more challenging. This project aims to develop a software platform for robust autonomous materials discovery that can shift the current trial-and-error practice to an informatics-driven one that can potentially expedite the discovery of novel materials at substantially reduced cost and time. Throughout the project, the PI and Co-PIs will mentor students and equip them with the skills necessary to tackle interdisciplinary problems that involve materials science, computing, optimization, and artificial intelligence. Research findings in the project will be incorporated into the courses taught by the PI and Co-PIs, thereby enriching the learning experience of students.<br/><br/>The objective of this project is to develop an effective in-silico experimental design platform to accelerate the discovery of novel materials. The platform will be built on optimal Bayesian learning and experimental design methodologies that can translate scientific principles in materials, physics, and chemistry into predictive models, in a way that takes model and data uncertainty into account. The optimal Bayesian experimental design framework will enable the collection of smart data that can help exploring the material design space efficiently, without relying on slow and costly trial-and-error and/or high-throughput screening approaches. The developed methodologies will be integrated into MSGalaxy, a modular scientific workflow management system, resulting in an accessible, reproducible, and transparent computational platform for accelerated materials discovery that allows easy and flexible customization as well as synergistic contributions from researchers across different disciplines.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1639529","INFEWS/T1: Mesoscale Data Fusion to Map and Model the U.S. Food, Energy, and Water (FEW) System","OAC","Track 1 INFEWS, HYDROLOGIC SCIENCES, DATANET","09/01/2016","01/17/2018","Benjamin Ruddell","AZ","Northern Arizona University","Standard Grant","Amy Walton","08/31/2020","$3,463,681.00","John Sabo, Kevin Gurney, Michael Hanemann, Shade Shutters","benjamin.ruddell@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","020Y, 1579, 7726","004Z, 043Z, 7433, 8048","$0.00","The Food, Energy, and Water (FEW) system is complex, vulnerable to societal and environmental changes, yet critical for national well-being. This project's major contribution is to create and exploit the first detailed mapping of the Food, Energy, and Water System of the United States. Using this capability will improve understanding of how local Food, Energy, and Water policy decisions and technologies cause ripple effects throughout the system (for example, how electricity usage in an American city affects rivers hundreds of miles away).  Policies and technologies often pose trade-offs between Food, Energy, and Water systems, and this project is measuring those trade-offs so costs and benefits may be understood and balanced in future decisions.  By studying how past events like droughts, storms, wars, or economic crises have affected the nation's Food, Energy, and Water System, this project is developing the capacity to anticipate the impacts of future events.  <br/><br/>The project provides an empirical basis for advances in theory and scientific modeling of the complete food-energy-water (FEW) system of the United States.   The system is primarily composed of mesoscale phenomena in which regional trade, river basins and aquifers, irrigation districts, crop belts, states, tribes, counties and cities, power grids, climate gradients, and seasonal timescales interact in a dynamic, inter-connected coupled natural-human system. To advance understanding of these interactions, a reliable and complete empirical description of the FEW system is needed. This requires a dataset containing consumption, production, and bilateral trade data for the United States, with sub-county resolution.  A retrospective version of this dataset (containing data from the mid-20th century to the present), will serve as a model network for the FEW system's emergent performance metrics, sustainability metrics, and supply-chain teleconnections, along with observed historical dynamics of system response, vulnerability, and resilience to stresses and shocks. A wide range of diverse and disparate (but mostly pre-existing) economic, climate, and environmental data will be assembled to create the first comprehensive empirical map of the U.S. Food, Energy, and Water system (the FEWSion v1.0-US database).  This capability will then be used to achieve four high-value science and modeling objectives: (1) quantify the multiple-objective trade-offs between performance and sustainability metrics, (2) analyze historical sensitivity, vulnerability, resilience, and evolution of the FEW network with attribution to observed stresses and shocks, (3) establish the role of cities within the FEW system, and (4) provide a standards-based benchmarking assessment capability that can be used by other projects awarded under Track 1 (FEW System Modeling) and Track 3 (Research to Enable Innovative System Solutions) of this INFEWS solicitation.  A public online educational tool uses this information to visualize how individual and local decisions create environmental footprints, and how those decisions create impacts throughout the food, energy, and water system."
"1835785","Framework: Software: Collaborative Research: CyberWater--An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","XC-Crosscutting Activities Pro, DATANET, EarthCube","01/01/2019","09/07/2018","Xu Liang","PA","University of Pittsburgh","Standard Grant","Stefan Robila","12/31/2022","$437,232.00","","xuliang@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","7222, 7726, 8074","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835817","Framework: Software: Collaborative Research: CyberWater--An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","DATANET","01/01/2019","09/07/2018","Yao Liang","IN","Indiana University","Standard Grant","Stefan Robila","12/31/2022","$514,249.00","Sudhakar Pamidighantam, Fengguang Song","yliang@cs.iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835602","Framework: Software: Collaborative Research: CyberWater-An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","DATANET","01/01/2019","09/07/2018","Lan Lin","IN","Ball State University","Standard Grant","Stefan Robila","12/31/2022","$39,996.00","","llin4@bsu.edu","2000 West University Avenue","Muncie","IN","473060155","7652851600","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835338","Framework: Software: Collaborative Research: CyberWater: An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","DATANET","01/01/2019","09/07/2018","Ibrahim Demir","IA","University of Iowa","Standard Grant","Stefan Robila","12/31/2022","$125,378.00","Witold Krajewski, Ricardo Mantilla","ibrahim-demir@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835592","Framework: Software: Collaborative Research: CyberWater-An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","DATANET","01/01/2019","09/07/2018","Anthony Castronova","MA","Consortium of Universities for the Advancement of Hydrologic Sci","Standard Grant","Stefan Robila","12/31/2022","$156,168.00","","acastronova@cuahsi.org","150 Cambridge Park Drive","Cambridge","MA","021402479","3392215400","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835656","Framework: Sofware: Collaborative Research: CyberWater -An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","DATANET","01/01/2019","09/07/2018","Yang Zhang","NC","North Carolina State University","Standard Grant","Stefan Robila","12/31/2022","$95,000.00","","yang_zhang@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7726","062Z, 077Z, 7925","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1234983","SAVI: PRAGMA - Enabling Scientific Expeditions and Infrastructure Experimentation for Pacific Rim Institutions and Researchers","OAC","CROSS-EF ACTIVITIES, NAT ECOLOGICAL OBSERVATORY NET, INFO INTEGRATION & INFORMATICS, INTERNATIONAL RES NET CONNECT, Science Across Virtual Instits","10/01/2012","09/24/2018","Shava Smallen","CA","University of California-San Diego","Standard Grant","Kevin Thompson","09/30/2019","$6,466,479.00","Philip Papadopoulos, Nadya Williams","ssmallen@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7275, 7350, 7364, 7369, 8077","1061, 5919, 5924, 5927, 7350, 7364, 7369, 7433, 8058, 9200, 9251","$0.00","The Pacific Rim Applications and Grid Middleware Assembly (PRAGMA), which began as a workshop series, explores the technical, organizational, and trust dimensions that enable small-to-medium-sized international networks of research scientists to address common scientific questions.<br/><br/>This award uses international scientific expeditions to forge teams of domain scientists and cyberinfrastructure researchers. Together, they develop and test the underlying technologies that are needed to create usable, international-scale, cyber environments.  This award includes not only technology developers but also domain scientists in lake ecology, biodiversity and computer-aided drug discovery.  In technology development, the award will: rebuild PRAGMA's technical infrastructure as a multi-provider/multi-institution cloud with a unique control approach; test and develop new analysis and provenance tools to track how data are utilized by the expeditions; enhance data sharing with user-controlled trust envelopes enabled by IPv4 and IPv6 overlay networks;  and advance sensor network cyberinfrastructure.  Education and training programs will be dramatically expanded through an international student association. New collaborations with a refined focus on common questions that affect India, China, and Southeast Asia will be developed as expeditions. This award broadens engagement of US researchers through a partnership that includes: University of Florida, Indiana University, University of Wisconsin, and led by the University of California, San Diego.<br/><br/>PRAGMA complements key international research network activities and large-scale production resources. It leverages significant investments in people, expertise, tools and infrastructure made by international members.<br/><br/>The intellectual merit is developing practical approaches to enable groups to collaborate through the cyberinfrastructure. The broader impacts are to fundamentally enable large numbers of small groups to work together on scientific problems where international perspective is essential; better inform the US research community of tools and experts out-side of the US; and create professional networks for the next generation of students. The transformational impact will be a model for building people networks to conduct science across international boundaries.<br/><br/>This award is designated as a Science Across Virtual Institutes (SAVI) and is being co-funded by NSF's Office of Cyberinfrastructure; Directorate for Biological Sciences; Directorate for Computer and Information Science and Engineering; and Office of International Science and Engineering."
"1547249","CICI:  Secure Data Architecture: Shared Intelligence Platform for Protecting our National Cyberinfrastructure","OAC","Cyber Secur - Cyberinfrastruc","12/01/2015","11/05/2018","Alexander Withers","IL","University of Illinois at Urbana-Champaign","Standard Grant","Micah Beck","11/30/2019","$499,206.00","Ravishankar Iyer, Adam Slagell, James Marsteller, Alexander Withers","alexw1@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8027","7434","$0.00","This research is expected to significantly enhance the security of campus and research networks. It addresses the emerging security challenge of open, unrestricted access to campus research networks, but beyond that it lays the foundation for an evolvable intelligence sharing network with the very real potential for national scale analysis of that intelligence. Further it will supply cyber security researchers with a rich real-world intelligence source upon which to test their theories, tools, and techniques.   The research will produce a new kind of virtual security appliance that will significantly enhance the security posture of open science networks so that advanced high-performance network-based research can be carried out free of performance lags induced by more traditional security controls.<br/><br/>This research will integrate prior research results, expertise and security products from from both the National Science Foundation and the Department of Energy to advance the security infrastructure available for open science networks, aka Science DMZs.  Further the effort will actively promote sharing of intelligence among science DMZ participants as well as with national academic computational resources and organizations that wish to participate.  Beyond meeting the security needs of campus-based DMZs, the effort will lay the foundation for an intelligence sharing infrastructure that will provide a significant benefit to the cybersecurity research community, making possible the collection, annotation, and open distribution of a national scale security intelligence to help test and validate on-going security research."
"1642142","Collaborative Research: CICI: Secure and Resilient Architecture: Creating Dynamic Superfacilities the SAFE Way","OAC","Cyber Secur - Cyberinfrastruc","12/01/2016","08/08/2016","Paul Ruth","NC","University of North Carolina at Chapel Hill","Standard Grant","Micah Beck","11/30/2019","$448,700.00","","pruth@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8027","","$0.00","Modern computational science is becoming increasingly collaborative as scientists utilize remote shared facilities, including instruments, compute resources, and data repositories. Department of Energy (DOE) researchers have coined the term ""superfacility"" to describe an integration of two or more existing facilities using high-performance networks and data management software in order to increase scientific output.  Currently, superfacilities are purpose-built manually for a specific scientific application or community, limiting their use to large projects that are long-lived.  Recent advances in campus science networks (Science DMZs) and federated Infrastructure-as-a-Service, as in NSF GENI, provide the basic building blocks to enable construction of dynamic superfacilities on demand. Automating the creation of superfacilities reduces their cost but introduces new security challenges. By design, their dynamic network links bypass campus security appliances in order to maintain a friction-free network path; security for these paths is typically addressed by managing interconnections manually.   This project creates a framework to automate, authorize, and monitor end-to-end connectivity across dynamic superfacilities, bringing this technology to a wider range of scientists.<br/><br/>The SAFE Superfacilities project brings together researchers and IT support organizations from RENCI/UNC Chapel Hill, Duke University and DOE/ESnet.  The goal of this project is to generalize support for stitching dynamic network circuits by providing the authorization and security monitoring necessary to enable general, dynamic, and safe interconnections as a foundational building block for Science DMZ, Software Defined Exchanges (SDX), and superfacilities.  One element of the project focuses on using the SAFE logical trust system to authorize dynamic stitching of network links in two systems developed, deployed, and operated by the researchers and their collaborators: the ExoGENI testbed and Duke's Software-Defined Science Network (SDSN) campus network exchange.   A second element addresses dynamic out-of-band security monitoring of traffic over these links.  The project serves as a model for improving security while maintaining high-performance friction-free network paths between campus scientists and remote facilities."
"1642140","Collaborative Research: CICI: Secure and Resilient Architecture: Creating Dynamic Superfacilities the SAFE Way","OAC","Cyber Secur - Cyberinfrastruc","12/01/2016","08/08/2016","Jeffrey Chase","NC","Duke University","Standard Grant","Micah Beck","11/30/2019","$550,000.00","","chase@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8027","","$0.00","Modern computational science is becoming increasingly collaborative as scientists utilize remote shared facilities, including instruments, compute resources, and data repositories. Department of Energy (DOE) researchers have coined the term ""superfacility"" to describe an integration of two or more existing facilities using high-performance networks and data management software in order to increase scientific output.  Currently, superfacilities are purpose-built manually for a specific scientific application or community, limiting their use to large projects that are long-lived.  Recent advances in campus science networks (Science DMZs) and federated Infrastructure-as-a-Service, as in NSF GENI, provide the basic building blocks to enable construction of dynamic superfacilities on demand. Automating the creation of superfacilities reduces their cost but introduces new security challenges. By design, their dynamic network links bypass campus security appliances in order to maintain a friction-free network path; security for these paths is typically addressed by managing interconnections manually.   This project creates a framework to automate, authorize, and monitor end-to-end connectivity across dynamic superfacilities, bringing this technology to a wider range of scientists.<br/><br/>The SAFE Superfacilities project brings together researchers and IT support organizations from RENCI/UNC Chapel Hill, Duke University and DOE/ESnet.  The goal of this project is to generalize support for stitching dynamic network circuits by providing the authorization and security monitoring necessary to enable general, dynamic, and safe interconnections as a foundational building block for Science DMZ, Software Defined Exchanges (SDX), and superfacilities.  One element of the project focuses on using the SAFE logical trust system to authorize dynamic stitching of network links in two systems developed, deployed, and operated by the researchers and their collaborators: the ExoGENI testbed and Duke's Software-Defined Science Network (SDSN) campus network exchange.   A second element addresses dynamic out-of-band security monitoring of traffic over these links.  The project serves as a model for improving security while maintaining high-performance friction-free network paths between campus scientists and remote facilities."
"1708299","Collaborative Research: ACI-CDS&E: Highly Parallel Algorithms and Architectures for Convex Optimization for Realtime Embedded Systems (CORES)","OAC","CDS&E-MSS, CDS&E","09/01/2017","08/24/2017","Saeid Nooshabadi","MI","Michigan Technological University","Standard Grant","Vipin Chaudhary","08/31/2020","$349,988.00","","saeid@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","8069, 8084","026Z, 7433, 8084, 9263","$0.00","Embedded processors are ubiquitous, from toasters and microwave ovens, to automobiles, planes, drones and robots and are typically very small processors that are compute and memory constrained. Real-time embedded systems have the additional requirement of completing tasks within a certain time period to accurately and safely control appliances and devices like automobiles, planes, robots, etc. Convex optimization has emerged as an important mathematical tool for automatic control and robotics and other areas of science and engineering disciplines including machine learning and statistical information processing.  In many fields, convex optimization is used by the human designers as optimization tool where it is nearly always constrained to problems solved in a few hours, minutes or seconds. Highly Parallel Algorithms and Architectures for Convex Optimization for Realtime Embedded Systems (CORES) project takes advantage of the recent advances in embedded hardware and optimization techniques to explore opportunities for real-time convex optimization on the low-cost embedded systems in these disciplines in milli- and micro-seconds. The development of novel algorithms and their high-performance implementations for the real-time solution of practical engineering and scientific optimization problems on the embedded system will open new opportunities in the area of emerging computational science and engineering for cyber physical systems on low-cost platforms. Equally important is the CORES contributions to the education of the next generation of researchers and creators of future infrastructure for realtime computational systems for problems involving engineering optimization. Foremost, CORES will provide undergraduate and graduate level educational opportunities with a multidisciplinary breadth spanning areas as diverse as optimization theory, parallel algorithms for numerical optimization, embedded computer systems, and heterogeneous computing architectures.  Interactions with the control engineering and auto industries in the State of Michigan confirms the need for the development of expertise in this area for present and future engineering research and development. The results from CORES research will have an impact in the fields of engineering optimization and computing infrastructure for cyber physical systems.<br/><br/><br/><br/>The current algorithms for realtime convex optimization can only solve the problem with about a hundred unknowns in the Karush Kuhn Tucker (KKT) convex optimization matrices. This is because the realtime solution enforces a strict time limit on the linear solver (e.g., in microseconds) and  the current algorithms are not designed to fully utilize the limited compute power of the embedded system (e.g., a few CPU cores, plus a GPU). The CORES project will analyze the structure of complex multi-dimensional convex optimization algorithms and replaces the existing sequential implementations, which are the current performance bottleneck, with implementations of new tracking algorithms. Efficient implementations of the algorithms that can effectively leverage the compute power of the scalable heterogeneous system  architecture (SHSA) of the embedded system will be developed. The goal is to speed up the solution process and scale up the size of the optimization problems by orders of magnitude for realtime embedded applications such as control of complex cyber-physical systems (CPS). Specifically, CORES will focus on: (1) Development of high performance linear solvers that exploit the structures of the KKT matrices and leverage the compute power of SHSA and (2) Development of automatic code generation and analysis tools that analyze the structure of the convex optimization problem from a high level modeling language like MATLAB or PYTHON, perform a mapping to a decomposed parallel algorithm, and generate a hybridized multicore CPU and GPU code in OpenCL/CUDA format. Tools that CORES aims to develop come with hierarchical parallel-feature extraction, targeted for various computing elements of SHSA e.g. CPUs and GPU) in a way that eliminates the inefficiencies of inter-processors data sharing. Emerging SHSA combines general-purpose low-latency CPU cores with programmable high-bandwidth vector processing engines on a single platform, connected through a high speed data transfer engines that could still become the performance bottleneck. This feature creates unique opportunities for CORES, and others, to develop sophisticated and specialized computational algorithms and tools for engineering applications such as machine learning and autonomous vehicles  that can exploit such architectures for significantly enhancing performance and scaling up the problem size, while reducing the cost.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1709069","Collaborative Research: ACI-CDS&E: Highly Parallel Algorithms and Architectures for Convex Optimization for Realtime Embedded Systems (CORES)","OAC","CDS&E-MSS, CDS&E","09/01/2017","08/24/2017","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Vipin Chaudhary","08/31/2020","$412,083.00","","dongarra@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","8069, 8084","026Z, 7433, 8084, 9263","$0.00","Embedded processors are ubiquitous, from toasters and microwave ovens, to automobiles, planes, drones and robots and are typically very small processors that are compute and memory constrained. Real-time embedded systems have the additional requirement of completing tasks within a certain time period to accurately and safely control appliances and devices like automobiles, planes, robots, etc. Convex optimization has emerged as an important mathematical tool for automatic control and robotics and other areas of science and engineering disciplines including machine learning and statistical information processing.  In many fields, convex optimization is used by the human designers as optimization tool where it is nearly always constrained to problems solved in a few hours, minutes or seconds. Highly Parallel Algorithms and Architectures for Convex Optimization for Realtime Embedded Systems (CORES) project takes advantage of the recent advances in embedded hardware and optimization techniques to explore opportunities for real-time convex optimization on the low-cost embedded systems in these disciplines in milli- and micro-seconds. The development of novel algorithms and their high-performance implementations for the real-time solution of practical engineering and scientific optimization problems on the embedded system will open new opportunities in the area of emerging computational science and engineering for cyber physical systems on low-cost platforms. Equally important is the CORES contributions to the education of the next generation of researchers and creators of future infrastructure for realtime computational systems for problems involving engineering optimization. Foremost, CORES will provide undergraduate and graduate level educational opportunities with a multidisciplinary breadth spanning areas as diverse as optimization theory, parallel algorithms for numerical optimization, embedded computer systems, and heterogeneous computing architectures.  Interactions with the control engineering and auto industries in the State of Michigan confirms the need for the development of expertise in this area for present and future engineering research and development. The results from CORES research will have an impact in the fields of engineering optimization and computing infrastructure for cyber physical systems.<br/><br/><br/><br/>The current algorithms for realtime convex optimization can only solve the problem with about a hundred unknowns in the Karush Kuhn Tucker (KKT) convex optimization matrices. This is because the realtime solution enforces a strict time limit on the linear solver (e.g., in microseconds) and  the current algorithms are not designed to fully utilize the limited compute power of the embedded system (e.g., a few CPU cores, plus a GPU). The CORES project will analyze the structure of complex multi-dimensional convex optimization algorithms and replaces the existing sequential implementations, which are the current performance bottleneck, with implementations of new tracking algorithms. Efficient implementations of the algorithms that can effectively leverage the compute power of the scalable heterogeneous system  architecture (SHSA) of the embedded system will be developed. The goal is to speed up the solution process and scale up the size of the optimization problems by orders of magnitude for realtime embedded applications such as control of complex cyber-physical systems (CPS). Specifically, CORES will focus on: (1) Development of high performance linear solvers that exploit the structures of the KKT matrices and leverage the compute power of SHSA and (2) Development of automatic code generation and analysis tools that analyze the structure of the convex optimization problem from a high level modeling language like MATLAB or PYTHON, perform a mapping to a decomposed parallel algorithm, and generate a hybridized multicore CPU and GPU code in OpenCL/CUDA format. Tools that CORES aims to develop come with hierarchical parallel-feature extraction, targeted for various computing elements of SHSA e.g. CPUs and GPU) in a way that eliminates the inefficiencies of inter-processors data sharing. Emerging SHSA combines general-purpose low-latency CPU cores with programmable high-bandwidth vector processing engines on a single platform, connected through a high speed data transfer engines that could still become the performance bottleneck. This feature creates unique opportunities for CORES, and others, to develop sophisticated and specialized computational algorithms and tools for engineering applications such as machine learning and autonomous vehicles  that can exploit such architectures for significantly enhancing performance and scaling up the problem size, while reducing the cost.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1664198","SI2-SSI: Lightweight Infrastructure for Land Atmosphere Coupling (LILAC): A Tool for Easy Integration of the Community Land Model into Multiple Modeling Systems","OAC","Software Institutes, CR, Earth System Models, EarthCube","10/01/2017","09/15/2017","A Denning","CO","Colorado State University","Standard Grant","Vipin Chaudhary","09/30/2020","$1,600,000.00","L. Darrell Whitley","denning@atmos.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","8004, 8012, 8074","026Z, 4444, 7433, 8004, 8009","$0.00","Everyone on Earth lives on the ground, so interactions between the atmosphere and the land surface are a critically important part of the climate system and understanding these interactions is important for weather prediction, agriculture, and urban water management. Earth System Models (ESMs) are complex software systems representing complex natural systems. They are critical tools for diagnosing, understanding, and predicting interactions and change in the atmosphere, oceans, and land ecosystems. This project will develop a new software system for coupling the land and atmosphere components of Earth System models, specifically for the most widely-used climate model in the world: the Community Earth System Model (CESM). The new system will be capable of simulating climates near the ground including exchanges of heat, water ,and carbon between vegetated land and the air as well as streamflow and soil moisture. As an officially-supported component of CESM, it will be used by thousands of scientists and students around the world. Unlike its predecessor, the new system will be able to simulate small areas at high resolution for important applications and testing. The project will also support a computer science graduate student as well as academics and scientists who will help develop and test the software. The PIs will engage a global community of software developers and users through a series of workshops and webinars as well as through professional societies and publications. The PIs recognize the chronic under-representation of women and ethnic minorities in both Computer Science and Atmospheric Science. To address this, they will host a computer science summer camp for middle school students from underrepresented groups (URGs), and close collaboration with existing climate courses for K-12 teachers, science outreach in K-12 schools, and a highly successful REU-Site operated by the PI. This project will dramatically improve the usability of the most widely-used climate model in existence. Hundreds of developers and thousands of users around the world will benefit from the far greater flexibility and use cases for this model. The Community Land Model will be coupled to a much greater range of atmospheric codes for weather prediction, air quality applications, and climate projections that enhance the quality of life for people everywhere. <br/>    <br/><br/><br/>The Community Earth System Model CESM) is a uniquely open ESM with a distributed community of hundreds of developers and thousands of users around the world. Compared to other ESMs, CESM is the ""Linux of climate models."" The land component of CESM, the Community Land Model (CLM), has its own vibrant and diverse user and development community, which has supported the construction of a particularly comprehensive terrestrial system model. It includes a rich array of processes that enable examination of the physical, biological, and chemical processes by which natural terrestrial ecosystems and human-managed land affect and are affected by climate and weather. CESM can only be run globally, but there is widespread interest in coupling CLM to alternative high-resolution atmosphere models to study the challenging scientific problems that exist at the interface between small and large spatial scales. A barrier to this research, however, is that CLM cannot readily be coupled to alternative atmosphere models. The CLM coupling interface only supports communication with the CESM coupling infrastructure (CPL7), which imposes strict requirements on how an atmospheric component can communicate with CLM. One key requirement is for the atmosphere model to complete a full time step before coupling can occur. This requirement necessitates significant refactoring of many atmospheric models in order for them to couple to CLM via CPL7. In addition, the tools to build and configure CLM are currently difficult to use outside of the CESM context. For all of these reasons, CLM has never been coupled to alternative atmospheric models in a sustainable fashion. Colorado State University (CSU), in partnership with the National Center for Atmospheric Research (NCAR), proposes to develop a Lightweight Infrastructure for Land-Atmosphere Coupling (LILAC) that will significantly simplify the coupling of CLM to alternative atmospheric models. The LILAC coupler and the associated proposed streamlining and simplification of the CLM tool chain will be developed and tested with a prototype high-resolution atmosphere model, the CSU SAM Cloud Resolving Model, and will be extended for use with any arbitrary Target Atmosphere Model. The development of LILAC and associated tools will enable numerous groups to couple CLM to their atmospheric models, and to quickly update to new state-of-the-art CLM model versions as they become available. This will open up new avenues of land-atmosphere research that can exploit the combined biogeophysical and biogeochemical capabilities of CLM with the strengths of high-resolution atmosphere models. It will enable research into land-atmosphere interactions across a variety of scales, ranging from turbulence-resolving simulations of tower and aircraft data to cloud-resolving simulations to study how small-scale land features such as hillslopes, valleys, lakes, rivers, urban areas and farms conspire to alter surface climate and atmospheric boundary layer characteristics to continental-scale simulations of the impact of land cover and land use change on weather and climate.<br/><br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Office of Polar Programs and Division of Atmospheric and Geospace Science in the Directorate for Geosciences."
"1450451","SI2-SSI: Community Software for Extreme-Scale Computing in Earthquake System Science","OAC","GEOPHYSICS, GEOINFORMATICS, Software Institutes, EarthCube, CDS&E","09/01/2015","03/21/2019","Yehuda Ben-Zion","CA","University of Southern California","Standard Grant","Micah Beck","08/31/2019","$2,200,000.00","Thomas Jordan, Kim Olsen, Yifeng Cui, Ricardo Taborda, Eric Daub","benzion@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1574, 7255, 8004, 8074, 8084","7433, 8004, 8009","$0.00","The Software Environment for Integrated Seismic Modeling (SEISM) Project of the Southern California Earthquake Center (SCEC) will develop advanced earthquake simulation software capable of using high-performance computing to produce new information about earthquakes and the hazards they present. SCEC's SEISM project is developing an integrated, sustainable community software framework for earthquake system science to serve diverse communities of earthquake scientists and engineers, computer scientists, and at-risk stakeholders. The SEISM project is a collaboration among several diverse user communities with shared interests in reducing seismic risk and enhancing seismic resilience. SCEC SEISM researchers are addressing scientific problems that limit the accuracy and scale in current numerical representations of earthquake processes. SEISM computational improvements in seismic hazard calculations will benefit earthquake system science worldwide. The SCEC SEISM project will educate a diverse STEM workforce from the undergraduate to early-career level, and it will cross-train scientists and engineers in a challenging high-performance environment. As one application of SEISM, the researchers will develop new simulations for the Great California ShakeOut, which is engaging millions of people in earthquake preparedness exercises.<br/><br/>Earthquake simulations at the spatiotemporal scales required for probabilistic seismic hazard analysis present some of the toughest computational challenges in geoscience, requiring extreme-scale computing. The Southern California Earthquake Center is creating a Software Environment for Integrated Seismic Modeling (SEISM) that will provide the extreme-scale simulation capability needed to transform probabilistic seismic hazard analysis into a physics-based science. This project will advance SEISM through a user-driven research and development agenda that will push validated SEISM capabilities to higher seismic frequencies and towards extreme-scale computing. It will develop an integrated, sustainable community software framework for earthquake system science to serve diverse communities of earthquake scientists and engineers, computer scientists and at-risk stakeholders. A new SEISM-T framework will support both in-situ and post-hoc data processing to make efficient use of available heterogeneous architectures. The main goal of the project is to increase the 4D outer-scale/inner-scale ratio of simulations at constant time-to-solution by two orders of magnitude above current capabilities. The software development plan will use an agile process of test-driven development, continuous software integration, automated acceptance test suites for each application, frequent software releases, and attention to user feedback. The researchers will take advantage of the SCEC Implementation Interface to develop a dialog among user communities regarding the application of SEISM to the reduction of seismic risk and enhancement of seismic resilience. This research will address fundamental scientific problems that limit the accuracy and scale range in current numerical representations of earthquake processes, which will benefit earthquake system science worldwide. This project will educate a diverse STEM workforce from the undergraduate to early-career level, and it will cross-train scientists and engineers in a challenging high-performance environment. As one application of SEISM, the project team will develop new simulations for the Great California ShakeOut, which is engaging millions of people in earthquake preparedness exercises."
"1835607","Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations","OAC","SPECIAL INITIATIVES, Software Institutes","01/01/2019","09/06/2018","Kristen Fichthorn","PA","Pennsylvania State Univ University Park","Standard Grant","Bogdan Mihaila","12/31/2021","$365,621.00","","fichthorn@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 8007, 9102","$0.00","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study.  Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations.  By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.<br/><br/>This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles.  Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale.  There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources.  The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of  Chemistry  within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1238993","Sustained-Petascale In Action: Blue Waters Enabling Transformative Science And Engineering","OAC","PETASCALE - TRACK 1","10/01/2013","11/02/2018","William Kramer","IL","University of Illinois at Urbana-Champaign","Cooperative Agreement","Edward Walker","07/31/2020","$133,778,949.00","William Gropp, Gregory Bauer, Brett Bode, Cristina Beldica","wtkramer@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","7781","$0.00","This a renewal award to the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign (UIUC) to operate Blue Waters, which is a leadership class compute, network, and storage system, that will deliver unprecedented large scale and highly usable computing capabilities to the national research community.  Blue Waters provides the capability for researchers to tackle much larger and more complex research challenges across a wide spectrum of domain than can be done now, and opens up entirely new possibilities and frontiers in science and engineering.   This system is located at the newly constructed National Petascale Computing Facility at UIUC. <br/><br/>This award enables investigators across the country to conduct innovative research in a number of areas including: using three-dimensional, compressible, finite difference, magnetohydrodynamic (MHD) codes to understand how internal solar magnetoconvection powers the Sun's activity, and how that activity heats the chromosphere and corona and accelerates charged particles to relativistic energies;  applying adaptive mesh refinement (AMR) technologies to study flows of partially ionized plasma in the outer heliosphere;  implementing multiscale methods to study protein induced membrane remodeling key steps of the HIV viral replication cycle and clathrin coated pit formation in endocytosis; testing of the hypothesis that transport fluxes and other effects associated with cloud processes and ocean mesoscale eddy mixing are significantly different from the theoretically derived averages embodied in the parameterizations used in current-generation climate models; and, exploring systems-of-systems engineering design challenges to discover optimal many-objective satellite constellation design tradeoffs that include Earth science applications. Large allocations of resources on the new system have been awarded to scientists and engineers by NSF through a separate peer-reviewed competition. <br/><br/>The Blue Waters system and project are aligned with NSF's Advanced Computing Infrastructure Strategy to promote next generation computational and data intensive applications.  These applications are being developed by multiple teams of researchers who will revolutionize and transform our knowledge of science and engineering across many disciplines. The system supports new modalities of computation, new programming models, enhanced system software, accelerator technologies and novel storage. The robust design and configuration of Blue Waters ensures that it will meet the evolving needs of the diverse science and engineering communities over the full lifetime of the system. <br/><br/>The broader impacts of this award include: provisioning unique infrastructure for research and education; accelerating education and training in the use of advanced computational science; training  new users on how to use petascale computing techniques; promoting an exchange of information between academia and industry about the petascale applications; and  broadening participation and collaborations in computational science with other research institutions and projects nationally and internationally."
"1659880","REU Site:  Undergraduate Studies in Earthquake Information Technology (SCEC/UseIT)","OAC","RSCH EXPER FOR UNDERGRAD SITES, EDUCATION AND HUMAN RESOURCES","04/01/2017","03/14/2017","Thomas Jordan","CA","University of Southern California","Standard Grant","Sushil Prasad","03/31/2020","$360,000.00","","tjordan@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1139, 1575","026Z, 9102, 9250","$0.00","Undergraduate Studies in Earthquake Information Technology (UseIT) is a mentored summer research program administered by the Southern California Earthquake Center (SCEC) at its lead institution, the University of Southern California (USC).  The goal of UseIT is to motivate undergraduates with diverse educational backgrounds towards STEM careers through team-based research in the exciting field of earthquake information technology.  UseIT enables undergraduates to participate in interdisciplinary research enabled by high-performance computing (HPC), visualization software, and other IT tools to address grand challenges in earthquake system science.  An objective is to close the gap between computer science and geoscience by cross-training students in the modes of understanding distinct to these disciplines.  The project particularly seeks to enhance the competency and diversity of the STEM workforce by increasing the research skills and self-confidence of women, under-represented minorities, first-generation college students, and undergraduates from institutions with limited research opportunities, particularly community colleges.  UseIT aims to enhance the computational science expertise that SCEC needs to achieve the practical objectives of predicting strong ground motions and understanding earthquake predictability, thus aligning with the NSF's mission of promoting science and advancing nations welfare.<br/><br/>Undergraduate Studies in Earthquake Information Technology (UseIT) has matured into a program with an experienced staff and a well-equipped Undergraduate Earthquake Research Laboratory at USC.  UseIT's goal of motivating a diverse cadre of undergraduates towards STEM careers is approached by supporting team-based research in the field of earthquake information technology, highly interdisciplinary research enabled by high-performance computing (HPC), visualization software, and other IT tools through which UseIT participants address grand challenges in earthquake system science.  SCEC was awarded its first REU-site grant in 2004 and subsequent grants in 2007, 2009 and 2012.  The UseIT interns have developed SCEC-VDO, a powerful visualization platform for the 4D display of earthquake information that has seen wide application, e.g., by the Working Group on California Earthquake Probabilities.  The UseIT toolkit has expanded in recent years to include geographic information systems (GIS), the OpenSHA hazard-analysis platform, the HAZUS risk-estimation platform, and the physics-based RSQSim rupture simulator.  Team-based research on grand challenges with explicit HPC components can demonstrate to students how difficult system-level problems can be solved by using the HPC facilities as precise scientific instruments. This next phase of UseIT incorporates new elements of HPC-enabled research, probabilistic forecasting, and visualization-assisted data analysis of physics-based earthquake rupture simulators to provide probabilistic earthquake forecasts.  UseIT received 19,500 node-hours of allocation on the Blue Waters supercomputer, which the students used to run simulated earthquake catalogs hundreds of thousands of years in length, from which they produced time-dependent earthquake forecasts, and the project aims to continue this partnership.  UseIT enhances the STEM workforce that SCEC will require to make fundamental progress in earthquake system science."
"1835780","Collaborative Research: NSCI Framework. Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations","OAC","SPECIAL INITIATIVES, Software Institutes","01/01/2019","09/06/2018","Peter Kasson","VA","University of Virginia Main Campus","Standard Grant","Bogdan Mihaila","12/31/2021","$763,289.00","","kasson@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 8007, 9102","$0.00","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study.  Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations.  By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.<br/><br/>This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles.  Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale.  There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources.  The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of  Chemistry  within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835449","Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations","OAC","SPECIAL INITIATIVES, Software Institutes","01/01/2019","09/06/2018","Matteo Turilli","NJ","Rutgers University New Brunswick","Standard Grant","Bogdan Mihaila","12/31/2021","$611,994.00","","matteo.turilli@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 8007, 9102","$0.00","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study.  Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations.  By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.<br/><br/>This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles.  Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale.  There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources.  The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of  Chemistry  within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835720","Collaborative Research: NSCI Framework: Software: SCALE-MS - Scalable Adaptive Large Ensembles of Molecular Simulations","OAC","SPECIAL INITIATIVES, Software Institutes","01/01/2019","09/06/2018","Michael Shirts","CO","University of Colorado at Boulder","Standard Grant","Bogdan Mihaila","12/31/2021","$286,504.00","","michael.shirts@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","1642, 8004","026Z, 077Z, 7925, 8004, 8007, 9102","$0.00","Molecular simulations are becoming important tools in understanding nanoscale processes in science and engineering. Such processes include the motions of proteins and nucleic acids that will enable design of better drugs, the interactions of liquids and metals in photovoltaic and catalytic applications, and the behavior of complex polymers used in industrial materials. Although national cyberinfrastructure investments are increasing raw computational power, the molecular timescales that scientists can simulate are not increasing proportionately. This means that most simulations are significantly shorter than the physical processes they are designed to study.  Fortunately, many researchers have developed powerful algorithms that combine multiple simulations to overcome this molecular timescale problem, but these algorithms can still be very difficult to use effectively. This project, called SCALE-MS, will develop computing tools to simplify the process of writing algorithms that use large collections of molecular simulations to simulate the long timescales needed for scientific and industrial understanding. These tools will make it much simpler to have simulations interact adaptively, so simulation results can automatically guide the creation and running of new simulations.  By making these complex multi-simulation algorithms easier to create and run, this project will enable users to run existing methods in computational molecular science more easily and make it possible for researchers to create and test new, even more powerful, methods for molecular modeling. This project also brings together researchers from biophysics, chemical engineering, and materials science, combining expertise from multiple simulation fields to develop important new ensemble simulation algorithms. This adaptive ensemble framework will enable communities of molecular simulation users in chemistry, chemical engineering, materials science, and biophysics to more easily exchange advanced methods and best practices. Many aspects of this framework can also be applied to aid societal problems requiring modeling in other domains, such as climate and earthquake modeling and prediction.<br/><br/>This project addresses a fundamental need across molecular simulation communities from chemistry to biophysics to materials science: the ability to easily simulate long-timescale phenomena and slowly equilibrating ensembles.  Researchers are increasingly developing high-level parallel algorithms that utilize simulation ensembles, loosely coupled molecular simulations that exchange information on a slower time scale than standard parallel computing techniques. However, most existing molecular simulation software cannot express ensemble simulation algorithms in a general manner and execute them at scale.  There is thus a need for (i) the ability to express ensemble-based methods in a simple, easy- to-use manner that is agnostic of the underlying simulation code, (ii) support for adaptive and asynchronous execution of ensembles, and (iii) a scalable runtime system that encapsulates the complexity of executing and managing jobs seamlessly on different resources.  The project will develop an extensible framework, including a simple high-level API and a sophisticated runtime system, to meet these design objectives on NSF?s production cyberinfrastructure. A key element of this design is the ability to specify ensemble-based patterns of work- and data-flow in a fashion independent of the challenges and complexity of the runtime management of the ensembles. This project will develop a framework consisting of a simple adaptive ensemble API with an underlying runtime platform that enables expression of ensemble simulation methods in a fashion agnostic of the underlying simulation code. This will facilitate design of new ensemble-based methods by the community and enable scientific end users to simply encode complex adaptive workflows. This approach separates the complexity of compute job management from the expression of sophisticated methods. The framework will support adaptive and asynchronous execution of ensembles, removing synchronization blocks that have restricted peta- and exa-scaling of simulation methods. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of  Chemistry  within the NSF Directorate for Mathematical and Physical Sciences and the Division of Chemical, Bioengineering, Environmental, and Transport Systems within the NSF Directorate for Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827151","CC* Network Design and Implementation for Small Institutions: The Tommie Science Network- A dedicated research network for the University of St. Thomas","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/29/2018","Edmund Clark","MN","University of St. Thomas","Standard Grant","Kevin L. Thompson","06/30/2020","$390,671.00","Chih Lai, Eric Tornoe, William Bear, Kemal Badur","edmund.clark@stthomas.edu","2115 Summit Avenue","St. Paul","MN","551051096","6519626038","CSE","8080","","$0.00","The University of St. Thomas, in conjunction with the University of Minnesota's Gopher Science Network Team, is building the Tommie Science Network, a dedicated research network that transforms the campus research environment by providing a reliable and secure high-speed research network capable of achieving sustained transmission rates of up to 100 Gigabits per second (Gbps) between campus research locations and Internet2 (I2). This project creates efficiencies in end-to-end workflows between existing instrumentation facilities and research centers, centralized computing and data storage facilities, and partner institutions. It significantly increases bandwidth available to researchers, allowing the St. Thomas community to fully participate in collaborative, global research activity, and provides a world-class working environment for professors and students to practice distributed research and collaboration techniques to learn skills they will use in their future employment.<br/><br/>The Tommie Science Network connects Owens Science Hall, O'Shaughnessy Science Hall and the St. Thomas E-learning and Research center (STELAR) via high-speed access layer switches linked to the network core at 80Gbps. Traffic on the Tommie Science Network then bypasses the firewall to connect directly to I2 at 100Gbps via the Northern Lights Gigapop. Laboratories and research locations in the science halls and STELAR are connected at 10Gbps to the access layer switches. Globus File Transfer Protocol is used to allow secure, reliable high-speed transfers between the science buildings, the DMZ, and I2. PerfSonar is used to monitor performance between the science buildings and the and Science Demilitarized Zone (Science DMZ) as well as from the DMZ to Internet2. The St. Thomas networking staff is responsible for ongoing operations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642409","SI2-SSE: Scaling up Science on Cyberinfrastructure with the Cooperative Computing Tools","OAC","Software Institutes","09/01/2016","05/17/2018","Douglas Thain","IN","University of Notre Dame","Standard Grant","Stefan Robila","08/31/2020","$510,711.00","","dthain@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8004","026Z, 062Z, 7433, 8004, 8005, 9251","$0.00","This project will support the continued development of the Cooperating Computing Tools (CCTools)software, which provides scientists with the ability to distribute existing simulation and analysis software codes to large numbers of computers in commercial clouds, advanced supercomputers, or both.  As a result, research in fields such as physics, biology, and chemistry can be accelerated, in some cases, by a hundred- or even a thousand-fold.  Staff and students involved in this project will develop new software capabilities and work closely with scientific communities to improve their productivity in advanced computing facilities.  The project will also help to develop a technical workforce by training graduate and undergraduate students in advanced software engineering skills.<br/><br/>Today's computational scientist has access to an extraordinary range of computing facilities, including thousands of cores and petabytes of storage drawn from commercial cloud providers, university clusters, or national computing resources. However, end users often want to connect these systems in ways that the designers did not anticipate. They may wish to access data at one facility, run software at a second facility, and store the results at yet another. However, combining infrastructure is much harder than it ought to be, because most resources assume that their users only live within that particular closed world. Programs built in one environment may not run in another; data stored in one system may not be accessible in another; specialized programming models may be fast on one machine, but useless on another machine. Researchers do not want to be tied down to a single system; rather, they want standard programs to execute easily across all environments. This project will create new features and support users of the Cooperating Computing Tools (CCTools) software which provides scientific users with the ability to harness large scale cyber-infrastructure for data intensive scientific applications. The key components are the Parrot virtual filesystem, the Makeflow workflow system, and the Work Queue application framework. These components can be used separately or together to take existing POSIX applications and scale them up from a single laptop all the way to national scale infrastructure. This project will advance the development of the CCTools software to meet the changing technology landscape in three key respects: exploiting container technologies, making efficient use of local concurrency, and performing capacity management at the workflow scale. This grant will support a small team of students and staff who will engage user communities to understand their needs, sustain and extend the software on current cyberinfrastructure, and perform outreach and education to users both new and old. The project will focus on active user communities in high energy physics, which rely on Parrot for global scale filesystem access in campus clusters and the Open Science Grid; bioinformatics users executing complex workflows via the VectorBase, LifeMapper, and CyVerse disciplinary portals, and ensemble molecular dynamics applications that harness GPUs from XSEDE and commercial clouds."
"1547580","S2I2: Impl: The Molecular Sciences Software Institute","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","08/01/2016","08/27/2018","Thomas Crawford","VA","Virginia Polytechnic Institute and State University","Cooperative Agreement","Vipin Chaudhary","07/31/2021","$14,380,491.00","Theresa Windus, Vijay Pande, Teresa Head-Gordon, Shantenu Jha","crawdad@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1253, 1712, 8004","1711, 7237, 7433, 7504, 7569, 8004, 8211, 8396, 8607, 9216, 9263","$0.00","The Molecular Sciences Software Institute (MolSSI) will become a focus of scientific research, education and scientific collaboration for the worldwide community of computational molecular scientists.  The MolSSI aims to reach these goals by engaging the computational molecular science community in multiple ways to remove barriers between innovations that often occur in small single-researcher groups and the implementation of these ideas in software that is used in the production of science by the entire community. Thus, great ideas will not languish in the ""just get the science right"" mode, but be incorporated into usable software for the wider community to enable bigger and better molecular science. The MolSSI will catalyze significant advances in software infrastructure, education, standards, and best-practices. These advances are critical because they are needed to address the next set of grand challenges in molecular science. Activities catalyzed by the Institute will improve the interoperability of the software used by the community, make easier the use of this software on the varied and heterogenous computing architectures that currently exist, enable greater scalability of existing and emerging theoretical models, as well as substantially improving the training of molecular-science students in software design and engineering. Through the range of outreach efforts by its multiple institutions, the MolSSI will engage the community to increase the diversity of its workforce by more effectively attracting and retaining students and faculty from underrepresented groups.  All of these endeavors will result in fundamentally and dramatically improved molecular science software and its usage, that will reduce or eliminate the current delays - often by years - in the practical realization of theoretical innovations.  Ultimately, the Institute will enable computational scientists to more easily navigate future disruptive transitions in computing technology, and most importantly, tackle problems that are orders of magnitude larger and more complex than those currently within their grasp and to realize new, more ambitious scientific objectives. This will accelerate the translation of basic science into new technologies essential to the vitality of the economy and environment, and to compete globally with Europe, Japan, and other countries that are making aggressive investments in advanced cyber-infrastructure.<br/><br/>The MolSSI aims to reach these goals by engaging the computational molecular science community in multiple ways to remove barriers between innovations that often occur in small single- principle investigator groups and the implementation of these ideas in software that is used in the production of science by the entire community. The MolSSI will create a sustainable Molecular Sciences Consortium that will develop use cases and standards for code and data sharing across the software ecosystem and become a focus of scientific research, education and scientific collaboration for the worldwide community of computational molecular scientists.  The Institute will create an interdisciplinary team of Software Scientists who will help develop software frameworks, interact with community code developers, collaborate with partners in cyber-infrastructure, form mutually productive coalitions with industry, government labs, and international efforts, and ultimately serve as future experts and leaders. In addition, the Institute will support and mentor a cohort of Software Fellows actively developing code infrastructure in research groups across the U.S., and, in turn, they will engage in MolSSI outreach and education activities within the larger molecular science community.  Through a range of multi-institutional outreach efforts, the Institute will engage the community to increase the diversity of its workforce by more effectively attracting and retaining students and faculty from underrepresented groups.  The Institute will educate the next generation of software developers by providing workshops, summer schools, on-line forums, and a Professional Master's program in molecular simulation and software engineering.  MolSSI will be guided by an internal Board of Directors and an external Science and Software Advisory Board, both comprised of leaders in the field, who will work together with the Software Scientists and Fellows to establish the key software priorities.  MolSSI will be sustained by a mix of labor contributed by the community, revenue from education programs and license revenues. In summary, the MolSSI's ultimate impact will be in the translation of basic science into future technological advances essential to the economy, environment, and human health."
"1440677","SI2-SSE: RADICAL Cybertools: Scalable, Interoperable and Sustainable Tools for Science","OAC","Software Institutes","01/01/2015","04/19/2018","Shantenu Jha","NJ","Rutgers University New Brunswick","Standard Grant","Bogdan Mihaila","12/31/2019","$511,916.00","Matteo Turilli","shantenu.jha@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8004","026Z, 7433, 8004, 8005, 9251","$0.00","To support science and engineering applications that are the basis of many societal and intellectual challenges in the 21st century, there is a need for comprehensive, balanced, and flexible distributed cyberinfrastructure (DCI).  The process of designing and deploying such large-scale DCI, however, presents a critical and challenging research agenda.  One specific challenge is to produce tools that provide a step change in the sophistication of problems that can be investigated using DCI, while being extensible, easy to deploy and use, as well as being compatible with a variety of other established tools.  RADICAL Cybertools will meet these requirements by providing an abstractions-based suite of well-defined capabilities that are architected to support scalable, interoperable and sustainable science on a range of high-performance and distributed computing infrastructure.  RADICAL Cybertools builds upon important theoretical advances, production-software-development best practices, and carefully-analyzed usage and programming models.  RADICAL Cybertools is posed to play a role in grand-challenge problems, ranging from personalized medicine and health to understanding long-term global and regional climate.  All software developed through the project will be open source and will be licensed under the MIT License (MIT).  Version control on the SVN repository will be accessible via http://radical.rutgers.edu.<br/><br/>Existing and current utilization of RADICAL Cybertools is built upon preliminary research prototypes of RADICAL Cybertools.  There is a significant difference, however, in the quality and capability required to support scalable end-usage science, compared to that of a research prototype.  It is the aim of this project to bridge this gap between the ability to serve as a research prototype versus the challenges of supporting scalable end-usage science.  This will be achieved by addressing existing limitations of usability, functionality, and scalability.  We will do so by utilizing conceptual and theoretical advances in the understanding of distributed systems and middleware, resulting in a scalable architecture and robust design.  We will employ advances in performance engineering, data-intensive methods, and cyberinfrastructure to deliver the next generation of RADICAL Cybertools.  This project will take the existing research prototypes of RADICAL Cybertools to the next level towards becoming a hardened, extensible, and sustainable tool that will support a greater number of users, application types, and resource types."
"1831393","2018 Software Infrastructure for Sustained Innovation (SI2) Principal Investigators Workshop","OAC","Software Institutes","06/01/2018","09/15/2018","Francis Timmes","AZ","Arizona State University","Standard Grant","Vipin Chaudhary","05/31/2019","$85,065.00","Paul Bauman, Rafael Ferreira da Silva, Kyle Niemeyer, Sandra Gesing","fxt44@mac.com","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8004","026Z, 7556, 8004","$0.00","This project will host a 2-day workshop in Washington, DC, which will bring together the community of Software Infrastructure for Sustained Innovation (SI2) awardees (with the goal of involving one principal investigator from each Scientific Software Elements (SSE), Scientific Software Integration (SSI), and Scientific Software Innovation Institutes (S2I2) project, many of which are collaborative awards) from approximately 250 awards. The workshop will have participation from Computational and Data-Enabled Science and Engineering (CDS&E), Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP), and Venture funded PIs as well as SI2 Early Concept Grants for Exploratory Research (EAGER) and Rapid Response Research (RAPID) awardees. In addition, the proximity to NSF will encourage participation by Program Officers from across the Foundation. Goals of this workshop include: (a) providing a focused forum for PIs to share technical information with each other and with NSF Program Officers, (b) encouraging exploration of emerging topics, (c) identifying emerging best practices across the supported software projects, (d) stimulating thinking on new ways of achieving software sustainability, and (d) disseminating the shared experiences of the researchers via an online web portal. The workshop is expected to host close to 150 SI2 and other awardees, other speakers and panelists. <br/><br/>The proposed workshop will support the exchange of ideas among the current software cyberinfrastructure development projects. It will provide guidance on issues related to the development of robust software and to the problem of software sustainability with the broader agenda for national software ecosystem. Involvement of program officers across NSF is expected to help the interdisciplinary SI2 awardees understand the relevance and impact of cyberinfrastructure throughout the NSF. The participation of these researchers and program officers in a common forum will help ensure that the cyberinfrastructure software developed as part of SI2 projects will be relevant and broadly applicable to the most science and engineering domains possible. The results of this workshop thus have the potential to guide cyberinfrastructure development and cyberinfrastructure driven research for both the participating projects and for the wider software development community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1614673","ArcticDEM Production","OAC","PETASCALE - TRACK 1","05/01/2016","05/18/2016","Paul Morin","MN","University of Minnesota-Twin Cities","Standard Grant","Edward Walker","04/30/2019","$28,494.00","Michael Willis, Claire Porter, Charles Nguyen, Myoung-Jong Noh","lpaul@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7781","","$0.00","The project will use the sustained, petascale computing capabilities of Blue Waters to construct the first high-resolution (2m), high-quality (<5m uncertainty), and openly distributed digital elevation model (DEM) set of all Arctic land masses above 60 degree N, including all of Greenland, Alaska, and Kamchatka. Such a DEM set is needed for measuring and understanding rapid, ongoing changes to the Arctic landscape resulting from climate change and human use, as well as mitigation and adaptation planning for Arctic communities. Resources for obtaining and maintaining the imagery, tools for post-processing the results, software development time, and methods for distributing the DEM have been secured. The compute time used on Blue Waters is needed to provide DEM coverage of the entire Arctic. DEMs will be provided at no cost to the science community and public at large, and will fulfill the United States' commitment to its Arctic partners as part of its tenure as chair of the Arctic Council. <br/><br/>The production of a comprehensive, fine-scale Arctic DEM will transform the Arctic research community and Arctic society. The DEMs will provide time-stamped observations of ice extent and ice surface height and can be examined within the context of changing environmental forcings. The detail of the DEMs will allow the evolution of supraglacial hydrology to be examined down to the level of individual lakes and streams. As published recently in Nature, surface subsidence caused by subglacial hydrological changes will also be available for study (I. M. Howat et al. 2015; Palmer, McMillan, and Morlighem 2015; Willis et al. 2015). <br/><br/>Furthermore, the extent of Arctic DEM almost exactly matches the extent of permafrost in the northern hemisphere, so the DEM set will make it possible to examine major thermokarst changes and periglacial hill slope evolution. Examination of the static, dynamic, macro- and micro-surface topography of permafrost regions is a major recommendation of the U.S. National Academy of Sciences to enable<br/>further understanding of how permafrost regions and their trapped carbon are responding to a warming climate (National Research Council 2014). <br/><br/>Additionally, the DEMs will enable detailed examination of coastal evolution, highlighting shoreline erosion at constant rates and in response to extreme events such as a shoreline ice-shove, or ivu. Changing distribution of lake size and location (and possibly depth) and river and delta morphology will be apparent in the data set, with ramifications for wetlands, ecosystems, and wildlife management and sustainability. Ice/volcano interactions, process geomorphology, tectonic geomorphology, plate boundary interactions, fault expressions, fault evolution, glacial geomorphology, and solid earth geodynamics derived from bending lake shorelines will all be provided from the new data. <br/><br/>Moreover, atmospheric science pursuits such as the study of microclimates, valley-scale atmospheric dynamics and foehn winds will use the new fine-scale topographies for downscaled or micro-region atmospheric models. Communities are also likely to use the DEMs to anticipate and address permafrost impacts, rising sea level, floodplain delimitation and disaster management."
"1726500","MRI: Acquisition of a Hybrid CPU/GPU High Performance Computing Cluster for Research and Education at Lamar University","OAC","MAJOR RESEARCH INSTRUMENTATION, CYBERINFRASTRUCTURE","10/01/2017","06/27/2018","Jing Zhang","TX","Lamar University Beaumont","Standard Grant","Stefan Robila","09/30/2020","$540,031.00","Tao Wei, Yueqing Li, Sujing Wang","jing.zhang@lamar.edu","4400 Port Arthur Road","Beaumont","TX","777055748","4098807670","CSE","1189, 7231","1189, 9251","$0.00","As traditional data processing devices are no longer adequate to handle complex data sets and large computations due to the continuing information explosion, a high performance computing cluster (HPCC) has become an essential instrument for a wide variety of leading-edge research and educational activities. Lamar University (LU), a medium-sized non-PhD granting four-year university with more than 15,000 students will acquire and deploy a HPCC to enhance compute-intensive and data-intensive studies and to facilitate discipline-specific and multidisciplinary research through a shared state-of-the-art computing platform. The instrumentation will strongly support LU's high priority current and future research needs as well as benefit a variety of regional academic institutions and industries.<br/><br/>Specifically, the project will acquire a hybrid CPU/GPU HPCC which will make it possible to deploy the best suited computing nodes to perform traditional CPU-based, GPU-based, and hybrid CPU/GPU-based data-intensive computing tasks at LU.  The resource will enable the exploration of creative research areas and establish new cross-disciplinary studies in the areas of imaging genomics, deep learning, big data, computational neuroscience, molecular physics, advanced materials research, scientific optimization, water and air quality analysis, transportation systems, electronic structure calculations, nucleic acid biomarker discovery and epigenetics, and many more. <br/><br/>Furthermore, as a shared research resource, the HPCC will not only promote cross-disciplinary collaborations among faculty members from different departments within the university, but also enable LU to promote and strengthen collaborative opportunities with other research institutions. In addition, the instrument will also become an essential educational tool with the potential to foster interest among faculty in the development of new courses that will integrate state-of-the-art research into undergraduate and graduate curricula.  Additionally, the project will provide access to the resource to users from other academic institutions and industrial partners in the Golden Triangle area in Southeast Texas.  Finally, the project will organize outreach activities for K-12 students from local Independent School Districts (ISDs) that have high minority and low-income ratios to study in science, technology, engineering, and mathematics (STEM) areas."
"1440800","SI2-SSE: Synthesizing Self-Contained Scientific Software","OAC","SPECIAL PROJECTS - CCF, Software Institutes","10/01/2014","08/08/2014","Ashish Gehani","CA","SRI International","Standard Grant","Bogdan Mihaila","09/30/2019","$499,919.00","","ashish.gehani@sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","6508592651","CSE","2878, 8004","2878, 7433, 8004, 8005","$0.00","Computational aspects of scientific experiments have been growing steadily. This creates an increasing need to be able to reproduce the results. Science is also increasingly performed by exploring diverse sets of data. Unsurprisingly, there is a demand for being able to easily repeat the numerous transformations performed. Software packaged with tools from this project will allow scientists to publish their code in a form that can be utilized by others with minimal effort. By eliminating many of the challenges of building, configuring, and running software, it will allow members of the scientific community to more easily reproduce each others' computational results.<br/><br/>Increasingly, entire virtual machines are published to ensure that a recipient does not have to replicate the compute environment, retrieve data and code dependencies, or invest effort into configuring the system. However, this approach scales poorly with the growth in size of the included data sets, the extraneous functionality in applications that utilize versatile software libraries, and the irrelevant code in stock operating system distributions. This project will design, develop, and evaluate a toolchain that allows scientists to transform their software into specialized applications with all the necessary environmental conditions and portions of required data sets built directly into the code. The resulting scientific appliances can be distributed for others to explore and verify results without the overhead of shipping extraneous data and code."
"1550547","Collaborative Research: SI2-SSI: Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion","OAC","PROBABILITY, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, Software Institutes, CDS&E-MSS","09/01/2016","08/10/2016","Noemi Petra","CA","University of California - Merced","Standard Grant","Micah Beck","08/31/2020","$475,000.00","","npetra@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092598670","CSE","1263, 1266, 1271, 8004, 8069","4444, 7433, 8004, 8009, 8251, 9263","$0.00","Scientists often use mathematical models to predict the behavior of natural and engineered systems. These models are therefore fundamental to scientific and engineering progress and hence relevant to NSF's science mission. Most models of realistic physical systems  use complex formulae (such as, partial differential equations) involving many variables. When using such a model for predicting the future behavior of a system, a scientist has to provide initial values for all the variables.  This can be difficult because input values may not be directly measureable. Thus, scientists often must use ""inverse"" computations to calculate the initial input values of the variables of a system model based on external observations of the real world. In other words, scientists seek to infer inputs to a computer model of a physical process from real observational data of the outputs. There are many examples of inverse computations, ranging from computing the important dimensions of an organ from its CAT scan, reconstructing the source of a sound by measuring its volume and frequency at various places, calculating the density of the Earth from measurements of its gravity field, or calculating the initial condition of the atmosphere (temperature, pressure, etc.) from satellite and weather station observations over a time interval. Inverse problems are ubiquitous across all of science and engineering (and beyond). Many solutions exist for inverse problems, i.e. solutions that fit the data to the observations. However, there are variations in the solutions identified. That is, the solutions of an inverse problem are subject to uncertainty. Bayesian inferencing provides a systematic mathematical framework for characterizing this uncertainty. However, the Bayesian solution of inverse problems for large-scale complex models require enormous computational power. Only recently have algorithms begun to emerge that are computationally tractable. However, these algorithms have remained out of the reach of the mainstream of scientists who solve inverse problems, due to their complexity and the need for deeper information from the forward model. This project aims to develop, distribute, and support open-source software that encodes state-of-the-art algorithms for the solution of large-scale complex Bayesian inverse problems and is robust, scalable, flexible, modular, widely accessible, and easy to use.<br/><br/>The project builds heavily on two complementary open-source software libraries the team has been developing: MUQ at MIT, and hIPPYlib at UT-Austin/UC-Merced. MUQ provides a spectrum of powerful Bayesian inversion models and algorithms, but expects forward models to come equipped with gradients/Hessians to permit large-scale solution. hIPPYlib implements powerful large-scale gradient/Hessian-based inverse solvers in an environment that can automatically generate needed derivatives, but it lacks full Bayesian capabilities. By integrating these two complementary libraries, the project will result in a robust, scalable, and efficient software framework that realizes the benefits of each to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. The resulting software, that will be distributed under an open-source license, will provide an environment for rapid development of inverse models equipped with gradient/Hessian information; benchmark problems for evaluation and comparison of algorithms; and tutorial problems for training and testing purposes."
"1638863","IRNC: Backbone: NEAAR: Networks for European, American, and African Research","OAC","INTERNATIONAL RES NET CONNECT","09/01/2016","12/11/2018","Jennifer Schopf","IN","Indiana University","Continuing grant","Kevin Thompson","08/31/2020","$3,250,000.00","Edward Moynihan, Cathrin Stover, Andoh Hoba","jmschopf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7369","","$0.00","The Networks for European, American, and African Research (NEAAR) collaboration is a powerful, cross organizational project that will provide services and bandwidth connecting researchers in the US with their counterparts in Europe and Africa. This project will have an immediate impact on the research environment and also supports future application and technology advances. <br/><br/>Indiana University (IU) jointly leads the NEAAR collaboration with GANT, the European research and education network (REN), in a cooperative partnership with the African regional RENs: the UbuntuNet Alliance, the Arab States Research and Education Network (ASREN), the West and Central African Research and Education Network (WACREN), as well as the South African National Research Network (SANReN) and the Tertiary Education and Research Network of South Africa (TENET). IU also coordinates domestically with Internet2 and the Energy Sciences Network (ESnet). Through its extensive regional partnerships, NEAAR has the potential to reach research and education (R&E) communities in over 80 countries across three continents.<br/><br/>NEAAR has the potential to be transformational to NSF-funded science by providing not only the network but also the human expertise to make the most of international collaborations and data sharing, thereby increasing research and educational opportunities. Through additional capacity to Europe and targeted support of data and networking support services in Africa, NEAAR will be able to support the majority of the NSF-funded research sharing between Africa and the US. IU has identified over 60 preliminary groups to work directly with to increase their data capacity and ability to collaborate. Advanced data sharing fundamentally changes and extends the research that is feasible between partners, and NEAAR has the potential to have an unprecedented impact on US-Africa collaboration."
"1550593","Collaborative Research: SI2-SSI: Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion","OAC","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, Software Institutes, CDS&E-MSS","09/01/2016","08/10/2016","Omar Ghattas","TX","University of Texas at Austin","Standard Grant","Micah Beck","08/31/2019","$350,885.00","Umberto Villa","omar@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1266, 1271, 8004, 8069","4444, 7433, 8004, 8009, 8251, 9263","$0.00","Scientists often use mathematical models to predict the behavior of natural and engineered systems. These models are therefore fundamental to scientific and engineering progress and hence relevant to NSF's science mission. Most models of realistic physical systems  use complex formulae (such as, partial differential equations) involving many variables. When using such a model for predicting the future behavior of a system, a scientist has to provide initial values for all the variables.  This can be difficult because input values may not be directly measureable. Thus, scientists often must use ""inverse"" computations to calculate the initial input values of the variables of a system model based on external observations of the real world. In other words, scientists seek to infer inputs to a computer model of a physical process from real observational data of the outputs. There are many examples of inverse computations, ranging from computing the important dimensions of an organ from its CAT scan, reconstructing the source of a sound by measuring its volume and frequency at various places, calculating the density of the Earth from measurements of its gravity field, or calculating the initial condition of the atmosphere (temperature, pressure, etc.) from satellite and weather station observations over a time interval. Inverse problems are ubiquitous across all of science and engineering (and beyond). Many solutions exist for inverse problems, i.e. solutions that fit the data to the observations. However, there are variations in the solutions identified. That is, the solutions of an inverse problem are subject to uncertainty. Bayesian inferencing provides a systematic mathematical framework for characterizing this uncertainty. However, the Bayesian solution of inverse problems for large-scale complex models require enormous computational power. Only recently have algorithms begun to emerge that are computationally tractable. However, these algorithms have remained out of the reach of the mainstream of scientists who solve inverse problems, due to their complexity and the need for deeper information from the forward model. This project aims to develop, distribute, and support open-source software that encodes state-of-the-art algorithms for the solution of large-scale complex Bayesian inverse problems and is robust, scalable, flexible, modular, widely accessible, and easy to use.<br/><br/>The project builds heavily on two complementary open-source software libraries the team has been developing: MUQ at MIT, and hIPPYlib at UT-Austin/UC-Merced. MUQ provides a spectrum of powerful Bayesian inversion models and algorithms, but expects forward models to come equipped with gradients/Hessians to permit large-scale solution. hIPPYlib implements powerful large-scale gradient/Hessian-based inverse solvers in an environment that can automatically generate needed derivatives, but it lacks full Bayesian capabilities. By integrating these two complementary libraries, the project will result in a robust, scalable, and efficient software framework that realizes the benefits of each to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. The resulting software, that will be distributed under an open-source license, will provide an environment for rapid development of inverse models equipped with gradient/Hessian information; benchmark problems for evaluation and comparison of algorithms; and tutorial problems for training and testing purposes."
"1826967","CC* NPEO: Toward the National Research Platform","OAC","Campus Cyberinfrastrc (CC-NIE)","10/01/2018","09/12/2018","Larry Smarr","CA","University of California-San Diego","Standard Grant","Kevin Thompson","09/30/2021","$2,500,000.00","Philip Papadopoulos, Frank Wuerthwein, Tajana Rosing, Ilkay Altintas","lsmarr@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8080","","$0.00","Academic researchers need a simple data sharing architecture with end-to-end 10-to-100Gbps performance to enable virtual co-location of large amounts of data with computing. End-to-end is a difficult problem to solve in general because the networks between ends (campuses, data repositories, etc.) typically traverse multiple network management domains: campus, regional, and national.  No one organization owns the responsibility for providing scientists with high-bandwidth disk-to-disk performance. Toward the National Research Platform (TNRP), addresses issues critical to scaling end-to-end data sharing. TNRP will instrument a large federation of heterogeneous ""national-regional-state"" networks (NRSNs) to greatly improve end-to-end network performance across the nation. <br/><br/>The goal of improving end-to-end network performance across the nation requires active participation of these distributed intermediate-level entities to reach out to their campuses. They are trusted conveners of their member institutions, contributing effectively to the ""people networking"" that is as necessary to the development of a full National Research Platform as is the stability, deployment, and performance of technology. TNRP's collaborating NRSNs structure leads to engagement of a large set of science applications, identified by the participating NRSNs and the Open Science Grid. <br/><br/>TNRP is highly instrumented to directly measure performance. Visualizations of disk-to-disk performance with passive and active network monitoring show intra- and inter-NSRN end-to-end performance. Internet2, critical for interconnecting regional networks, will provide an instrumented dedicated virtual network instance for the interconnection of TNRP's NRSNs. Cybersecurity is a continuing concern; evaluations of advanced containerized orchestration, hardware crypto engines, and novel IPv6 strategies are part of the TNRP plan.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835712","Elements:Software:Open-Source Robust Geometry Toolkit for Black-Box Finite Element Analysis","OAC","DATANET","09/01/2018","09/06/2018","Daniele Panozzo","NY","New York University","Standard Grant","Vipin Chaudhary","08/31/2021","$599,967.00","Denis Zorin","panozzo@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7726","026Z, 077Z, 7923, 8004","$0.00","The numerical solution of partial differential equations (PDEs) is ubiquitous in science and engineering applications, including simulation of elastoplastic deformations, fluids, and light scattering. The finite element method (FEM) is the most commonly used discretization of PDEs, especially in the context of structural and thermal analysis, due to its generality and rich selection of off-the-shelf commercial implementations. Ideally, a PDE solver should be a ``black box'': the user provides as input the domain boundary, boundary conditions, and the governing equations, and the code computes the value of the solution at a set of user-specified points of the input domain. This is surprisingly far from being the case for all existing open-source or commercial software, despite the research efforts in this direction and the large academic and industrial interest. To a large extent, this is due to treating meshing and FEM basis construction as two disjoint problems, often exposing the user to the technical issues of interfacing the meshing software with FEM basis construction, both of which, strictly speaking, are technical issues internal to the solver. This state of matters presents a fundamental problem for applications that require fully automatic, robust processing of large collections of meshes of varying sizes, an increasingly common situation as large collections of geometric data become available. This proposal introduces an integrated pipeline, considering meshing and element design as a single challenge, and developing a software platform to enable black box analysis on complex geometric models represented as point clouds, triangle meshes, or CAD (Computer Aided Design) models, opening the door to new shape design technique to a wide range of new applications in sciences and engineering.<br/><br/>This project proposes to develop a set of software components based on a set of novel approaches the investigators have developed combined with ""filtered"" use of rational or multi-precision numerical representations to handle robustness problems while maintaining practical performance. The proposed set of geometry processing techniques, while slower than existing ones, are fully robust in a sense of always produce a valid result with minimal assumptions on the input. The geometric toolkit will allow to automatically convert geometrical data in the form of range scans, CAD models, or voxel grids into a surface or volumetric representation, directly usable in widely used open-source finite element method (FEM) packages. It will include mesh generation, in addition to tetrahedral meshes, for other common types of discretizations: hexahedral meshes, and hex-dominant hybrid meshes. The key innovation is to achieve numerical robustness with minimal added algorithmic complexity by carefully mixing higher precision representations for the critical part, while relying on standard fixed-precision floating point representation for the rest and designing algorithms amenable to this approach. As in overwhelming majority of cases higher accuracy is needed for a vanishingly small fraction of computation, this approach allows the users to achieve sensible running time while ensuring output validity and algorithmic correctness on imperfect, real world data. Secondly, the invetigators will integrate FEM basis construction with meshing decoupling accuracy from mesh quality. The software toolkit developed in this proposal has potential for a major impact in all domains that require computational simulation of physical phenomena in complex geometries, enabling the automation of data acquisition, reconstruction, and simulation pipelines. The expectation of this project is that the outcome will not only be a reduction in human time, but the opportunity to fully automate this pipeline will open new research venues. The release of all the software with a MPL2 license will facilitate integration of the results of the work into commercial software, in addition to academic/non-profit research use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1550487","Collaborative Research: SI2-SSI: Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion","OAC","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, Software Institutes, CDS&E-MSS","09/01/2016","08/10/2016","Youssef Marzouk","MA","Massachusetts Institute of Technology","Standard Grant","Micah Beck","08/31/2019","$524,968.00","Matthew Parno","ymarz@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1266, 1271, 8004, 8069","4444, 7433, 8004, 8009, 8251, 9263","$0.00","Scientists often use mathematical models to predict the behavior of natural and engineered systems. These models are therefore fundamental to scientific and engineering progress and hence relevant to NSF's science mission. Most models of realistic physical systems  use complex formulae (such as, partial differential equations) involving many variables. When using such a model for predicting the future behavior of a system, a scientist has to provide initial values for all the variables.  This can be difficult because input values may not be directly measureable. Thus, scientists often must use ""inverse"" computations to calculate the initial input values of the variables of a system model based on external observations of the real world. In other words, scientists seek to infer inputs to a computer model of a physical process from real observational data of the outputs. There are many examples of inverse computations, ranging from computing the important dimensions of an organ from its CAT scan, reconstructing the source of a sound by measuring its volume and frequency at various places, calculating the density of the Earth from measurements of its gravity field, or calculating the initial condition of the atmosphere (temperature, pressure, etc.) from satellite and weather station observations over a time interval. Inverse problems are ubiquitous across all of science and engineering (and beyond). Many solutions exist for inverse problems, i.e. solutions that fit the data to the observations. However, there are variations in the solutions identified. That is, the solutions of an inverse problem are subject to uncertainty. Bayesian inferencing provides a systematic mathematical framework for characterizing this uncertainty. However, the Bayesian solution of inverse problems for large-scale complex models require enormous computational power. Only recently have algorithms begun to emerge that are computationally tractable. However, these algorithms have remained out of the reach of the mainstream of scientists who solve inverse problems, due to their complexity and the need for deeper information from the forward model. This project aims to develop, distribute, and support open-source software that encodes state-of-the-art algorithms for the solution of large-scale complex Bayesian inverse problems and is robust, scalable, flexible, modular, widely accessible, and easy to use.<br/><br/>The project builds heavily on two complementary open-source software libraries the team has been developing: MUQ at MIT, and hIPPYlib at UT-Austin/UC-Merced. MUQ provides a spectrum of powerful Bayesian inversion models and algorithms, but expects forward models to come equipped with gradients/Hessians to permit large-scale solution. hIPPYlib implements powerful large-scale gradient/Hessian-based inverse solvers in an environment that can automatically generate needed derivatives, but it lacks full Bayesian capabilities. By integrating these two complementary libraries, the project will result in a robust, scalable, and efficient software framework that realizes the benefits of each to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. The resulting software, that will be distributed under an open-source license, will provide an environment for rapid development of inverse models equipped with gradient/Hessian information; benchmark problems for evaluation and comparison of algorithms; and tutorial problems for training and testing purposes."
"1827139","CC* Networking Infrastructure: The Roadrunner High-Performance Science, Engineering, and Business DMZ","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","09/15/2018","Brent League","TX","University of Texas at San Antonio","Standard Grant","Kevin L. Thompson","06/30/2019","$500,000.00","Harry Millwater, Bernard Arulanandam, Brent League","BRENT.LEAGUE@UTSA.EDU","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","8080","","$0.00","World-class research in cyber security, bioinformatics, cloud computing, machine learning, artificial intelligence, real-time computing and other related areas requires efficient and often near-immediate access to large data sets in order to reduce the time from theory to discovery. In addition, discoveries are often interdisciplinary and multi-institutional. As a result, a critical enabling feature for impactful, collaborative research is a dedicated high-speed network that is omnipresent across a campus. The University of Texas at San Antonio (UTSA), a Hispanic Serving Institution, is implementing a dedicated research network (DMZ) to facilitate data-intensive computation and research collaboration endeavors. This infrastructure fills a gap that currently exists in the ""last mile"" bottleneck from a research lab to the DMZ. <br/><br/>In particular, the project calls for installation of 10 Gb/s switches across campus that, in concert with the dedicated research network, provide 5-10X faster data transfer rates. These improvements foster access by UTSA faculty and students to the campus' high-performance computing facility, high-speed data storage, and visualization laboratory as well as the Texas Advanced Computing Center (TACC) in Austin, Texas. The new network also enhances experiential learning activities such as UTSA's annual CyberPatriot competition, undergraduate research projects in cloud and high-performance computing, cloud computing ELab training for undergraduates, as well as certification programs such as the Master's level certification in cloud computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1550225","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, APPLIED MATHEMATICS, COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/25/2018","Steffen Bass","NC","Duke University","Continuing grant","Bogdan Mihaila","06/30/2020","$522,190.00","Robert Wolpert","bass@phy.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1253, 1266, 7244, 8004","026Z, 4444, 7433, 7569, 8009, 8084, 8251","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1827127","CC* Networking Infrastructure: Bulldog Connectivity and Research","OAC","Campus Cyberinfrastrc (CC-NIE)","08/01/2018","09/06/2018","Damian Clarke","SC","South Carolina State University","Standard Grant","Kevin Thompson","07/31/2020","$443,726.00","Nikunja Swain, Donald Walter","damian.clarke@aamu.edu","300 College Street NE","Orangeburg","SC","291170001","8035367000","CSE","8080","","$0.00","The Bulldog Connectivity and Research Network  (BCR net) project at South Carolina State University (SCSU) creates a network infrastructure to accommodate increasing research activities in STEM and non-traditional areas such as the library, visual arts, museum and the planetarium. The campus enterprise network, plagued with bandwidth bottlenecks, supports only administrative, and academic needs, which competes for simultaneous use of limited resources. Disparate silos of unconnected computing resources have supported research computing in research labs with little scalability to the evolution of research needs. The BCR net provides a frictionless redundant design dedicated to research computing that delivers high-speed connectivity. Its design focuses on isolating high-throughput research functions through data paths on operationally efficient high-speed connections to research partners.<br/><br/>BCR net, separate from the campus enterprise network, gives SCSU researchers access to on-demand high bandwidth science DMZ connections to sharable storage of research data connected to high throughput data transfer nodes with secure digital connections. Transfers of large datasets and instructions will be enabled under projects such as the Robotically Controlled Telescope Consortium, the NSF Partnership in Observational and Computational Astronomy (POCA), the NSF SCI-STEPS INCLUDES Project. Other areas affected include; the Physics and Chemistry computational labs for astrophysics and cancer research, the digital media lab, the historical collection and archiving Orangeburg massacre collection, applied radiation sciences lab, the Nuclear Engineering program reactor simulation lab, and the computer science security lab.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835768","Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Matthew Hasselfield","PA","Pennsylvania State Univ University Park","Standard Grant","Bogdan Mihaila","08/31/2021","$40,000.00","","mhasse@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.<br/><br/>This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1649759","Computational Infrastructure for Brain Research: Workshops on Methods and Infrastructure for Scalable Computing in Neuroscience","OAC","ETF, IntgStrat Undst Neurl&Cogn Sys","10/01/2016","08/30/2016","Lauren Michael","WI","University of Wisconsin-Madison","Standard Grant","William Miller","09/30/2019","$49,988.00","Robert Nowak, Rebecca Willett, Tim Rogers","lmichael@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7476, 8624","026Z, 7556, 8089, 8091","$0.00","Computational and data-intensive neuroscience has been growing rapidly. However, many neuroscientists are not familiar with scalable computational approaches and the diverse, widely available shared computational resources that can be used to accelerate neuroscientific data analysis and modeling, such as the advanced computing resources of the Open Science Grid and the XSEDE network, among others. Many computing infrastructure experts also know little about the computational and data challenges in contemporary neuroscience. This project will address these issues through a pair of multidisciplinary workshops and development of training materials designed to bring together and educate both the neuroscientists and computational infrastructure communities. In the first workshop, leading neuroscientists and computing infrastructure experts will share computational practices to identify critical theory and best practices for matching neuroscience computational approaches with appropriate infrastructure configurations. The second workshop will be timed to coincide with the 2018 annual meeting of the Cognitive Science Society, to be held in Madison, Wisconsin. The overall goal is dissemination of these best practices and accompanying practical training on advanced computing resources to accelerate neuroscience discovery. This project therefore aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/>At the Center for High Throughput Computing (CHTC) at the University of Wisconsin-Madison, breakthrough progress in neuroscience has been enabled by integrating computational expertise and domain expertise via Research Computing Facilitators. This model has demonstrated that scientific paradigm shifts can be achieved by human-driven matchmaking to pair research programs, computational methods experts, and appropriate computational infrastructure configurations. The workshops will foster replication of this matchmaking approach to impact the broader neuroscience community. The overall goal is to empower brain researchers to select the scalable computational resources that are most appropriate for their specific research programs, to achieve dramatic rapid progress in the field.<br/>This award by the CISE Division of Advanced Cyberinfrastructure is jointly supported by the CISE Division of Information and Intelligent Systems, with funds associated with the NSF Understanding the Brain and BRAIN Initiative activities, and for developing national research infrastructure for neuroscience. This project also aligns with NSF objectives under the National Strategic Computing Initiative."
"1642391","SI2-SSE: Hearing the Signal through the Static: Realtime Noise Reduction in the Hunt for Binary Black Holes and other Gravitational Wave Transients","OAC","LIGO RESEARCH SUPPORT, COMPUTATIONAL PHYSICS, Software Institutes","11/01/2016","09/04/2018","Chad Hanna","PA","Pennsylvania State Univ University Park","Continuing grant","Bogdan Mihaila","10/31/2019","$400,000.00","","crh184@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1252, 7244, 8004","7433, 7483, 7569, 8004, 8005, 8084","$0.00","Gravitational waves - tiny ripples in the fabric of space - were detected for the first time in history on September 14, 2015 by the US-led gravitational wave observatory, LIGO. This watershed event has ushered in a new era of gravitational wave astronomy, which will transform our understanding of the Universe by providing information through a previously inaccessible channel. LIGO operates at the very edge of the sensitivity required to detect gravitational waves, and therefore, non-stationary noise caused by the environment, e.g., weather and man-made noise, limits LIGO's sensitivity to short-duration gravitational wave transient signals such as the one detected in September. In order to ensure the most opportunity for the advancement of science, this project aims to mine the extensive auxiliary information available in the LIGO observatories in realtime in order to mitigate the impact of non-stationary noise and increase the rate of transient gravitational wave detections. Doing so will afford increased opportunities for joint gravitational wave and electromagnetic observations, which are thought to be rare.<br/><br/>This project aims to address a key piece of missing software infrastructure to use machine learning and inference techniques to utilize auxiliary information such as seismometers, microphones and various control loop signals, to identify non-stationary noise that couples to the gravitational-wave channel in near realtime. The software will be broken into three components: a signal decomposer, a signal classifier and a signal trainer. The signal classifier will determine, given the decomposed, instantaneous output of auxiliary channels and the training data, the probability that non-stationary noise is present. These software components will be built from reusable resources that can be applied across the time-domain gravitational wave community in data calibration, data transfer, and analysis.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1450412","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","OAC","GEOMORPHOLOGY & LAND USE DYNAM, Software Institutes, EarthCube","08/01/2015","07/14/2015","Erkan Istanbulluoglu","WA","University of Washington","Standard Grant","Stefan Robila","07/31/2020","$676,836.00","","erkani@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7458, 8004, 8074","7433, 8009","$0.00","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population."
"1450409","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","OAC","GEOMORPHOLOGY & LAND USE DYNAM, Software Institutes, EarthCube","08/01/2015","07/14/2015","Gregory Tucker","CO","University of Colorado at Boulder","Standard Grant","Stefan Robila","07/31/2020","$789,777.00","Daniel Hobley","gtucker@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7458, 8004, 8074","7433, 8009","$0.00","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population."
"1550228","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","02/01/2019","Barbara Jacak","CA","University of California-Berkeley","Standard Grant","Bogdan Mihaila","06/30/2020","$497,683.00","Peter Jacobs, Xin-Nian Wang","bvjacak@lbl.gov","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1450338","Collaborative Research: SI2-SSI: Landlab: A Flexible, Open-Source Modeling Framework for Earth-Surface Dynamics","OAC","GEOMORPHOLOGY & LAND USE DYNAM, Software Institutes, EarthCube","08/01/2015","07/14/2015","Nicole Gasparini","LA","Tulane University","Standard Grant","Stefan Robila","07/31/2020","$532,320.00","","ngaspari@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","7458, 8004, 8074","7433, 8009, 9150","$0.00","Earth scientists discover and predict the behavior of the natural world in part through the use of computer models. Models are used to study a wide range of phenomena, such as soil erosion, flooding, plant growth, landslide occurrence, and many other processes. Models can be used to develop scientific understanding by comparing model calculations with the real world. They can also be used to make predictions about how nature will behave under certain conditions. In the areas of geosciences that deal with the earth's surface, one bottleneck in the development and use of models is the effort required to build, test, and debug the necessary software. This project contributes to scientific research and discovery by creating open source software tools that scientists can use to more efficiently create, modify, or combine computer models that represent portions of earth's surface and the processes occurring thereon. The project combines software development with user training and web-based resources, so that the products will be openly accessible, well documented, and widely disseminated within the relevant scientific communities. The project also contributes to K-12, undergraduate, and graduate-level education in computational modeling and earth-surface processes.<br/><br/>This project catalyzes research in earth-surface dynamics by developing a software framework that enables rapid creation, refinement, and reuse of two-dimensional (2D) numerical models. The phrase earth-surface dynamics refers to a remarkably diverse group of science and engineering fields that deal with our planet's surface and near-surface environment: its processes, its management, and its responses to natural and human-made perturbations. Scientists who want to use an earth-surface model often build their own unique model from the ground up, re-coding the basic building blocks of their model rather than taking advantage of codes that have already been written. Whereas the end result may be novel software programs, many person-hours are lost rewriting existing code, and the resulting software is often idiosyncratic, poorly documented, and unable to interact with other software programs in the same scientific community and beyond, leading to lost opportunities for exploring an even wider array of scientific questions than those that can be addressed using a single model. The Landlab model framework seeks to eliminate these redundancies and lost opportunities, and simultaneously lower the bar for entry into numerical modeling, by creating a user- and developer-friendly software library that provides scientists with the fundamental building blocks needed for modeling earth-surface dynamics. The framework takes advantage of the fact that nearly all surface-dynamics models share a set of common software elements, despite the wide range of processes and scales that they encompass. Providing these elements in the context of a popular scientific programming environment, with strong user support and community engagement, contributes to accelerating progress in the diverse sciences of the earth's surface.<br/><br/>The Landlab modeling framework is designed so that grid creation, data storage, and sharing of data among process components is done for the user. The framework is generic enough so that the coupling of two process components, whether they are squarely within a geoscience subdiscipline, or they cross subdisciplines, is the same. This architecture makes it easy to explore a wide range of questions in the geosciences without the need for much coding. Further, the model code is primarily written in Python, a language that is relatively easy for casual programmers to learn. Because of these attributes, the Landlab modeling framework has the potential to add computational modeling to the toolbox of a wide array of geoscientists, and to clear a path for trans-disciplinary earth-surface modeling that explores societally relevant topics, such as land-cover changes in response to climate change, as well as modeling of topics that currently require the coupling of sophisticated but harder-to-use models, such as sediment source-to-sink dynamics. The project will generate several proof-of-concept studies that are interesting in their own right, and demonstrate the capabilities of the modeling framework. Community engagement is fostered by presenting clinics and demonstrations at professional venues aimed at hydrologists, sedimentologists, critical zone scientists, and the broader earth and environmental sciences community. The team also supports visits by individual scientists for on-site training beyond these clinics. Input/output tools allow compatibility with data from relevant NSF-supported research. The project supports four graduate students and three postdoctoral researchers, and also includes modeling workshops for fifth to seventh grade girls in a community that has a greater than 50% minority population."
"1550482","SI2-SSI: Collaborative Research: ENKI: Software Infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","PETROLOGY AND GEOCHEMISTRY, Software Institutes, EarthCube","09/01/2016","08/19/2016","Mark Ghiorso","WA","OFM Research","Standard Grant","Micah Beck","08/31/2019","$734,907.00","","ghiorso@ofm-research.org","28430 NE 47th Place","Redmond","WA","980538841","4258804418","CSE","1573, 8004, 8074","7433, 8004","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1550346","SI2-SSI:Collaborative Research: ENKI: Software infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","Software Institutes, EarthCube","09/01/2016","08/19/2016","Dimitri Sverjensky","MD","Johns Hopkins University","Standard Grant","Micah Beck","08/31/2019","$217,827.00","","sver@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","8004, 8074","7433, 8004, 8009","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1550337","SI2-SSI: Collaborative Research: ENKI: Software infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","Software Institutes, EarthCube","09/01/2016","08/19/2016","Marc Spiegelman","NY","Columbia University","Standard Grant","Micah Beck","08/31/2019","$245,093.00","","mspieg@ldeo.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8004, 8074","7433, 8004, 8009","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1549970","SI2-SSI:Collaborative Research: ENKI: Software Infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","PETROLOGY AND GEOCHEMISTRY","09/01/2016","08/19/2016","George Bergantz","WA","University of Washington","Standard Grant","Micah Beck","08/31/2019","$141,957.00","","bergantz@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1573","7433, 8004, 8009","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1550229","SI2-SSI: Collaborative Research: ENKI: Software infrastructure that ENables Knowledge Integration for Modeling Coupled Geochemical and Geodynamical Processes","OAC","Software Institutes, EarthCube","09/01/2016","08/19/2016","Everett Shock","AZ","Arizona State University","Standard Grant","Micah Beck","08/31/2019","$213,345.00","","eshock@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","8004, 8074","7433, 8004, 8009","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1835526","Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Thomas Crawford","IL","University of Chicago","Standard Grant","Bogdan Mihaila","08/31/2021","$39,994.00","","tcrawfor@kicp.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.<br/><br/>This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1558219","Collaborative Research: S2I2: Cncp: Conceptualization of an S2I2 Institute for High Energy Physics","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","08/22/2017","Michael Sokoloff","OH","University of Cincinnati Main Campus","Standard Grant","Vipin Chaudhary","06/30/2019","$409,991.00","","mike.sokoloff@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","7244, 8004","026Z, 7433, 7569, 8004, 8084, 8211","$0.00","Facilities such as the Large Hadron Collider at CERN represent a huge step forward in the quest to understand the fundamental building blocks of nature, and their interactions. The discovery of the Higgs boson and the observation of very rare B meson decays, as predicted by the Standard Model (SM) of particle physics, demonstrate the strong scientific reach of LHC experiments. Despite these achievements, fundamental questions remain, including: Why does nature express the symmetries embodied in the SM, and not other symmetries? Why are there exactly three generations of basic building blocks (quarks and leptons)? Why are the masses of these building blocks so different from each other, both within a generation and between generations? What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the three spatial dimensions we know? Planning for upgrades for a High Luminosity LHC (HL-LHC) in the 2020s is well underway. These upgrades will lead to a 100-fold increase in data volume. Exploiting the increased data volume and the large investments in upgraded detectors requires a commensurate investment in software to assure that the scientific goals are achieved.<br/><br/>The HL-LHC is expected to take data starting 10 years from now, and continue for many years. This conceptualization project will develop a Strategic Plan for an eventual S2I2 Institute which can address the software development and sustainability challenges of software for the HL-LHC era. This conceptualization project will support a series of workshops to prepare both the S2I2 Strategic Plan and, together with the international community, a Community White Paper providing an overall roadmap for both U.S. and international software efforts to realize the full science potential of the HL-LHC. This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1550172","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/06/2016","Gunther Roland","MA","Massachusetts Institute of Technology","Standard Grant","Bogdan Mihaila","06/30/2020","$189,500.00","","Gunther.Roland@cern.ch","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1835725","Frameworks: Software NSCI-Open OnDemand 2.0: Advancing Accessibility and Scalability for Computational Science through Leveraged Software Cyberinfrastructure","OAC","DATANET","11/01/2018","09/12/2018","David Hudak","OH","Ohio State University","Standard Grant","Amy Walton","10/31/2023","$3,345,802.00","Thomas Furlani, Brad Chalker, Robert Settlage","dhudak@osc.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7726","026Z, 062Z, 077Z, 7925","$0.00","Reducing barriers that limit the adoption of high performance computing (HPC) addresses an important problem that broadly affects the science, engineering, and humanities communities.  This effort builds on existing capabilities with large and varied user communities, and on national scale cyberinfrastructure and high-performance computing resources.  The approach has several benefits:<br/> - It increases the use of HPC resources among communities that are not well represented on HPC yet, but have growing needs for HPC.   <br/> - It is also beneficial to HPC providers, by supporting advanced features for monitoring and visualization of the states of systems.<br/> - The resulting framework will be used for training and workforce development, expanding the future ability to use advanced cyberinfrastructure for science.<br/>This project builds on the strengths of existing efforts, and has the potential to benefit a broad user community. <br/><br/>The project develops Open OnDemand 2.0, an open-source software that enables access to high-performance computing, cloud, and remote computing resources via the web, and lower the barriers to access HPC systems. The project combines two widely used HPC resources: <br/> - Open OnDemand 1.0  - an existing open-source, web-based project for accessing HPC services; and <br/> - Open XDMoD - an open-source tool that facilitates the management of high performance computing resources.<br/>Project activities include enhancing an existing web portal-to-HPC system (OnDemand), integrating XDMoD, extending the portal to provide other methods of access for other science domains, and improving the scaling of the system.  The software employs a unique per-user web server architecture. This gives a user full system-level access to an HPC cluster through a web browser.  Job performance visibility is provided by XDMoD, which enables users to make more efficient usage of HPC resources. Innovation and discovery will be integrated through a study which investigates ways to leverage the system-level access provided by Open OnDemand with science gateways.  The integrated platform will enhance resource utilization visibility, extend to more resource types and institutions, and support a smooth and easy utilization of HPC resources with intuitive web interfaces.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1550281","SI2-SSI: Collaborative Research: ENKI: Software infrastructure that ENables Knowledge Integration for Modelling Coupled Geochemical and Geodynamical Processes","OAC","Software Institutes, EarthCube","09/01/2016","08/19/2016","Peter Fox","NY","Rensselaer Polytechnic Institute","Standard Grant","Bogdan Mihaila","08/31/2019","$450,001.00","","foxp@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8004, 8074","7433, 8004","$0.00","Earth scientists seek to understand the mechanisms of planetary evolution from a process perspective in order to promote the progress of science.  They model the chemistry of melting of the interiors of planets as a result of heat flow within the body.  They calculate the flows of energy and mass from the interior to the surface. They model the interaction of fluids and rocks, which drives chemical weathering and the formation of ore deposits.  They seek to understand the synthesis and stabilities of organic compounds and their economic and biological roles.  They study the interactions of atmosphere, oceans, biosphere and land as a dynamically coupled evolving chemical system. To achieve this level of understanding of planetary evolution, Earth scientists use software tools that encode two fundamentally different types of models: (1) thermodynamic models of naturally occurring materials, and (2) models of transport that track physical flows of both fluids and solids.  Much of the fundamental science of planetary evolution lies in understanding coupled thermodynamic and transport models. This grant funds development of a software infrastructure that supports this coupled modeling of the chemical evolution of planetary bodies.   It is their aim to establish an essential and active community resource that will engage a large number of researchers, especially early career scientists, in the exercise of model building and customization.  <br/><br/>This is a project to create ENKI, a collaborative model configuration and testing portal that will transform research and education in the fields of geochemistry, petrology and geophysics.  ENKI will provide software tools in computational thermodynamics and fluid dynamics.  It will support development and access to thermochemical models of Earth materials, and establish a standard infrastructure of web services and libraries that permit these models to be integrated into fluid dynamical transport codes. This infrastructure will allow scientific questions to be answered by quantitative simulations that are presently difficult to impossible because of the lack of interoperable software frameworks.  ENKI, via the adoption of state-of-the-art model interfacing (OpenMI) and deployment environments (HubZero), will modernize how thermodynamic and fluid dynamic models are used by the Earth science community in five fundamental ways: (1) provenance tracking will enable automatic documentation of model development and execution workflows, (2) new tools will assist users in updating thermochemical models as new data become available, with the ability to merge these data and models into existing repositories and frameworks, (3) automated code generation will eliminate the need for users to manually code web services and library modules, (4) visualization tools and standard test suites will facilitate validation of model outcomes against observational data, (5) collaborative groups will be able to share and archive models and modeling workflows with associated provenance for publication. With these tools we seek to transform the large community of model users, who currently depend on a small group of dedicated and experienced researchers for model development and maintenance, into an empowered ensemble of model developers who take ownership of the process and bring their own expertise, intuition and perspective to shaping the software tools they use in daily research.  ENKI development will be community driven.  Participation of a dedicated and diverse group of early career professionals will guide us in user interface development - insuring portal capabilities are responsive to user needs, and in development of a rich set of documentation, tutorials and examples.  All software associated with this project will be released as open source."
"1550300","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","08/15/2018","Abhijit Majumder","MI","Wayne State University","Continuing grant","Bogdan Mihaila","06/30/2020","$2,241,626.00","Ron Soltz, Loren Schwiebert, Joern Putschke","abhijit.majumder@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","1253, 7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1835536","Collaborative Research: Elements: Software: NCSI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Colin Bischoff","OH","University of Cincinnati Main Campus","Standard Grant","Bogdan Mihaila","08/31/2021","$39,005.00","","colin.bischoff@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.<br/><br/>This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835865","Collaborative Research: Elements: Software: NSCI: HDR: Building An HPC/HTC Infrastructure For The Synthesis And Analysis Of Current And Future Cosmic Microwave Background Datasets","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Julian Borrill","CA","University of California-Berkeley","Standard Grant","Bogdan Mihaila","08/31/2021","$481,001.00","Akito Kusaka, Nathan Whitehorn","borrill@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The photons created in the Big Bang have experienced the entire history of the Universe, and every step in the evolution of the Universe has left its mark on their statistical properties. Observations of these photons have the potential to unlock the secrets of fundamental physics and cosmology, and to provide key insights into the formation and evolution of cosmic structures such as galaxies and galaxy clusters. Since the traces of these processes are so faint, one must gather enormous datasets to be able to detect them above the unavoidable instrumental and environmental noise. This in turn means that one must be able to use the most powerful computing resources available to be able to process the volume of data. These computing resources include both highly localized supercomputers and widely distributed grid and cloud systems. The PI and Co-Is will develop a common computing infrastructure able to take advantage of both types of resource, and demonstrate its suitability for ongoing and planned experiments by adapting the analysis pipelines of four leading Big Bang observatories to run within it. In addition to enabling the full scientific exploitation of these extraordinarily rich data sets, the investigators will mentor students engaged in this research and run summer schools in applied supercomputing.<br/><br/>This project seeks to enable the detection of the faintest signals in Cosmic Microwave Background radiation, and in particular the pattern of peaks and troughs in the angular power spectra of its polarization field. In order to obtain these spectra one must first reduce the raw observations to maps of the sky in a way the preserve the correlations in the signal and characterizes the correlation in the noise. While the algorithms to perform this reduction are well-understood, applying them to data sets with quadrillions to quintillions of observations is a very serious computational challenge. The computational resources available to the project to address this include both high performance and high throughput computing systems, and one will need to take advantage of both of them. This project will develop a joint high performance/high throughput computational framework, and deploy within it analysis pipelines currently being fielded by the ongoing Atacama Cosmology Telescope, BICEP/Keck Array, POLARBEAR, and South Pole Telescope experiments. By doing so one will also demonstrate the frameworks efficacy for the planned Simons Observatory and CMB-S4 experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1550221","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","08/04/2018","Rainer Fries","TX","Texas A&M University Main Campus","Continuing grant","Bogdan Mihaila","06/30/2020","$244,282.00","","rjfries@comp.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1550223","SI2-SSI: Collaborative Research: Jet Energy-Loss Tomography with a Statistically and Computationally Advanced Program Envelope (JETSCAPE)","OAC","COMPUTATIONAL PHYSICS, Software Institutes","07/01/2016","07/06/2016","Ulrich Heinz","OH","Ohio State University","Standard Grant","Bogdan Mihaila","06/30/2020","$263,673.00","","heinz@mps.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7244, 8004","026Z, 7433, 7569, 8009, 8084","$0.00","Microseconds after the Big Bang, the universe was filled with an extremely hot fluid called the Quark-Gluon Plasma. As the universe expanded, this plasma cooled and condensed into the building blocks of ordinary matter around us: protons, neutrons, and atomic nuclei. Droplets of this fluid, which exists only at temperatures above 2 trillion Kelvin, are generated and studied in the laboratory today using collisions of high-energy heavy ions at Brookhaven National Laboratory and CERN. A key method to study the Quark-Gluon Plasma is the generation of high-energy quarks and gluons in the collision, which interact with the hot plasma and emerge as ""jets"" of particles that are measured by experiments. These jets provide powerful tools to study the internal structure of the plasma, analogous to tomography in medical imaging. However, interpretation of jet measurements requires sophisticated numerical modeling and simulation, and comparison of theory calculations with experimental data demands advanced statistical tools. The JETSCAPE Collaboration, an interdisciplinary team of physicists, computer scientists, and statisticians, will develop a comprehensive software framework that will provide a systematic, rigorous approach to meet this challenge. Training programs, workshops, summer schools and MOOCs, will disseminate the expertise needed to modify and maintain this framework.<br/><br/>The JETSCAPE Collaboration will develop a scalable and portable open source software package to replace a variety of existing codes. The modular integrated software framework will consist of interacting generators to simulate (i) wave functions of the incoming nuclei, (ii) viscous fluid dynamical evolution of the hot plasma, and (iii) transport and modification of jets in the plasma. Integrated advanced statistical analysis tools will provide non-expert users with quantitative methods to validate novel theoretical descriptions of jet modification, by comparison with the complete set of current experimental data. To improve the efficiency of this computationally intensive task, the collaboration will develop trainable emulators that can accurately predict experimental observables by interpolation between full model runs, and employ accelerators such as Graphics Processing Units (GPUs) for both the fluid dynamical simulations and the modification of jets. The collaboration will create this framework with a user-friendly envelope that allows for continuous modifications, updates and improvements of each of its components. The effort will serve as a template for other fields that involve complex dynamical modeling and comparison with large data sets. It will open a new era for high-precision extraction of the internal structure of the Quark-Gluon Plasma with quantifiable uncertainties. <br/><br/>This project advances the objectives of the National Strategic Computing Initiative (NSCI), an effort aimed at sustaining and enhancing the U.S. scientific, technological, and economic leadership position in High-Performance Computing (HPC) research, development, and deployment. <br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division and the Division of Mathematical Sciences in the Directorate of Mathematical and Physical Sciences."
"1638990","IRNC: Backbone: Americas Africa Research and eduCation Lightpaths (AARCLight)","OAC","INTERNATIONAL RES NET CONNECT","01/01/2017","02/11/2019","Julio Ibarra","FL","Florida International University","Continuing grant","Kevin Thompson","12/31/2019","$400,000.00","Heidi L. Morgan, Donald Cox, Luis Lopez","julio@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7369","","$0.00","Florida International University and AmLight consortium partners are planning, designing, and defining a strategy for high capacity connectivity research and education network connectivity between the US and West Africa, called AARCLight: Americas Africa Research and eduCation Lightpaths. Science is being conducted in an era of information abundance.  Sharing science resources, such as data, instrumentation, technology, and best practices, across national borders, can promote expanded scientific inquiry, and has the potential to advance discovery. Linking the U.S. and the nations of Africa's researcher and education communities is an increasingly strategic priority. Africa offers research and education communities with unique biological, environmental, geological, anthropological, and cultural resources. Research challenges in atmospheric and geosciences, materials sciences, tropical diseases, biology, astronomy, and other disciplines will benefit by enhancing the technological and social connections between the research and education communities of the US and Africa.<br/><br/>The planning project is largely based on the planned availability of submarine cable spectrum for use by research and education communities.  It creates an unprecedented opportunity for the stakeholders in the U.S., Africa, and Brazil to coordinate planning efforts to strategically make use of the offered spectrum towards serving the broadest communities of interest in research and education. The AmLight consortium partners are the (Brazilian Education and Research Network  (RNP), Academic Network of So Paulo (ANSP), Latin American Cooperation of Advanced Networks (CLARA), Chile's Red Universitaria Nacional (REUNA), Florida LambdaRail (FLR), Association of Universities for Research in Astronomy (AURA), and Latin American Nautilus)"
"1743188","SI2-S2I2 Conceptualization: Conceptualizing a US Research Software Sustainability Institute (URSSI)","OAC","Software Institutes","12/15/2017","12/21/2017","Karthik Ram","CA","University of California-Berkeley","Standard Grant","Vipin Chaudhary","06/30/2019","$499,999.00","Daniel Katz, Jeffrey Carver, Sandra Gesing, Nicholas Weber","karthik.ram@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8004","026Z, 8004, 9102","$0.00","Many science advances have been possible thanks to use of software. This software, also known as ""research software"", has become essential to progress in science and engineering. The scientists who develop the software are experts in their discipline, but do not have sufficient understanding of the practices that make software development easier, and the software more robust, reliable, maintainable and sustainable.  This project will work with these scientists to understand how the research community can best work together to design and maintain better software with lower effort, so that they and others can continue to use it over long periods of time.  This project will conduct several workshops and a survey in order to gather and understand the community's needs and the software expertise of its members. These needs will be widely disseminated via newsletters and via a website. The primary deliverable of this project will be a design and strategic plan for a US Research Software Sustainability Institute (URSSI) which will serve as a community hub and provide services to scientists that will help them create improved, more sustainable software. This software in turn will accelerate the progress of science, thus serving NSF's mission.<br/><br/>Modern research is inescapably digital, with data and publications most often created, analyzed, and stored electronically, using tools and methods expressed in software. This ""research software"" is essential to progress in science, engineering, and all other fields, but it is not developed in an efficient or sustainable way. The researchers who develop this software, while well-versed in their discipline, generally do not have sufficient training and understanding of best practices that ease development and maintainability and that encourage sustainability and reproducibility. In response, this project is conceptualizing a US Research Software Sustainability Institute that will validate and address at least three classes of concerns (functioning of the individual and team, the research software, and the research field itself), impacting all software development and maintenance projects across all of NSF. URSSI conceptualization includes workshops and a widely-distributed survey that engages important stakeholder communities to learn about the software they produce and use, and the ways they contemplate sustaining it, following the paths blazed by other successful software institutes. Communication is a key component of this project, with newsletters, a web site, survey outputs, and social media used to provide broad dissemination and engagement. The workshops, survey, and community management approach allow the conceptualization project to iteratively build on existing, extensive understanding of the challenges for sustainable software and its developers.  The project also addresses how URSSI could formalize, diversify, and improve the pipeline under which students enter universities, learn about and contribute to software, then graduate to full-time positions where they make use of their software skills, to increase the diversity of those entering research software development and to retain diversity over their university careers. The conceptualization team has accumulated hundreds of person-years of combined experience by thinking, researching, and living scientific software; this will be combined with feedback from the broader community. It leverages existing collaborations to expand both the community and the project's knowledge of its needs, to plan the best possible URSSI. The results will create an eager supportive community, a concrete institute plan configured to offer valued services, and a published survey and data that demonstrates community need."
"1550476","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","09/13/2016","Mitchell Wayne","IN","University of Notre Dame","Standard Grant","Micah Beck","09/30/2019","$81,000.00","","wayne.1@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1550475","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","09/13/2016","Martin McCullagh","CO","Colorado State University","Standard Grant","Micah Beck","09/30/2019","$50,000.00","","Martin.McCullagh@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1550562","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","09/13/2016","Gerrick Lindberg","AZ","Northern Arizona University","Standard Grant","Micah Beck","09/30/2019","$50,000.00","","gerrick.lindberg@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1550528","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","09/13/2016","Joseph Baker","NJ","The College of New Jersey","Standard Grant","Micah Beck","09/30/2019","$68,009.00","","bakerj@tcnj.edu","P.O. Box 7718","Ewing","NJ","086280718","6097713255","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1841768","EAGER: Advanced Cyberinfrastructure Training for Modeling Physical Systems Research","OAC","CyberTraining - Training-based, COMPUTATIONAL PHYSICS","09/01/2018","07/20/2018","Joel Giedt","NY","Rensselaer Polytechnic Institute","Standard Grant","Sushil Prasad","08/31/2020","$300,000.00","Christopher Carothers, Vincent Meunier","giedtj@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","044Y, 7244","026Z, 7361, 7569, 7916, 9179","$0.00","There is a critical national need for the development of scientists with expertise in modeling and simulating physical systems on the most powerful computers available.  Many pressing problems require this scale of extreme computing in order to make significant advances that contribute to the national welfare and security, such as stewardship of the nuclear stockpile, developing advanced energy technologies, and discovering new physical interactions and materials that may revolutionize technological sectors of the economy.  In order to address this need, a series of two-week summer schools, each preceded by a three-day computational boot camp, is being hosted at Rensselaer Polytechnic Institute.  This series is aimed at graduate students and advanced undergraduate students.  The project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.  The boot camp focuses on basic computer skills that are needed for the more advanced material and methods that are to be covered in the summer school.  It will be offered to a subset of the summer school students who need this basic training, since many students from across the country come from colleges and physics programs where adoption of cyberinfrastructure, computational methods and data-intensive computing is far below the level desired by practitioners in advanced scientific computing.  The project is specifically addressing the compartmentalization between computer science versus physics curriculum by bridging that gap through an intensive hands-on learning experience from experts who have developed this type of interdisciplinary agility.  This project is performing significant outreach to encourage women and underrepresented minorities to participate, coupled with specific recruiting targets for this cyber-training series.<br/><br/>The boot camp and summer school build on existing activities at Rensselaer, which have successfully integrated computation into the physics curriculum, including three computational courses and a computational physics concentration that have been developed for physics majors.  This NSF project is scaling out these types of activities to a national stage, where students and lecturers are brought from across the country to interact and learn around lectures and activities that illustrate cutting edge physical modeling with advanced algorithms and hardware.  Participants in the summer school have access to the significant supercomputing facility at Rensselaer, the Computational Center for Innovations (CCI), which includes an IBM BlueGene/Q platform.  This facility is being used for hands-on training activities for the students, thereby exposing them to computing at the highest levels, in preparation for a career at the forefront of modeling and simulating physical systems.  The boot camp and summer school are resulting in world-class instruction in basic computer skills and the use and design of highly optimized and efficient codes and algorithms for solving important problems in the physical sciences.  Each summer school is culminating with a competition in which students work in teams to solve significant computational problems related to the physical sciences, with a poster session and prizes awarded. Domain specific topics that are included in the summer school are density functional theory, lattice gauge theory, numerical relativity, big data applications in physics and astronomy, quantum finite elements and quantum many-body simulations.  Since the goal for attendance is 36 summer school students per year (18 to the boot camp), the project activities are having a direct transformative impact on 72 scientists over the two-year period of this project.  In order to create an even greater impact, on-demand tutorials are being posted on an RPI website, based on the video lectures, exercises and activities that are developed through the course of the boot camp and summer school.  The course materials and videos of the lectures are also being made available on the website, along with e-mail announcements to physics departments around the country, in order to encourage integration and adoption of the curriculum.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827186","CC* Networking Infrastructure: Building a high-performance, flexible DMZ backbone at the University of Nevada, Reno","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/25/2018","Graham Kent","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Kevin L. Thompson","06/30/2020","$495,255.00","Scotty Strachan, Jeff Springer, Jeffrey LaCombe","gkent@seismo.unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","8080","9150","$0.00","Advances in science and engineering and the use of science-driven technology are transforming the scale and complexity of research at the University of Nevada, Reno (UNR). Fundamental to supporting these shifts in data volume and research workflows are core networking and computing infrastructures maintained by the central campus Office of Information Technology. As part of a strategic plan to increase technology support of research activity at the university, a collaboration of faculty and IT professionals are building a dedicated, high-speed research network backbone across campus with connections to off-campus high-performance-compute facilities, remote sensor networks, and national peering points. The primary tasks of this project are to: 1) increase external connectivity to Internet2 and Pacific Wave from 10G to 100G; 2) extend a set of 40G dedicated research network paths with data transfer management and monitoring to geographically distributed locations on campus to serve critical research efforts; 3) implement a ""scienceDMZ"" network architecture for scientific and research workflows across the UNR network and as a model for Nevada's higher education community; and 4) augment key wide-area network end-to-end research workflows impacting rural areas all the way to UNR's high-performance research computing colocation partner Switch, a new state-of-the-art world-class datacenter.<br/><br/>This project solves current and near-future connectivity problems for science-focused researchers at UNR by enabling larger-scale science, saving time, and increasing the rate of discovery, and creating the capability for the institution to participate in the emerging Pacific Wave and National Research Platforms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829771","CyberTraining:CIC: DeapSECURE: A Data-Enabled Advanced Training Program for Cyber Security Research and Education","OAC","CyberTraining - Training-based","09/01/2018","07/02/2018","Hongyi Wu","VA","Old Dominion University Research Foundation","Standard Grant","Sushil K Prasad","08/31/2021","$500,000.00","Masha Sosonkina, Wirawan Purwanto","h1wu@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","As the volume and sophistication of cyber-attacks grow, cybersecurity researchers, engineers and practitioners heavily rely on advanced cyberinfrastructure (CI) techniques such as big data, machine learning, and parallel programming, as well as advanced CI platforms, e.g., cloud and high-performance computing to assess cyber risks, identify and mitigate threats, and achieve defense in depth. However, advanced CI techniques have not been widely introduced in undergraduate and graduate cybersecurity curricula. This lack creates a hurdle for many senior undergraduates and early-stage graduate cybersecurity students who are keen to conduct cutting-edge cybersecurity research and/or participate in advanced industrial cybersecurity projects. This project introduces a unique Data-Enabled Advanced Training Program for Cyber Security Research and Education (DeapSECURE), aimed to prepare undergraduate and graduate students with advanced CI techniques and teach them to use CI resources, tools, and services to succeed in cutting-edge cybersecurity research and industrial cybersecurity projects. The project responds to the urgent need for well-prepared cybersecurity workforce in the Hampton Roads metropolitan region, the Commonwealth of Virginia, and the Nation. It, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.<br/><br/>This project develops six new CI training modules which emphasize the practical use of the advanced CI techniques, especially the tools that implement them, in the context of cybersecurity research. Each training module includes three sections: (1) an overview presented by an invited cybersecurity faculty about his/her research, concluding with a research problem that heavily depends on CI techniques; (2) an introduction of corresponding CI skills, tools and platforms; (3) a hands-on lab session where students will apply the CI techniques to solve the research problem formerly introduced by the cybersecurity faculty. The modules will be delivered via two distinct means: monthly workshops and summer institutes. Six monthly workshops are conducted during academic year, primarily targeting students enrolled at Old Dominion University (ODU). The summer institutes present these six modules to students from local community colleges, Research Experiences for Undergraduates program at ODU, and other Virginia universities; they also include special activities such as field trips, open house for K-12 students, Cyber Night events, cybersecurity career panels, and student competitions. Complementing the workshops and summer institutes, an online continuous learning community is created, which includes a virtual computer lab and a student forum, as a place for students to continue their learning engagement after the face-to-face sessions. Archived workshop materials, as well as additional learning materials are also posted on this online platform as open educational resources, to be made available to the cybersecurity research and education communities. The open-source style development of the learning modules facilitates a wide-range of adoption, adaptations, and contributions in an efficient manner. The project leverages existing and new partnerships to ensure broad participation, and accordingly broaden the adoption of advanced CI techniques in the cybersecurity community. The project employs a rigorous assessment and evaluation plan rooted in diverse metrics of success to improve the curricula and demonstrate its effectiveness. The metrics, which are based on the students' outcomes and exit surveys, are assessed by an independent evaluator. The adoption of the learning modules outside of the training program is also considered as a metric of success.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1541108","CRISP Type 2: Collaborative Research: Towards Resilient Smart Cities","OAC","CIVIL INFRASTRUCTURE SYSTEMS, INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE, Computer Systems Research (CSR, Software Institutes","01/01/2016","12/14/2017","Arif Sarwat","FL","Florida International University","Standard Grant","William Miller","12/31/2019","$511,999.00","Ismail Guvenc","asarwat@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","1631, 1640, 7231, 7354, 8004","008Z, 029E, 036E, 039E, 9251","$0.00","Realizing the vision of truly smart cities is one of the most pressing technical challenges of the coming decade. The success of this vision requires synergistic integration of cyber-physical critical infrastructures (CIs) such as smart transportation, wireless systems, water networks, and power grids into a unified smart city. Such smart city CIs have significant resource dependence as they share energy, computation, wireless spectrum, users and personnel, and economic investments, and as such are prone to correlated failures due to day-to-day operations, natural disasters, or malicious attacks. Protecting tomorrow's smart cities from such failures requires instilling resiliency into the processes that manage the city's common CI resources. Such processes must be able to adaptively and optimally reallocate smart city resources to recover from failure. The goal of this Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP) collaborative research project is to address this fundamental challenge via a coordinated and interdisciplinary approach that relies on machine learning, operations research, behavioral economics, and cognitive psychology to lay the mathematical foundations of resilient smart cities. The anticipated results will break new ground in the understanding of synergies between multiple cyber-physical infrastructure and resilient resource management thus catalyzing the global deployment of smart cities. This research will yield advances to the areas of resilient systems, cyber-physical systems, security and privacy engineering, game theory, computer and network science, behavioral economics, data analytics, and psychology. The project will involve students from diverse backgrounds across engineering, computer science, economics, and psychology that will be trained on pertinent research issues related to smart cities and resiliency. The project will also contribute to fostering trust between residents and the various technological processes that are fundamental to the operation of a smart city.<br/><br/>This research will introduce a foundational, transformational, analytical framework for leveraging synergies between a city's CIs to yield resilient resource management schemes cognizant of both technological and human factors. By bringing together researchers from interdisciplinary fields, this framework yields several advances: 1) Rigorous mathematical tools for delineating the inter-dependencies between CIs via a complementary mix of novel tools from graph theory, power indices, machine learning, and random spatial models; 2) Resilient resource management mechanisms that advance notions from frameworks such as behavioral game theory to enable optimized management of shared CI resources in face of failures stemming from agents of varying intelligence levels; 3) Behavioral models for characterizing the trust relationships between the residents of a smart city and the CIs; 4) Behavioral studies that provides guidelines on how to influence the users of the CIs in such a way so as to improve the resiliency of the CIs; and 5) Large-scale smart city simulators coupled with realistic experiments that will bridge the gap between theory and practice. The insights from this project will apply to the future scientific cyber-infrastructures that are likely to be interconnected as well as interdependent. The simulator will be a software artifact that would be a useful component of a scientific cyberinfrastructure aimed at understanding (for example) smart cities."
"1659845","REU Site: Parallel and Distributed Computing","OAC","RSCH EXPER FOR UNDERGRAD SITES, Integrative Activities in Phys, EPSCoR Co-Funding","05/15/2017","03/27/2017","Sanjeev Baskiyar","AL","Auburn University","Standard Grant","Sushil K Prasad","04/30/2020","$291,590.00","Alvin Lim","baskisa@auburn.edu","310 Samford Hall","Auburn University","AL","368490001","3348444438","CSE","1139, 9134, 9150","026Z, 9150, 9250, SMET","$0.00","This project establishes a Research Experiences for Undergraduates (REU) Site to promote early engagement of undergraduate students in research.  A multi-disciplinary team of faculty from the departments of Computer Science and Software Engineering, Electrical and Computer Engineering, and Physics at Auburn University collaborate to provide participating students with research experiences in computational aspects of multiple disciplines.  It will expose students to high-performance computing and other cyberinfrastructure resources, provide hands-on experience in a collaborative research environment, and inspire them towards advanced STEM education and research careers.  Thus, the project will promote participation of students in graduate studies in computer science, electrical engineering and physics and will help maintain US leadership in computing education and research.  The society will benefit from the trained workforce in critical areas of national need of cyberinfrastructure, parallel and distributed computing and neuroimaging informatics.  Students from underrepresented groups will be encouraged to participate in the REU site.  Faculty will mentor students in carefully planned research projects which pose a range of scientific and technological challenges. The program fosters long-term mentoring relationships between students and faculty through collaboration.  The research outcomes can lower energy costs and carbon footprint in operating data centers and secure smart utility networks in US power grids.  The research aims for better understanding of enhancing urban traffic control, homeland security and location information to vehicles in a GPS degraded environment, plasma physics and better diagnosis of mental health diseases.<br/><br/>The objective of this project is to offer research opportunities to undergraduate students around a coherent theme of parallel and distributed computing.  The students will use cyberinfrastructure to solve problems in computer science, electrical and computer engineering and physics.  As energy consumption in computing is of current importance, students will solve problems with a common focus on energy reduction, reinforcing the cohort experience.  The multidisciplinary interaction will provide experience in crosscutting research.  The students will participate in training activities at the beginning of summer.  Next, they will conduct research under the supervision of mentors and write reports and deliver oral presentations.  The research project aims to contribute to the design of new thermal conscious computer systems to improve energy-efficiency and thereby longer component lifespan.  It will investigate novel data dissemination algorithms in ad-hoc cyberinfrastructure to correct GPS information using minimal bandwidth and time.   It explores innovative real-time distributed analytics on mobile cyberinfrastructure to support multi-user coordinated actions, which balance risk and reward.   It can contribute to the understanding of ion velocity ring instabilities in plasmas, novel deep learning algorithm using channel state finger-printing to reduce power for indoor location detection and machine learning algorithms in neuro-informatics to advance brain science."
"1541105","CRISP Type 2: Collaborative Research: Towards Resilient Smart Cities","OAC","CIVIL INFRASTRUCTURE SYSTEMS, INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE","01/01/2016","08/24/2015","Walid Saad","VA","Virginia Polytechnic Institute and State University","Standard Grant","William Miller","12/31/2019","$1,100,000.00","Sheryl Ball, Danfeng Yao, Myra Blanco","walids@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1631, 1640, 7231","008Z, 029E, 036E, 039E","$0.00","Realizing the vision of truly smart cities is one of the most pressing technical challenges of the coming decade. The success of this vision requires synergistic integration of cyber-physical critical infrastructures (CIs) such as smart transportation, wireless systems, water networks, and power grids into a unified smart city. Such smart city CIs have significant resource dependence as they share energy, computation, wireless spectrum, users and personnel, and economic investments, and as such are prone to correlated failures due to day-to-day operations, natural disasters, or malicious attacks. Protecting tomorrow's smart cities from such failures requires instilling resiliency into the processes that manage the city's common CI resources. Such processes must be able to adaptively and optimally reallocate smart city resources to recover from failure. The goal of this Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP) collaborative research project is to address this fundamental challenge via a coordinated and interdisciplinary approach that relies on machine learning, operations research, behavioral economics, and cognitive psychology to lay the mathematical foundations of resilient smart cities. The anticipated results will break new ground in the understanding of synergies between multiple cyber-physical infrastructure and resilient resource management thus catalyzing the global deployment of smart cities. This research will yield advances to the areas of resilient systems, cyber-physical systems, security and privacy engineering, game theory, computer and network science, behavioral economics, data analytics, and psychology. The project will involve students from diverse backgrounds across engineering, computer science, economics, and psychology that will be trained on pertinent research issues related to smart cities and resiliency. The project will also contribute to fostering trust between residents and the various technological processes that are fundamental to the operation of a smart city.<br/><br/>This research will introduce a foundational, transformational, analytical framework for leveraging synergies between a city's CIs to yield resilient resource management schemes cognizant of both technological and human factors. By bringing together researchers from interdisciplinary fields, this framework yields several advances: 1) Rigorous mathematical tools for delineating the inter-dependencies between CIs via a complementary mix of novel tools from graph theory, power indices, machine learning, and random spatial models; 2) Resilient resource management mechanisms that advance notions from frameworks such as behavioral game theory to enable optimized management of shared CI resources in face of failures stemming from agents of varying intelligence levels; 3) Behavioral models for characterizing the trust relationships between the residents of a smart city and the CIs; 4) Behavioral studies that provides guidelines on how to influence the users of the CIs in such a way so as to improve the resiliency of the CIs; and 5) Large-scale smart city simulators coupled with realistic experiments that will bridge the gap between theory and practice. The insights from this project will apply to the future scientific cyber-infrastructures that are likely to be interconnected as well as interdependent. The simulator will be a software artifact that would be a useful component of a scientific cyberinfrastructure aimed at understanding (for example) smart cities."
"1541069","CRISP Type 2: Collaborative Research: Towards Resilient Smart Cities","OAC","CIVIL INFRASTRUCTURE SYSTEMS, INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE, Computer Systems Research (CSR","01/01/2016","06/27/2018","Narayan Mandayam","NJ","Rutgers University New Brunswick","Standard Grant","William Miller","12/31/2019","$926,000.00","Arnold Glass, Janne Lindqvist","narayan@winlab.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1631, 1640, 7231, 7354","008Z, 029E, 036E, 039E, 7218, 7433, 8004, 9251","$0.00","Realizing the vision of truly smart cities is one of the most pressing technical challenges of the coming decade. The success of this vision requires synergistic integration of cyber-physical critical infrastructures (CIs) such as smart transportation, wireless systems, water networks, and power grids into a unified smart city. Such smart city CIs have significant resource dependence as they share energy, computation, wireless spectrum, users and personnel, and economic investments, and as such are prone to correlated failures due to day-to-day operations, natural disasters, or malicious attacks. Protecting tomorrow's smart cities from such failures requires instilling resiliency into the processes that manage the city's common CI resources. Such processes must be able to adaptively and optimally reallocate smart city resources to recover from failure. The goal of this Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP) collaborative research project is to address this fundamental challenge via a coordinated and interdisciplinary approach that relies on machine learning, operations research, behavioral economics, and cognitive psychology to lay the mathematical foundations of resilient smart cities. The anticipated results will break new ground in the understanding of synergies between multiple cyber-physical infrastructure and resilient resource management thus catalyzing the global deployment of smart cities. This research will yield advances to the areas of resilient systems, cyber-physical systems, security and privacy engineering, game theory, computer and network science, behavioral economics, data analytics, and psychology. The project will involve students from diverse backgrounds across engineering, computer science, economics, and psychology that will be trained on pertinent research issues related to smart cities and resiliency. The project will also contribute to fostering trust between residents and the various technological processes that are fundamental to the operation of a smart city.<br/><br/>This research will introduce a foundational, transformational, analytical framework for leveraging synergies between a city's CIs to yield resilient resource management schemes cognizant of both technological and human factors. By bringing together researchers from interdisciplinary fields, this framework yields several advances: 1) Rigorous mathematical tools for delineating the inter-dependencies between CIs via a complementary mix of novel tools from graph theory, power indices, machine learning, and random spatial models; 2) Resilient resource management mechanisms that advance notions from frameworks such as behavioral game theory to enable optimized management of shared CI resources in face of failures stemming from agents of varying intelligence levels; 3) Behavioral models for characterizing the trust relationships between the residents of a smart city and the CIs; 4) Behavioral studies that provides guidelines on how to influence the users of the CIs in such a way so as to improve the resiliency of the CIs; and 5) Large-scale smart city simulators coupled with realistic experiments that will bridge the gap between theory and practice. The insights from this project will apply to the future scientific cyber-infrastructures that are likely to be interconnected as well as interdependent. The simulator will be a software artifact that would be a useful component of a scientific cyberinfrastructure aimed at understanding (for example) smart cities."
"1740263","SI2-SSE: Expanding the Scope of Materials Modeling with Electron Phonon Wannier (EPW) Software","OAC","DMR SHORT TERM SUPPORT, Software Institutes, DMREF","09/01/2017","08/24/2017","Elena Roxana Margine","NY","SUNY at Binghamton","Standard Grant","Vipin Chaudhary","08/31/2020","$500,000.00","Madhusudhan Govindaraju","rmargine@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","1712, 8004, 8292","026Z, 054Z, 7433, 8004, 8005, 8400, 9102, 9216","$0.00","Introduction of efficient non-empirical computational methods for modeling and predicting advanced materials properties is at the heart of the on-going effort to accelerate theory-guided materials discovery. The open-source software Electron-Phonon-Wannier (EPW) code offers unique capabilities for high-accuracy calculations of properties at the quantum mechanical level. In particular, EPW provides insight into the microscopic mechanisms that govern the interaction between electrons and atomic vibrations. Within this project, the existing capabilities will be extended to model a wider range of materials with complex electronic and magnetic properties. The knowledge can be used to design new-generation materials for harvesting of solar and thermal energy, making transition from electronics to spintronics, or realizing exotic states of matter. EPW will serve the broad electronic structure community of physicists, materials scientists, chemists, and engineers who work on modeling and designing next-generation materials for thermoelectric, photovoltaic, superconducting, spintronic, and other applications. The development of high-performance materials is crucial for addressing emergent societal challenges related to energy and environment, transportation, and information and communication technologies. The developed computational tools will be released under the GNU General Public License to ensure that the scientific community will directly and timely benefit from this technology. A broad spectrum of educational and outreach activities proposed within the project will promote and popularize scientific research in diverse communities. Planned hands-on workshops on EPW in the US and Europe will help create a strong EPW community for further development of the code and foster new research collaborations among participants from different countries. Interactive events for elementary school students in the upstate New York area will help attract a new generation of scientists from underrepresented groups into the STEM disciplines.<br/><br/><br/>The current focus of the electronic structure community is to introduce new capabilities enabling the design of emerging high-performance materials for thermoelectric, photovoltaic, superconducting, spintronic, and other applications. Function-defining properties in these applications are notoriously difficult to evaluate with desired accuracy using present density functional theory-based methods. The aim of this project is to expand the functionalities and broaden the impact of the open-source software Electron-Phonon-Wannier (EPW) in the area of materials research. EPW, now distributed as part of the Quantum ESPRESSO suite, has emerged as a unique computational tool that offers functionalities not available in standard electronic structure packages. By combining density-functional perturbation theory and maximally-localized Wannier functions methods, EPW makes it computationally feasible to calculate millions of electron-phonon matrix elements. The proposed work will expand the current capabilities of the EPW code to modeling an important class of spin-dependent materials properties. The proposed methodological, and user-oriented objectives are chosen to align with the focal directions of the SSE program and the DMR pertaining to creation, expansion, and deployment of robust software targeting a large user base. In particular, predictive calculations of spin transport, spin relaxation, and spin dynamics can yield fundamental insights into processes at the atomic scale and provide the necessary foundation to rationally design new materials and guide experimental work. The project will also provide easy management and execution of day to day scientific experiments on large-scale computing infrastructures. The introduction of workflows for automating, storing, managing, and sharing simulations will facilitate data transparency and communication as well as advance data-driven materials design to new frontiers. Successful implementation of these objectives will substantially enhance the functionalities of the EPW code and ensure the continued growth of the EPW user community. The developed computational tools will be released under the GNU General Public License to ensure that the scientific community will directly and timely benefit from this technology. The research program will be tightly integrated with educational and outreach activities. It will enable interdisciplinary training of students in advanced electronic structure methods, computational material science, and high-performance computing. Other efforts will include science demonstrations to elementary school students, development of a special course in materials modeling to incorporate computer simulations in the Binghamton University's undergraduate and graduate curriculum, and organization of workshops to teach the underlying theory and optimal usage of the EPW code.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1550588","Collaborative Research: SI2-SSI: Swift/E: Integrating Parallel Scripted Workflow into the Scientific Software Ecosystem","OAC","Software Institutes","10/01/2016","01/09/2018","Kyle Chard","IL","University of Chicago","Standard Grant","Micah Beck","09/30/2019","$2,749,446.00","Michael Wilde, Daniel Katz, Dmitry Karpeev, Ravi Madduri, Justin Wozniak","chard@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","8004","7433, 8004, 8009, 9102","$0.00","Science and engineering research increasingly relies on repeated execution of a complex series of steps (i.e., workflows) to form hypotheses; conduct experiments; analyze results; and refine theory.   Computation is often essential throughout the workflow and in this case, software can improve productivity by managing the computational and data workflow.  Swift is one such open-source workflow system that has been developed and widely used in diverse areas ranging from materials simulations and climate modeling to neuroscience and genomics. This project extends the capabilities of Swift by integrating it with other software systems that enable collaboration, usability, maintainability, and productivity. The new ecosystem, Swift/E, will enable scientists and engineers to more productively create and run computational workflow campaigns of larger scale, and debug, execute, adapt, and disseminate them faster and easier than has been possible to date. These workflows embody and communicate the computational methods specific to each domain of scientific inquiry. Swift/E achieves community engagement and extensive productivity benefits for a large user community through an integrated program of research, education, and software dissemination. The project engages and serves science and engineering communities by creating patterns of practice for building and sharing reusable workflow libraries, and by training students, educators, and researchers in their use.  To advance the education of the next generation of computationally trained scientists, Swift/E powers a network of NSF-supported ""e-Labs"" that teach the concepts of collaborative parallel computational science at high school and undergraduate levels, reaching over a thousand students annually.<br/><br/>The open-source Swift/E ""ecosystem"" integrates Swift with several scientific software elements that play a major role in the national and global cyberinfrastructure of today. These elements are: Swift for the parallel scripting of scientific workflow; Globus for data cataloging, management, and high-speed wide-area transport; the Web-based Galaxy workflow portal for workflow composition, execution, and collaborative sharing; Jupyter for the interactive development, testing, debugging, and assembly of high level programming and workflow languages; Python and R for productively expressing high-level computational logic; and ""git"" and related tools and Web portals for revision control, code dissemination and sharing, and for the collaborative engagement of developers.  Swift's implicitly parallel programming language is minimal and compact.  Swift provides a facility for embedding other scripting languages (currently Python, R, Julia and Tcl) into its runtime environment.  This project merges newer extreme-scale ""Swift/T"" capabilities with the flexible and portable original ""Swift/K"" version to make the core Swift/E software element more powerful and flexible while lowering it?s ongoing support cost. Swift/E enhances usability by extending Swift's troubleshooting and inter-language integration facilities.  And with enhanced and innovative workflow sharing archives, new training materials, and a sustained program for user support and self-sustaining and expanding community engagement, the Swift/E project engages, supports, and sustains a large global science and engineering user base."
"1828163","MRI: Acquisition of Hardware for the Enhancement of the ELSA High Performance Computing Cluster to Enable Computational Research at The College of New Jersey","OAC","MAJOR RESEARCH INSTRUMENTATION","09/01/2018","08/23/2018","Joseph Baker","NJ","The College of New Jersey","Standard Grant","Stefan Robila","08/31/2021","$651,032.00","Michael Ochs, Wendy Clement, Paul Wiita, Michael Bloodgood","bakerj@tcnj.edu","P.O. Box 7718","Ewing","NJ","086280718","6097713255","CSE","1189","026Z, 1189","$0.00","The College of New Jersey (TCNJ) will acquire equipment to significantly upgrade and enhance the Electronic Laboratory for Science and Analysis (ELSA) High Performance Computing cluster. TCNJ is a primarily undergraduate institution promoting a deep engagement of undergraduate students in research. Many of TCNJ's School of Science faculty members are working at the cutting edge of computational research in their fields, which include a broad range of areas including biochemistry/biophysics, genetics, bioinformatics, astrophysics, machine learning, and mathematical biology. In order to maintain a diverse and state of the art resource that meets the current and future computational needs of TCNJ's faculty and undergraduate students the current ELSA cluster requires targeted hardware enhancements. The new instrument will (1) enhance the research capacity and resulting scientific discovery of TCNJ's School of Science faculty members and their undergraduate research teams; (2) expose a greater number of undergraduate students and researchers to this powerful computational infrastructure through a series of newly developed High Performance Computing and data visualization short courses and workshops; and (3) improve access to the ELSA cluster for students traditionally underrepresented in STEM, as well as to researchers beyond TCNJ through a new collaboration with Open Science Grid.<br/><br/>This project will expand the research programs of more than 13 faculty members (many of whom are early career faculty) spanning all five of TCNJ's School of Science departments. The computationally intensive work that will be supported through this project includes a diverse array of scientific efforts, including studies of pilus biomechanics, estimations of cell signaling processes, methods for investigating the strength of passwords and security of password systems, and improving our understanding of the most energetic objects in the universe. Currently, the research programs of TCNJ faculty members in these and other areas are restricted by inadequate graphic processing unit (GPU) resources and by the slow speed aging central processing unit (CPU) servers part of the current instrumentation. The new ELSA cluster will allow faculty and student researchers at TCNJ to run workflows ranging from embarrassingly parallel computations, to those that necessitate high levels of parallelization over hundreds of cores, intensive GPU computations, and remote visualization of simulation results. The instrument will thus ensure that these research programs are able to reach their full potential. This project will also benefit nearly 100 undergraduate student researchers each year who are part of these labs and work directly on the cluster. As a result, in addition to improving the capacity for scientific discovery, the proposed acquisition will help TCNJ meet the demands of developing an undergraduate workforce that is ready to leverage increasingly powerful High-Performance Computing resources, now and in their future careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1451050","IRNC: RXP - Pacific Wave Expansion Supporting SDX & Experimentation","OAC","INTERNATIONAL RES NET CONNECT","09/01/2015","09/19/2018","Louis Fox","CA","Corporation for Education Network Initiatives in California","Cooperative Agreement","Kevin Thompson","08/31/2020","$4,174,160.00","Ronald Johnson","lfox@cenic.org","16700 Valley View Ave, Suite 400","La Mirada","CA","906385830","7142203400","CSE","7369","5942, 7369","$0.00","Pacific Wave is a distributed open internet exchange that provides high performance connectivity among US research and education networks and their international counterparts through points of presence in Los Angeles, Sunnyvale, and Seattle. These facilities are essential infrastructure providing high-performance advanced networking that enables the large-scale data flows, access to high performance computing, and remote visualization necessary for collaborative research in all areas of science and engineering including astronomy, biology, bio-medical engineering, materials science, medicine, earth sciences, high-energy physics, and oceanography.<br/><br/>This award funds enhancement and expansion of the Pacific Wave production infrastructure to support multiple 100 gigabit-per-second connections among US-based research and education networks and international counterparts from countries of the Pacific Rim including Australia, Canada, Japan, Mexico, New Zealand, Singapore, South Korea, and Taiwan. Many research and education networks are developing novel network architectures based on Software Defined Networking to better support the demands of the science and engineering communities that they serve. Project activities also include the development of a parallel set of network connection facilities comprising a distributed Software Defined Exchange to interconnect these emerging Software Defined Networks.  <br/><br/>The Pacific Wave Expansion project supported by this award achieves these objectives through engagement with the 17 major international research and education networks currently connected to the Pacific Wave exchange and through information and technology exchange with other awardees of the International research network connections program and the US science and engineering research community."
"1659169","CC* Storage: Implementation of a Distributed, Shareable, and Parallel Storage Resource at San Diego State University to Facilitate High-Performance Computing for Climate Science","OAC","DATANET","06/01/2017","05/26/2017","Christopher Paolini","CA","San Diego State University Foundation","Standard Grant","Amy Walton","05/31/2019","$199,998.00","Aram Kalhori, Jose Castillo, Gustaaf Jacobs, John Abraham","paolini@engineering.sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","CSE","7726","7433","$0.00","This project develops a high-capacity, high-performing storage system closely matched to the identified needs of climate science, but also of interest to a wide range of research and engineering fields such as brain and behavior research, high-energy physics, renewable energy, genomics, bioinformatics, computational chemistry, and the study of dark matter.  The two-tiered design of the storage system is consistent with best practices and would maximize research capacity and utilization.<br/><br/>The effort would create a distributed, shareable and parallel cluster running the BeeGFS file system (a European development), delivering about 1.8 PB (usable) storage and capable of up to 5GB/sec of streaming performance.  The project would support twelve science, technology, engineering and mathematics (STEM) projects in a broad range of disciplines; co-investigators in mechanical, environmental, civil and aerospace engineering, computer science, and biology are participating.  The results would improve input-output operations associated with a number of research and engineering computation bottlenecks: parallel sequential writing during distributed-parallel geochemical, computational fluid dynamics, and structural mechanics simulations; new file creation during real-time data acquisition; and random read operations during post-processing and visualization."
"1750865","CAREER:  GPU-Accelerated Framework for Integrated Modeling and Biomechanics Simulations of Cardiac Systems","OAC","CAREER: FACULTY EARLY CAR DEV","03/01/2018","02/05/2018","Adarsh Krishnamurthy","IA","Iowa State University","Continuing grant","Sushil K Prasad","02/28/2023","$288,286.00","","adarsh@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","1045","026Z, 062Z, 1045","$0.00","Cardiovascular diseases, such as heart failure, are one of the leading cause of death in the U.S. and pose a severe burden to the healthcare system. Most current treatments for cardiovascular diseases are based on rough estimates of outcomes from the results of clinical trials, which might not apply to individual patients due to patient-specific variations. Computational models of the cardiovascular system, developed from patient-specific clinical data, can help refine the diagnosis and personalize the treatment, significantly improving patient care and reducing mortality. The current patient-specific methods for cardiovascular diseases have been demonstrated mainly in simple, isolated examples. For widespread adoption of personalized medicine, a flexible and easy-to-use framework for integrating patient data and simulating cardiac biomechanics needs to be developed. This project focuses on creating an integrative framework with simulation, analysis, and visualization tools that will significantly advance the state-of-the-art in personalized medicine, ultimately improving patient care and treatment outcomes. Results from this research will benefit the U.S. healthcare system, society, and economy, while supporting the NSF mission to promote the progress of science and advance the national health. The tools developed as a part of this research involves several disciplines including computer science, bioengineering, and mechanical engineering. The multidisciplinary components of the project is being integrated into a larger educational effort that offers the students a solid foundation in developing computational tools and algorithms, while also broadening the participation of underrepresented groups in research.<br/><br/>The primary objective of this research is the advancement of the state-of-the-art in translational medicine with the help of computational modeling and interactive analysis tools to improve the basic understanding of the cardiac muscle and personalize treatment of cardiovascular diseases in patients. The research focuses on creating a novel computational framework to automate biomechanics finite-element simulation and analysis of patient-specific cardiac systems. Further, it aims to advance the knowledge of disease and therapeutic mechanisms by developing advanced multiscale methods to model muscle contraction and growth. Some of the key computational tools and methods proposed as part of this framework include: (1) a geometric mesh generation tool for systematic generation of patient-specific finite element meshes from clinical data; (2) an algorithm for accelerating high-order finite-element simulations using the graphics processing unit (GPU) for fast tuning of model parameters to match the patients' baseline cardiac function; (3) new methods for multiphysics simulations of cardiac systems to model multi-scale muscle mechanics and tissue growth; and (4) new visualization and virtual reality tools to enable animated volume rendering and visual analytics of the results of the cardiac simulations. Successful development of these open-source tools will enable faster adoption of patient-specific computational models by the research community to understand therapeutic mechanisms. This framework can significantly advance the state-of-the-art in personalized medicine, ultimately improving patient care and treatment outcomes. The multidisciplinary components of the project is being integrated into a larger educational effort to offer students a solid foundation in combining biomedical engineering with scientific computing. The education and outreach plans of this research can inform the community about the crucial role of computational models in improving patient-care.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842042","Pilot Study for a Cyberinfrastructure Center of Excellence","OAC","CYBERINFRASTRUCTURE","10/01/2018","09/09/2018","Ewa Deelman","CA","University of Southern California","Continuing grant","William Miller","09/30/2020","$1,500,000.00","Jaroslaw Nabrzyski, Anirban Mandal, Robert Ricci, Valerio Pascucci","deelman@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7231","020Z, 062Z","$0.00","NSF's major multi-user research facilities (large facilities) are sophisticated research instruments and platforms - such as large telescopes, interferometers and distributed sensor arrays - that serve diverse scientific disciplines from astronomy and physics to geoscience and biological science. Large facilities are increasingly dependent on advanced cyberinfrastructure (CI) - computing, data and software systems, networking, and associated human capital - to enable broad delivery and analysis of facility-generated data. As a result of these cyber infrastructure tools, scientists and the public gain new insights into fundamental questions about the structure and history of the universe, the world we live in today, and how our plants and animals may change in the coming decades.  The goal of this pilot project is to develop a model for a Cyberinfrastructure Center of Excellence (CI CoE) that facilitates community building and sharing and applies knowledge of best practices and innovative solutions for facility CI.<br/><br/>The pilot project will explore how such a center would facilitate CI improvements for existing facilities and for the design of new facilities that exploit advanced CI architecture designs and leverage establish tools and solutions. The pilot project will also catalyze a key function of an eventual CI CoE  - to provide a forum for exchange of experience and knowledge among CI experts. The project will also gather best practices for large facilities, with the aim of enhancing individual facility CI efforts in the broader CI context.  The discussion forum and planning effort for a future CI CoE will also address training and workforce development by expanding the pool of skilled facility CI experts and forging career paths for CI professionals.  The result of this work will be a strategic plan for a CI CoE that will be evaluated and refined through community interactions: workshops and direct engagement with the facilities and the broader CI community.<br/> <br/>This project is being supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Emerging Frontiers in the Directorate for Biological Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1450273","SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Perfomance, Security, and Correctness Tools","OAC","Software Institutes","08/01/2015","07/20/2015","John Mellor-Crummey","TX","William Marsh Rice University","Standard Grant","Bogdan Mihaila","07/31/2020","$1,500,000.00","","johnmc@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","8004","7433, 8009","$0.00","Software has become indispensable to society, used by computational scientists for science and engineering, by analysts mining big data for value, and to connect society over the Internet. However, the properties of software systems for any of these purposes cannot be understood without accounting for code transformations applied by optimizing compilers used to compose algorithm and data structure templates, and libraries available only in binary form. To address this need, this project will overhaul, integrate, and enhance static binary analysis and runtime technologies to produce components that provide a foundation for performance, correctness, and security tools. The project will build upon three successful and widely adopted open source software packages: the DynInst library for analysis and transformation of application binaries, the MRNet infrastructure for control of large-scale parallel executions and data analysis of their results, and the HPCToolkit performance analysis tools. The project team will engage the community to participate in the design and evaluation of the emerging components, as well as to adopt its components. <br/><br/>This project will have a wide range of impacts. First, software components built by the project will enable the development of sophisticated, high-quality, end-user performance, correctness, and security tools built by the project team, as well as others in academia, government, and industry. Software developed by the project team will help researchers and developers tackle testing, debugging, monitoring, analysis, and tuning of applications for systems at all scales. Second, end-user tools produced by the project have a natural place in the classroom to help students write efficient, correct, and secure programs. Third, components produced by the project will lower the barrier for new researchers to enter the field and build tools that have impact on production applications without years of investment. Fourth, the project will provide training for graduate students and interns in the area of software for performance, correctness, and security. Finally, through workshops and tutorials, the project will disseminate project results, provide training to enable others to leverage project software, and grow a community of tool researchers who depend on project components and thus have a strong motivation to help sustain project software into the future.<br/><br/>Modernizing open-source software components and tools for binary analysis will enable static analysis of application characteristics at the level of executable machine code, transformation of binaries to inject monitoring code, measurement to capture a detailed record of application?s interactions with all facets of a target platform, analysis of recorded data in parallel, and attribution of analysis results back to application source code in meaningful ways. Providing innovative, software components that support development of robust performance, correctness, and security tools will accelerate innovation by tools researchers and help them grapple with the increasing complexity of modern software. Of particular note, helping tools researchers and computational scientists grapple with the challenges of software for modern parallel systems and producing training materials that help people use this software, addresses several of the needs identified in the NSF Vision for Cyberinfrastructure for the 21st Century."
"1449918","SI2-SSI: Collaborative Research: A Sustainable Infrastructure for Performance, Security, and Correctness Tools","OAC","Software Institutes","08/01/2015","07/20/2015","Barton Miller","WI","University of Wisconsin-Madison","Standard Grant","Bogdan Mihaila","07/31/2020","$1,500,000.00","","bart@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8004","7433, 8009","$0.00","Software has become indispensable to society, used by computational scientists for science and engineering, by analysts mining big data for value, and to connect society over the Internet. However, the properties of software systems for any of these purposes cannot be understood without accounting for code transformations applied by optimizing compilers used to compose algorithm and data structure templates, and libraries available only in binary form. To address this need, this project will overhaul, integrate, and enhance static binary analysis and runtime technologies to produce components that provide a foundation for performance, correctness, and security tools. The project will build upon three successful and widely adopted open source software packages: the DynInst library for analysis and transformation of application binaries, the MRNet infrastructure for control of large-scale parallel executions and data analysis of their results, and the HPCToolkit performance analysis tools. The project team will engage the community to participate in the design and evaluation of the emerging components, as well as to adopt its components. <br/><br/>This project will have a wide range of impacts. First, software components built by the project will enable the development of sophisticated, high-quality, end-user performance, correctness, and security tools built by the project team, as well as others in academia, government, and industry. Software developed by the project team will help researchers and developers tackle testing, debugging, monitoring, analysis, and tuning of applications for systems at all scales. Second, end-user tools produced by the project have a natural place in the classroom to help students write efficient, correct, and secure programs. Third, components produced by the project will lower the barrier for new researchers to enter the field and build tools that have impact on production applications without years of investment. Fourth, the project will provide training for graduate students and interns in the area of software for performance, correctness, and security. Finally, through workshops and tutorials, the project will disseminate project results, provide training to enable others to leverage project software, and grow a community of tool researchers who depend on project components and thus have a strong motivation to help sustain project software into the future.<br/><br/>Modernizing open-source software components and tools for binary analysis will enable static analysis of application characteristics at the level of executable machine code, transformation of binaries to inject monitoring code, measurement to capture a detailed record of application?s interactions with all facets of a target platform, analysis of recorded data in parallel, and attribution of analysis results back to application source code in meaningful ways. Providing innovative, software components that support development of robust performance, correctness, and security tools will accelerate innovation by tools researchers and help them grapple with the increasing complexity of modern software. Of particular note, helping tools researchers and computational scientists grapple with the challenges of software for modern parallel systems and producing training materials that help people use this software, addresses several of the needs identified in the NSF Vision for Cyberinfrastructure for the 21st Century."
"1807563","CDS&E: Optimization of Advanced Cyberinfrastructure through Data-Driven Computational Modeling","OAC","CDS&E","09/01/2018","12/11/2018","Patrick Bridges","NM","University of New Mexico","Standard Grant","Micah Beck","08/31/2021","$523,644.00","Majeed Hayat, Trilce Estrada-Piedra","bridges@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","8084","026Z, 8084, 9150","$0.00","Modern scientific and big-data computing systems must balance multiple system attributes such as power, <br/>performance, and reliability to meet application science demands. These systems and their designers are, however,<br/>constrained by the lack of clear and well-defined methods to guide system design and tuning. This reduces the <br/>overall effectiveness of modern strategic computing systems. This project is developing new<br/>techniques for quantitatively characterizing and optimizing system tradeoffs in a wide range of<br/>modern strategic computing systems. The short term goal of this project is to develop models, analyses,<br/>and optimizations that will enable system software, applications, and system architects to make effective <br/>end-to-end performance tradeoffs. The desired long-term impact of this research is to increase the<br/>overall efficiency and effectiveness of current and emerging strategic computing systems. The techniques<br/>developed as part of this project will also be integrated with large-scale computing educational programs<br/>at the University of New Mexico and across the country.<br/><br/>This research is grounded in stochastic inter-collective-interval model for characterizing <br/>the performance of application/system-software configurations. To this end, the project is <br/>investigating techniques for estimating how different system software and applications mechanisms change <br/>application model parameters, providing a general means for understanding application/system software <br/>performance tradeoffs. In addition, the project is examining optimization techniques that leverage <br/>these models to improve resource allocation decisions in system schedulers and runtime systems. <br/>These techniques are being evaluated based on their ability to improve the overall performance of modern <br/>strategic computing systems in areas such as scheduling, power management, and data locality. The techniques<br/>developed as part of this project will also be integrated with large-scale computing educational programs<br/>at the University of New Mexico and across the country.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757659","REU Site: Research Experience in Cyber-Security for Cyber-Physical Systems","OAC","RSCH EXPER FOR UNDERGRAD SITES","03/01/2018","12/21/2017","Jeremy Straub","ND","North Dakota State University Fargo","Standard Grant","Sushil Prasad","02/28/2021","$359,999.00","Kendall Nygard","jeremy.straub@ndsu.edu","Dept 4000 - PO Box 6050","FARGO","ND","581086050","7012318045","CSE","1139","9150, 9250","$0.00","This research experience for undergraduates (REU) site has a focus on security of cyber-physical systems such as self-driving cars, unmanned aerial vehicle systems, robots, smart electrical grid systems and internet of things devices that are among the most important devices to secure due to their real-world interaction with humans, other equipment and the environment.These devices are also critical to manufacturing efficiency and national competitiveness. This program provides student participants an opportunity to gain experience in developing and analyzing cyber-security-related software for such cyber-physical systems.Through this experience, students learn about the process of and techniques for conducting research in cybersecurity, cyber-physical systems and the computing disciplines in general.They also come to better understand their own capabilities and interest in pursuing research careers, refining and potentially changing their career and educational goals. The project, thus, promotes the progress of science by allowing students to evaluate the pursuit of careers in research through immersive participation in a real-world research environment, andit advances national security and welfare through its focus on the area of cybersecurity.<br/><br/>The projects focusing on topics such as self-driving cars and robots excite student participants and provide the opportunity for hands on work with hardware, allowing students to see the effect of decisions they have made regarding software design and security. Sample projects involve 1) the design of secure coordination systems, intrusion, and malicious data detection, 2) issue prevention and detection, 3) data trustworthiness, 4) sensor data trust assessment, and 5) supervisory control and data acquisition system security, and reliability analytics.Through these, students gain exposure to multiple sub-disciplines related to these projects, allowing them to experience in multiple related areas. They also gain an appreciation for the importance of each sub-discipline as well as the opportunity to work on a large project from start to finish in conjunction with their cadre-mates.Participating students complete research projects, including software development, results analysis and technical write-ups. They are mentored by North Dakota State University faculty through cadre-group, scheduled one-on-one and ad-hoc meetings. They gain appreciation of the importance of quality assurance processes, particularly for ensuring software security and the safety of cyber-physical systems, and they learn about the software lifecycle and proper software development practices. The projects are designed so that students, despite working on their own research goal and serving as the ""lead scientist"" on this goal, rely on other students for their success.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844565","CAREER: Spatial Network Database approach for Emergency Management Information Systems","OAC","CAREER: FACULTY EARLY CAR DEV","03/15/2019","02/21/2019","KwangSoo Yang","FL","Florida Atlantic University","Continuing grant","Sushil K Prasad","02/29/2024","$251,818.00","","yangk@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","CSE","1045","1045","$0.00","Emergency Management Information Systems (EMIS) are an increasingly important tool for understanding, managing, and governing transportation-related systems, as well as for testing the stability or vulnerability of these systems against interference. Recently, EMIS have benefitted from both volunteer geographic information (VGI) and crowdsourcing as powerful methods of collecting user-generated datasets. However, these data sources are challenging due to their very large size, variety, and update rates required to ensure the timely and accurate delivery of useful emergency information and response for disastrous events. Developing fundamental data processing components for advanced relevant queries which can clearly and succinctly deliver critical information in the case of an emergency is critically important and challenging. This research focuses on three interrelated domains: 1) evacuation route planning 2) resource assignment, and 3) transportation resilience. This research investigates innovative queries in these three domains in the context of emergency management. The outcome of this project has potential benefit to a wide range of societal applications, such as transportation management, logistics, public safety, resource assignment, and service delivery and thus aligns well with the NSF mission: to promote the progress of science; to advance the national health, prosperity and welfare . Educational objectives of this project include broadening participation of Hispanic women, increasing undergraduate research opportunities including research-intensive course development, and promotion of team science skills. <br/><br/>The goals of this project are to identify promising solutions for addressing the challenge of EMIS and to develop an advanced spatial query processing platform that clearly and succinctly delivers critical information in emergencies. First, this project designs and develops the problem-solving framework that can integrate different technical components, including geometry, topology, graph theory, and optimization techniques. Second, this project investigates multiple inherent constraints for spatial networks and identifies main bottlenecks for query processing. Third, this project develops fast and scalable query processing mechanisms that overcome these bottlenecks and produce simple and concise information for emergency management. A key research challenge is to identify structural patterns or optimal substructures of the spatial network optimization problem that can enhance the scalability and efficiency of spatial network query processing. The components of the query processing framework include frequent suffix tree mining, graph simplification, bi-partite graph clustering, minimum polygon covering, graph partitioning, spectral method, random walk, and expander graph mining. These components are integrated to develop fast and scalable spatial network queries and to provide simple and concise information for EMIS. The outcomes of this project include data processing tools, spatial and spatial network optimization algorithms, queries, and visualization tools. This project validates the performance of new spatial network queries using historical and real-time datasets and provides a web-based educational system to enhance student learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1450934","IRNC:NOC - Global Research Network Operations Center at Indiana University Enabling International Science and Innovation","OAC","INTERNATIONAL RES NET CONNECT","04/15/2015","10/04/2018","David Jent","IN","Indiana University","Cooperative Agreement","Kevin Thompson","03/31/2020","$3,608,030.00","Ronald Johnson, Luke Fowler","djent@iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7369","1061, 5921, 5924, 7369","$0.00","The International Research Network Connections Network Operations Center (IRNC NOC) serves as a cooperative point of contact and communications for IRNC network management, providing consolidated network monitoring, reporting, and operational visibility for the IRNC program.  The IRNC NOC facilitates a single set of operational expectations for all IRNC funded infrastructure programs; this enables greater availability of IRNC infrastructure and improves results in troubleshooting multi-domain network issues.  A central data repository created by the IRNC NOC provides critical operational information; monitoring data and performance metrics in support of NSF funded science and research.<br/><br/>The IRNC NOC end-to-end Performance Engagement Team (E2E PET) provides centrally guided and well-coordinated end-to-end performance incident management. The E2E PET coordinates activities with the IRNC Exchange Point and Backbone programs as well as international partners, enabling efficient and timely troubleshooting of end to end performance issues spanning the continents.  The IRNC NOC works together with these programs to create a common framework for handling end-to-end performance problems.  An IRNC NOC Advisory Council with representation from each of the awarded backbone, exchange point, and measurement projects guides interaction to ensure consistent operational practices across the IRNC programs."
"1725729","MRI: Development of an Instrument for Deep Learning Research","OAC","MAJOR RESEARCH INSTRUMENTATION, CYBERINFRASTRUCTURE","10/01/2017","09/14/2017","William Gropp","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","09/30/2020","$2,721,983.00","Roy Campbell, Volodymyr Kindratenko, Jian Peng","wgropp@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1189, 7231","1189","$0.00","This project will develop and deploy a novel instrument for accelerating deep learning research at the University of Illinois (UI). The instrument will integrate the latest computing, storage, and interconnect technologies in a purpose-built shared-use system. This Instrument will deliver unprecedented performance levels for extreme data intensive emerging fields of research with far-reaching impacts in many areas, such as computer vision, natural language processing, artificial intelligence, healthcare and education. The instrument development will be driven by the UI deep learning (DL) community needs and will be carried out in collaboration with IBM and Nvidia. The instrument will serve as a focal point for the rapidly growing DL research community at UI, enable expansion of several research programs at UI, and contribute to STEM education and training.<br/><br/>Specifically, the proposed instrument is a far-reaching cyberinfrastructure development for the research community and industry engaged with deep learning. The work will result in an advanced high-performing scalable instrument with capabilities far beyond those currently deployed in academia or industry to tackle large-scale deep learning projects. This instrument will serve as a focal point for a community-driven effort to advance the field of DL, integrating the work of computer scientists, systems engineers, and software developers. This project is transformative both in the systems architecture and domain science fields it will imbue, with new knowledge to be developed via new interactions and synergies that will emerge as part of this effort.<br/><br/>The proposed development of this well-integrated instrument will improve the quality and expand the scope of research and training, provide inter- and intra-organizational use amongst many disciplines, and engage private sector partners. The work will have deep and long-lasting effects on future computer architectures for compute- and data-intensive applications by making the blueprints of the novel system architecture of the instrument publically available. Access to the high-performance software developed through this project will aid numerous science domains that utilize DL frameworks. The unprecedented computational capabilities of many applications will make it possible to tackle complex science, engineering and societal problems in many important fields ranging from education, to healthcare, to artificial intelligence (AI). This project will strive to include participants from under-represented minority and female students, to make new discoveries, train, and educate a new generation of users fluent with DL tools and methodologies, contributing to the development of a highly educated, and diverse workforce with specialized skillsets. Finally, the work will enable new industry-academic collaborations benefiting both the scientific community and industry nationwide."
"1531814","MRI: Acquisition of a High Performance Computing System for Science and Engineering Research and Education at the University of Houston","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC","08/15/2015","11/07/2016","Margaret Cheung","TX","University of Houston","Standard Grant","Stefan Robila","07/31/2019","$950,000.00","Eric Bittner, Barbara Chapman, Pradeep Sharma, Edgar Gabriel, Lars Grabow","mscheung@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","1189, 1640","1189, 2886","$0.00","This Major Research Instrument (MRI) project provides for the acquisition of a high-performance computing (HPC) instrument to support research from the University of Houston (UH) system as well as other minority-serving institutions in the Houston region. It enables cross-disciplinary research efforts in: multiscale biomolecular simulations; atomistic molecular dynamics simulations; computational chemistry for alternative energy; simulations of gas-phase, condensed-phase and nanostructured materials; and large-program restructuring for multicore and graphics processing unit (GPU) based systems. This facility also develops simulations of early universe cosmology; simulations of stochastic networks; quantum chromodynamics lattice simulations for particle physics; space radiation analysis and simulations; seismic characterization of wave functions; evolutionary simulations; computational analysis of genomic data; and cloud computing and image processing. <br/>   <br/>The instrument significantly increases resources available to researchers in science, technology, engineering, and mathematics (STEM) disciplines at the participating organizations. On-campus access to a heterogeneous computing platform fosters collaboration between computational scientists and programming experts to advance the use of graphics processing units (GPUs) for scientific computing. Training materials and strategies are developed to support code development and fully exploit the performance benefits. A new generation of open-source, directive-based GPU-enabled application codes are created that demonstrate scalability and portability. <br/><br/>This project greatly enriches the university research infrastructure and increases the use of new HPC techniques and systems at the University of Houston, a Hispanic Serving Institution, and collaborating institutions. UH partners with two Historically Black Colleges (HBCUs) to broaden the participation of students from underrepresented groups in STEM disciplines. The impact is particularly pronounced for undergraduate research. Students and researchers are exposed to a vibrant and multi-disciplinary research environment and receive valuable, advanced training in HPC - skills that are readily transferrable between scientific domains. Aspects of algorithms, computer codes, and data sets developed under this award are made publicly available."
"1453864","CAREER: Organizational Capacity and Capacity Building for Cyberinfrastructure Diffusion","OAC","CAREER: FACULTY EARLY CAR DEV","08/01/2015","03/11/2015","Kerk Kee","CA","Chapman University","Standard Grant","Sushil K Prasad","07/31/2020","$519,753.00","","kee@chapman.edu","One University Drive","Orange","CA","928661099","7146287383","CSE","1045","1045","$0.00","The vision behind advanced cyberinfrastructure (CI) is that its development, acquisition, and provision will transform science and engineering in the 21st century. However, CI diffusion is full of challenges, because the adoption of the material objects also requires the adoption of a set of related behavioral practices and philosophical ideologies. Most critically, CI-enabled virtual organizations (VOs) often lack the full range of organizational capacity to effectively integrate and support the complex web of objects, practices, and ideologies as a holistic innovation.<br/><br/>This project examines the various manifestations of CI related objects, practices, and ideologies, and the ways they support CI implementation in scientific VOs. Using grounded theory analysis of interviews and factor analysis of survey data, this project will develop and validate a robust framework/measure of organizational capacity for CI diffusion. The project's empirical focus will be the NSF-funded Extreme Science and Engineering Discovery Environment (XSEDE; https://www.xsede.org/), a nationwide network of distributed high-performance computing resources. Interviews and surveys will solicit input from domain scientists, computational technologists, and supercomputer center administrators (across e-science projects, institutions, and disciplines) who have experience with adopting and using CI tools within the XSEDE ecosystem. The project will generate a series of capacity building strategies to help VOs increase the organizational capacity necessary to fully adopt CI. Findings will help NSF and other federal agencies to improve existing and future CI investments. This project may also have implications for open-source and commercial technologies that harness big data for complex simulations, modeling, and visualization analysis."
"1540931","Stampede 2: The Next Generation of Petascale Computing for Science and Engineering","OAC","EQUIPMENT ACQUISITIONS","06/01/2016","03/22/2018","Daniel Stanzione","TX","University of Texas at Austin","Cooperative Agreement","Robert Chadduck","05/31/2020","$30,000,000.00","William Barth, Niall Gaffney, Kelly Gaither, Tommy Minyard","dan@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7619","","$0.00","The Texas Advanced Computing Center (TACC) at the University of Texas at Austin will acquire and deploy Stampede 2, a new, nearly 20 petaflop High Performance Computing (HPC) system.   This system will be available to and accessed by thousands of researchers across the country.  It will enable new computational and data-driven scientific and engineering, research and educational discoveries and advances.  As a national resource, Stampede 2 will replace and surpass the current highly successful Stampede system.  The new system will deliver over twice the overall performance as the current system in many dimensions most important to scientific computing, including computing capability, storage capacity, and network bandwidth.  TACC and its academic partners will team with Dell, Inc. and Intel Corp. to procure and provide this system.  <br/> <br/>HPC is intrinsic to discovery across the science and engineering disciplines served by the NSF.  This resource allows researchers to explore those scientific and engineer frontiers that require very large scale computations not otherwise possible.  Over the life of Stampede 2, the system is expected to serve many thousands of researchers spanning all NSF-supported disciplines, as the current system has done.  In addition to being an immediately productive resource for a large community of computational engineers and scientists,  Stampede 2 will also continue the community on an evolutionary path to future ""many core"" computing technologies.  <br/> <br/>Stampede 2 will employ upcoming generations of Intel's Xeon and Xeon Phi processors, as well as the Intel Omni-Path network fabric.   The system will maintain a familiar Linux-based software environment to insure a smooth migration of the large existing user base to the new system.   The system and its software stack will be designed to support traditional large scale simulation users, users performing data intensive computations, as well as emerging classes of new and non-traditional users to high performance computing.  Stampede 2 will support breakthrough discoveries and advances across a wide range of research topics."
"1914715","NSF Student Travel Grant for 2019 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation (PADS)","OAC","EDUCATION AND WORKFORCE","04/01/2019","03/21/2019","Dong Jin","IL","Illinois Institute of Technology","Standard Grant","Sushil K Prasad","03/31/2020","$5,000.00","Philippe Giabbanelli","djin6@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7361","026Z, 7556, 9179","$0.00","Discrete simulations are widely employed across a variety of settings, from the large-scale models that are commonplace in national laboratories (e.g., to simulate particles or populations) to industrial applications (e.g., in healthcare or transportation). The growing interest in simulations is also fueled by dynamic research in big data, which is both used and generated by simulations. In line with ongoing national efforts to train and diversify the workforce in science, this award supports students in the United States to attend the annual Conference on Principles of Advanced Discrete Simulation held in Chicago in June, 2019, with special emphasis on members of under-represented groups such as female and minorities. This serves the national interest, as stated by NSF's mission, to promote the progress of science as it provides a forum to disseminate research efforts, connect researchers, and train the next generation of scholars.<br/><br/>Selected students have access to traditional opportunities to support their career success and gain tools that support the advancement of science in theoretical as well as applied research on discrete simulations. Such traditional opportunities include presenting their work at the event (through oral presentations as well as posters), developing their scientific networks, and acquiring new state-of-the-art methods. Although the conference builds on a rich history dating back to 1985, the innovative format of the 2019 edition goes beyond traditional opportunities offered to students. In particular, the conference actively supports workforce development and the advancement of science by organizing (i) panels on careers beyond a doctoral program, with contributors from national laboratories, liberal arts colleges, and research universities; and (ii) round tables with editors of several prominent journals in the field. The funding provided by NSF thus has a significant impact on the careers of the future generation of researchers in discrete simulation, while encouraging diversity in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1728868","Collaborative Research: CyberTraining: CDL: Preparing Instructors to Offer Experimental Courses in an Updated PDC Curriculum, and Broadening Participation","OAC","CyberTraining - Training-based","08/01/2017","12/21/2018","Alan Sussman","MD","University of Maryland College Park","Standard Grant","Almadena Chtchelkanova","07/31/2019","$42,740.00","","als@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","044Y","7361","$0.00","This effort will develop a cadre of college faculty to pioneer a shift in the computer science curriculum to ensure students are educated in the use of 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly built on the principle of a single processor, executing a sequence of operations. That model is tightly bound into curricula, even though the last decade has seen widespread deployment of multi-core processors, graphics processors, online servers, and the internet of things, all of which depend on the much different PDC mindset for problem solving and programming. Financial, technical, scientific, engineering and medical companies, government laboratories, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the older sequential model. Even new graduates continue to learn the old approach because of the considerable inertia in the educational system. Thus, by turning the tide toward incorporation of PDC into the early stages of computer science education, through teaching the teachers, this project will strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) with the IEEE Computer Society Technical Committee on Parallel Processing, developed curriculum guidelines for parallel and distributed computing (PDC) that guided PDC aspects of the ACM Computer Science Curricula 2013. The guideline sought to shift courses in the first two years from the sequential model toward the PDC paradigm. In the modern world, students must see PDC as an aspect of computational problem solving from the very beginning. <br/>This project will update the curriculum guideline, with special foci on big data, energy, distributed computing, and exemplars. It will offer grants (encouraging participation by institutions serving underrepresented groups) for faculty to work on course development and attend a workshop where they will be trained in the use of PDC, and in experimental course design and evaluation. The workshops, offered each summer of the grant period, will also support attendance by industrial and government stakeholders to help build a network of relationships. Additional instructors will be offered travel support to attend the workshops, beyond the course development grants."
"1730527","Collaborative Research: CyberTraining: CDL: Preparing Instructors to Offer Experimental Courses in an Updated PDC Curriculum, and Broadening Participation","OAC","CyberTraining - Training-based","08/01/2017","06/28/2017","Charles Weems","MA","University of Massachusetts Amherst","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$367,260.00","Neena Thota, Arnold Rosenberg","weems@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","044Y","7361","$0.00","This effort will develop a cadre of college faculty to pioneer a shift in the computer science curriculum to ensure students are educated in the use of 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly built on the principle of a single processor, executing a sequence of operations. That model is tightly bound into curricula, even though the last decade has seen widespread deployment of multi-core processors, graphics processors, online servers, and the internet of things, all of which depend on the much different PDC mindset for problem solving and programming. Financial, technical, scientific, engineering and medical companies, government laboratories, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the older sequential model. Even new graduates continue to learn the old approach because of the considerable inertia in the educational system. Thus, by turning the tide toward incorporation of PDC into the early stages of computer science education, through teaching the teachers, this project will strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) with the IEEE Computer Society Technical Committee on Parallel Processing, developed curriculum guidelines for parallel and distributed computing (PDC) that guided PDC aspects of the ACM Computer Science Curricula 2013. The guideline sought to shift courses in the first two years from the sequential model toward the PDC paradigm. In the modern world, students must see PDC as an aspect of computational problem solving from the very beginning. <br/>This project will update the curriculum guideline, with special foci on big data, energy, distributed computing, and exemplars. It will offer grants (encouraging participation by institutions serving underrepresented groups) for faculty to work on course development and attend a workshop where they will be trained in the use of PDC, and in experimental course design and evaluation. The workshops, offered each summer of the grant period, will also support attendance by industrial and government stakeholders to help build a network of relationships. Additional instructors will be offered travel support to attend the workshops, beyond the course development grants."
"1727395","Collaborative Research: CyberTraining: CDL: Preparing Instructors to Offer Experimental Courses in an Updated PDC Curriculum, and Broadening Participation","OAC","CyberTraining - Training-based","08/01/2017","06/28/2017","Ramachandran Vaidyanathan","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Almadena Chtchelkanova","07/31/2019","$34,856.00","","vaidy@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","044Y","7361, 9150","$0.00","This effort will develop a cadre of college faculty to pioneer a shift in the computer science curriculum to ensure students are educated in the use of 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly built on the principle of a single processor, executing a sequence of operations. That model is tightly bound into curricula, even though the last decade has seen widespread deployment of multi-core processors, graphics processors, online servers, and the internet of things, all of which depend on the much different PDC mindset for problem solving and programming. Financial, technical, scientific, engineering and medical companies, government laboratories, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the older sequential model. Even new graduates continue to learn the old approach because of the considerable inertia in the educational system. Thus, by turning the tide toward incorporation of PDC into the early stages of computer science education, through teaching the teachers, this project will strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) with the IEEE Computer Society Technical Committee on Parallel Processing, developed curriculum guidelines for parallel and distributed computing (PDC) that guided PDC aspects of the ACM Computer Science Curricula 2013. The guideline sought to shift courses in the first two years from the sequential model toward the PDC paradigm. In the modern world, students must see PDC as an aspect of computational problem solving from the very beginning. <br/>This project will update the curriculum guideline, with special foci on big data, energy, distributed computing, and exemplars. It will offer grants (encouraging participation by institutions serving underrepresented groups) for faculty to work on course development and attend a workshop where they will be trained in the use of PDC, and in experimental course design and evaluation. The workshops, offered each summer of the grant period, will also support attendance by industrial and government stakeholders to help build a network of relationships. Additional instructors will be offered travel support to attend the workshops, beyond the course development grants."
"1727556","Collaborative Research: CyberTraining: CDL: Preparing Instructors to Offer Experimental Courses in an Updated PDC Curriculum, and Broadening Participation","OAC","CyberTraining - Training-based","08/01/2017","06/28/2017","Rajshekhar Sunderraman","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Almadena Chtchelkanova","07/31/2019","$19,922.00","Semir Sarajlic, Neranjan Edirisinghe Pathirannehe","raj@cs.gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","044Y","7361","$0.00","This effort will develop a cadre of college faculty to pioneer a shift in the computer science curriculum to ensure students are educated in the use of 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly built on the principle of a single processor, executing a sequence of operations. That model is tightly bound into curricula, even though the last decade has seen widespread deployment of multi-core processors, graphics processors, online servers, and the internet of things, all of which depend on the much different PDC mindset for problem solving and programming. Financial, technical, scientific, engineering and medical companies, government laboratories, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the older sequential model. Even new graduates continue to learn the old approach because of the considerable inertia in the educational system. Thus, by turning the tide toward incorporation of PDC into the early stages of computer science education, through teaching the teachers, this project will strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) with the IEEE Computer Society Technical Committee on Parallel Processing, developed curriculum guidelines for parallel and distributed computing (PDC) that guided PDC aspects of the ACM Computer Science Curricula 2013. The guideline sought to shift courses in the first two years from the sequential model toward the PDC paradigm. In the modern world, students must see PDC as an aspect of computational problem solving from the very beginning. <br/>This project will update the curriculum guideline, with special foci on big data, energy, distributed computing, and exemplars. It will offer grants (encouraging participation by institutions serving underrepresented groups) for faculty to work on course development and attend a workshop where they will be trained in the use of PDC, and in experimental course design and evaluation. The workshops, offered each summer of the grant period, will also support attendance by industrial and government stakeholders to help build a network of relationships. Additional instructors will be offered travel support to attend the workshops, beyond the course development grants."
"1730115","Collaborative Research: CyberTraining: CDL: Preparing Instructors to Offer Experimental Courses in an Updated PDC Curriculum, and Broadening Participation","OAC","CyberTraining - Training-based","08/01/2017","06/28/2017","Martina Barnas","IN","Indiana University","Standard Grant","Almadena Y. Chtchelkanova","07/31/2019","$68,532.00","","mbarnas@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","044Y","7361","$0.00","This effort will develop a cadre of college faculty to pioneer a shift in the computer science curriculum to ensure students are educated in the use of 21st century platforms that pervasively incorporate parallel and distributed computing (PDC). Twentieth century computers were mostly built on the principle of a single processor, executing a sequence of operations. That model is tightly bound into curricula, even though the last decade has seen widespread deployment of multi-core processors, graphics processors, online servers, and the internet of things, all of which depend on the much different PDC mindset for problem solving and programming. Financial, technical, scientific, engineering and medical companies, government laboratories, the department of defense, the intelligence community, and many other sectors are desperately seeking employees who can exploit PDC systems, because the existing workforce was heavily steeped in the older sequential model. Even new graduates continue to learn the old approach because of the considerable inertia in the educational system. Thus, by turning the tide toward incorporation of PDC into the early stages of computer science education, through teaching the teachers, this project will strategically serve the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; and to secure the national defense. It will be a significant step toward modernizing the emerging workforce to have the computing skills needed for the United States to maintain leadership in all of these areas.<br/><br/>The Center for Parallel and Distributed Computing Curriculum Development and Educational Resources (CDER) with the IEEE Computer Society Technical Committee on Parallel Processing, developed curriculum guidelines for parallel and distributed computing (PDC) that guided PDC aspects of the ACM Computer Science Curricula 2013. The guideline sought to shift courses in the first two years from the sequential model toward the PDC paradigm. In the modern world, students must see PDC as an aspect of computational problem solving from the very beginning. <br/>This project will update the curriculum guideline, with special foci on big data, energy, distributed computing, and exemplars. It will offer grants (encouraging participation by institutions serving underrepresented groups) for faculty to work on course development and attend a workshop where they will be trained in the use of PDC, and in experimental course design and evaluation. The workshops, offered each summer of the grant period, will also support attendance by industrial and government stakeholders to help build a network of relationships. Additional instructors will be offered travel support to attend the workshops, beyond the course development grants."
"1450455","SI2-SSI: CRESCAT, A Computational Research Ecosystem for Scientific Collaboration on Ancient Topics, Spanning the Full Data Life Cycle","OAC","METHOD, MEASURE & STATS, Software Institutes","09/01/2015","08/14/2017","David Schloen","IL","University of Chicago","Standard Grant","Bogdan Mihaila","08/31/2019","$1,750,000.00","Thomas Levy, Kathleen Morrison, Hakizumwami B. Runesha","d-schloen@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1333, 8004","7433, 8004, 8009","$0.00","This project integrates, tests, and documents a suite of interoperable software tools to support collaborative research. The tools are collectively called CRESCAT (Computational Research Ecosystem for Scientific Collaboration on Ancient Topics). The initial focus is on disciplines that deal with dynamic interactions and structural changes within spatially situated populations over long time spans in the past, e.g., paleobiology, archaeology, and economic history. Despite their differences, these disciplines have similar computational needs for modeling and analyzing data. Moreover, the same software can be used in many other disciplines, enabling economies of scale by building and maintaining a common set of interoperable tools to serve a wide range of researchers, while spanning the full research data life cycle, consisting of (1) acquisition, (2) integration, (3) analysis, (4) publication, and (5) archiving of data. An intuitive graphical user interface is provided for end-user researchers to work with their data in all stages of the life cycle without cumbersome manual data transfers and transformations. The project will address a major computational problem that affects many scientific disciplines due to the challenge of integrating and analyzing data of diverse origins based on heterogeneous spatial, temporal, and taxonomic ontologies. Thus it will have a broad impact in the sciences and beyond by showing how to represent explicitly the full variability of individual judgments and the divergent conceptualizations and terminologies through which those judgments are expressed, with explicit attribution of each observation, interpretation, and conceptual ontology to a particular named person or group. Unlike many computational tools for scientific research, which assume a degree of ontological consensus that does not exist, CRESCAT conforms to actual research practices. It does not impose a standardized ontology, thereby ignoring or suppressing the inevitable disagreements and conflicting interpretations that arise among researchers. Instead, it represents ontological diversity, observational uncertainty, and interpretive disagreement explicitly within a larger common framework in which end users can query, analyze, and compare the full range of observations, interpretations, and terminologies to inform their own judgments about the evidence. CRESCAT is designed to allow scientific disagreements and observational and interpretive uncertainties to be represented digitally in a way that exposes these differences themselves as data for analysis and debate. Thus, in addition to the practical goal of building a more efficient shared framework for advanced research, the proposed work will provoke theoretical reflection about how computational tools should relate to scientific practice.<br/><br/>The CRESCAT project is an interdisciplinary collaboration between computer scientists, paleobiologists, geoscientists, archaeologists, economic historians, and other social scientists. The goal is to demonstrate the value of an integrative software ecosystem that spans the social and natural sciences and can facilitate any research characterized by overlapping models of temporal and spatial relations or by conflicting terminologies and taxonomies. CRESCAT's representation of scientific knowledge eschews forced standardization, which is impractical in many cases due to lack of an enforcement mechanism and is also questionable in principle since divergent ontologies often legitimately reflect different theoretical assumptions and research agendas. Central to the CRESCAT suite of tools is an innovative data-integration system that represents explicitly both research data and the ontologies inherent in the data. An ontology is defined here as a conceptual model of entities and the relationships among them in a given domain of knowledge, in contrast to a schema,&#148;which is the implementation of an ontology in logical data structures within a working system. CRESCAT's data-integration system operates at a level of abstraction sufficient to provide a predictable and efficiently queryable database structure based on an abstract global schema, which in turn is based on an upper ontology specified in terms of fundamental concepts and relationships applicable to all scientific and scholarly disciplines. The data-integration system is implemented in an enterprise-class XML/XQuery DBMS that serves as a data warehouse (using the non-relational graph data model), in which is stored diverse data from a wide range of research projects representing many disciplines. The terminology and conceptual distinctions of each research project are fully preserved. The approach to research data taken in the CRESCAT project is (1) coherent, tightly integrating software tools and data formats within a single analytical framework; (2) open-ended, interconnecting existing tools while allowing the addition of new tools in the future; (3) non-exclusive, in no way preventing its component tools from participating in other software ecosystems; (4) scalable, designed to handle large-scale data management, analysis, and visualization; and (5) sustainable, maintaining shared resources to meet common needs for software and technical support and thus enabling substantial economies of scale."
"1642102","Collaborative Research:  CICI: Regional: SouthEast SciEntific Cybersecurity for University REsearch (SouthEast SECURE)","OAC","Cyber Secur - Cyberinfrastruc","10/01/2016","06/28/2017","Jill Gemmill","SC","Clemson University","Standard Grant","Micah Beck","03/31/2019","$158,004.00","Jan Holmevik, Hongxin Hu, Nuyun Zhang","gemmill@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","8027","9102, 9150, 9251","$0.00","Threats to scientific instruments and data that are accessible via the Internet are ubiquitous.  The SouthEast SciEntific Cybersecurity for University REsearch (SECURE) project helps protect the National Science Foundation's investments in scientific research while providing scientists with tools to safeguard intellectual property and ensure data integrity. The project team provides education, training, and selected cybersecurity services to NSF-funded researchers across the Southeast.  The team is multidisciplinary, comprised of cybersecurity experts (both research and practitioner), scientists, and experts in communication.  Team members are located in South Carolina, Alabama and Mississippi, with strong representation from Historically Black Colleges and Universities (HBCU).  This program raises investigators' awareness of their essential role in creating a secure and trustworthy cyberspace and offers concrete assistance in risk assessment, vulnerability testing, and mitigation tailored to NSF-funded scientists? workflow and program size.  Through past collaborations, the team is well positioned to leverage both national and regional cybersecurity organizations and programs to effectively reach the target audience.<br/> <br/>SouthEast SECURE impacts the region by raising cybersecurity awareness; providing concise training, assessment, tools and one-on-one help; and assisting in preparation of select cybersecurity metrics.  Student interns are conducting many of these activities by means of practicum-based deployment and support, thus developing capabilities in the next generation of cyber professionals.  An online survey of NSF-funded investigators in the region will be conducted to learn about their primary cybersecurity challenges and concerns.  Training is then tailored to provide concrete and practical assistance in how to do right-sized risk assessment and mitigation.  A ""toolkit"" is provided to test and validate local cybersecurity, and measures of cybersecurity are created and field-tested.  The team?s approach facilitates communication between research faculty and university IT/Data Security staff.  A long-term goal is building communities with common interests in cybersecurity and a commitment to helping others; and building connections with other regions and with national centers and programs."
"1445806","XD Metrics Service (XMS)","OAC","ETF","07/01/2015","07/21/2018","Thomas Furlani","NY","SUNY at Buffalo","Cooperative Agreement","Edward Walker","06/30/2020","$9,754,161.00","Abani Patra, Gregor von Laszewski, Matthew Jones, Steven Gallo","furlani@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7476","7476, 9251","$0.00","The XD Metrics Service (XMS) is a renewed project of the Technology Audit Service (TAS), which aims at improving the operational efficiency and management of NSF's XD network of computational resources. XMS builds on and expands the successes of the TAS project, such as the development of the XDMoD tool. This tool provides stakeholders of XD and its largest project, XSEDE, with ready access to data about utilization, performance, and quality of service for XD resources and XSEDE-related services. While the initial project focus was the XD community, the ongoing effort realized that such a resource management tool would also be of great utility to high performance computing centers in general, as well as to other data centers managing complex IT infrastructure. To pursue this opportunity, Open XDMoD was being developed, which is an open source version of the tool. Open XDMoD is already in use by numerous academic and industrial HPC centers. The XMS project expands XDMoD beyond its original goals, so as to increase its utility to XD and move it into the realm of a comprehensive resource management tool for cyberinfrastructure. One example is the incorporation of job-level performance data through ""TACC_Stats"" into XDMoD. This functionality provides XDMoD with the ability to identify poorly performing applications, improve throughput, characterize the system's workload, and provide metrics critical for the specification of future resource acquisitions. Given the scale of today's HPC systems, even modest increases in throughput can have a substantial impact on science and engineering research. For example, with respect to the XD network, every 1% increase in system performance translates into an additional 15 million CPU hours of computer time that can be allocated for research."
"1531594","MRI: Acquisition of High Performance Scientific Computing Cluster at Trinity University","OAC","MAJOR RESEARCH INSTRUMENTATION","08/01/2015","07/28/2015","Matthew Hibbs","TX","Trinity University","Standard Grant","Stefan Robila","07/31/2019","$623,730.00","Steven Bachrach, Kwan Cheng, Hoa Nguyen","mhibbs@trinity.edu","One Trinity Place","San Antonio","TX","782127200","2109997246","CSE","1189","1189, 9229","$0.00","This instrument supports large-scale, high-performance computing (HPC) at Trinity University, which enables a broad range of scientific research efforts spanning mathematics, computer science, chemistry, biology, and physics.  Many ongoing projects at Trinity relate to questions of 'big data' analysis, including such diverse areas as weak interactions in host-guest chemistry, multi-scale simulations of protein interactions, models of fluid dynamics related to cellular motility in membranes, analysis of high-throughput genomic sequencing data, multi-agent modeling of security systems, and simulations of particle collisions and gravity within the rings of Saturn. Each of these research areas requires considerable computational power to analyze extremely large datasets, whether generated through traditional laboratory experiments or through robust computational simulations. <br/><br/>The HPC cluster provides a shared computational resource to all researchers at Trinity, and includes both traditional central processing units (CPUs) and newer general-purpose graphics processor unit (GPU) systems optimized for large quantities of floating-point numeric operations.  The cluster operates on the Linux operating system, with a hybrid queuing/priority system so that computational jobs submitted by any researcher are quickly and efficiently executed.  This resource enables Trinity University, an undergraduate institution, to more effectively train and educate students on tangible aspects of large-scale computational projects and research endeavors, such as the value of test-driven development, efficient algorithm design, memory management, and data visualization, while still tackling the challenges of open research questions on a modern scale."
"1730695","CyberTraining: CIP: CiSE-ProS: Cyberinfrastructure Security Education for Professionals and Students","OAC","CyberTraining - Training-based","08/01/2017","07/05/2017","Dhruva Chakravorty","TX","Texas A&M University Main Campus","Standard Grant","Sushil K Prasad","07/31/2020","$499,996.00","Jeffrey Froyd, Donald McMullen, Daniel Ragsdale, Jinsil Seo","chakravorty@tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","044Y","7361","$0.00","The rapidly changing nature of the cybersecurity landscape underscores the need for trained professionals who are prepared to efficiently identify and mitigate new threats.  The Cyberinfrastructure Security and Education for Professionals and Students (CiSE-ProS) program adopts well-established teaching and learning practices to provide in-person and online training avenues for practitioners to develop core competencies in secure computing relevant to advanced cyberinfrastructure.  CiSE-ProS works with an international consortium of professionals in research computing, academia and workforce training to define a common standard for certification.  Evidence-generating learning activities are designed around concepts utilized in cybersecurity.  These activities include hands-on exercises with computing clusters in both laboratory settings and virtual reality environments. Scoring schemes developed by CiSE-ProS help evaluate a participant?s effectiveness at achieving the desired learning outcomes.  Overall, CiSE-ProS aims to have a significant impact on cybersecurity preparedness and as stated by NSF's mission, it will help promote the progress of science and advance the national security.<br/><br/>The CiSE-ProS project will be implemented through continuing education programs, and as a new cybersecurity minor along a cyberinfrastructure track at Texas A&M University.  At its core, the project increases awareness of cybersecurity issues in research computing and actively prepares participants from a variety of backgrounds for productive cybersecurity careers.  Learning modules, virtual laboratories, and the cybersecurity minor developed by CiSE-ProS can be adapted by other institutions to support various learning levels.  The project will leverage an academic-industry partnership to train more than 400 undergraduate students in STEM+C, through a combination of workshops, e-learning and community engagement activities.  The training program will implement a wide range of activities including a community workshop, a series of hackathons, a set of workshops aimed at delivering core technical competencies in cybersecurity, online learning modules, and integration of those curriculum as part of a cybersecurity minor program at TAMU.  The training program will develop core competency in a number of areas including system and data security/privacy, how to set up secure computing and data directories, secure networking, secure remote access, auditing maintenance for secure data, intrusion testing, and secure cloud-based computing environments.  By engaging stakeholders during the development process, project intends to increase the likelihood of widespread adoption of CiSE-ProS materials by academic and professional communities."
"1642069","Collaborative Research: CICI: Regional: SouthEast SciEntific Cybersecurity for University REsearch (SouthEast SECURE)","OAC","Cyber Secur - Cyberinfrastruc","10/01/2016","06/08/2017","Sara Graves","AL","University of Alabama in Huntsville","Standard Grant","Micah Beck","03/31/2019","$92,000.00","","sgraves@itsc.uah.edu","301 Sparkman Drive","Huntsville","AL","358051911","2568242657","CSE","8027","9102, 9150, 9251","$0.00","Threats to scientific instruments and data that are accessible via the Internet are ubiquitous.  The SouthEast SciEntific Cybersecurity for University REsearch (SECURE) project helps protect the National Science Foundation's investments in scientific research while providing scientists with tools to safeguard intellectual property and ensure data integrity. The project team provides education, training, and selected cybersecurity services to NSF-funded researchers across the Southeast.  The team is multidisciplinary, comprised of cybersecurity experts (both research and practitioner), scientists, and experts in communication.  Team members are located in South Carolina, Alabama and Mississippi, with strong representation from Historically Black Colleges and Universities (HBCU).  This program raises investigators' awareness of their essential role in creating a secure and trustworthy cyberspace and offers concrete assistance in risk assessment, vulnerability testing, and mitigation tailored to NSF-funded scientists' workflow and program size.  Through past collaborations, the team is well positioned to leverage both national and regional cybersecurity organizations and programs to effectively reach the target audience.<br/> <br/>SouthEast SECURE impacts the region by raising cybersecurity awareness; providing concise training, assessment, tools and one-on-one help; and assisting in preparation of select cybersecurity metrics.  Student interns are conducting many of these activities by means of practicum-based deployment and support, thus developing capabilities in the next generation of cyber professionals.  An online survey of NSF-funded investigators in the region will be conducted to learn about their primary cybersecurity challenges and concerns.  Training is then tailored to provide concrete and practical assistance in how to do right-sized risk assessment and mitigation.  A ""toolkit"" is provided to test and validate local cybersecurity, and measures of cybersecurity are created and field-tested.  The team's approach facilitates communication between research faculty and university IT/Data Security staff.  A long-term goal is building communities with common interests in cybersecurity and a commitment to helping others; and building connections with other regions and with national centers and programs."
"1615206","Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with a Multi-Scale Fluid-Kinetic Simulation Suite","OAC","PETASCALE - TRACK 1","05/01/2016","04/29/2016","Nikolai Pogorelov","AL","University of Alabama in Huntsville","Standard Grant","Edward Walker","04/30/2019","$20,087.00","Jacob Heerikhuisen","np0002@uah.edu","301 Sparkman Drive","Huntsville","AL","358051911","2568242657","CSE","7781","9150","$0.00","Flows of partially ionized plasma are frequently characterized by the presence of both thermal<br/>and nonthermal populations of ions. For example, this occurs in the outer heliosphere: the part of interstellar space beyond the solar system whose properties are determined by the solar wind (SW) interaction with the local<br/>interstellar medium (LISM). Simulation of the SW-LISM interaction problem with data-driven boundary<br/>conditions, requires the application of adaptive mesh refinement technologies and petascale supercomputers.<br/>The objective of this proposal is to use the Blue Waters system to model solar winds (SW) flows in the inner and outer heliosphere, <br/>and compare the simulation results with observational data, thereby revealing the fundamental physics of non-thermal plasma-neutral flows.<br/><br/>The project will address a variety of physical phenomena occurring throughout the solar system, e.g., charge exchange processes between neutral<br/>and charged particles, the birth of pick-up ions (PUIs), the origin of energetic neutral atoms (ENAs),<br/>turbulence, the interplay of the heliopause instability and magnetic reconnection at the SW-LISM interface,<br/>properties of the heliotail, etc. Additionally, the project will fit the simulation results with observational data to make it possible<br/>to constrain the properties of the LISM and refine time-dependent SW models. <br/><br/>The work proposed here is expected to have major impact in several areas. The project<br/>will provide a leap forward in the computation and simulation of complex charged and neutral gas systems.<br/>The development of codes that embrace ""coupling complexity"" via the self-consistent incorporation of multiple<br/>physical scales and processes in models is viewed as a pivotal development in the different plasma<br/>physics areas for the current decade. The ubiquity of the underlying model suggests numerous applications<br/>in space physics, astrophysics, and, in general, plasma physics problems. The components of our physical<br/>model and corresponding code routines in the publicly accessible simulation suite may be also useful to<br/>model plasma-beam interactions in Tokamak fusion devices to be implemented in the burning plasma experiment<br/>ITER. <br/><br/>Besides the impact on the modeling of complex physical systems, the project anticipates that their<br/>approach to computational resource management for complex codes utilizing multiple algorithm technologies<br/>will be a major advance on current approaches.  Additionally, collaboration with the Blue Waters team will further promote the application of adaptive technologies to contemporary plasma physics problems through the development of publicly available packages suitable<br/>for multiple applications. <br/><br/>Finally, the project will provide leadership in promoting computational science and plasma<br/>physics within the UAH campus and, through the training of a broad spectrum of specialists, foster new<br/>technologies within an EPSCoR state."
"1730653","CyberTraining: CDL: Cyber Infrastructure Training and Mentoring (CI-TraM)","OAC","CyberTraining - Training-based, EPSCoR Co-Funding","08/01/2017","07/12/2017","Diana Dugas","NM","New Mexico State University","Standard Grant","Sushil K Prasad","07/31/2020","$467,170.00","Satyajayant Misra, Abdel-Hameed Badawy","dugasdvt@nmsu.edu","Corner of Espina St. & Stewart","Las Cruces","NM","880038002","5756461590","CSE","044Y, 9150","7361, 9102, 9150","$0.00","The Cyber Infrastructure Training and Mentoring (CI-TraM) project serves the national interest by developing and increasing the cyberinfrastructure (CI) literacy of undergraduates who are interested in STEM careers.  It does this through technical training and career mentoring focused around building knowledge in CI.  CI-TraM provides direct mentoring of new undergraduates by active STEM researchers and IT professionals.  The training will be provided by a diverse group of researchers in CI-intensive domains and CI professionals.  Topics will include network and data security, high performance computing, computer architecture, and data analytics tools to provide the background to explore computing in a) cybersecurity, b) patterns discovery in biological data, c) computational physics, d) neuroscience, e) game design and human-computer interaction, etc.  This will help address the challenge of creating a future STEM workforce that has the technical and professional skills needed to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.  The CI-TraM program, developed in a Hispanic Serving Institution (NMSU), cultivates untapped talent within a growing, yet underrepresented, population in STEM careers. The model used for community engagement and student recruitment addresses the transitions between secondary education, post-secondary education, and individual career planning and management. The CI-TraM model will be designed to become institutionalized with the ability to be replicated in other communities.<br/><br/>The main goals of the CI-TraM program are: 1) To develop a program that exposed and retains students in CI-STEM fields while teaching them valuable technical and life skills needed for successfully pursing, attaining, and maintaining careers, and 2) To develop a program that is sustainable and scalable to other institutions so that other student populations may benefit from the program.  The program utilizes common job site internship procedures in area high schools that provide dual/concurrent college credit.  The CI-TraM program will create a cohort of students interns each academic semester and summer (50/yr) that have demonstrated interest/ability in STEM career pathways.  To achieve Goal 1, technical training and career planning modules developed and taught by working STEM researchers and IT professionals will be completed by each student intern.  At the start and finish of the program a general assessment of a student's knowledge will be performed, and each module would have a tangible deliverable to evaluate student understanding and progress throughout the program.  The program itself will be assessed by a longitudinal follow up that will track students into their futures.  Assessment results will be used by the program coordinators in a continual improvement process.  To achieve Goal 2, the modules will be generated using a standard Learning Management System allowing them to be transferable to other universities.  External consultant-created rubrics will be used to evaluate the modules and their efficacy as well as the efficacy of the modules together as the body of the CI-TraM program and available for others to use."
"1812404","Collaborative Research: CICI: Regional: SouthEast SciEntific Cybersecurity for University Research (SouthEast SECURE)","OAC","Cyber Secur - Cyberinfrastruc","10/01/2017","02/01/2018","Anthony Skjellum","TN","University of Tennessee Chattanooga","Standard Grant","Micah Beck","09/30/2019","$76,949.00","","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","8027","9102, 9150, 9251","$0.00","Threats to scientific instruments and data that are accessible via the Internet are ubiquitous.  The SouthEast SciEntific Cybersecurity for University REsearch (SECURE) project helps protect the National Science Foundation's investments in scientific research while providing scientists with tools to safeguard intellectual property and ensure data integrity. The project team provides education, training, and selected cybersecurity services to NSF-funded researchers across the Southeast.  The team is multidisciplinary, comprised of cybersecurity experts (both research and practitioner), scientists, and experts in communication.  Team members are located in South Carolina, Alabama and Mississippi, with strong representation from Historically Black Colleges and Universities (HBCU).  This program raises investigators' awareness of their essential role in creating a secure and trustworthy cyberspace and offers concrete assistance in risk assessment, vulnerability testing, and mitigation tailored to NSF-funded scientists' workflow and program size.  Through past collaborations, the team is well positioned to leverage both national and regional cybersecurity organizations and programs to effectively reach the target audience.<br/> <br/>SouthEast SECURE impacts the region by raising cybersecurity awareness; providing concise training, assessment, tools and one-on-one help; and assisting in preparation of select cybersecurity metrics.  Student interns are conducting many of these activities by means of practicum-based deployment and support, thus developing capabilities in the next generation of cyber professionals.  An online survey of NSF-funded investigators in the region will be conducted to learn about their primary cybersecurity challenges and concerns.  Training is then tailored to provide concrete and practical assistance in how to do right-sized risk assessment and mitigation.  A ""toolkit"" is provided to test and validate local cybersecurity, and measures of cybersecurity are created and field-tested.  The team's approach facilitates communication between research faculty and university IT/Data Security staff.  A long-term goal is building communities with common interests in cybersecurity and a commitment to helping others; and building connections with other regions and with national centers and programs."
"1840043","CICI: RDP: Supporting Controlled Unclassified Information with a Campus Awareness and Risk Management Framework","OAC","Cyber Secur - Cyberinfrastruc","09/01/2018","08/17/2018","Baijian Yang","IN","Purdue University","Standard Grant","Micah Beck","08/31/2020","$598,373.00","Preston Smith","byang@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8027","","$0.00","Protecting Controlled Unclassified Information (CUI) is mandated by the executive order 13356, and today is required for research in sectors such as defense and aerospace. Regulatory requirements for research will increase, with CUI regulations covering categories including Agriculture, Financial, Legal Records, and Business information. When combined with existing regulations already seen by universities, such as HIPAA, and the European Union's GDPR, a well-defined and consistent framework for working with regulated data is critical for institutions of higher education. This project describes a cost-effective ecosystem (REED+) to manage regulated data that meets the compliance requirements found in a campus environment.  <br/><br/>The REED+ framework integrates NIST SP 800-171 and other related NIST publications as the foundation of the framework. This framework serves as a standard for campus IT to align with security regulations and best practices, and create a single process for intake, contracting, and facilitate easy mapping of controlled research to CI resources for the sponsored programs office, human subjects office, and export control office. The framework allows researchers to experience faster intake of new funded projects and be more competitive for research dollars. Using student-developed training materials and instruction, researchers, administrators, and campus IT are now able to more clearly understand previously complicated data security regulations affecting research projects. The ecosystem developed from this project enables new partnerships with government agencies, and industry partners from the defense, aerospace, and life science sectors. Experiences and best practices in providing cyberinfrastructure and security awareness developed from this collaboration are documented and shared with the broader CI and campus community through conferences, journals and workshop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642038","Collaborative Research: CICI: Regional: SouthEast SciEntific Cybersecurity for University REsearch (SouthEast SECURE)","OAC","Cyber Secur - Cyberinfrastruc","10/01/2016","04/12/2018","Gadsden Veronical","SC","Voorhees College","Standard Grant","Micah Beck","03/31/2019","$113,496.00","","vgadsden@voorhees.edu","PO Box 678","Denmark","SC","290420678","8037801149","CSE","8027","2886, 9150","$0.00","Threats to scientific instruments and data that are accessible via the Internet are ubiquitous.  The SouthEast SciEntific Cybersecurity for University REsearch (SECURE) project helps protect the National Science Foundation's investments in scientific research while providing scientists with tools to safeguard intellectual property and ensure data integrity. The project team provides education, training, and selected cybersecurity services to NSF-funded researchers across the Southeast.  The team is multidisciplinary, comprised of cybersecurity experts (both research and practitioner), scientists, and experts in communication.  Team members are located in South Carolina, Alabama and Mississippi, with strong representation from Historically Black Colleges and Universities (HBCU).  This program raises investigators' awareness of their essential role in creating a secure and trustworthy cyberspace and offers concrete assistance in risk assessment, vulnerability testing, and mitigation tailored to NSF-funded scientists' workflow and program size.  Through past collaborations, the team is well positioned to leverage both national and regional cybersecurity organizations and programs to effectively reach the target audience.<br/> <br/>SouthEast SECURE impacts the region by raising cybersecurity awareness; providing concise training, assessment, tools and one-on-one help; and assisting in preparation of select cybersecurity metrics.  Student interns are conducting many of these activities by means of practicum-based deployment and support, thus developing capabilities in the next generation of cyber professionals.  An online survey of NSF-funded investigators in the region will be conducted to learn about their primary cybersecurity challenges and concerns.  Training is then tailored to provide concrete and practical assistance in how to do right-sized risk assessment and mitigation.  A ""toolkit"" is provided to test and validate local cybersecurity, and measures of cybersecurity are created and field-tested.  The team's approach facilitates communication between research faculty and university IT/Data Security staff.  A long-term goal is building communities with common interests in cybersecurity and a commitment to helping others; and building connections with other regions and with national centers and programs."
"1835443","Framework: Software: Next-Generation Cyberinfrastructure for Large-Scale Computer-Based Scientific Analysis and Discovery","OAC","Software Institutes","01/01/2019","08/24/2018","Alan Edelman","MA","Massachusetts Institute of Technology","Standard Grant","Vipin Chaudhary","12/31/2023","$3,498,560.00","Juan Pablo Vielma Centeno","EDELMAN@MATH.MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Recent revolutions in data availability have radically altered activities across many fields within science, industry, and government. For instance, contemporary simulations medication properties can require the computational power of entire data centers, and recent efforts in astronomy will soon generate the largest image datasets in history. In such extreme environments, the only viable path forward for scientific discovery hinges on the development and exploitation of next-generation computational cyberinfrastructure of supercomputers and software. The development of this new computational infrastructure demands significant engineering resources, so it is paramount to maximize the infrastructure's potential for high impact and wide adoption across as many technical domains as possible. Unfortunately, despite this necessity, existing development processes often produce software that is limited to specific hardware, or requires additional expertise to use properly, or is overly specialized to a specific problem domain. Such ""single-use"" software tools are limited in scope, leading to underutilization by the wider scientific community. In contrast, this project seeks to develop methods and software for computer-based scientific analysis that are sufficiently powerful, flexible and accessible to (i) enable domain experts to achieve significant advancements within their domains, and (ii) enable innovative use of advanced computational techniques in unexpected scientific, technological and industrial applications. This project will apply these tools to a wide variety of specific scientific challenges faced by various research teams in astronomy, medicine, and energy management. These teams plan on using the proposed work to map out new star systems, develop new life-saving medications, and design new power systems that will deliver more energy to a greater number of homes and businesses at a lower cost than existing systems. Finally, this project will seek to leave a legacy of sustained societal benefit by educating students and practitioners in the broader scientific and engineering communities via exposure to state-of-the-art computational techniques. <br/><br/>Through close collaboration with research teams in statistical astronomy, pharmacometrics, power systems optimization, and high-performance computing, this project will deliver cyberinfrastructure that will effectively and effortlessly enable the next generation of computer-based scientific analysis and discovery. To ensure the practical applicability of the developed cyberinfrastructure, the project will focus on three target scientific applications: (i) economically viable decarbonization of electrical power networks, (ii) real-time analysis of extreme-scale astronomical image data, and (iii) pharmacometric modeling and simulation for drug analysis and discovery. While tackling these specific problems will constitute an initial stress test of the proposed cyberinfrastructure, it is the ultimate goal of the project that the developed tools be sufficiently performant, accessible, composable, flexible and adaptable to be applied to the widest possible range of problem domains. To achieve this vision, the project will build and improve various software tools for computational optimization, machine learning, parallel computing, and model-based simulation. Particular attention will be paid to the proposed cyberinfrastructure's composability with new and existing tools for scientific analysis and discovery. The pursuit of these goals will require the design and implementation of new programming language abstractions to allow close integration of high-level language features with low-level compiler optimizations. Furthermore, maximally exploiting proposed cyberinfrastructure will require research into new methods that combine state-of-the-art techniques from optimization, machine learning, and high-performance computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835821","Elements: Software: NSCI: A high performance suite of SVD related solvers for machine learning","OAC","DATANET","01/01/2019","09/06/2018","Andreas Stathopoulos","VA","College of William and Mary","Standard Grant","Vipin Chaudhary","12/31/2021","$600,000.00","Zhenming Liu","andreas@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7726","026Z, 077Z, 7923, 8004","$0.00","The accrual of vast amounts of data is one of the defining characteristics of our century. With the help of computers, scientists use this data to make and test hypotheses, draw inferences, predict complex phenomena, and make educated policy decisions. Machine learning (ML) is an area in computer science that uses statistical methods to allow computers to ""learn"" from data, with and without human supervision. Central to the application of machine learning methods is the numerical computation of the Singular Value Decomposition (SVD) of matrices of very large dimension, often larger than a million or even a billion. Since ""off-the-shelf"" algorithms and SVD software, however, cannot handle matrices of very large dimension, iterative methods used in scientific computing are more appropriate. Yet their stringent approximation quality requirements are often excessive for downstream applications, and result in slow execution times. Recently, methods based on randomization have improved execution times, but their implementations relax the approximation quality, often to detrimental levels. This project proposes to develop a software package that unifies randomized and iterative methods with a particular focus on the specific requirements of various ML applications and with high performance optimizations for modern computing platforms. This will allow scientists to analyze significantly larger datasets, ML researchers to study large models that could not be tackled before, and ML service providers to use the new solvers to reduce their operational cost. <br/><br/>This project proposes to develop a software package that unifies randomized and iterative methods with a particular focus on the specific requirements of various ML applications and with high performance optimizations for modern computing platforms. This will allow scientists to analyze significantly larger datasets, ML researchers to study large models that could not be tackled before, and ML service providers to use the new solvers to reduce their operational cost. Specifically, the software package builds upon the state-of-the-art eigenvalue/singular value software package PRIMME that integrates cutting-edge iterative methods and high-performance implementations. The development of the package consists of two thrusts: (T1) Unifying state-of-the-art algorithmic techniques including randomized, streaming, and iterative methods, to deliver consistent experience for a diverse range of matrices with different quality requirements, hardware platforms and precisions, and programming environments. (T2) Developing software devices that enable downstream systems and SVD solvers to interoperate so that users can tune and customize solvers without being experts in numeric linear algebra.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640775","CIF21 DIBBs: PD: Accelerating Comparative Metagenomics through an Ocean Cloud Commons","OAC","ADVANCES IN BIO INFORMATICS, DATANET, EarthCube","01/01/2017","07/21/2016","Bonnie Hurwitz","AZ","University of Arizona","Standard Grant","Amy Walton","12/31/2019","$496,064.00","John Hartman","bhurwitz@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","1165, 7726, 8074","7433, 8048, 9102","$0.00","The Tara Oceans Expedition has provided the largest publicly available contiguous dataset available in genomics for any scientific project in the world.   Using the research schooner Tara and modern sequencing and state-of-the-art imaging technologies, a multinational team of scientists sampled microscopic plankton at hundreds of sites and depths in all the major oceanic regions.  The Tara Oceans Expedition data have been released, but it is a challenge for researchers to access, manipulate, and analyze such large-scale resources.  This project creates an Ocean Cloud Commons (OCC), a cloud-based resource and repository allowing researchers to query the Tara Oceans Expedition Data in the cloud; it also makes available comparative metagenomic tools through the Ocean Treasure Box (OTB).<br/><br/>The Ocean Cloud Commons and Ocean Treasure Box build upon established partnerships with organizations such as CyVerse Cyberinfrastructure, Agave Platform, OpenCloud, and computing facilities at the Texas Advanced Computing Center.  The Ocean Cloud Commons uses an algorithm based on MapReduce to create a comparative metagenomics data resource in a Hadoop big data framework.  The OCC can be widely accessed by researchers using tools developed in the Ocean Treasure Box and implemented as Apps in the CyVerse Cyberinfrastructure.  Specifically, OTB tools deploy and compute on OCC data in OpenCloud via the Agave Platform and Developer API from CyVerse.  Taken together, the OTB tools and OCC data resources enable researchers to address global-scale questions about the distribution of microbes across the sea that affect climate and ecosystem function.<br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Biological Sciences (Division of Biological Infrastructure), and the NSF Directorate for Geosciences."
"1740333","SI2-SSE: Gunrock: High-Performance GPU Graph Analytics","OAC","Software Institutes","10/01/2017","08/24/2017","John Owens","CA","University of California-Davis","Standard Grant","Stefan Robila","09/30/2020","$400,000.00","","jowens@ece.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Many sets of data can be represented as ""graphs"". Graphs express relationships between entities, and those entities and relationships can be used to solve problems of interest in many fields. For instance, a social graph (like Facebook's) links people (entities) by friendships (relationships), and with that graph, Facebook can suggest people to you who might be your friends. Amazon might use a graph made of people and items for sale (entities) connected by who bought those items (relationships) to suggest items you might want to buy. A credit card company might look at your pattern of purchases and detect possible fraud even before you know your credit card was stolen. Graphs are also useful in many fields of science, such as genomics, epidemiology, and economics. This project uses an emerging programmable processor, the graphics processor (GPU), to solve graph problems. GPUs are rapidly moving into our nation's largest data centers and supercomputers. The project team is building a system for computation on graphs that will significantly improve performance on these problems. In this project, the team will work with the computing community and the scientific community, both of whom have numerous interesting, challenging graph computation problems that this system will target. The system is open-source software and can be used freely by researchers and industry all over the world.<br/><br/><br/>This project, supported by the Office of Advanced Cyberinfrastructure seeks to develop the ""Gunrock"" programmable, high-performance, open-source graph analytics library for graphics processors (GPUs) from a working prototype to a robust, sustainable, open-source component of the GPU computing ecosystem. Gunrock's strengths are its programming model and highly optimized implementation. With this work the project team hopes to address Gunrock's usability in the computing and scientific communities by improving Gunrock's scalability, capabilities, core operators, and supported graph computations. In this work the team will collaborate with the GPU Open Analytics Initiative and the NSF-sponsored CINET project for network science to ensure that our work has the broadest possible impact.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1657364","CRII: ACI: 4D Dynamic Anisotropic Meshing and Applications","OAC","CRII CISE Research Initiation","07/01/2017","12/21/2016","Zichun Zhong","MI","Wayne State University","Standard Grant","Sushil Prasad","06/30/2019","$174,969.00","","zichunzhong@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","026Y","8228","$0.00","There is an emerging need in healthcare, transportation, and simulation areas to realistically reconstruct, visualize, and capture space-time (4D) models/images (dynamic objects) from a complicated scenario in real-time. For example, high-fidelity dynamically modeling and visualizing 4D deformable shapes and variations of organs and their surrounding tissues in real-time become important for building an effective 4D model planning/capturing for radiation therapy (e.g., 4D-Doctor system). It requires dynamic anisotropic modeling and multi-modality imaging techniques for accurate registration, segmentation, and visualization. The goal of this project is to develop a tool for efficiently computing high-quality 4D dynamic anisotropic meshing models for complicated 4D objects with features and details in the large-scale volume image data. This project involves several disciplines, such as geometric modeling, computer graphics, and medical image processing, and has a potential to provide a high-quality platform for both interdisciplinary research and education integration. This project will aim to enrich the computer science and engineering curriculum at Wayne State University in both the undergraduate and graduate levels. Therefore, this research aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>A theoretical and computational framework for 4D dynamic anisotropic meshing with high quality and efficiency is essential for tackling challenges in ef&#64257;ciently representing and capturing the 4D data. The key strength of this project focuses on the transformative research ideas and approaches in particle-based approach for Riemannian metric mesh modeling, serving as a foundation for geometry-guided 3D/4D imaging informatics. The proposed exploratory research activities will address the following major themes and objectives: (1) to develop a novel particle-based method for high-quality 3D and 4D anisotropic tetrahedral meshing; (2) to evaluate the mesh quality and apply the proposed theoretical meshing approaches in applications to medical imaging, and develop a testbed system to evaluate its capability and potential in 4D-Doctor system, including 4D image registration and segmentation. The unified theoretical particle-based meshing framework, integrating Gaussian energy, dynamic Riemannian metrics, and high-dimensional embedding theory, can enable efficient generation of dynamic anisotropic meshes from a brand new perspective. This research initiative is innovative as it will establish a novel geometric modeling framework supporting 3D/4D imaging informatics."
"1443019","CIF21 DIBBs: DIBBs for Intelligence and Security Informatics Research Community","OAC","DATANET, Cyber Secur - Cyberinfrastruc","10/15/2014","09/08/2014","Hsinchun Chen","AZ","University of Arizona","Standard Grant","Amy Walton","09/30/2019","$1,499,531.00","Mark Patton, Catherine Larson","hchen@eller.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","7726, 8027","7433, 7434, 7726, 8027, 8048","$0.00","The growing number of cyber attacks on the Internet and other critical infrastructure has led to an increased sense of urgency in developing a better understanding of the motivation and methods behind such incursions. This project develops a research infrastructure for the Intelligence and Security Informatics (ISI) community comprised of experts across the computer, information, and social sciences. <br/><br/>The infrastructure consists of online archives and analysis tools. The archives contain a wide array of open source data including: discussions in online forums run by hackers, data from botnet command and control servers used to stage computer attacks, video streams and tweets and news summaries from economically and politically unstable states and regions. The analysis tools developed for this project support a range of research investigations. The social network analysis tool allows researchers to study how organizations form and how people interact with one another both virtually and in person. The data visualization tools are important for helping researchers pick out important patterns and trends in large sets of data of different types and from disparate sources. A new tool for adversarial data mining and deception detection allows researchers to deepen their enquiries and analysis of the intentions behind cyber-attacks. <br/><br/>Integrating these divergent data sources allows the security research community to more easily collaborate with other members of the community, rapidly test hypotheses, evaluate detection techniques, track down malicious actors, and identify weaknesses in a cyberinfrastructure network."
"1835782","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, DATANET","11/01/2018","08/23/2018","Wei Chen","IL","Northwestern University","Standard Grant","Amy Walton","10/31/2023","$599,778.00","","weichen@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","1712, 7726","054Z, 062Z, 077Z, 7925","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1649475","Cyberinfrastructure Leadership Academy","OAC","EDUCATION AND WORKFORCE","09/01/2016","09/13/2016","Henry Neeman","OK","University of Oklahoma Norman Campus","Standard Grant","Sushil Prasad","08/31/2019","$49,300.00","","hneeman@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","7361","026Z, 7556, 9150","$0.00","The Cyberinfrastructure Leadership Academy workshop's objective is to prepare a new generation of national Cyberinfrastructure (CI) leaders who can shape the national CI agenda, in order to promote the progress of science, which depend heavily and increasingly on the advanced digital capabilities that CI provides. The mechanism for accomplishing this goal is establishing mentoring relationships between senior CI leaders who are nearing retirement or recently retired, and emerging midcareer CI leaders. A mushrooming fraction of Science, Technology, Engineering and Mathematics (STEM) education, research, development and commerce is computing- and/or data-intensive, and therefore depends on CI. The CI workforce pipeline has several stages, and one of the most vital, senior CI leaders, is rapidly narrowing as Baby Boomers retire. This outmigration presents a significant challenge to the burgeoning computing- and data-intensive research community, which depends heavily on senior CI leaders not only to oversee CI design, development, implementation and facilitation, but also to shape the research CI ecosystem and agenda, both locally at their own institutions and especially nationally. Crucial to surviving this expertise drain is capturing as much of that expertise as possible, before it is irretrievably lost. Establishing mentoring relationships between senior CI leaders and emerging midcareer CI leaders is invaluable, because such relationships not only enable the transfer of knowledge, experience and wisdom, but also serve to solidify the ability of emerging CI leaders to continue to draw on the expertise of senior CI leaders even after the latter retire.  To date, little emphasis has been placed on the senior end of the pipeline. The CI Leadership Academy workshop's intends to address this gap, by stanching the outflow of CI agenda/ecosystem/community expertise.<br/><br/>The Cyberinfrastructure Leadership Academy workshop takes the approach of convening a combination of senior CI leaders, many of whom are nearing retirement or have recently retired, and emerging midcareer CI leaders, to meet the following objectives: (1) Transfer knowledge, experience and especially wisdom from senior CI leaders to emerging CI leaders, in order to enable emerging CI leaders to shape the national research CI landscape. (2) Initiate mentoring relationships between senior CI leaders and emerging CI leaders, in order to foster longer term professional development. (3) Establish peer mentoring relationships among emerging CI leaders, in order to prepare and position them for national leadership, as senior CI leaders reduce their day to day engagement. Important components of this workshop's process include: (a) White papers from the senior and emerging CI leaders on topics directly relevant to the national research CI ecosystem/community are a valuable component in mentorship matchmaking, by helping senior and emerging CI leaders to understand each other. (b) Perspective panels offer the viewpoints of stakeholders such as research funding agencies, industry, and researcher constituencies. (c) Brief talks by senior CI leaders on their experiences, and sit-downs between senior CI leaders and emerging midcareer leaders, are designed to establish both professional and personal linkages. This workshop includes a focus on emerging CI leaders from populations that have traditionally been underrepresented in CI leadership, including (a) women, (b) underrepresented minorities, (c) Minority Serving Institutions, (d) non-PhD-granting institutions, and (e) institutions in EPSCoR jurisdictions."
"1835677","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, DATANET, Software Institutes","11/01/2018","08/23/2018","L. Brinson","NC","Duke University","Standard Grant","Amy Walton","10/31/2023","$2,252,971.00","Cynthia Rudin","cate.brinson@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1712, 7726, 8004","026Z, 054Z, 062Z, 077Z, 7925","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835778","Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling","OAC","DATANET","11/01/2018","08/23/2018","Ryan Abernathey","NY","Columbia University","Standard Grant","Amy Walton","10/31/2022","$254,103.00","","ra2697@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7726","062Z, 077Z, 7925","$0.00","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation.  A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. <br/><br/>The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations.  The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems.  Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis.   The software framework from this project is expected to handle petascale to exascale datasets for users.  Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store.  The broader target is next generation simulation software in the geosciences and other disciplines.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642336","SI2-SSE: Enabling Chemical Accuracy in Computer Simulations: An Integrated Software Platform for Many-Body Molecular Dynamics","OAC","Software Institutes","04/01/2017","03/27/2017","Francesco Paesani","CA","University of California-San Diego","Standard Grant","Stefan Robila","03/31/2020","$499,918.00","Andreas Goetz, Andrea Zonca","fpaesani@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8004","026Z, 7433, 8004, 8005, 9216, 9263","$0.00","This project is jointly funded by the Office of Advanced Cyberinfrastructure and and the Division of Chemistry within the Directorate of Mathematical and Physical Sciences. As attested by the 2013 Nobel Prize in Chemistry awarded to Martin Karplus, Michael Levitt, and Arieh Warshel, molecular-level computer simulations have become indispensable in many research areas, including chemistry, physics, materials science, and biochemistry, and often provide fundamental insights that are otherwise difficult to obtain. Nowadays, computer simulations are used to complement, guide, and sometimes replace experimental measurements, reducing the amount of time and money spent on research to bring ideas from the lab to practical applications. In the pharmaceutical industry computer simulations play a key role in structure-based drug design as demonstrated by the development of HIV protease inhibitors. In the chemical industry, computer modeling guides the design of new catalysts as well as novel materials for applications in more efficient batteries, and fuel and solar cells. More recently, there has been significant success in using of computer simulations to design more effective chemical processes as well as to provide information on safety issues. However, both the realism and the predictive power of a molecular-level computer simulation directly depend on the accuracy with which the interactions between molecules are described. To address the limitations of existing simulation approaches, the project group has recently developed a new theoretical/computational methodology that has been shown to display unprecedented accuracy when applied to a variety of molecular systems. The overarching goal of the proposed research is the implementation of this new methodology into an integrated and publicly available software platform that will allow the scientific community to address a broad range of problems through computer simulations. Potential applications include, but are not limited to, the rational design of new drugs as well as novel materials for water purification and the detection of toxic compounds and explosives, the virtual screening of catalysts for more efficient chemical processes, the development of new batteries, solar and fuel cells, and biomolecular structure prediction. A diverse group of high school, undergraduate, and graduate students will be directly involved in different aspects of the proposed research. The students will thus acquire critical knowledge about computer simulations and programming, which will significantly enhance their competitiveness in today's computer-driven job market. Given its multidisciplinary and multifaceted nature, the proposed research will promote scientific progress at different levels and contribute to the development of new technologies that will advance the national health, prosperity and welfare, as well as secure the national defense.<br/><br/>The proposed research focuses on the development and implementation of unique software elements that will enable computer simulations on both CPU and GPU architectures using the so-called many-body molecular dynamics (MB-MD) methodology developed by the Paesani group. These software elements will be made publicly available to the scientific community through an integrated platform. MB-MD is a new simulation methodology that has already been shown to provide unprecedented accuracy in molecular simulations of a variety of molecular systems from the gas to the condensed phase. The new software elements comprise three components integrated in a unique software platform: a suite of publicly available computational tools for the automated generation of many-body potential energy functions from electronic structure data; a client-server architecture for the calculation of the required electronic structure data through volunteer computing; independent CPU and GPU plugins for the OpenMM toolkit which will enable MB-MD simulations of generic molecular systems across different phases. In parallel with the proposed research and software engineering projects, outreach and mentoring activities to promote STEM disciplines among students from underprivileged and underrepresented minorities through the PI and Co-PI direct involvement in several outreach programs at UC San Diego and the San Diego Supercomputer Center. These activities are specifically designed to increase the involvement and advancement of women, minorities, and economically disadvantaged groups across different education levels, from high school to undergraduate and graduate students."
"1535070","SI2-SSE: AttackTagger: Early Threat Detection for Scientific Cyberinfrastructure","OAC","Software Institutes","09/01/2015","09/08/2015","Alexander Withers","IL","University of Illinois at Urbana-Champaign","Standard Grant","Stefan Robila","08/31/2019","$499,136.00","Ravishankar Iyer, Randal Butler, Zbigniew Kalbarczyk, Adam Slagell","alexw1@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004","7433, 8004, 8005","$0.00","The cyber infrastructure that supports science research (such as the cyberinfrastructure that provides access to unique scientific instrumentation such as a telescope, or an array of highly distributed sensors placed in the field, or a computational supercomputing center) faces the daunting challenge of defending against cyber attacks. Modest to medium research project teams have little cyber security expertise to defend against the increasingly diverse, advanced and constantly evolving attacks. Even larger facilities that have with security expertise are often overwhelmed with the amount of security log data they need to analyze in order to identify attackers and attacks, which is the first step to defending against them.  The challenges of the traditional approach of identifying an attacker are amplified by the lack of tools and time to detect attacks skillfully hidden in the noise of ongoing network traffic. The challenge is not necessarily in deploying additional monitoring but to identify this malicious traffic by utilizing all available information found in the plethora of security, network, and system logs that are already being actively collected.  This project proposes to build and deploy,  is needed in research environments, an advanced log analysis tool, named AttackTagger, that can scale to be able to address the dramatic increase in security log data, and detect emerging threat patterns in today's constantly evolving security landscape. AttackTagger will make science research in support of national priorities more secure.<br/><br/>AttackTagger will be a sophisticated log analysis tool designed to find potentially malicious activity, such as credential theft, by building factor graph models for advanced pattern matching.  AttackTagger will integrate with existing security software so as to be easily deployable within existing security ecosystems and to offload processing and computational work onto better suited components.  It can consume a wide variety of system and network security logs.  AttackTagger accomplishes advanced pattern matching by utilizing a Factor Graph model, which is a type of probabilistic graphical model that can describe complex dependencies among random variables using an undirected graph representation, specifically a bipartite graph. The bipartite graph representation consists of variable nodes representing random variables, factor nodes representing local functions (or factor functions , and edges connecting the two types of nodes. Variable dependencies in a factor graph are expressed using a global function, which is factored into a product of local functions. In the practice of the security domain, using factor graphs is more flexible to define relations among the events and the user state compared to Bayesian Network and Markov Random Field approaches. Specifically, using factor graphs allows capturing sequential relation among events and enables integration of the external knowledge, e.g., expert knowledge or a user profile."
"1725938","MRI: Acquisition of a High Performance Computing Cluster for Multi-Disciplinary Research & Education at a Primarily Undergraduate Institution","OAC","MAJOR RESEARCH INSTRUMENTATION","10/01/2017","09/18/2017","Gillian Ryan","MI","Kettering University","Standard Grant","Stefan Robila","09/30/2020","$395,975.00","James Cohen, Farnaz Ghazi Nezami, Matthew Causley, Salomon Turgman Cohen","gryan@kettering.edu","1700 University Ave","Flint","MI","485046214","8107629677","CSE","1189","1189","$0.00","In this project, faculty from Kettering University will acquire and operate a centralized High Performance Computing (HPC) resource called the Kettering University High Performance Computing cluster (KUHPC). KUHPC will provide capabilities to Kettering University to facilitate both computation- and data- intensive research on campus. Kettering is a primarily undergraduate institution specializing in experiential learning and cooperative education within science, engineering, math, and business fields. Current computing resources available on campus are insufficient to support the scope of computational programs faculty wish to pursue both in research and teaching. Reliable, dedicated computational and data-storage resources are needed on campus to support and grow existing faculty research programs, and to train and recruit high-quality students in the future. The establishment of a centralized computational resource will support ongoing research projects, allow for the development of new research projects, and advance the use of computational techniques both in research and in undergraduate coursework across multiple Science, Technology, Engineering, and Math (STEM) disciplines.<br/><br/>KUHPC will enable a growing number of multi-disciplinary faculty at Kettering University to use computational tools in their research. The project team represents nine different research groups across a broad range of STEM disciplines that would immediately benefit from the acquisition of the HPC resource. From mapping generational differences in DNA sequences and simulating dynamic protein structures, to modeling the passage of electromagnetic waves through complex media and optimizing industrial manufacturing processes - the diverse projects will all directly benefit from a centralized HPC resource. KUHPC will deliver large-scale parallel computation that will significantly shorten the modeling, simulation, and analysis time for all the research projects.<br/><br/>Moreover, KUHPC will facilitate large-scale student exposure to applied computational research projects not currently possible on campus. Computational research techniques and tools are essential for state-of-the art work in multiple STEM fields, and future Kettering students will gain these essential skills through both coursework and on-campus research experiences. The acquisition of the HPC resource would allow for the inclusion of computationally intensive projects or modules in coursework across a wide range of disciplines. Access to an HPC resource would provide students the opportunity to work with more accurate and complex computational models than are currently possible. Allowing students to work with a HPC resource as undergraduates will provide them with strong technical and computational skills that are transferrable to both industry and graduate studies across disciplines."
"1740250","SI2:SSE: MAtrix, TEnsor, and Deep-Learning Optimized Routines (MATEDOR)","OAC","Software Institutes","09/01/2017","09/25/2018","Azzam Haidar","TN","University of Tennessee Knoxville","Standard Grant","Stefan Robila","08/31/2020","$400,000.00","Stanimire Tomov","haidar@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","8004","7433, 8004, 8005","$0.00","A number of scientific software applications from important fields, including applications in deep learning, data mining, astrophysics, image and signal processing, hydrodynamics, and more, do many computations on small matrices (also known as ""tensors"") and using widely available standard linear-algebra software libraries. Scientists are trying to make these applications run faster by running them on advanced high performance computing (HPC) systems, that are heterogeneous systems that use processors of many different types, such as ""accelerators"" - that use of specialized computer hardware to perform some functions more efficiently than standard, general-purpose processors - and ""co-processors"" - that can run certain specialized functions in parallel with the central processor. However, standard linear algebra software libraries cannot make use of these specialized hardware components, and so the scientific applications mentioned above do not become much faster. Many existing linear algebra libraries, including libraries supplied by commercial vendors of computing technology have been tried to no avail. This issue is now critical because advancements in science from important fields are being held back due to the lack of progress in speeding up software. This project will address this through research and development that will create efficient software that can repetitively execute tensor operations grouped together in ""batches"" and which can be written to run very efficiently and quickly on the types of hardware components that exist in HPC systems. In addition to the research and development, several students will be engaged in the project, thus helping develop a critically needed component of the U.S. workforce.<br/><br/>The trend in high performance computing (HPC) toward large-scale, heterogeneous systems with GPU accelerators and coprocessors has made the near total absence of linear algebra software for small matrix or tensor operations especially noticeable. Given the fundamental importance of numerical libraries to science and engineering applications of all types, the need for libraries that can perform batched operations on small matrices or tensors has become acute. This MAtrix, TEnsor, and Deep-learning Optimized Routines (MATEDOR) project seeks to provide a solution to this problem by developing a sustainable and portable library for such small computations. Future releases of MATEDOR are expected to have a significant impact on application areas that use small matrices and tensors and need to exploit the power of advanced computing architectures. Such application areas include deep-learning, data mining, metabolic networks, computational fluid dynamics, direct and multi-frontal solvers, image and signal processing, and many more. This team has a proven record of providing software infrastructure that is widely adopted and used, that supports ongoing community contributions, and that becomes incorporated in vendor libraries (e.g., Intel's MKL and NVIDIA's CUBLAS) and other software tools and frameworks (e.g., MATLAB and R). Students will be regularly integrated into the project activities, and this group of PIs has an exceptionally strong record of community outreach, having given numerous performance optimization and software tutorials at conferences and Users Group meetings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659142","CC* Network Design: Network Infrastructure to Support Computational Biology, Computational Science, and Computing Across the Curriculum","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2017","12/21/2016","Bruce Maxwell","ME","Colby College","Standard Grant","Kevin Thompson","06/30/2019","$380,246.00","Charles Wray, Bruce Segee, Andrea Tilden, David Angelini","bmaxwell@colby.edu","4000 Mayflower Hill","Waterville","ME","049018840","2078594342","CSE","8080","9150","$0.00","The research partnership of Colby College, the University of Maine, and the Jackson Laboratory are building a dedicated research network to support projects in computational biology, computational physics and astrophysics, computational chemistry, and machine learning, The network is also expanding the capabilities available to faculty in the classroom, enabling more complex and modern classroom and laboratory activities.<br/><br/>The project provides 10Gb network connectivity to all academic buildings at Colby College to give all faculty access to state-of-the-art computing and data facilities, which is critical given the trends in all academic disciplines towards greater use of digital and computing resources. The project also creates a secure dedicated 10Gb research network connecting the Advanced Computing Group resources at the University of Maine and the large genomic databases at the Jackson Laboratory with researchers in variety of disciplines at Colby College.<br/><br/>The new research network provides the capability for researchers at Colby to obtain fast access to gigabyte and terabyte scale data sets from Jackson Laboratory or other sources and make use of advanced computing resources at the University of Maine with a high bandwidth and high availability connection. Using this infrastructure, researchers are able to execute analyses and simulations, and professors are able to assign projects and exercises in courses, that were not previously possible with the old infrastructure."
"1649658","Computational Infrastructure for Brain Research: EAGER: A Computationally Enabled Knowledge Infrastructure for Cognitive Neuroscience","OAC","ETF, IntgStrat Undst Neurl&Cogn Sys","03/01/2017","08/22/2016","Russell Poldrack","CA","Stanford University","Standard Grant","William Miller","02/29/2020","$291,928.00","","poldrack@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7476, 8624","026Z, 7916, 8089, 8091","$0.00","The Cognitive Atlas is a major online resource for testing hypotheses about the association of neural function to anatomical structures in the human brain. The goal of this project is to greatly enhance the capabilities and performance of the Atlas using state-of-the-art information technologies and national high performance computing (HPC) resources.  The result will be a powerful computational tool to assist neuroscience researchers in understanding cognitive and psychological processes and making discoveries about how the brain works, in alignment with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>The first aim of the project is to restructure the current Cognitive Atlas database so that it will be able to interface more directly with other computing systems, and so that it can be changed in a more flexible way based on new research.  The second aim is to utilize a powerful knowledge query system called Deep Dive to extract new knowledge from neuroscience publications and integrate this knowledge into the Cognitive Atlas.  To address the intensive computational demands of Deep Dive, an implementation will be piloted on Wrangler, a high performance data analytics computing system hosted at the Texas Advanced Computing Center.  The proposed project will enhance the computing infrastructure for neuroscience in multiple ways. By updating the system to a state-of-the-art graph database infrastructure, this work will allow much greater use of the system for automated analyses. In addition to enabling analysis of the cognitive neuroscience literature at scale for the current project, the implementation of Deep Dive on Wrangler will allow researchers in many other fields to also take advantage of this state-of-the-art application, which could have important benefits across many different areas of science and technology. <br/><br/>This Early-concept Grants for Exploratory Research (EAGER) award by the CISE Division of Advanced Cyberinfrastructure is jointly supported by the SBE Division of Behavioral and Cognitive Sciences and the CISE Division of Information and Intelligent Systems, with funds associated with the NSF Understanding the Brain, BRAIN Initiative activities, and for developing national research infrastructure for neuroscience. This project also aligns with NSF objectives under the National Strategic Computing Initiative."
"1839321","CICI: SSC: Real-Time Operating System and Network Security for Scientific Middleware","OAC","Cyber Secur - Cyberinfrastruc","10/01/2018","08/17/2018","Gedare Bloom","DC","Howard University","Standard Grant","Micah Beck","09/30/2021","$999,915.00","","gedare.bloom@howard.edu","2400 Sixth Street N W","Washington","DC","200599000","2028064759","CSE","8027","","$0.00","Remote monitoring and control of industrial control systems are protected using firewalls and user passwords. Cyberattacks that get past firewalls have unfettered access to command industrial control systems with potential to harm digital assets, environmental resources, and humans in proximity to the compromised system.  To prevent and mitigate such harms in scientific industrial control systems, this project enhances the security of open-source cyberinfrastructure used for high energy physics, astronomy, and space sciences. The results of this project enhance the security of scientific instruments used in particle accelerators, large-scale telescopes, satellites, and space probes. The benefits to science and the public include greater confidence in the fidelity of experimental data collected from these scientific instruments, and increased reliability of scientific cyberinfrastructure that reduces the costs associated with accidental misconfigurations or malicious cyberattacks.<br/><br/>The objective of this project is to enhance the security of the open-source Real-Time Executive for Multiprocessor Systems (RTEMS) real-time operating system and the Experimental Physics and Industrial Control System (EPICS) software and networks; RTEMS and EPICS are widely used cyberinfrastructure for controlling scientific instruments. The security enhancements span eight related project activities: (1) static analysis and security fuzzing as part of continuous integration; (2) cryptographic security for the open-source software development life cycle; (3) secure boot and update for remotely-managed scientific instruments; (4) open-source cryptographic libraries for secure communication; (5) real-time memory protection; (6) formal modeling and analysis of network protocols; (7) enhanced security event logging; and (8) network-based intrusion detection for scientific industrial control systems. The project outcomes provide a roadmap for enculturating cybersecurity best practices in open-source, open-science communities while advancing the state-of-the-art research in cyberinfrastructure software engineering and industrial control system security.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1550481","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","OAC","Software Institutes","08/01/2016","07/21/2016","Toru Shiozaki","IL","Northwestern University","Standard Grant","Micah Beck","07/31/2020","$600,000.00","","shiozaki@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","8004","7433, 8009, 9216","$0.00","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance:  industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products.  This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting."
"1657286","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","OAC","Software Institutes","07/21/2016","09/20/2016","Garnet Chan","CA","California Institute of Technology","Standard Grant","Bogdan Mihaila","07/31/2020","$600,000.00","","garnetc@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","8004","7433, 8009, 9216","$0.00","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance:  industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products.  This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting."
"1550456","Collaborative Research: SI2-SSI: Software Framework for Electronic Structure of Molecules and Solids","OAC","Software Institutes","08/01/2016","07/21/2016","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Standard Grant","Bogdan Mihaila","07/31/2020","$600,000.00","","evaleev@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8004","7433, 8009, 9216","$0.00","Many traditionally experimental disciplines such as chemistry and materials science are rapidly changing due to our increasing ability to predict properties of molecules and materials purely by simulation. This is particularly true when molecules meet solid surfaces - due to the particular challenges of experiments in such a setting. Yet the molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance:  industrial applications facilitated by surface processes are estimated to produce globally more than than 15 trillion USD worth of goods and products.  This research will improve our ability to simulate the physics and chemistry of molecules on surfaces by extending the advanced simulation methodologies that were originally developed by for modeling electrons in molecules. This project will not only advance our fundamental understanding of the surface science but also open a road to technological applications relevant to producing and storing clean energy and in designing improved catalysts. The research may result in a new computer software framework for simulating electrons in molecules and materials. This software will be a unique contribution to the U.S. cyberinfrastructure and spur further innovation by other researchers in the US and worldwide, who will be able to access its source code for free. The software framework will also serve as an education platform for training computational chemists and materials scientists.<br/><br/>A frontier simulation challenge lies at the intersection of the two domains of chemistry and materials science - namely to determine, with predictive accuracy, the properties and chemistry of molecules on solid surfaces. The molecule-on-surface frontier encompasses a vast class of problems of tremendous practical importance: heterogeneous catalysis, photovoltaics, and emerging electronic materials. Yet, from a simulation perspective, it is not currently possible to efficiently combine the recent advances in highly accurate many-body molecular and periodic condensed phase methodologies in these problems, due to a significant gap between how the electronic structure theories of molecules and materials are formulated, as reflected in distinct algorithms and disjoint codebases. The goal of this project is to reduce and/or completely eliminate the gap between molecular and solid-state electronic structure methodologies, in theory, algorithms, and in usable community software implementations. This will be achieved by building an ambitious Electronic structure for Molecules and Solids (EMOS) software framework that will permit accurate computation of the first-principles electronic structure of both molecules and solids on an equivalent footing - and with the high efficiency necessary for high-throughput screening or ab initio molecular dynamics. These efforts build on the leading track-record of the principal investigators in developing open-source quantum chemistry software as well as automated computer implementation and high-performance parallel libraries. The project will allow the advances from molecular electronic structure - embedding, reduced-scaling many-body methodology, accurate excited-state electronic structure, and others - to be applied routinely to molecules, materials, and combinations of the two as relevant to surface chemistry. This has great potential to advance the state-of-the-art in treatment of electronic structure and open new lines of theoretical inquiry. The resulting open-source production-quality toolkit will be validated against experimental data for a host of surface phenomena, from exciton dynamics to surface spectroscopy and catalysis. An open-source US-based advanced materials code is a long-standing omission in U.S. cyberinfrastructure. As a high-performance framework for simulation of electronic structure of molecules, solids, and their interfaces with unprecedented accuracy, EMOS will be a significant contribution to this effort. Further, the modular component based structure will be able to be integrated with other major electronic structure packages through the reuse of the modules. This project will provide invaluable training opportunities to the students and postdocs who will develop the software framework under the direct supervision of principal investigators. In addition, each project site will contribute to the development of a stakeholder network for EMOS by hosting, each summer, visiting students and faculty representing the broader theoretical community, to train them on the use of EMOS in research and education. The project team will also use EMOS in teaching classes and summer schools, building on already established efforts in this area; these efforts will also be extended to an online setting."
"1835426","Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E)","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Brian O'Shea","MI","Michigan State University","Standard Grant","Bogdan Mihaila","08/31/2021","$480,055.00","","oshea@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7925, 8004","$0.00","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, PIs at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics.  The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. <br/><br/>The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The PIs have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1652541","CAREER: Numerical Methods and Computational Infrastructure for Simulating Prosthetic Heart Valve Function and Dysfunction","OAC","CAREER: FACULTY EARLY CAR DEV, COMPUTATIONAL MATHEMATICS, MSPA-INTERDISCIPLINARY","08/01/2017","02/17/2017","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Standard Grant","Sushil Prasad","07/31/2022","$500,000.00","","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1045, 1271, 7454","1045, 8007, 9263","$0.00","This project will advance mathematical methods and computational software for simulating the dynamics of prosthetic heart valves.  In the United States, heart valve diseases affect approximately 8.5% of the population aged 65 to 74 years, and approximately 13.3% of those aged 75 years or older.  Treatment for severe aortic valve disease is generally to replace the heart valve with a mechanical or bioprosthetic valve, and approximately 70,000 aortic valve replacements are performed in the U.S. each year.  Bioprosthetic heart valves (BHVs) degrade over time, however, and these devices have a typical durable lifetime of only 10-15 years.  To date, there have been relatively few studies on modeling BHV fatigue, and there are no predictive models of BHV failure.  The methods created in this project can yield advanced fluid-structure interaction (FSI) models of heart valve function as well as the first FSI modeling tools for simulating BHV valve failure.  By advancing broadly applicable FSI modeling technologies and their specific application to heart valve dynamics, the project aligns with NFS's mission of promoting the progress of science and advancing national health.<br/><br/>This project will integrate higher-order immersed boundary (IB) methods, peridynamics, and turbulence modeling to predict prosthetic heart valve function and dysfunction (fatigue and failure).  The project will advance cardiovascular FSI simulation accuracy by developing and analyzing two classes of sharp-interface immersed boundary (IB) methods capable of using experimentally constrained elasticity models with complex, three-dimensional geometries and integrating large-eddy simulation (LES) turbulence models with these new FSI methods.  This project also aims to develop new FSI methods for simulating tissue failure by integrating IB methods with peridynamics, which is a nonlocal formulation of solid mechanics that is well suited for modeling structural failure.  These models will be validated using experimental data obtained in ongoing studies at the FDA Heart Valve Lab.  These methods will be implemented within the open-source IBAMR software, which is used by many independent research groups in a broad range of fields.  This project also integrates research aims with an educational plan leverages UNC's outstanding high-performance computing environment, experimental facilities available to UNC's applied math group, and UNC's major new makerspace initiative, BeAM (Be a Maker).  Finally, research and educational products of this project will be incorporated into ongoing activities at UNC's main science outreach unit, the Morehead Planetarium & Science Center."
"1835560","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Jeremy Palmer","TX","University of Houston","Standard Grant","Bogdan Mihaila","09/30/2021","$147,746.00","","jcpalmer@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835067","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Joern Ilja Siepmann","MN","University of Minnesota-Twin Cities","Standard Grant","Bogdan Mihaila","09/30/2021","$238,958.00","","siepmann@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835630","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Edward Maginn","IN","University of Notre Dame","Standard Grant","Bogdan Mihaila","09/30/2021","$372,486.00","","ed@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835593","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Eric Jankowski","ID","Boise State University","Standard Grant","Bogdan Mihaila","09/30/2021","$235,000.00","","ericjankowski@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835612","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Sharon Glotzer","MI","University of Michigan Ann Arbor","Standard Grant","Bogdan Mihaila","09/30/2021","$450,000.00","","sglotzer@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835120","Collaborative Research: Elements: Software: Accelerating Discovery of the First Stars through a Robust Software Testing Infrastructure","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/31/2018","Jonathan Pober","RI","Brown University","Standard Grant","Bogdan Mihaila","08/31/2021","$250,933.00","","Jonathan_Pober@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The birth of the first stars and galaxies 13 billions years ago -- our ""Cosmic Dawn"" -- is one of the last unobserved periods in the history of the Universe. Scientists are working to observe the 21 cm radio light emitted by the primeval neutral hydrogen fog as the first stars formed.  These observations are considered one of the grand challenges of modern astrophysics. This project will provide critical software infrastructure for the field of 21 cm cosmology, enabling rapid vetting of the new analyses and techniques developed for these observations and increasing their robustness, rigor, and reproducibility. Under this project The invetigators will train students in the best practices for software and code development, preparing them to develop robust, reproducible software for their own research, contribute to large open source projects, and develop software in a professional setting.<br/><br/>One of the biggest challenges for the detection of the Epoch of Reionization is the presence of bright astrophysical foregrounds that obscures the signal of interest, requiring extraordinarily precise modeling and calibration of the radio telescopes performing these observations. The 21 cm cosmology community is rapidly developing new techniques for instrument calibration, foreground removal, and analysis, but thorough testing and integration into existing data analysis pipelines has been slow. This project will provide a software infrastructure that can enable rigorous, seamless testing of novel algorithmic developments within a unified framework. This infrastructure will ensure a level of reliability and reproducibility not possible with current tools and accelerate the speed at which developments become integrated into production level code, providing an invaluable foundation for bringing our field into the next decade and for leveraging the current NSF investments in these experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835421","Collaborative Research: Elements: Software: Accelerating Discovery of the First Stars through a Robust Software Testing Infrastructure","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/31/2018","Bryna Hazelton","WA","University of Washington","Standard Grant","Bogdan Mihaila","08/31/2021","$344,613.00","Miguel Morales","brynah@phys.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7923, 8004","$0.00","The birth of the first stars and galaxies 13 billions years ago -- our ""Cosmic Dawn"" -- is one of the last unobserved periods in the history of the Universe. Scientists are working to observe the 21 cm radio light emitted by the primeval neutral hydrogen fog as the first stars formed.  These observations are considered one of the grand challenges of modern astrophysics. This project will provide critical software infrastructure for the field of 21 cm cosmology, enabling rapid vetting of the new analyses and techniques developed for these observations and increasing their robustness, rigor, and reproducibility. Under this project The investigators will train students in the best practices for software and code development, preparing them to develop robust, reproducible software for their own research, contribute to large open source projects, and develop software in a professional setting.<br/><br/>One of the biggest challenges for the detection of the Epoch of Reionization is the presence of bright astrophysical foregrounds that obscures the signal of interest, requiring extraordinarily precise modeling and calibration of the radio telescopes performing these observations. The 21 cm cosmology community is rapidly developing new techniques for instrument calibration, foreground removal, and analysis, but thorough testing and integration into existing data analysis pipelines has been slow. This project will provide a software infrastructure that can enable rigorous, seamless testing of novel algorithmic developments within a unified framework. This infrastructure will ensure a level of reliability and reproducibility not possible with current tools and accelerate the speed at which developments become integrated into production level code, providing an invaluable foundation for bringing our field into the next decade and for leveraging the current NSF investments in these experiments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835402","Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E)","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Michael Norman","CA","University of California-San Diego","Standard Grant","Bogdan Mihaila","08/31/2021","$479,999.00","James Bordner","mlnorman@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7925, 8004","$0.00","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics.  The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. <br/><br/>The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835509","Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E)","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","Greg Bryan","NY","Columbia University","Standard Grant","Bogdan Mihaila","08/31/2021","$439,350.00","","gbryan@astro.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7925, 8004","$0.00","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics.  The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. <br/><br/>The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835213","Collaborative Research:Framework:Software:NSCI:Enzo for the Exascale Era (Enzo-E)","OAC","OFFICE OF MULTIDISCIPLINARY AC, , Software Institutes","09/01/2018","08/14/2018","John Wise","GA","Georgia Tech Research Corporation","Standard Grant","Bogdan Mihaila","08/31/2021","$481,436.00","","jwise@physics.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1253, 1798, 8004","026Z, 077Z, 1206, 7569, 7925, 8004","$0.00","The earliest stages of the formation of galaxies and quasars in the universe are soon to be explored with a powerful new generation of ground and space-based observatories. Broad and deep astronomical surveys of the early universe beginning in the next decade using the Large Synoptic Survey Telescope and the James Webb Space Telescope will revolutionize our understanding of the origin of galaxies and quasars, and help constrain the nature of the dark matter which is the dominant matter constituent in the universe. Detailed physical simulations that model the formation of these objects are an indispensible aid to understanding the coming glut of observational data, and to maximize the scientific return of these instruments. In this project, investigators at the Univ. California San Diego, Columbia Univ., Georgia Tech, and Michigan State Univ. are collaborating with the goal of developing a next generation community simulation software framework for the coming generation of supercomputers for cosmological simulations of the young universe. Undergraduate and graduate students will be directly involved in the software development as well as its application to several frontier cosmological research topics.  The software framework that will be produced will be disseminated as open source software to enable a much broader range of scientific explorations of astrophysical topics. <br/><br/>The project brings together the key developers of the open source Enzo adaptive mesh refinement (AMR) hydrodynamic cosmology code, who will port its software components to a newly developed AMR software framework called Cello. Cello implements the highly scalable array-of-octrees AMR algorithm on top of the powerful Charm++ parallel object system. Designed to be extensible and scalable to millions of processors, the new framework, called Enzo-E, will target exascale high performance computing (HPC) systems of the future. Through this project, the entire Enzo community will have a viable path to exascale simulations of unprecedented size and scope. The investigators have chosen three frontier problems in cosmology to drive the development of the Enzo-E framework: (1) the assembly of the first generation of stars and black holes into the first galaxies; (2) the role of cosmic rays in driving galactic outflows; and (3) the evolution of the intergalactic medium from cosmic dawn to the present day. Annual developer workshops and software releases will keep the broader research community informed and involved in the developments.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Astronomical Sciences in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659182","CC* Network Design: Upgrading the University of Guam Network to Connect to Internet2 and Create a Science DMZ","OAC","Campus Cyberinfrastrc (CC-NIE)","06/15/2017","06/06/2017","Rommel Hidalgo","GU","University of Guam","Standard Grant","Kevin L. Thompson","05/31/2019","$250,000.00","Romina King, Terry Donaldson","rhidalgo@triton.uog.edu","UOG Station","Mangilao","GU","969131800","6717352994","CSE","8080","","$0.00","The University of Guam (UoG) is partnering with two leadership institutions, the University of Hawai'i (UH) and the Network Startup Resource Center (NRSC) at University of Oregon to develop an advanced network designed to support research, education, and scientific discovery at the University of Guam. Together with UH and the NSRC, this project supports the implementation of an advanced campus network with a 10 Gigabit per second network connection between UoG and Internet2, capable of supporting high-speed exchanges of very large scientific data sets between Guam and other US research institutions. This project leverages two other NSF projects including the UH IRNC PIREN: the Pacific Islands Research and Education Network (ACI-1451058), and the NSRC's IRNC ENgage: Building Network Expertise and Capacity for International Science Collaboration (ACI-1451045). <br/><br/>The success of this Campus CyberInfrastructure project will lead to the creation of critical networking infrastructure and a skilled cyber-team capable of connecting the University of Guam to the global and regional research and education networking fabric. Over the long term, this project enables Guam to participate as a Trans-Pacific Research and Education Networking Hub for the Pacific Island Region, one of the most underserved regions worldwide. This investment advances the university's research and education mission by enabling UoG to participate with its national peers and improve the productivity and opportunities of its faculty."
"1847753","Network Embedded Storage and Compute (NESCO)","OAC","Campus Cyberinfrastrc (CC-NIE)","08/15/2018","02/14/2019","Xi Yang","MD","University of Maryland College Park","Standard Grant","Kevin Thompson","07/31/2020","$299,817.00","Xi Yang","maxyang@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","8080","7916","$0.00","The Network Embedded Storage and Compute (NESCO) project is motivated by a premise that future Research and Education networks should include ""embedded compute and storage"" functions as a mechanism to provide advanced services for domain science application workflows. This is a natural follow-on to the current cyberinfrastructure evolution revolving around ""Cloud"" and ""Edge"" computing.  The NESCO project is developing the next important trend based on a distributed ecosystem of compute and storage facilities which are embedded deeply within networks.  These will be similar in function to on-premise edge cloud deployments but will be customized for placement within networks.  Application workflow developers will have access to a new class of services which can be rapidly tailored to specific flows and use cases.  One result anticipated is the ""democratization of middlebox"" functions in a manner where application workflow developers can customize their workflows and data processing on real-time basis with flow level granularity.  This project includes a climate science workflow as a prototype use case.  It is anticipated that these new services will be of interest to a broad range of domain science applications.  Individual and small research groups who cannot afford access to dedicated resources may find these services of particular value.<br/><br/>A NESCO facility is defined which can be placed inside of Wide Area Networks, Regional Networks, and Exchange Points.  These facilities will contain advanced programmable hardware-based network functions, compute and storage resources, and be architected to provide services on a highly customizable per flow basis.  An open source orchestration system known as StackV is being modified to develop a new class of realtime ""Flow Management"" services.  The StackV system includes a realtime modeling framework to ""datafy"" infrastructure, modular pluggable computation elements, and intelligent workflow processes to build on-demand topologies across heterogeneous resource types and service providers.  This system also includes a user facing application programming interface which allows workflow agents to request resource topologies which span network, compute, and storage resources via a single integrated request.  This project is developing a NESCO reference implementation which will be deployed for testing and experimentation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835874","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Peter Cummings","TN","Vanderbilt University","Standard Grant","Bogdan Mihaila","09/30/2021","$1,089,470.00","Clare McCabe, Akos Ledeczi, Christopher Iacovella","peter.cummings@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","1253, 1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 8009, 9216, 9263","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835713","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Jeffrey Potoff","MI","Wayne State University","Standard Grant","Bogdan Mihaila","09/30/2021","$230,637.00","","jpotoff@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835613","Collaborative Research: NSCI Framework: Software for Building a Community-Based Molecular Modeling Capability Around the Molecular Simulation Design Framework (MoSDeF)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2018","09/08/2018","Arthi Jayaraman","DE","University of Delaware","Standard Grant","Bogdan Mihaila","09/30/2021","$235,702.00","","arthij@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","1712, 8004","026Z, 054Z, 077Z, 7237, 7569, 7925, 8004, 9216","$0.00","As molecular-based computer simulations of both naturally occurring and man-made (synthetic) materials become increasingly used to predict their properties, the reproducibility of these simulations becomes an increasingly important issue. These simulations are complex, require large amounts of computer time, and are usually performed manually - i.e., put together one at a time, from all the components that go into such a simulation, including the models for how molecules interact with each other (known as forcefields). In addition, there has been much interest in being able to perform such computational simulations on large sets of different but related systems in order to screen for desirable properties, leading to the discovery of new materials and their incorporation into applications twice as rapidly and at half the cost of existing, primarily experimental, methods. This ambition is the basis for the national Materials Genome Initiative (MGI), making reproducibility even more important. In this project, nine research groups from eight universities are combining their expertise to create a software environment, called the Molecular Simulation Design Framework (MoSDeF) that will enable the automation of molecular-based computer simulations of soft materials (such as fluids, polymers, and biological systems) and will enable MGI-style screening of such systems. MoSDeF is open source and the use of MoSDeF will enable reproducibility in molecular-based computer simulations, because all simulation steps, all input data, and all codes used will be publicly accessible to anyone to reproduce a published simulation. MoSDeF will contribute to reproducibility through standardization and maintaining the provenance of forcefields, one of the most common sources of irreproducibility in molecular-based simulations.<br/><br/>Reproducibility in scientific research has become a prominent issue. Computational scientists, along with the rest of the scientific community, are grappling with the central question: How can a study be performed and published in such a way that it can be replicated by others? Answering this question is essential to the scientific enterprise and increasingly urgent, as reproducibility issues faced in small-scale studies will only be compounded as researchers look to harness the ever expanding computational power to perform large-scale Materials Genome Initiative (MGI) inspired screening studies, thus growing the number of simulations by orders of magnitude. Addressing the issues of reproducibility in soft matter simulation is particularly challenging, given the complexity of the simulation inputs and workflows, and the all-to-common usage of closed-source software. In this proposal, nine leading research groups (from Vanderbilt, U Michigan, Notre Dame U, U Delaware, Boise State U, U Houston, Wayne State U, and U Minnesota), representing a broad range of expertise, and an equally broad range of science applications, simulation codes, algorithms and analysis tools, along with computer scientists from Vanderbilt's Institute for Software Integrated Systems (ISIS), are committing to invest their expertise and capabilities to transform the mindset of molecular simulationists to perform and publish their simulations in such a way as to be Transparent, Reproducible, Usable by others, and Extensible (TRUE). Most of the investigators are recent or current holders of grants from the software program (i.e., S2I2, SSI or SSE grants); thus, the project builds upon, and brings synergy to, an existing large investment in molecular simulation software by NSF. To drive the community towards performing simulation that are TRUE, new software tools to facilitate best practices will be developed. Specifically, this will be achieved by expanding the capabilities of the open-source molecular simulation design framework (MoSDeF), which was initiated at Vanderbilt with support from two NSF grants. MoSDeF is a modular, scriptable Python framework that includes modules for programmatic system construction, encoding and applying force field usage rules, and workflow management, allowing the exact procedures used to setup and perform a simulation to be capture, version-controlled, and preserved. Continued development of the existing MoSDeF modules will be performed to support a wider range of force fields, molecular models, and open-source simulation engines. The creation of a plugin architecture for community extension, and the development of new modules for force field optimization, free energy calculations, and screening, will further allow MoSDeF can achieve these goals.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1547324","CICI: Data Provenance: Collaborative Research: CY-DIR Cyber-Provenance Infrastructure for Sensor-Based Data-Intensive Research","OAC","Cyber Secur - Cyberinfrastruc","01/01/2016","09/14/2015","Murat Kantarcioglu","TX","University of Texas at Dallas","Standard Grant","Micah Beck","12/31/2019","$219,571.00","David Lary","muratk@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","8027","7434","$0.00","Today scientists in many disciplines, including biology, medicine, agronomy, energy management, hydrology, and earth sciences, rely on the use of massive datasets collected from various sources. Such collections are made possible by advances in computer technology such as sensors which collect data such as humidity, air quality and so forth, and powerful computer systems that analyze the data. The increased use of data for scientific research poses some important challenges. Data can have errors which impact conclusions derived from the data. Scientific research has to be reproducible to support validation and detection of scientific misconduct. Addressing these challenges requires tracking data used in research projects. Examples include: tracking the source that originated the data -- for example a mobile phone that acquired some images- and its geographic location; tracking which computer systems processed the data; tracking how scientists modified given data. Such a set of information is referred to as provenance -- very much like the provenance of artistic artifacts. Managing provenance is technically complex; yet it is key for data-intensive research. This project makes important advances in this direction by developing software systems for securely managing provenance. <br/><br/>This project develops a provenance management system for cyberinfrastructure that includes different types of hosts, devices, and data management systems. The proof-of-concept system, referred to as Cyber-provenance Infrastructure for Sensor-based Data-Intensive Research (CY-DIR), will support scientists throughout the life-cycle of their sensor-based data collection processes, including the continuous monitoring of sensors to ensure that provenance is collected and recorded, and the traceable use and processing of the data across different data management systems. CY-DIR provides researchers with provenance and metadata about data being collected by sensors. Provenance security will be assured by the use of efficient encryption techniques for use in sensors, secure logging techniques, and secure processors. Research from this project will provide novel results in several areas: provenance techniques for sensor data; cryptographic key management for sensors, mobile devices, and unmanned aircraft systems; provenance aware streaming data processing techniques; protection of provenance data against tempering; provenance data integration across different data management systems."
"1739491","SI2-SSE:GeoVisuals Software: Capturing, Managing, and Utilizing GeoSpatial Multimedia Data for Collaborative Field Research","OAC","METHOD, MEASURE & STATS, Software Institutes","09/01/2017","09/25/2018","Ye Zhao","OH","Kent State University","Standard Grant","Stefan Robila","08/31/2020","$500,000.00","Xiang Lian, Xinyue Ye, Andrew Curtis","zhao@cs.kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","442420001","3306722070","CSE","1333, 8004","7433, 8004, 8005","$0.00","Videos, photos, and narratives (audio, text, graphics) enriched with geospatial coordinates can be used to capture spatial data and associated contextual information for a variety of challenging environments. These data can fill gaps where no data exists, capture ephemeral or constantly changing information, and provide contextual insights that elevate local knowledge beyond more traditional methods. Example applications might include how a slum in Haiti changes after a vaccination intervention following a cholera outbreak, or identifying patterns of homeless camps (and their temporal stability) in Los Angeles. The GeoVisuals software system and a Web-based GeoVisuals Data Repository bridge the gap between the diversity of researcher needs and data infrastructure challenges often encountered ""in the field"". The computing infrastructures help domain researchers and decision-makers capture, manage, query and visualize such big, dynamic data to conduct exploratory and analytical tasks. The utilization of the tools spans across disciplines as diverse as anthropology, clinical health, criminology, disaster management, epidemiology, geography, planning, and sociology. GeoVisuals is being developed with collaboration from many domain researchers in academia, various branches of government, and non-profit sectors, and funded by the NSF Office of Advanced Cyberinfrastructure and the Directorate of Social, Behavioral and Economic Sciences (SBE). It is shared to users as public-licensed software with free access.<br/><br/>The GeoVisuals software consists of a mobile data collection module, a data transmission module, a data storage and computing module, and a visual analytics module. The software helps users to: (1) Capture and transfer geospatial multimedia data in a variety of different field settings ranging from developed countries with advanced IT infrastructures, to countries that still lack a reliable access to the Internet; (2) Manage and explore the collected data, when the data scale and dimensions impose technical challenges for non-IT professionals. It allows for the easy merging and query of GPS, video, audio, narratives, and so on - with flexibility for multiple input types; (3) Apply qualitative, quantitative, and spatial data analysis with mining algorithms, visual representations, and interactions. This robust and easy-to-use software enables the users to conduct information foraging and sense making on geospatial multimedia data. The GeoVisuals Data Repository allows field researchers to share their own data and access the analytical and visualization functions through a web-based platform, being inspired by the open access GenBank for biomedical researchers. The software and the data repository advance the research activities and grant applications of worldwide researchers and educators related to field studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835648","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, DATANET","11/01/2018","12/11/2018","Deborah McGuinness","NY","Rensselaer Polytechnic Institute","Standard Grant","Amy Walton","10/31/2023","$1,448,223.00","Deborah McGuinness","dlm@cs.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1712, 7726","054Z, 062Z, 077Z, 7925","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835618","Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling","OAC","DATANET","11/01/2018","08/23/2018","Christopher Hill","MA","Massachusetts Institute of Technology","Standard Grant","Amy Walton","10/31/2022","$728,217.00","Paul O'Gorman","cnh@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7726","062Z, 077Z, 7925","$0.00","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation.  A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. <br/><br/>The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations.  The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems.  Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis.   The software framework from this project is expected to handle petascale to exascale datasets for users.  Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store.  The broader target is next generation simulation software in the geosciences and other disciplines.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835735","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, DATANET","11/01/2018","08/23/2018","Chiara Daraio","CA","California Institute of Technology","Standard Grant","Amy Walton","10/31/2023","$555,834.00","","Daraio@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1712, 7726","054Z, 062Z, 077Z, 7925","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835640","Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling","OAC","PHYSICAL OCEANOGRAPHY, DATANET, EarthCube","11/01/2018","08/23/2018","Thomas Haine","MD","Johns Hopkins University","Standard Grant","Amy Walton","10/31/2022","$1,850,538.00","Renske Gelderloos, Gerard Lemson, Alexander Szalay","Thomas.Haine@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","1610, 7726, 8074","062Z, 077Z, 1324, 7925","$0.00","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation.  A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. <br/><br/>The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations.  The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems.  Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis.   The software framework from this project is expected to handle petascale to exascale datasets for users.  Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store.  The broader target is next generation simulation software in the geosciences and other disciplines.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640840","CIF21 DIBBs: PD: Ontology-Enabled Polymer Nanocomposite Open Community Data Resource","OAC","Mechanics of Materials and Str, DMR SHORT TERM SUPPORT, DATANET, CDS&E, DMREF","09/01/2016","06/27/2018","Linda Schadler","NY","Rensselaer Polytechnic Institute","Standard Grant","Amy Walton","08/31/2019","$520,890.00","Wei Chen, L. Brinson, Deborah McGuinness","linda.schadler@uvm.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1630, 1712, 7726, 8084, 8292","022E, 024E, 1765, 7433, 8021, 8048, 8396, 8399, 8400, 9102, 9251, 9263","$0.00","The project develops an open access data resource for the polymer nanocomposites community, facilitating faster nanocomposite design and insertion into advanced applications.  The capability would support three major types of users: researchers interested in comparing their results in detail to other work; researchers exploring, discovering, and quantifying fundamental scientific principles that govern processing-structure-property (p-s-p) relationships; and materials designers interested in using nanocomposites for advanced applications.  The resource would accelerate the comparison, exploration, and design of polymer nancomposite materials, for applications ranging from energy to healthcare.<br/><br/>The goal is to create an open access, easy to use, persistent, flexible data resource for the polymer nanocomposites community that is scalable and will enable improved understanding of p-s-p relationships and design of commercially relevant polymer nanocomposites.  The resource builds upon earlier work using the Material Data Curator System (MDCS) at the National Institute of Standards and Technology (NIST), and a prototype (NanoMine) developed through the NSF Division of Materials Research.  A robust ontology will be integrated with an expanded version of the NanoMine prototype to create an open linked data community resource to support organization, integration, mining, and analysis services. Improved data analytics tools are integrated into NanoMine for quantitative image analysis, microstructure reconstruction, knowledge discovery, and materials design recommendation.  A set of standards provide quality metrics for the data; these metrics will be integrated into the ontology and used to validate data at the time of data ingest. The project also leverages open source visualization tools and provides semantically enhanced browsing and visualization capabilities to aid users in discovery of new relationships. <br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Engineering Directorate (Division of Civil, Mechanical & Manufacturing Innovation), and the NSF Directorate for Mathematical & Physical Sciences (Division of Materials Research)."
"1835321","Elements: Software. icepack: an open-source glacier flow modeling library in Python","OAC","POLAR CYBERINFRASTRUCTURE, EarthCube","11/01/2018","08/15/2018","Daniel Shapero","WA","University of Washington","Standard Grant","Stefan Robila","10/31/2021","$388,314.00","Ian Joughin","shapero@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","5407, 8074","026Z, 062Z, 072Z, 077Z, 1079, 7923, 8004","$0.00","This project supports the development of a software package named ""icepack"", that will enable simulations of how glaciers, such as those in Greenland, Antarctica, and mountain ranges around the world, will flow in response to the environment around them. Glaciologists use software tools to run simulations so that they can make predictions of how large the Greenland and Antarctic ice sheets will be in the future. With these predictions, scientists can give policy-makers and the public better predictions on the sea level rise in the coming decades. While the ability to run simulations is essential for advancing our understanding of science, doing so requires a significant programming and scientific expertise. The goal of this project is to lower this barrier to entry. Led by an early career scientist, the team, from University of Washington will develop a tool that is easier to use for researchers and students, whether they are experts or novices. The software applications will be freely available and an open source license.<br/><br/>icepack allows for estimating parameters, such as a basal friction or internal rheology, that are not observable via remote sensing. Glaciologists use simulation tools like icepack for (1) exploring aspects of the physics of ice sheets that are not completely understood, (2) drawing inferences from observational data, and (3) making predictions of the future state of the ice sheets in order to estimate future sea-level rise. While modeling is an essential tool for practicing glaciologists, it is still a complex endeavor. In addition to supporting development of more features and improvements to icepack, we will create an extensive set of tutorial materials for a workshop aimed at graduate students and early-career researchers on how to use icepack. Additionally, the investigators will implement novel algorithms for parameter estimation and uncertainty quantification in icepack. These will allow the investigators to leverage the entire time series of observations of the ice sheets, while current algorithms are limited in how much data they can use, and to get a better idea of the statistical spread on estimates of the current and future states of the ice sheets.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1827314","MRI: Acquisition of a GPU Accelerated Vermont Advanced Computing Core","OAC","MAJOR RESEARCH INSTRUMENTATION","09/01/2018","08/23/2018","Adrian Delmaestro","VT","University of Vermont & State Agricultural College","Standard Grant","Stefan Robila","08/31/2020","$893,120.00","Joshua Bongard, Yolanda Chen, Hugh Garavan, Juan Vanegas","Adrian.Delmaestro@uvm.edu","85 South Prospect Street","Burlington","VT","054050160","8026563660","CSE","1189","026Z, 062Z, 075Z, 1189, 8089, 8091, 9150","$0.00","This project will enable interdisciplinary science through the acquisition of a high-performance computer cluster, named DeepGreen.  Based on cutting-edge massively parallel graphics processing unit (GPU) technologies, DeepGreen will be utilized by the over 300 users from six Colleges at the University of Vermont, and throughout the Northeast. The unique hybrid architecture was designed to optimize artificial intelligence (AI) applications and will allow for rapid progress on problems of great societal importance. They include: quantum computing, drug discovery and design, safe robotics, control of adaptive crop pests, and new computer vision tools for use in the health care and transportation industries.  As an example, DeepGreen will allow the training of neural networks on the world's largest brain imaging datasets of illicit drug users, yielding novel health and policy strategies to combat the opioid epidemic.  A focus of the scientific and technical team is to broaden the number of personnel able to exploit GPU hardware for problem solving, producing the highly trained and diverse technical workforce required for the current and future AI economy. <br/><br/>DeepGreen was designed by a team of experts from the physical, medical, biological, computational, and agricultural sciences, partnered with an experienced group of information technology professionals.  It will be capable of over 8 petaflops of mixed precision calculations based on the latest NVIDIA Tesla V100 architecture with a hybrid design allowing high bandwidth message passing across heterogeneous compute nodes.  Its extreme parallelism will facilitate research in three interconnected areas: quantum many-body systems, molecular simulation and modeling, and deep learning, artificial intelligence and evolutionary algorithms.  DeepGreen will forge transformative research pipelines. It will enable the study of thousands of quantum entangled atoms, and millions of interacting components in biological systems providing insights into structure-function mechanisms.  Machine learning and deep neural networks will exploit DeepGreen's Tensor Cores to solve diverse problems. These problems include: the development of coarse grained potentials for use in molecular dynamics simulations, real time dynamic processing of crowd sourced decision making for robotics, genomic sequencing of invasive pests, and feature recognition in medical imaging to distinguish cancerous tumors from benign nodules.  Software designed for use on DeepGreen will be released to the public as open source, with other scientists and researchers being able to immediately use and extend it. This project will also support the next generation of data scientists. Training workshops focused on GPU computing and machine learning frameworks, new university courses, and partnerships with existing local NSF-funded graduate training initiatives, will drive broad utilization of DeepGreen.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750549","CAREER: HiPer: A CFD solver for High-Performance Turbulent Flow Simulations on Massively Parallel Machines","OAC","CAREER: FACULTY EARLY CAR DEV, SOFTWARE & HARDWARE FOUNDATION","03/01/2018","02/05/2018","Aparna Chandramowlishwaran","CA","University of California-Irvine","Continuing grant","Sushil K Prasad","02/28/2023","$303,574.00","","amowli@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","1045, 7798","026Z, 062Z, 1045, 9102","$0.00","Evolution of physics-based simulations and Computational Fluid Dynamics (CFD) in particular has fundamentally reshaped the design and engineering process in the last several decades.  However, in spite of noteworthy success, today's CFD still remains limited to a small design space.  One of the grand challenge problems is the simulation of a full aircraft envelope.  Today's computing platforms are based on massive parallelism and heterogeneous processor designs to deliver petascale performance (10^15 floating point operations per second).  This research has the potential to effectively harness this level of performance and significantly address grand challenge problems through advances in numerical schemes, efficient parallel algorithms, and implementation strategies to enable underlying simulations that are currently infeasible.  It also aims at transforming CFD simulation capabilities to dramatically reduce the cost and time needed to solve complex multi-physics problems.  As such, this research fills a gap in the understanding that lies at the intersection of computational fluid dynamics and parallel computing.  This interdisciplinary research also shapes the next-generation of students and researchers with a multi-faceted skill set required to solve challenging problems at the boundary of domain sciences, applied mathematics, and computer science to promote the progress of science and advance national prosperity and welfare as stated by NSF's mission.<br/><br/>This research creates HiPer, a CFD solver for high-performance turbulent flow simulations on massively parallel machines.  HiPer solves the Navier-Stokes equations on multi-block structured grids for complex geometries by combining the following key components: (i) a novel time-delayed implicit time-marching scheme tailored for heterogeneous architectures;  (ii) parallelization strategies for shared- and distributed-memory systems aimed at reducing the synchronization and communication time; and  a hybrid multi-block structured grid enhanced by geometric multigrid to increase convergence as well as reduce the number of grid cells.  Applications that are drivers in the near- and long-term serve as benchmarks for continually measuring progress towards the grand challenge goals.  In particular, two application case studies serve as drivers for the near-term: (a) the simulation of the complex unsteady flow through multi-stage compressors and turbines, and (b) noise generation and propagation in a high-speed turbulent jet.  The intellectual merit of this work is the development of novel numerical techniques, parallelization strategies, and scalable software that enable turbulent-separated flow simulations that are computationally intractable today.  To engage and inspire young generations to this approach, this project strives to (i) organize hands-on workshops at relevant conferences, (ii) design and develop an educational kit targeted to teaching CFD and high-performance computing (HPC) concepts, (iii) design lab-based courses on HPC for computational scientists, including hands-on labs using HiPer on large-scale systems for on-site students, and (iv) release videos and guest lectures through University of California's Early Academic Outreach Program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1663671","SI2-SSI Collaborative Research: The SimCardio Open Source Multi-Physics Cardiac Modeling Package","OAC","FLUID DYNAMICS, Engineering of Biomed Systems, Biomechanics & Mechanobiology, Software Institutes, Smart and Connected Health, CDS&E","09/01/2017","05/04/2017","Alison Marsden","CA","Stanford University","Standard Grant","Vipin Chaudhary","08/31/2021","$1,431,169.00","","amarsden@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","1443, 5345, 7479, 8004, 8018, 8084","026Z, 028E, 7433, 7479, 8004, 8009, 9102, 9263","$0.00","Cardiovascular (CV) simulations have become a crucial component of fundamental research in surgical planning, device design, diagnosis, and disease mechanisms. The project team has previously developed SimVascular (www.simvascular.org), which is currently the only open source software package providing a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis in arteries and veins. The SimCardio open source project will extend and enhance the functionality of SimVascular to the realm of heart modeling, providing the first fully integrated computer model of cardiac physiology and function. This will help basic science and medical researchers perform computer modeling in numerous diseases affecting heart function in children and adults. This computer modeling software will enable researchers to build models of the heart and vascular anatomy directly from medical imaging data, which can be used for personalized treatment planning and medical device design, ultimately leading to new treatments for patients with cardiovascular disease. <br/><br/>The SimCardio project will create a unique open source software package for multi-physics cardiac modeling and simulations. SimCardio will include a new multi-physics finite element solver with capabilities for large-deformation fluid-structure interaction (FSI) to capture ventricular contraction and heart valve dynamics, non-linear and visco-elastic material models, cardiac mechanics models of active heart contraction, and electrophysiology. This will be facilitated by sustainable software infrastructure bridging the cardiovascular fluid and solid mechanics communities. The project will provide a new user-interface for high-throughput construction of patient-specific cardiac and vascular models. SimCardio will broaden the applicability of SimVascular to problems including heart valves, heart failure, cardiomyopathy, aortic dissection, structural congenital heart defects and medical devices. To facilitate adoption, the project will publicly provide accompanying educational and training materials, and maintain a sustainable software ecosystem to increase the user community and ensure continued availability and evolution."
"1839011","EAGER: Collaborative Research: Synchronization Across Terrestrial and Aquatic Ecosystems","OAC","NSF Public Access Initiative","09/01/2018","08/13/2018","Grace Wilkinson","IA","Iowa State University","Standard Grant","Beth Plale","08/31/2020","$136,115.00","","wilkinso@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7414","7916","$0.00","Aquatic ecosystems are closely connected to their surrounding watersheds through the flux of water, nutrients, and organic carbon.  Similarly, the flux of exogenous carbon from the landscape causes lakes to be substantial sources of greenhouse gases as well as sinks for organic carbon buried in the sediment.  There are also important fluxes from inland waters to the terrestrial ecosystems. Despite the recognition of the importance of terrestrial-aquatic coupling, synchronization (persistent relatedness) of dynamics between these ecosystems has not been broadly investigated.  Employing data reuse techniques to use data from Lake Multiscaled Geospatial and Temporal Database (LAGOS) and Global Lake Ecological Observatory Network (GLEON) databases, the PIs will apply new analysis tools to answer questions related to the synchrony of lakes and their surrounding watersheds.<br/> <br/>To quantify synchronization between terrestrial and aquatic habitats wavelet coherence will be used to measure the strength of synchronization between terrestrial and lake ecosystems, as well as phase relationships, describing time lags. Time lags are hypothesized to reflect mechanisms of synchrony and may be related to lake size, water residence time, trophic status, and watershed area. These factors are hypothesized to affect the degree of aquatic-terrestrial synchrony. Random forest regression will be applied to test this idea, leveraging the range of lake and watershed properties. Multiple regression for wavelet transforms will be used to determine the fraction of aquatic-terrestrial synchrony that can be explained by climate drivers and assess synchrony between terrestrial ecosystems and lake nutrient fluctuations for improved inference into non-climate mechanisms of synchrony.<br/> <br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659235","CC* Network Design: Multiple Organization Regional One Oklahoma Friction Free Network (MORe OFFN)","OAC","Campus Cyberinfrastrc (CC-NIE)","06/15/2017","06/06/2017","Richard Reif","OK","Northeastern State University","Standard Grant","Kevin L. Thompson","05/31/2019","$333,859.00","James Deaton, Karl Frinkle, Hue Xiong, Jeremy Evert","reif01@nsuok.edu","600 N. Grand","Tahlequah","OK","744642301","9184565511","CSE","8080","9150","$0.00","The Multiple Organization Regional OneOklahoma Friction Free Network (MORe-OFFN) expands the network capabilities of Northeastern State University (NSU), Southwestern Oklahoma State University (SWOSU), Southeastern Oklahoma State University (SOSU), and Rogers State University (RSU) by connecting them to the OneOklahoma Friction Free Network (OFFN) through community-based ScienceDMZ technologies allowing the researchers at each institution to connect reliably to supercomputers and increase computational capabilities.<br/><br/>OFFN provides connection to Oklahoma's research organizations, including MORe OFFN's leadership institution for this grant, University of Oklahoma. Previously, researchers at these institutions were hampered with the lack of throughput speed when dealing with large data sets.   With the creation of a parallel research network, scholars at each of the institutions have steady access to a network that allows for the transfer of large data sets and faster information sharing. In addition to expanding research capability at each of the partnering institutions, SOSU and SWOSU are developing and team-teaching a course on parallel programming offered online to their students.  With expanded research opportunities and the development of a new course, MORe-OFFN advances undergraduate science research and education along with providing more educational and research opportunities for underrepresented communities."
"1850012","CRII: Algorithms and Methodologies for Real-Time Decision-Making of Mission-Critical Structures Experiencing High-Rate Dynamics","OAC","CRII CISE Research Initiation, EPSCoR Co-Funding","03/15/2019","02/21/2019","Austin Downey","SC","University of South Carolina at Columbia","Standard Grant","Sushil K Prasad","02/28/2021","$175,000.00","","ADOWNEY2@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","026Y, 9150","8228, 9150","$0.00","This project focuses on investigations into methodologies to enable real-time decision-making for mission-critical structural systems experiencing high-rate dynamics. Examples of mission-critical structures that experience high-rate dynamics include hypersonic vehicles, space crafts, ballistics packages, and active blast mitigation. Enabling real-time decision-making for these structures increases mission success rates by enhancing the structure's survivability, providing on-time guidance corrections, and adopting the mission goals/outcome to changing conditions. Additionally, the methodologies developed by this project increase the robustness, safety, and commercial viability of structures operating in extreme dynamic environments. The development of algorithms and methodologies for structures experiencing high-rate dynamics serves the national interest and fulfills the NSF's mission:  to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.  This project provides research experience and mentors a diverse and inclusive group of students, and introduces a graduate level class on surrogate modeling of complex systems.<br/><br/>A structural system operating in a high-rate dynamic environment can experience sudden and unmodeled plastic deformation of the structure that may further lead to damaged electronics, sensors, and/or delicate payloads. This research focuses on enabling a real-time decision-making module to take corrective actions. To achieve this goal, a parallelization framework is developed that enables a structural system to be decomposed into its constituent components where each component can be monitored, modeled, and extrapolated into the future. Once the degradation trajectories for each component have been estimated, they are recombined into a single system-level model to be used for real-time decision making.  The project is organized into two research thrusts and one experimental design challenge. Thrust 1 investigates surrogate modeling techniques with the goal of developing component-level models that can converge within the required time constraints. These surrogate models use data obtained from dense sensor networks to generate data-driven damage-sensitive features that can be used as the degradation parameters for component-level prognostics. Thrust 2 explores and formulates methodologies for real-time component-level prognostics. These component-level predictions are then recombined into a single structural model that is used to develop potential corrective actions. Lastly, the experimental design challenge validates the developed algorithms and methodologies using a high-rate dynamic test bench.<br/><br/>This project is jointly funded by Office of Advanced Cyberinfrastructure (OAC) and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740305","SI2-SSE: Improving Scikit-Learn Usability and Automation","OAC","Software Institutes","09/01/2017","07/28/2017","Andreas Mueller","NY","Columbia University","Standard Grant","Stefan Robila","08/31/2020","$399,356.00","","acm2248@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8004","7433, 8004, 8005","$0.00","Machine learning is a central component in many data-driven research areas, but its adoption is limited by the often complex choice of data processing, model, and parameter settings. The goal of this project is to create software tools that enable automatic machine learning, that is, solving predictive analytics tasks without requiring the user to explicitly specify the algorithm or model parameters used for prediction.  The software developed in this project will enable a wider use of machine learning, by providing tools to apply machine learning without requiring knowledge of the details of the algorithms involved.<br/><br/>The project, supported by the Office of Advanced Cyberinfrastructure, and the Division of Computing and Communication Foundations, extends the existing scikit-learn project, a machine learning library for Python, which is widely used in academic research across disciplines.  The project will add features to this library to lower the amount of expert knowledge required to apply models to a new problem, and to facilitate the interaction with automated machine learning systems. The project will also create a separate software package that includes models for automatic supervised learning, with a very simple interface, requiring minimal user interaction. In contrast to existing research projects, this project focuses on creating easy-to-use tools that can be used by researchers without extensive training in machine learning or computer science."
"1535232","SI2-SSE: EASE: Improving Research Accountability through Artifact Evaluation","OAC","SPECIAL PROJECTS - CCF, Software Institutes","09/01/2015","06/16/2015","Bruce Childers","PA","University of Pittsburgh","Standard Grant","Bogdan Mihaila","08/31/2019","$499,515.00","Daniel Mosse'","childers@cs.pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","2878, 8004","7433, 8005","$0.00","Research in computer systems, particularly in the first stages of creating a new innovation, relies almost exclusively on software prototypes, simulators, benchmarks, and data sets to understand the benefits and costs of new ideas in computers, ranging from consumer devices to exascale systems. These artifacts are used to evaluate new capabilities, algorithms, bottlenecks and trade-offs. Empirical study is behind the rapid pace of innovation in creating faster, lower energy and more reliable systems. This experimental approach lies at the core of development that fuels the nation's information economy.  Given the critical importance of experimental study to developing new computer systems, several efforts are underway to curate experimental results through accountable research.  One effort, Artifact Evaluation (AE), is being adopted to promote high quality artifacts  and experimentation, including making public the experimental information necessary for reproducibility. However, the rapid adoption of AE is hampered by technical challenges that create a high barrier to the process: there is no consistent or simple environment, or mechanism, to package and reproduce experiments for AE. Authors rely on their own approaches, leading to much time consumed, as well as considerable variability in the ways materials are prepared and evaluated, unnecessarily obstructing the AE process.  <br/><br/>To overcome the technical challenges with AE, and to more broadly encourage adoption of AE in computer science and engineering research, this project is developing a software infrastructure, Experiment and Artifact System for Evaluation (EASE), to create and run experiments specifically for AE, in which authors create, conduct and share artifacts and experiments. It allows for repeating,  modifying, and extending experiments. Authors may also use EASE to  package and upload their experiments for archival storage in a digital library. EASE is being developed and deployed for two use cases, namely compilers and real-time systems, keeping the project tractable to address specific needs. These communities have overlapping but also distinct requirements, helping to ensure EASE can also be extended and<br/>used by other computer systems research communities as well.<br/><br/>EASE will be release as open source software, based on an Experiment Management System (EMS) previously developed by the project investigator in a project call Open Curation for Computer Architecture Modeling (OCCAM), used to define and conduct experiments using computer architecture simulators. Using EMS as a starting point, EASE will provide AE support, by: 1) separating EMS from OCCAM's repository and hardware services, transforming the EMS infrastructure into EASE, a fully standalone, sustainable, and extensible platform for AE; 2) supporting record and replay (for repeating and reproducing results, as well as provenance) of artifacts and experiments as part of normal development and experimental practice to ease participation in AE by authors and evaluators; 3) supporting artifacts,workflows of artifacts and experiments that run directly on a machine, including specialized hardware and software, and run indirectly on a simulator or emulator; 4) allowing both user-level (artifacts and experiments as user processes) and system-level (artifacts and experiments involving kernel changes) innovations; 5) providing consistent/uniform access, whether locally or remotely, to artifacts and experiments; 6) simplifying viewing, running, modifying, and comparing experiments by innovators (i.e., during innovation development), artifact evaluators (during AE), and archive users (after publication); 7) enabling indexing (object locators and search tags) and packaging of artifacts and experiments for AE and for archival deployment (e.g., to ACM?s or IEEE?s Digital Library); and 8) refining, expanding, generalizing, and documenting EASE to ensure it is robust, maintainable and extensible, and that it can be used and sustained by different CSR communities (starting with real-time and compilers, given their different artifacts, data and methods)."
"1663747","SI2-SSI Collaborative Research: The SimCardio Open Ssource Multi-Physics Cardiac Modeling Package","OAC","FLUID DYNAMICS, Engineering of Biomed Systems, Biomechanics & Mechanobiology, Software Institutes, Smart and Connected Health, CDS&E","09/01/2017","05/04/2017","Shawn Shadden","CA","University of California-Berkeley","Standard Grant","Vipin Chaudhary","08/31/2021","$943,832.00","","shadden@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","1443, 5345, 7479, 8004, 8018, 8084","026Z, 028E, 7433, 7479, 8004, 8009, 9263","$0.00","Cardiovascular (CV) simulations have become a crucial component of fundamental research in surgical planning, device design, diagnosis, and disease mechanisms. The project team has previously developed SimVascular (www.simvascular.org), which is currently the only open source software package providing a complete pipeline from medical image data segmentation to patient specific blood flow simulation and analysis in arteries and veins. The SimCardio open source project will extend and enhance the functionality of SimVascular to the realm of heart modeling, providing the first fully integrated computer model of cardiac physiology and function. This will help basic science and medical researchers perform computer modeling in numerous diseases affecting heart function in children and adults. This computer modeling software will enable researchers to build models of the heart and vascular anatomy directly from medical imaging data, which can be used for personalized treatment planning and medical device design, ultimately leading to new treatments for patients with cardiovascular disease. <br/><br/>The SimCardio project will create a unique open source software package for multi-physics cardiac modeling and simulations. SimCardio will include a new multi-physics finite element solver with capabilities for large-deformation fluid-structure interaction (FSI) to capture ventricular contraction and heart valve dynamics, non-linear and visco-elastic material models, cardiac mechanics models of active heart contraction, and electrophysiology. This will be facilitated by sustainable software infrastructure bridging the cardiovascular fluid and solid mechanics communities. The project will provide a new user-interface for high-throughput construction of patient-specific cardiac and vascular models. SimCardio will broaden the applicability of SimVascular to problems including heart valves, heart failure, cardiomyopathy, aortic dissection, structural congenital heart defects and medical devices. To facilitate adoption, the project will publicly provide accompanying educational and training materials, and maintain a sustainable software ecosystem to increase the user community and ensure continued availability and evolution."
"1321762","Accomplisment Based Renewal (ABR) to the award Flight-Worthy Condor: Enabling Scientific Discovery","OAC","CYBERINFRASTRUCTURE, INTERNATIONAL RES NET CONNECT, PHYSICS AT THE INFO FRONTIER, Cyber Secur - Cyberinfrastruc","07/01/2013","07/19/2017","Miron Livny","WI","University of Wisconsin-Madison","Continuing grant","Micah Beck","06/30/2019","$7,499,960.00","Todd Tannenbaum","miron@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7231, 7369, 7553, 8027","7245, 7369, 7433, 7434, 7553","$0.00","In the foreseen future a mix of changes in technologies, user and application requirements and the business model of delivering computing capacity will continue to pose new challenges to the effectiveness of high throughput computing (HTC) technologies.  To address these challenges, this ongoing research and development effort will devise new policy-driven capabilities to increase throughput within a defined budget by effectively managing extremely large workloads of homogenous jobs running on homogenous machines provisioned by cloud services. These capabilities will be augmented with effective schedulers for servers that have multiple cores of execution, many disks, perhaps several GPUs each with different capabilities, and multiple networking interfaces. New communities will be introduced to the power of HTC through customized, easy to deploy and secure software. Novel tools for profiling the requirements and dependencies of scientific applications will expand the reach of distributed computing infrastructures that leverage advanced networks to cross institutional and national boundaries. <br/><br/>The advances in HTC technologies will be delivered through the widely adopted HTCondor software tools. More than 150 domestic universities, a growing number of national and international science communities and a wide spectrum of commercial organizations employ HTCondor to improve the throughput of their compute and data intensive applications. This project  will sustain a software engineering process that enables translational work to occur in a transitional manner, building upon the previous generation of software while simultaneously continuing to offer and support dependable software that is suitable to handle the ever growing amounts of experimental and simulated scientific data."
"1541457","CC*DNI Engineer:Integrating Campus Cyberinfrastructure and Research Computing at Northwestern University","OAC","Campus Cyberinfrastrc (CC-NIE)","10/01/2015","08/25/2015","Joseph Walsh","IL","Northwestern University","Standard Grant","Kevin L. Thompson","09/30/2019","$399,018.00","Sean Reynolds","jwalsh@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","8080","","$0.00","The growing complexity of research requires increasingly sophisticated tools, technologies, data options and solutions. Moreover, scientific inquiry is increasingly reliant on the ability to access a diversity of resources, on and off campus, based on comprehensive technology and infrastructure solutions along with capable networks that facilitate the sharing, management and analysis of data. However, to fully support scientific research it is not enough to simply provide cyberinfrastructure (CI). Expertise in matching resources to research requirements is essential.<br/><br/>The two-year project sustains the activities of a Campus Cyberinfrastructure Engineer (CIE) dedicated to supporting research inquiry. The CIE leads the provisioning and integration of local and remote resources, including those that are sited around the world, that meet the research and training needs of faculty and students while actively advocating for domain scientists' differing CI needs. This project identifies and addresses gaps in CI services and infrastructure, and incorporates new technologies and practices into the existing service portfolio and team skill set, allowing the broader CI team to better support research and training.<br/><br/>This project provides enabling and integrative capabilities across multiple disciplines and bridges activities between internal and external communities of practice through: (1) researcher support in pursuit and execution of research activities; (2) design, development, and integration advancement of core cyberinfrastructure services; (3) engagement of local, regional, and national partners in the development of communities of practice; (4) growth and expanded usage of the University's Science DMZ network; and (5) core CI services training."
"1839024","EAGER: Collaborative Research: Synchronization Between Terrestrial and Aquatic Ecosystems","OAC","NSF Public Access Initiative","09/01/2018","08/13/2018","Michael Pace","VA","University of Virginia Main Campus","Standard Grant","Beth Plale","08/31/2020","$163,673.00","","pacem@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7414","7916","$0.00","Aquatic ecosystems are closely connected to their surrounding watersheds through the flux of water, nutrients, and organic carbon.  Similarly, the flux of exogenous carbon from the landscape causes lakes to be substantial sources of greenhouse gases as well as sinks for organic carbon buried in the sediment.  There are also important fluxes from inland waters to the terrestrial ecosystems. Despite the recognition of the importance of terrestrial-aquatic coupling, synchronization (persistent relatedness) of dynamics between these ecosystems has not been broadly investigated.  Employing data reuse techniques to use data from Lake Multiscaled Geospatial and Temporal Database (LAGOS) and Global Lake Ecological Observatory Network (GLEON) databases, the PIs will apply new analysis tools to answer questions related to the synchrony of lakes and their surrounding watersheds.<br/> <br/>To quantify synchronization between terrestrial and aquatic habitats wavelet coherence will be used to measure the strength of synchronization between terrestrial and lake ecosystems, as well as phase relationships, describing time lags. Time lags are hypothesized to reflect mechanisms of synchrony and may be related to lake size, water residence time, trophic status, and watershed area. These factors are hypothesized to affect the degree of aquatic-terrestrial synchrony. Random forest regression will be applied to test this idea, leveraging the range of lake and watershed properties. Multiple regression for wavelet transforms will be used to determine the fraction of aquatic-terrestrial synchrony that can be explained by climate drivers and assess synchrony between terrestrial ecosystems and lake nutrient fluctuations for improved inference into non-climate mechanisms of synchrony.<br/> <br/>This project is supported by the National Science Foundation?s Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740251","SI2-SSE: Software for Semiconductor and Electrochemical Interfaces (SSEI)","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2017","09/11/2017","Richard Hennig","FL","University of Florida","Standard Grant","Vipin Chaudhary","09/30/2020","$322,051.00","","rhennig@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","1712, 8004","026Z, 054Z, 6863, 7433, 8004, 8005, 8396, 8399, 9216","$0.00","The importance of materials interfaces is apparent when considering key industrial segments such as the microelectronics, chemical, energy, and biomedical industries. Interfaces and surfaces often control the properties of materials and the interface structure and chemistry are one of the most important, yet least understood, aspects of materials synthesis and functionalization. Materials interfaces come in a broad variety, ranging from solid/solid interfaces in semiconductor hetero-structures to solid/liquid interfaces in electrochemistry and biomaterials, and solid/vapor interfaces in chemical vapor deposition and free-standing 2D materials. Despite their diversity, all interfaces present a common set of challenges for computational studies. This project will develop a sustainable software package that enables ab initio simulations of materials interfaces in various environments that extends the current state of the art by adding functionality and increasing performance. The software tools will impact the development and the design of novel materials that broadly benefit society in the fields of catalysis, electrochemistry, battery technologies, and electronic devices.<br/><br/><br/>The goal of this project is to develop a comprehensive and sustainable Software for Semiconductor and Electrochemical Interfaces (SSEI) that provides various continuum models and couples them to ab initio simulations. The SSEI package will enable an efficient and accurate description of a wide variety of materials interfaces that are important for application in energy technologies, corrosion research, electronic devices, and 2D materials. The SSEI is based on the VASPsol module developed by the PI and will be developed primarily for direct use as a module in VASP. For future extension of the software to other DFT codes, the project will provide a portable software interface and detailed documentation. The software design goals are to develop SSEI into (i) a sustainable scientific tool, (ii) significantly extend its functionality to nonlinear models and arbitrary electrostatic boundary conditions, and (iii) increase its performance. These complementary goals will be achieved through an expansion of the developer and user base, a transition to portable software interfaces and data structures, and the addition of modular algorithms for functionality and performance enhancements. All developments will make extensive use of object-oriented programming principles and software design patterns to speed up the development process and aid maintainability. The SSEI package provides the predictive tools for materials interfaces that have the potential to transform the use of ab initio methods to advance our fundamental knowledge and enable the design and optimization of materials interfaces for better catalysts, battery electrodes, electronic junctions, and corrosion resistance.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1740142","NSCI SI2-SSE: The N-Jettiness Software Framework for Precision Perturbative QCD Calculations in Particle and Nuclear Physics","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/29/2017","Francis Petriello","IL","Northwestern University","Standard Grant","Vipin Chaudhary","08/31/2020","$476,976.00","","f-petriello@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","1253, 7244, 8004","026Z, 7433, 7569, 8004, 8005, 8084","$0.00","This project will develop computational tools needed to interpret increasingly precise instrument data from particle accelerators such as the world's largest and most powerful, the Large Hadron Collider (LHC).  These computations will advance our knowledge of physics at the smallest scales and may potentially reveal deviations between measurements and theory (the Standard Model of particle physics).  The detailed scrutiny of the recently-discovered Higgs boson and searches for deviations from the Standard Model will guide the physics community for the coming decades.  The success of this program will rely upon increasingly intricate and precise theoretical calculations.  In the words of the U.S. Particle Physics Project Prioritization Panel (P5) report which describes the next decade of high energy physics: ""The full discovery potential of the Higgs will be unleashed by percent-level precision studies of the Higgs properties.""  The difficulty in achieving predictions at this precision is an enormous theoretical and computational challenge.  The project members have developed a novel approach to the necessary calculations that is especially adapted to run on the nation's largest high-performance computing systems.  This method has made previously unobtainable results possible, and there is great promise for similar future rapid progress.  The software development in this project will provide the tools needed to answer some of the most outstanding issues facing fundamental physics: What is the underlying origin of the Higgs boson?  Can we discover dark matter at the LHC?  What is the microscopic mechanism which gives the proton its observed spin?  Through the involvement of junior scientists in answering these questions the younger generation will be trained in applying cutting-edge computing knowledge to answer future scientific questions.<br/><br/><br/>The primary goal of this project is the development and deployment of codes incorporating the N-jettiness subtraction approach to perturbative QCD calculations in order to address the ever-increasing precision needs of collider experiments in  particle and nuclear physics.  This theoretical framework very effectively uses previous community investments in software development by extending publicly-available next-to-leading-order (NLO) codes to next-to-next-to-leading order (NNLO), where the expansion parameter is the strong coupling constant.  This advance improves their achievable theoretical precision by an order of magnitude, while maintaining the interface familiar to the user community.  The specific objectives of this project are as follows: the public release of NNLO corrections for jet production processes at the LHC into a public simulation code that is both fast and user-friendly; the expansion of the functionality of DISTRESS, a new code designed for precision simulations for RHIC and a future electron-ion-collider; the preparation of these precision simulation tools for future multi-core computing architectures that feature smaller memory per core.  The N-jettiness subtraction approach is optimized for the massively-parallel computing architectures in which the United States government has invested heavily, and therefore advances the goals of the National Strategic Computing Initiative.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering, the Physics Division and Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences."
"1339893","SI2-SSI: Particle-In-Cell and Kinetic Simulation Center","OAC","OFFICE OF MULTIDISCIPLINARY AC, PHYSICS AT THE INFO FRONTIER, Software Institutes","09/01/2013","07/22/2017","Warren Mori","CA","University of California-Los Angeles","Continuing grant","Bogdan Mihaila","08/31/2019","$4,000,000.00","Frank Tsung, Michail Tzoufras, Viktor Decyk, Russel Caflisch","mori@physics.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","1253, 7553, 8004","7433, 7483, 8009, 8084","$0.00","Computer simulations using the particle-in-cell (PIC) method are widely used for basic and applied research involving plasma physics applications. For example, simulations that calculate the self-consistent interaction of charged particles aid in the development of new accelerator technologies, new radiation sources, are used in magnetic and inertial fusion research, and help understand the physics of the solar wind. The Particle-in-Cell and Kinetic Simulation Software Center (PICKSC) at UCLA will aim to significantly broaden the impact of PIC simulations by making available and documenting illustrative software programs for different computing hardware, a flexible Framework for rapid construction of parallelized PIC programs, and several distinct production programs. This project will also include activities on developing and comparing different PIC algorithms and documenting best practices for developing and using PIC programs. The activities fostered by this project will bring together an interdisciplinary team of faculty, research staff, post-doctoral scholars, and graduate students. Important goals of this project include also the development of educational software for undergraduate and graduate courses in plasma physics and computer science and will, to build a large community of users through outreach and an annual workshop. <br/><br/>The broader impact of the activities fostered by this project will be significant. A well-documented set of components and example codes for running on large computers and a set of basic production codes will allow students and researchers from all levels and many disciplines to understand the inner workings of optimized PIC and kinetic simulation software used to model plasmas. It will allow them to build their own software, or to make independent comparisons against commercially available codes, against their own codes, and against published simulation data. The availability of production codes to more researchers will increase the rate of scientific discovery. The software will allow computer scientists who are developing tools that allow existing software to use next generation hardware to compare their performances against highly optimized codes, and will provide new code developers a test-bed of parallelized and optimized software for performance comparison. Furthermore, this projet will make state-of-the-art research software available for education, both for physics and computer science courses, will help train the next generation of plasma physicists (in many sub disciplines) and computational scientists. Interactive tools based on simpler skeleton codes will also be useful for undergraduate and high school education. Documenting examples of best practices will save graduate students and new researchers significant time in learning how to best employ PIC simulations."
"1461264","REU Site: Computation Across the Disciplines","OAC","RSCH EXPER FOR UNDERGRAD SITES","04/01/2015","03/27/2015","Dennis Brylow","WI","Marquette University","Standard Grant","Sushil Prasad","03/31/2019","$357,552.00","Kim Factor","brylow@mscs.mu.edu","P.O. Box 1881","Milwaukee","WI","532011881","4142887200","CSE","1139","9250","$0.00","Student participants in the Marquette University (MU) Research Experience for Undergraduates (REU) site, ""Computation Across the Disciplines,"" form a research cohort for a 10-week summer program designed to tackle a variety of interdisciplinary, open research questions.  Each student works closely with a faculty mentor in MU's Mathematics, Statistics and Computer Science (MSCS) department to design research goals and milestones matched to their shared interests and aspirations.<br/><br/>MU's REU site focuses on recruiting talented undergraduates who are uncertain about future graduate studies, with little or no research opportunity at their home institution.  Working on different problems with diverse backgrounds, participants nevertheless can witness firsthand the commonalities of foundational research techniques across a wide spectrum of computational areas.  MSCS REU participants may develop tools for managing high-performance computing infrastructure, model oceanographic and volcanic fluid flows, build novel embedded computer systems, analyze the impact of species loss in a complex ecological food web, discover biologically important pathways for gene regulation, or help predict natural gas flow through commercial distribution networks to reduce everyone's utility bills.  Throughout, participants build essential research skills, such as problem solving, technical reading and writing, tool support, research poster construction, and oral presentation.  MSCS faculty mentor expertise spans high performance computing, mathematical models and simulations, data assimilation, and embedded, mobile and ubiquitous systems, as well as math and computer science education.  For more information on this ongoing program, see http://acm.mscs.mu.edu/reu/."
"1853982","CyberTraining: CIP: Collaborative Research: Enhancing Mobile Security Education by Creating Eureka Experiences","OAC","CyberTraining - Training-based","09/01/2018","09/21/2018","Wei Cheng","WA","University of Washington","Standard Grant","chun-hsi huang","08/31/2021","$100,000.00","","uwcheng@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","044Y","026Z, 062Z, 7361, 9179","$0.00","The rapid development and rollout of mobile infrastructure and applications not only bring convenience to people's daily lives, but also give birth to threats that can jeopardize each individual's privacy and national security. Therefore, it is critical to train and educate the future workforce on the fundamental aspects of mobile security relevant to advanced cyberinfrastructure, and to improve their ability to identify, prevent, and respond to emerging threats. This project designs and develops a wide variety of intriguing and challenging hands-on laboratories that aim to create Eureka Experiences in reference to the ""aha!"" moment of understanding a previously incomprehensible concept. Such an illuminating learning experience is created by incorporating Inquiry-Based Learning (IBL) activities to hands-on laboratories. Overall, this project meets the pressing and essential needs in the Computer Science and Information Technology curricula, has a strong impact on developing the future workforce' core competencies and preparedness in mobile security related to advanced cyberinfrastructure, and helps advance national security.<br/><br/>In this project, three types of hands-on laboratories are designed and developed: i) Exploratory; ii) Core; and, iii) Advanced. The primary purpose of exploratory labs is to spark the interests of high school and community college students from diverse backgrounds to pursue a career in cybersecurity in mobile ecosystems related to advanced cyberinfrastructure. Core labs help prepare both undergraduate and graduate students in STEM for productive cybersecurity careers by enabling enduring understanding of key security concepts and technologies through hands-on practice in an interactive setting. Advanced labs assist future research workforce development by not only introducing emerging security technologies and threats, but also inspiring student research in related fields. In addition, a universal lab platform that is affordable and flexible is designed and developed. This project helps develop core competencies in a number of areas relevant to advanced cyberinfrastructure including how to secure mobile devices and wireless systems, protect large scale and streaming data from mobile and other sources, ensure user privacy, and prevent intrusion. By engaging all stakeholders during the development process, this project increases the likelihood of wide adoption of the developed materials by academic and professional communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829674","CyberTraining: CIP: Collaborative Research: Enhancing Mobile Security Education by Creating Eureka Experiences","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Zhipeng Cai","GA","Georgia State University Research Foundation, Inc.","Standard Grant","chun-hsi huang","08/31/2021","$150,000.00","Yingshu Li","zcai@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","The rapid development and rollout of mobile infrastructure and applications not only bring convenience to people's daily lives, but also give birth to threats that can jeopardize each individual's privacy and national security. Therefore, it is critical to train and educate the future workforce on the fundamental aspects of mobile security relevant to advanced cyberinfrastructure, and to improve their ability to identify, prevent, and respond to emerging threats. This project designs and develops a wide variety of intriguing and challenging hands-on laboratories that aim to create Eureka Experiences in reference to the ""aha!"" moment of understanding a previously incomprehensible concept. Such an illuminating learning experience is created by incorporating Inquiry-Based Learning (IBL) activities to hands-on laboratories. Overall, this project meets the pressing and essential needs in the Computer Science and Information Technology curricula, has a strong impact on developing the future workforce' core competencies and preparedness in mobile security related to advanced cyberinfrastructure, and helps advance national security.<br/><br/>In this project, three types of hands-on laboratories are designed and developed: i) Exploratory; ii) Core; and, iii) Advanced. The primary purpose of exploratory labs is to spark the interests of high school and community college students from diverse backgrounds to pursue a career in cybersecurity in mobile ecosystems related to advanced cyberinfrastructure. Core labs help prepare both undergraduate and graduate students in STEM for productive cybersecurity careers by enabling enduring understanding of key security concepts and technologies through hands-on practice in an interactive setting. Advanced labs assist future research workforce development by not only introducing emerging security technologies and threats, but also inspiring student research in related fields. In addition, a universal lab platform that is affordable and flexible is designed and developed. This project helps develop core competencies in a number of areas relevant to advanced cyberinfrastructure including how to secure mobile devices and wireless systems, protect large scale and streaming data from mobile and other sources, ensure user privacy, and prevent intrusion. By engaging all stakeholders during the development process, this project increases the likelihood of wide adoption of the developed materials by academic and professional communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1829553","CyberTraining: CIP: Collaborative Research: Enhancing Mobile Security Education by Creating Eureka Experiences","OAC","CyberTraining - Training-based","09/01/2018","07/20/2018","Liran Ma","TX","Texas Christian University","Standard Grant","chun-hsi huang","08/31/2021","$250,000.00","Richard Alexander","l.ma@tcu.edu","2800 South University Drive","Fort Worth","TX","761290001","8172577516","CSE","044Y","026Z, 062Z, 7361, 9102, 9179","$0.00","The rapid development and rollout of mobile infrastructure and applications not only bring convenience to people's daily lives, but also give birth to threats that can jeopardize each individual's privacy and national security. Therefore, it is critical to train and educate the future workforce on the fundamental aspects of mobile security relevant to advanced cyberinfrastructure, and to improve their ability to identify, prevent, and respond to emerging threats. This project designs and develops a wide variety of intriguing and challenging hands-on laboratories that aim to create Eureka Experiences in reference to the ""aha!"" moment of understanding a previously incomprehensible concept. Such an illuminating learning experience is created by incorporating Inquiry-Based Learning (IBL) activities to hands-on laboratories. Overall, this project meets the pressing and essential needs in the Computer Science and Information Technology curricula, has a strong impact on developing the future workforce' core competencies and preparedness in mobile security related to advanced cyberinfrastructure, and helps advance national security.<br/><br/>In this project, three types of hands-on laboratories are designed and developed: i) Exploratory; ii) Core; and, iii) Advanced. The primary purpose of exploratory labs is to spark the interests of high school and community college students from diverse backgrounds to pursue a career in cybersecurity in mobile ecosystems related to advanced cyberinfrastructure. Core labs help prepare both undergraduate and graduate students in STEM for productive cybersecurity careers by enabling enduring understanding of key security concepts and technologies through hands-on practice in an interactive setting. Advanced labs assist future research workforce development by not only introducing emerging security technologies and threats, but also inspiring student research in related fields. In addition, a universal lab platform that is affordable and flexible is designed and developed. This project helps develop core competencies in a number of areas relevant to advanced cyberinfrastructure including how to secure mobile devices and wireless systems, protect large scale and streaming data from mobile and other sources, ensure user privacy, and prevent intrusion. By engaging all stakeholders during the development process, this project increases the likelihood of wide adoption of the developed materials by academic and professional communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1907321","Support of the Doctoral Symposium at the IEEE International Conference on Autonomic Computing (ICAC)","OAC","EDUCATION AND WORKFORCE","06/01/2019","03/21/2019","Gregory Ditzler","AZ","University of Arizona","Standard Grant","Sushil K Prasad","05/31/2020","$15,000.00","Salim Hariri","ditzler@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","7361","026Z, 7556, 9179","$0.00","Research in cloud and autonomic computing spans a variety of areas, from distributed systems, computer architecture, middleware services, databases and data-stores, networks, machine learning, and control theory. The purpose of the Fifth International Conference on Autonomic Computing (ICAC) is to bring together researchers and practitioners across these disciplines to address the multiple facets of cloud and autonomic computing. ICAC is holding a doctoral symposium during the conference. The doctoral symposium is meant to engage students attending the symposium with interactions that provide them with research and career mentoring opportunities. The overall impact that such a meeting can have on Ph.D. students, both early and late in their studies, can be significant. The research presentations at the Doctoral Symposium will be made publicly. This project, thus, serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity, and welfare; or to secure the national defense.<br/><br/>To enable students to increase their interactions with other researchers, experts from both industry and academia will be invited to participate in the symposium program and/or deliver keynote talks at the ICAC doctoral symposium. The primary objectives of the doctoral symposium are to (1) provide a venue for graduate students to present their research and obtain feedback from experts in the area; (2) allow Ph.D. students to present their work at the main conference?s poster session; and (3) mentor the PhD students by providing career advice from representatives in industry as well as academia. Furthermore, the ICCAC doctoral symposium will have a special session devoted to career mentoring for Ph.D. students in attendance. The ICAC organization committee understands the diversity in the background of the Ph.D. students that attend the conference - while some will go into industry there will be those who go into academia. Therefore, the career mentoring session will allow students to listen and interact with experts in both areas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659293","CC*Integration: BRACELET:  Robust Cloudlet Infrastructure for Scientific Instruments' Lifetime Connectivity","OAC","CISE RESEARCH RESOURCES","04/15/2017","04/10/2017","Klara Nahrstedt","IL","University of Illinois at Urbana-Champaign","Standard Grant","Deepankar Medhi","03/31/2020","$500,000.00","Roy Campbell, Paul Braun, John Dallesasse, Tracy Smith","klara@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","2890","","$0.00","Universities typically have many scientific instruments in interdisciplinary<br/>research laboratories to conduct high-quality research in science and engineering.<br/>Yet many of these older operating instruments are prematurely disconnected from campus networks<br/>because they cannot operate at the speed of a modern computing devices, or use legacy operating system software<br/>that is not updated with the latest security patches, creating potential vulnerabilities.<br/><br/>This project will develop a robust cloudlet-based infrastructure, called BRACELET. <br/>BRACELET is an integrated three-tier infrastructure that integrates the<br/>existing campus network, cloud, and security infrastructures with the NSF DIBBs program supported 4CeeD<br/>data file upload service. Each cloudlet will be placed alongside potentially vulnerable instruments to shape traffic<br/>and protect against external threats. The cloudlet will play a crucial role in keeping the instrument connected throughout its<br/>lifetime, continuously providing otherwise missing or new performance and security features for the<br/>instrument. BRACELET will extend the capabilities and useful lifetime of scientific instruments, helping to accelerate<br/>scientific innovation and discovery."
"1541106","CRISP Type 2: Interdependent Electric and Cloud Servies for Sustainable, Reliable, and  Open Smart Grids","OAC","CIVIL INFRASTRUCTURE SYSTEMS, INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CISE, CYBERINFRASTRUCTURE, EFRI RESEARCH PROJECTS","09/15/2015","09/11/2015","Manuel Rodriguez-Martinez","PR","University of Puerto Rico Mayaguez","Standard Grant","William Miller","08/31/2019","$1,499,988.00","Rafael Rodriguez, Efrain O'Neill-Carrillo, Marla Perez-Lugo, Fabio Andrade","manuel.rodriguez7@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","1631, 1640, 1714, 7231, 7633","008Z, 029E, 036E, 039E, 9102, 9150","$0.00","Electric energy networks are the cornerstone of the civil infrastructure of our society. These networks provide the energy essential to carrying out daily operations in education, health care, commerce, entertainment, defense, and government. However, electric energy markets, due to their vertical integration, often exclude customers from the processes associated with energy production, pricing, transmission and distribution. Smart grids and distributed generation schemes have been proposed as mechanisms to modernize energy grids and balance the current power structures in electric markets. In a smart grid, computers and communications networks are attached to the power generation, transmission, distribution and load elements, establishing a mechanism to gather information, control generation, control demand, diagnose problems, bid for prices in energy markets, and forecast energy consumption. However, a smart grid creates interdependencies between the energy network and the computer network since the energy network powers the computers that in turn control the operation of the energy grid. In this project, a team from the University of Puerto Rico, Mayaguez (UPRM) will study smart grids and the interdependency between the energy grid and the IT infrastructure that is setup to manage it. This project champions a transformation of the electric grid, moving it away from being centered on centralized utilities that supply most, if not all, power services. Instead, the grid becomes a marketplace of third-party power-service suppliers, who compete to sell their electric services over the Internet.  These services include energy block purchases, storage, billing, weather forecasting, energy demand forecasting, and other ancillary services.  This brings in an important societal element - it empowers common citizens, whose homes are now renewable energy generation systems, to become suppliers and key actors in the energy market. This project is thus aimed at designing and developing the basic science and technology for an Open Access Smart Grid in order to create truly sustainable energy markets.<br/><br/>In this project, the smart grid is modelled as a collection of interdependent electric and cloud services, whose collaborative interactions help manage the smart grid.  All the electric services (e.g., energy, storage, billing) are exposed to users as REST-based cloud services, enabling the development of algorithms and applications for customers, power producers, and other users to consume or subscribe to these electric services, collect operational data and customer feedback, and support analytics to predict electric energy demands. Microgrids and renewable energy systems will be important components in this framework, as they enable modularization of the grid into autonomous or semi-autonomous subsystems. The research team will develop methods to map reliable power microgrids into electric services that can be rapidly brought online to compensate for lost generation capacity or to obtain more affordable energy. A major challenge with microgrid systems is activating them without introduction major power disturbances in the system. Another challenge is forecasting the availability of renewable energy, which will be addressed this by developing rain-cell tracking frameworks for solar and wind output estimation services, and the determination of local sensors requirements to improve short-term forecasts services. Finally, the team will apply the social acceptance model to the development, implementation, management and assessment of the Open Access Smart Grid with the purpose of identifying the institutional change necessary for the integration of all stakeholders and the effective democratization of electric services."
"1713690","Collaborative Research:  Advancing first-principle symmetry-guided nuclear modeling for studies of nucleosynthesis and fundamental symmetries in nature","OAC","PETASCALE - TRACK 1","08/01/2017","05/05/2017","Jerry Draayer","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Edward Walker","07/31/2019","$24,864.00","Kristina Launey, Tomas Dytrych","draayer@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7781","9150","$0.00","Understanding the origin, structure, and phases of hadronic matter is key to comprehending the evolution of the universe. To fully achieve this, we need to model the complex dynamics of atomic nuclei. The scale of computational challenges that are inherent to modeling such intricate quantum many-body systems makes the utilization of petascale level resources, like Blue Waters, essential. The objective of this project is to use the Blue Waters system to advance large-scale modeling of light through intermediate-mass nuclei and, for the first time, medium-mass nuclei, including short-lived isotopes not yet accessible to experiment but key to breakthrough solutions to open questions in basic and applied research, as well as the focus of next-generation radioactive beam facilities. <br/><br/>Specifically, the project will use an innovative approach, the symmetry-adapted no-core shell model (SA-NCSM), which employs exact as well as approximate symmetries of nuclei to solve the nonrelativistic Schrodinger equation for a system of particles interacting via realistic nuclear forces. The proposed calculations will provide nuclear wave functions of unprecedented accuracy that are crucial for gaining further knowledge of fundamental symmetries in nature, as well as on extracting information from large datasets that is essential for neutrino physics, for probing physics beyond the standard model, and modeling of astrophysical processes.   <br/><br/>In addition, the project will train and educate graduate students and postdoctoral researchers in nuclear structure modeling using large-scale computational resources to address important problems in physics and related fields. The students will acquire and further their skills in the development of high performance computational physics software, laying the foundations for their careers in science and technology.  Moreover, the project will provide open access to software and data, including nuclear structure information of unprecedented accuracy and scope -- only possible when coupled with the Blue Waters computational capabilities -- as a publicly available database."
"1849970","CRII: OAC: RUI: Improving Software Deployments to Serverless Computing Environments","OAC","CRII CISE Research Initiation","03/15/2019","02/14/2019","Wes Lloyd","WA","University of Washington","Standard Grant","Sushil K Prasad","02/28/2021","$175,000.00","","wlloyd@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","026Y","8228","$0.00","A form of cloud computing, called serverless computing, has recently emerged as a new approach to abstract the management and configuration of cloud computing servers to deliver compute resources to end users, while promising performance improvements, around-the-clock availability, and energy savings to enable a better return on investment from computer hardware.  The benefits of serverless computing are offset, however, by new software development challenges arising from mitigation of resource constraints employed by cloud providers to share physical servers among many users, and by the complexity of understanding performance and cost implications of software designed for serverless platforms.  This project serves the national interest, as stated by NSF's mission: to promote the progress of science, and to promote national prosperity and welfare by developing a reusable framework enabling new data driven analytics and evaluation techniques that characterize the quality of software designed for serverless computing platforms.  These advancements serve to extend the applicability of serverless computing to a broader range of use cases enabling the next generation of cloud software to realize performance improvements and cost savings. This project additionally leverages the simplicity of serverless computing to develop an online hour-of-code tutorial to introduce cloud computing, client/server, and distributed systems concepts to high school students for dissemination.<br/><br/>This research develops the Serverless Application Analytics Framework (SAAF), a novel framework that supports the characterization of software deployed to serverless computing platforms enabling the collection of resource utilization (CPU, memory, disk/network I/O), infrastructure (type, state, and distribution), load balancing, and performance metrics.  SAAF enables an improved understanding of several factors that impact performance.  This research develops predictive models using statistical and machine learning approaches to automate the characterization of performance and hosting cost of serverless software leveraging metrics from SAAF, and software quality metrics from static analysis tools as features.  SAAF enables identification of best practices over serverless architectures (monolithic, composable, and RESTful APIs), software composition patterns (monolithic, fine grained, library composition, and switchboard), and workflow instrumentation (remote client, vendor service, microservice controller, and asynchronous call chains).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833317","Open Compass: Leveraging the Compass AI Engineering Testbed to Accelerate Open Research","OAC","ETF","05/01/2018","04/25/2018","Paola Buitrago","PA","Carnegie-Mellon University","Standard Grant","Robert Chadduck","04/30/2020","$300,000.00","Nicholas Nystrom","paola@psc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7476","7916","$0.00","Artificial intelligence (AI) has immense potential to contribute to advances spanning progress in science, the national health, prosperity and welfare, education, benefit society, or secure the nation's defense. Research initiatives, conferences, investments, and products based on AI abound and are expanding rapidly, while the methods, performance, and understanding of AI are in their infancy. Researchers face vexing issues such as how to improve performance, transferability, reliability, comprehensibility, and how better to train AI models with only limited data. Future progress depends on advances in hardware accelerators, software frameworks, system architectures, and creating cross-cutting expertise between scientific and AI domains. One way to accelerate progress on these topics is to create an engineering testbed, which provides a controlled environment that allows investigators to explore solutions to these - and other - challenges.  Open Compass is an exploratory research project to conduct academic pilot projects on an advanced engineering testbed for artificial intelligence, culminating in the development and publication of best practices. <br/><br/>Open Compass will: 1) engage pilot projects and research groups, documenting approaches, experiences, and lessons learned; 2) conduct training events, in-person and using the Pittsburgh Supercomputing Center's wide area classroom; 3) organize and conduct a workshop focusing on advanced AI technologies; 4) integrate experiences gained through open research collaboration with those of industry experiences to identify a comprehensive set of best practices; and 5) publish results and best practices in peer-reviewed journals, conferences, and technical reports. The broad community will benefit from publication of research results, experiences, and a knowledge base of best practices. The research community will gain access to new technologies on which to develop algorithms and applications, along with insight across new fields of those technologies' applicability and important trends for AI. Open Compass will promote workforce development through student involvement in pilot projects and training and provide feedback to industry for to enable more efficient future AI technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832257","Collaborative Research: Mentoring the Next Generation of Parallel Processing Researchers at IPDPS and other IEEE-CSTCPP Sponsored Conferences","OAC","INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE","05/01/2018","05/08/2018","Jaroslaw Zola","NY","SUNY at Buffalo","Standard Grant","Sushil K Prasad","04/30/2019","$25,000.00","","jzola@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1640, 7361","7556, 9102","$0.00","From their attendance to conferences, students can form professional connections and establish collaborations, market themselves to potential employers, get exposed to a wider network of mentors, and find inspiration for novel research avenues. Student mentoring programs play an important role in providing younger attendees with the necessary tools to take advantage of their participation at conferences. Student programs have emerged in several leading conferences, but are still far from being widely adopted. In particular the International Parallel and Distributed Processing Symposium (IPDPS) has been hosting a PhD forum and student program for over a decade. This program has evolved into a comprehensive training workshop that includes sessions on career planning and hands on tutorials on writing and presentation skills; it also provides effective opportunities for student networking. This project seeks to expand the IPDPS mentoring and outreach model into other conferences that fall under the broad field of parallel and distributed processing. Parallel and distributed processing has become increasingly important for a variety of disciplines where traditional computational methods lack the mechanisms to deal with large data volumes or expensive computations. As stated by NSF's mission, this project supports education and diversity, while promoting the progress of science by mentoring the next-generation workforce in the high performance computing discipline.<br/><br/>This project has two concrete goals: 1) supporting student participation at the 32nd IEEE International Parallel and Distributed Processing Symposium. IPDPS is an international conference for engineers and scientists from around the world to present their latest research findings in all aspects of parallel computation. 2) promoting the adoption of student mentoring programs in other IEEE TCPP conferences, such as: AICCSA, DCOSS, DS-RT, e-Energy, HiPC, ICCABS, ICPADS, NAS, PACT, PERCOM. The beneficiary conferences are required to facilitate an introductory session where students network with peers and mentors. Additionally, conference organizers are provided with the logistic plan of IPDPS?s PhD Forum and are encouraged to have similar proven activities; including a poster session and mentoring workshops. Only students currently studying at U.S. universities are eligible to receive support from this NSF-funded project. This project targets especially students that typically cannot attended these conferences without financial support, such as undergraduate students, graduate students in their first years, and students attending their first conference. In order to increase the diversity of attendees, the project strongly encourages the participation of females and other underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835499","Elements: Software: Roundoff-Error-Free Algorithms for Large-Scale, Sparse Systems of Linear Equations and Optimization","OAC","Software Institutes","06/01/2019","08/31/2018","Erick Moreno-Centeno","TX","Texas A&M Engineering Experiment Station","Standard Grant","Stefan Robila","05/31/2022","$600,000.00","Timothy Davis","emc@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Solving systems of linear equations is central to solving problems in numerous applications within healthcare, power generation, national defense, economics, physics, chemistry, mathematics, computer science and engineering.  Nowadays, a large number of these critical applications have an ever-increasing need for faster, more reliable, and more accurate solutions. For example, more accurate treatment plans are proven to be less costly, less invasive, safer, and more reliable outcomes for prostate-cancer brachytherapy (placement of radioactive ""seeds"" inside a tumor). Similarly, millions of dollars can be saved, and cleaner energy can be produced, by solving the power generation dispatch optimization problem with more accuracy. Paradoxically, today's state-of-the-art software tools are limited to calculating limited-precision solutions (e.g., treatment plans and power dispatches).  This is due in part to the prevalence of computing methods relying on floating-point arithmetic (i.e., arithmetic using truncated decimal numbers). At the same time, real-life problems in a wide range of applications are becoming larger and so more prone to incorrect results due to roundoff errors (errors introduced when truncating the decimal numbers). The primary goal of this project is to design, create, and deploy computational tools to solve large-scale, sparse systems of linear equations and optimization problems without any error at all. Because of the ubiquity of solving systems of linear equations and optimization problems, the outcomes of this project will directly translate in software that is more reliable for applications across academia, industry, and government.  <br/><br/>Large-scale, sparse systems of linear equations (SLEs) and linear optimization problems (LPs) are routinely solved and the accuracy/correctness of solvers is taken for granted. However, state-of-the-art solvers commonly report incorrect results, some as striking as misclassifying feasible problems as infeasible and vice versa or even failing altogether.  Moreover, exactly solving SLEs and LPs is of fundamental importance for applications where fixed-precision standards have been deemed inadequate, including specific applications in healthcare, power generation, biology, combinatorial auctions, and formal verification of mathematical proofs.  Therefore, the first objective of this project is to devise efficient algorithms and implement robust software to reliably and exactly solve large-scale, sparse SLEs, free of any roundoff error. This objective will build on our recently devised roundoff-error-free (REF) LU and Cholesky factorizations for dense matrices. The second objective of this project is to devise efficient algorithms and  implement robust software to reliably and exactly (REF) solve  large-scale, sparse LPs.  The specific outcomes of this project  include: (1) Devise an efficient REF factorization framework for  large-scale sparse matrices, including devising good fill-reducing  orderings that consider the bit-size growth of the entries; (2)  Devise REF optimization algorithms to exactly solve large-scale,  sparse linear programs; (3) Our software will be rigorously tested,  with a full 100% test coverage suite and scaffolding code to test  loop invariants and data sanity. The software products will be  submitted as algorithm papers to the ACM Transactions on Mathematical  software, where the code itself, test suite and documentation undergo  rigorous peer review. Finally, we will incorporate our solvers into  our existing SuiteSparse installations, including all Linux distros  with the ultimate goal of being integrated into MATLAB and thus  accessible to a wide user base.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642440","SI2-SSE: PAPI Unifying Layer for Software-Defined Events (PULSE)","OAC","Software Institutes","11/01/2016","09/08/2016","Anthony Danalis","TN","University of Tennessee Knoxville","Standard Grant","Stefan Robila","10/31/2019","$499,997.00","Heike Jagode","adanalis@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Leading scientific domains, such as physics, chemistry, climate science, and advanced materials design, utilize high-performance computing (HPC) to understand and solve problems of unprecedented complexity. Overcoming such challenges requires the ability to perform advanced scientific and engineering simulations, and to analyze the extreme amount of data these computer models involve. But the ever increasing scale of these problems also means that the complexity of software systems needed to address them is rising, and this fact raises new challenges for the scientific application communities that require HPC in order to achieve their goals. In particular, HPC application developers who want to understand the performance characteristics of their application had previously been able to monitor the way it interacted with the underlying hardware, but they had no access to a standardized way of accessing the behavior of the complex software stack that their application depends on. The PULSE project will fill this major software infrastructure gap. It offers an integrated solution that enables different layers of a complex software stack to communicate with one another and provide information about their internal behavior. Thus, PULSE makes it possible for the scientific applications of the future to harness ever increasing amounts of computing power, despite anticipated increases in the complexity of both future hardware and software technologies. By providing the infrastructure that developers need in order to achieve an analytical understanding of the behavior of whole programs, it will substantially improve the insight computational scientists have into how various modern software systems interact with one another and the underlying hardware technologies. &#8232;<br/><br/>The abstraction and standardization layer provided by the Performance Application Programming Interface (PAPI) has played a critical role in enabling application profiling for over a decade. It has enabled performance conscious developers to gain insights about their application by simply instrumenting their code using a handful of PAPI functions that interoperate across different hardware substrates. At the same time, the abstraction layer offered by PAPI has enabled sophisticated profiling toolkits to focus on combining, organizing and visualizing information in a way that is useful to the end user, instead of re-implementing the hardware access layer for every new platform that comes to market. However, this abstraction layer that PAPI offers has been limited to profiling information generated by hardware. Information regarding the behavior of the software stack underneath the application that is being profiled has to be acquired either through low level binary instrumentation, or through custom APIs. Now, through this PULSE project, abstraction and unification layer for profiling software events has emerged. PULSE will extend the abstraction and unification layer that PAPI has provided to hardware events to also encompass software events. On one end, it will provide a standard, well defined and well documented API that high level profiling software can utilize to acquire and present to application developers performance information about the libraries used by their application. On the other end, it will provide standard APIs that library and runtime writers can utilize to communicate to higher software layers information about the behavior of their software. The project is expected to have a direct influence on the state of the art in whole program profiling and understanding. The success of PULSE will substantially improve the insight of computational scientists and engineers into the way that different modules of modern software interact with one another and the underlying hardware. Broadening the applicability of PAPI, as proposed under PULSE, is expected to dramatically increase PAPI's impact in this area."
"1809073","Three-Dimensional Simulations of Core-Collapse Supernovae","OAC","PETASCALE - TRACK 1","04/01/2018","03/22/2018","Adam Burrows","NJ","Princeton University","Standard Grant","Edward Walker","03/31/2020","$8,100.00","","burrows@astro.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7781","","$0.00","Core-collapse supernovae dramatically announce the death of massive stars and the birth of neutron stars. These supernovae occur when the iron core of a massive star collapses to a neutron star. Releasing its gravitational binding energy in a violent explosion as neutrinos, the resulting neutron star, for a few seconds, outshines the rest of the observable universe.  Viewed as a nuclear physics laboratory, core-collapse supernovae produce the highest densities of matter and energy in the modern universe.  However, the precise mechanism of this explosion has not been unambiguously pinned down and this fifty-year old conundrum is one of the central remaining unsolved problems in theoretical astrophysics.  Supernova theory is an amalgam of much of physics, and represents one of the most complex computational problems of modern science. Supercomputers, such as Blue Waters, are essential to make progress in simulating and understanding the evolution of the supernova event and neutron star birth.  A solution to the core-collapse supernova problem would benefit ongoing efforts of observers and instrument designers in the U.S. and around the world engaged in projects to determine the origin of the elements, measure gravitational waves (LIGO), and other important physics experiments.<br/><br/>The project will conduct three-dimensional radiation/hydrodynamic simulations of core-collapse supernovae with the goal of constraining, and ultimately determining, the mechanism of explosion.  During this explosion process, a combination of high-density nuclear physics, multi-dimensional hydrodynamics, radiation transport, and neutrino physics determines whether and how the star explodes.   The project will use the newly developed and tested code FORNAX incorporating state-of-the-art microphysics and methodologies, with excellent scalability to beyond 100,000 cores per tasks.   Various observational diagnostics, such as neutrino and gravitational-wave signatures and residual neutron star masses and kicks, will be derived for all models calculated.  Supernova explosions and their products are central to the origin of the elements, the birth of pulsars and black holes, and the dynamics of galaxies and the interstellar medium, and a fundamental theoretical understanding of such explosions will inform the interpretation of data derived from, among other platforms, NASA's Chandra X-ray Observatory, the Hubble Space Telescope, Swift, NuStar, and the upcoming James Webb Space Telescope.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1713712","Collaborative Research: Advancing first-principle symmetry-guided nuclear modeling for studies of nucleosynthesis and fundamental symmetries in nature","OAC","PETASCALE - TRACK 1","08/01/2017","05/05/2017","William Tang","NJ","Princeton University","Standard Grant","Edward Walker","07/31/2019","$15,066.00","Bei Wang","wtang@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7781","","$0.00","Understanding the origin, structure, and phases of hadronic matter is key to comprehending the evolution of the universe. To fully achieve this, we need to model the complex dynamics of atomic nuclei. The scale of computational challenges that are inherent to modeling such intricate quantum many-body systems makes the utilization of petascale level resources, like Blue Waters, essential. The objective of this project is to use the Blue Waters system to advance large-scale modeling of light through intermediate-mass nuclei and, for the first time, medium-mass nuclei, including short-lived isotopes not yet accessible to experiment but key to breakthrough solutions to open questions in basic and applied research, as well as the focus of next-generation radioactive beam facilities. <br/><br/>Specifically, the project will use an innovative approach, the symmetry-adapted no-core shell model (SA-NCSM), which employs exact as well as approximate symmetries of nuclei to solve the nonrelativistic Schrodinger equation for a system of particles interacting via realistic nuclear forces. The proposed calculations will provide nuclear wave functions of unprecedented accuracy that are crucial for gaining further knowledge of fundamental symmetries in nature, as well as on extracting information from large datasets that is essential for neutrino physics, for probing physics beyond the standard model, and modeling of astrophysical processes.   <br/><br/>In addition, the project will train and educate graduate students and postdoctoral researchers in nuclear structure modeling using large-scale computational resources to address important problems in physics and related fields. The students will acquire and further their skills in the development of high performance computational physics software, laying the foundations for their careers in science and technology.  Moreover, the project will provide open access to software and data, including nuclear structure information of unprecedented accuracy and scope -- only possible when coupled with the Blue Waters computational capabilities -- as a publicly available database."
"1566259","CRII: ACI: Efficient Radiative Heat Transfer Modeling In Large-Scale Combustion Systems","OAC","CRII CISE Research Initiation, EDUCATION AND WORKFORCE","07/01/2016","07/17/2017","Xinyu Zhao","CT","University of Connecticut","Standard Grant","Sushil K Prasad","06/30/2019","$182,999.00","","xinyu.zhao@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","026Y, 7361","8228, 9102, 9251","$0.00","Thermal radiation is an important but less adequately understood heat transfer process for large-scale thermal systems. Radiative heat transfer accounts for more than 70% of the total heat transfer to the ambient environment in large-scale fires. Pollutants that are produced by combustion systems, such as particulate matters, NOx and SOx (main contributor to acid rain), are highly sensitive to the thermal effects of radiative heat transfer. To correctly predict fire propagation and pollutant emission, and to guide power plant retrofit, high-fidelity radiation modeling for large-scale combustion systems is needed. However, the expensive computational cost of high-fidelity radiation models, their intensive memory requirements, and poor scaling performances have traditionally prevented their applications beyond toy or small-scale problems. Modern high performance computing systems have evolved to a stage where massive parallelism can be harnessed but memory-per-core is decreasing. Therefore, new modeling and parallelism strategies for thermal radiation prediction are required to leverage the power of current and future cyber-infrastructure. To advance the understanding of the thermal radiation processes, and to enable the application of predictive models to practical engineering systems, this CRII project aims at optimizing the solution algorithms and parallelism strategies of high-fidelity radiation models for the modern heterogeneous many-core high performance computing systems.  Therefore, this research aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>The overarching goal of this project is to break the barrier of applying high-fidelity radiation models to practical large-scale systems, utilizing modern cyber-infrastructure. As a result, the heat flux on the computational boundaries as well as pollutant emissions can be better predicted, which can reduce the fire loss and alleviate the environmental concerns with pollutions. Specifically, the project focuses on enhancing the parallelism of a Monte Carlo based high-fidelity radiation model, using the hybrid computing environment provided by the many-integrated-core (MIC) co-processors. As proposed, the high-fidelity radiation model will be coupled to an open-source fire simulator, and will be validated against well-documented experimental data. By identifying the disparate time scales of different physical processes, solution algorithm is first optimized to enhance the overall efficiency of the proposed code. Hybrid parallelism with message passing interface (MPI) and OpenMP is then proposed to achieve the desired reduction in the ""time to solution."" Finally, the accuracy and efficiency of the developed fire-radiation code will be demonstrated through a large-scale fire simulation."
"1609842","CDS&E: Exploiting Multiple Levels of Parallelism in Quantum Chemistry Software","OAC","Theory, Models, Comput. Method, CDS&E","08/01/2016","03/28/2016","Edmond Chow","GA","Georgia Tech Research Corporation","Standard Grant","Micah Beck","07/31/2019","$690,710.00","Charles Sherrill","echow@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","6881, 8084","7433, 8084","$0.00","Quantum chemistry methods and software give scientists a computational tool to predict and study the properties of molecular systems.  These tools are used widely, ranging from fundamental chemistry and biochemistry to pharmaceutical and materials design.  Improvements in these methods and software give scientists an even more powerful tool for discovery.  This project will develop new methods and software for quantum chemistry aimed at fully exploiting the parallel capabilities of today's computer hardware.  Parallel capabilities within individual processor cores will be specifically exploited, as well as the ability to use multiple compute nodes concurrently for a single calculation.  Success in this project will promote the progress of science, specifically for efficiently studying very large molecular systems with quantum chemistry tools, by decreasing the computer time needed for studies with the same accuracy and increasing the accuracy of studies that use the same computational time.<br/> <br/>This project will first target the computation of electron repulsion integrals (ERIs) used in essentially all quantum chemistry software.  The calculation of ERIs using the Obara-Saika (OS) method will be reorganized to exploit the single instruction multiple data (SIMD) capabilities of modern computer processors.  The calculation of the Boys function used in the OS method will also be addressed.  An optimized, open source software library for ERI calculation will be released, which will include functionality for one-electron integrals and integral derivatives.  This project will also further develop the GTFock quantum chemistry framework to make it easier to use by adding several interfaces.  GTFock, which has efficient distributed parallel capabilities, will be extended to include symmetry-adapted perturbation theory.  GTFock will be used to study large protein-ligand systems.  In addition, this project will involve undergraduate students and will be used to motivate research in the classroom."
"1450300","Collaborative Research: SI2-SSI:Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","05/15/2015","05/20/2015","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Bogdan Mihaila","04/30/2019","$1,178,068.00","Thomas Herault","bosilca@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","1253, 1712, 8004","7433, 8009, 8084, 9150, 9216","$0.00","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US.  TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects.  TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries."
"1450262","Collaborative Research: SI2-SSI:Task-based Environment for Scientific Simulation at Extreme Scale (TESSE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","05/15/2015","05/20/2015","Edward Valeev","VA","Virginia Polytechnic Institute and State University","Standard Grant","Bogdan Mihaila","04/30/2019","$600,000.00","","evaleev@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1253, 1712, 8004","7433, 8009, 8084, 9216","$0.00","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US.  TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects.  TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries."
"1450344","Collaborative Research: SI2-SSI: Task-Based Environment for Scientific Simulation at Extreme Scale (TESSE)","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, EDUCATION AND WORKFORCE, Software Institutes","05/15/2015","04/17/2017","Robert Harrison","NY","SUNY at Stony Brook","Standard Grant","Bogdan Mihaila","04/30/2019","$622,808.00","","robert.harrison@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","1253, 1712, 7361, 8004","7433, 7556, 8004, 8009, 8084, 9216, 9251","$0.00","This project, TESSE, is developing a new-generation programming environment that uniquely address the needs of emerging computational models in chemistry and other fields, and allowing these models to reap the benefits of the computer hardware of tomorrow. The project thus is bringing closer the ability to predict properties of matter and design matter transformations of importance to the technological leadership and energy security of the US.  TESSE's impact has scientific, cyberinfrastructure, and interdisciplinary educational aspects.  TESSE helps carry widely used scientific applications (NWChem, MADNESS, MPQC, etc.) into a whole new generation of many-core and accelerated architectures, while at the same time dramatically improving programmer productivity. In terms of cyberinfrastructure, TESSE's PaRSEC runtime system delivers capabilities that many libraries and advanced applications will require for high performance on large scale and hybridized systems.<br/><br/>The goals of this project, Task-based Environment for Scientific Simulation at Extreme Scale (TESSE), are to design and demonstrate via substantial scientific simulations within chemistry and other disciplines a prototype software framework that provides a groundbreaking response to the twin problems of portable performance and programmer productivity for advanced scientific applications on emerging massively-parallel, hybrid, many-core systems. TESSE will create a viable foundation for a new generation of science codes, one which enables even more rapid exploration of new physical models, provides greatly enhanced performance portability through directed acyclic graph (DAG) scheduling and auto-tuned kernels, and works towards full interoperability between major chemistry packages through compatible runtimes and data structures. TESSE will mature to become a standard, widely available, community-based and broadly-applicable parallel programming environment complementing and rivaling MPI/OpenMP. This is needed due to the widely appreciated shortfalls of existing mainstream programming models and the already huge successes of the existing DAG-based runtimes that are the foundation of the next generation of NSF- and DOE-supported (Sca)LAPACK high-performance linear algebra libraries."
"1832190","Collaborative Research: Mentoring the Next Generation of Parallel Processing Researchers at IPDPS and other IEEE-CSTCPP Sponsored Conferences","OAC","INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE","05/01/2018","05/08/2018","Trilce Estrada-Piedra","NM","University of New Mexico","Standard Grant","Sushil Prasad","04/30/2019","$25,000.00","","estrada@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","1640, 7361","7556, 9102, 9150","$0.00","From their attendance to conferences, students can form professional connections and establish collaborations, market themselves to potential employers, get exposed to a wider network of mentors, and find inspiration for novel research avenues. Student mentoring programs play an important role in providing younger attendees with the necessary tools to take advantage of their participation at conferences. Student programs have emerged in several leading conferences, but are still far from being widely adopted. In particular the International Parallel and Distributed Processing Symposium (IPDPS) has been hosting a PhD forum and student program for over a decade. This program has evolved into a comprehensive training workshop that includes sessions on career planning and hands on tutorials on writing and presentation skills; it also provides effective opportunities for student networking. This project seeks to expand the IPDPS mentoring and outreach model into other conferences that fall under the broad field of parallel and distributed processing. Parallel and distributed processing has become increasingly important for a variety of disciplines where traditional computational methods lack the mechanisms to deal with large data volumes or expensive computations. As stated by NSF's mission, this project supports education and diversity, while promoting the progress of science by mentoring the next-generation workforce in the high performance computing discipline.<br/><br/>This project has two concrete goals: 1) supporting student participation at the 32nd IEEE International Parallel and Distributed Processing Symposium. IPDPS is an international conference for engineers and scientists from around the world to present their latest research findings in all aspects of parallel computation. 2) promoting the adoption of student mentoring programs in other IEEE TCPP conferences, such as: AICCSA, DCOSS, DS-RT, e-Energy, HiPC, ICCABS, ICPADS, NAS, PACT, PERCOM. The beneficiary conferences are required to facilitate an introductory session where students network with peers and mentors. Additionally, conference organizers are provided with the logistic plan of IPDPS?s PhD Forum and are encouraged to have similar proven activities; including a poster session and mentoring workshops. Only students currently studying at U.S. universities are eligible to receive support from this NSF-funded project. This project targets especially students that typically cannot attended these conferences without financial support, such as undergraduate students, graduate students in their first years, and students attending their first conference. In order to increase the diversity of attendees, the project strongly encourages the participation of females and other underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1713792","Improving Earthquake Forecasting and Seismic Hazard Analysis Through Extreme-Scale Simulations","OAC","PETASCALE - TRACK 1","05/01/2017","09/29/2017","John Vidale","CA","University of Southern California","Standard Grant","Edward Walker","04/30/2019","$40,000.00","Philip Maechling, Kim Olsen, Yifeng Cui, Ricardo Taborda","seismoguy@mac.com","University Park","Los Angeles","CA","900890001","2137407762","CSE","7781","","$0.00","Earthquakes emerge from complex, multiscale interactions across time scales that range from milliseconds to millions of years within active faults systems that are incredibly difficult to observe. Large-scale physics-based earthquake simulations are essential scientific tools that can be used to better understand these hazardous natural phenomena. This project will develop physics-based codes for simulating earthquakes on Blue Waters and apply these simulation capabilities to improve existing hazard analysis methods. The very large scale computing and data management capabilities of the  Blue Waters system will allow the project to develop and test earthquake models that capture physics in a more realistic manner, and to run simulations at finer resolutions and higher frequencies. The results will better quantify seismic hazards and their uncertainties. <br/><br/>This project will advance physics-based probabilistic seismic hazard analysis (PSHA) methods using numerical experimentation and large-scale simulations to increase the scale range in current representations of source physics, anelasticity, and geologic heterogeneity.  Specifically, the research project will work towards seven computational objectives defined to improve our understanding of earthquake processes and advance physics-based PSHA: (1) Develop an empirically-calibrated physics-based earthquake forecast. (2) Develop a statistically sufficient, but reduced, rupture set representative of the new Unified California Earthquake Rupture Forecast. (3) Implement a new dynamic-rupture based kinematic source model to compute ground motions up to 8 cycles per second. (4) Evaluate the basin connectivity phenomenon observed in previous simulations to establish the importance of waveguide modeling at low seismic frequencies. (5) Investigate and characterize the influence of material and source models on the accuracy of ground motion simulations. (6) Validate and calibrate the rheological models used in simulations. (7) Test the physics-based hazard capabilities on a vulnerable embankment dam. The goal of this research is to improve the physical representations of earthquake processes and the deterministic codes for simulating earthquakes, which will improve PSHA practice in the United States and benefit earthquake system science worldwide."
"1810976","Petascale Polar Topography Production","OAC","PETASCALE - TRACK 1","05/01/2018","03/22/2018","Paul Morin","MN","University of Minnesota-Twin Cities","Standard Grant","Edward Walker","04/30/2019","$14,999.00","Claire Porter, Charles Nguyen, Ian Howat, Myoung-Jong Noh","lpaul@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7781","","$0.00","The polar regions are changing faster than any other region of the Earth, with accelerating rates of coastal erosion, permafrost loss, glacier thinning, and other changes. Yet the poles are also among the least well observed regions, due to both their relative inaccessibility and coverage limitations in prior global surveys by remote sensing.  The project will use the high performance computing capabilities of Blue Waters to produce high-resolution, high-quality, time-dependent, and openly distributed digital elevation models (DEMs) for the Earth's poles. This data set is needed for measuring and understanding rapid, ongoing changes to the polar landscapes, as well as for impact mitigation and adaptation planning by polar communities. The produced DEMs will be distributed at no cost to the science community and public at large.  <br/><br/>The project will use Blue Waters to produce 2m resolution DEMs with absolute accuracies of approximately 1m or better and relative accuracies of 0.2m over the Antarctic and Arctic regions.  The project incorporates the capability for repeat, high-resolution and high-precision topographic mapping in the form of sub-meter stereoscopic imagery from the DigitalGlobe constellation of five polar-orbiting sun-synchronous Earth imagers (WorldView 1-4 and GeoEye-1), as well as the archive of IKONOS and QuickBird.  These satellites images allow for precise stereo photogrammetric DEM construction from overlapping image pairs over a wide range of terrain types and light conditions, including the ""flat white"" interior of ice sheets and shadowed mountain faces. The time-dependent collection of DEMs will be invaluable to scientists studying change in the polar region.   Resources for obtaining and maintaining the imagery, tools for post-processing the results, software development time, and methods for distributing the DEMs have been secured by the project.  All DEMs will be publically distributed through the Polar Geospatial Center (PGC) website and a consortium including the National Geospatial Agency (NGA) and NASA, as well as Amazon web services and the geospatial software company ESRI.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1713711","Simulating Faintest Galaxies in the JWST Era","OAC","PETASCALE - TRACK 1","06/01/2017","05/05/2017","Nickolay Gnedin","IL","University of Chicago","Standard Grant","Edward Walker","05/31/2019","$16,368.00","","gnedin@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7781","","$0.00","The study of cosmic reionization -- the process of ionization of the bulk of cosmic gas by high energy radiation from early galaxies -- is one of the most promising areas of astrophysical research in the current decade.  The end of this decade will see an amazing increase in observational capabilities for the epoch of reionization. The breakthrough is spearheaded by the James Webb Space Telescope (JWST), the flagship NASA mission for this and next decade, as well as several other observational developments to follow.  All there observational probes will flood the field with high quality observational data, whose reach will be limited unless the observational progress can be met with equivalent advances in theory, of which numerical modeling is a primary tool.<br/><br/>One of the JWST primary science goals is to push the detection limit of high redshift galaxies to sufficiently faint levels, where the luminosity function is expected to ""turn over"".  The primary scientific goal of this project is to use the petascale capabilities of Blue Waters to explore the physical mechanisms for the turnover at the faint end of the galaxy UV luminosity function. Since the JWST reach is sufficient to detect the expected turnover in cluster-lensed ""Frontier Fields"", theory must be ready to confront such observations with adequate interpretive tools.  This project is part of the Cosmic Reionization On Computers (CROC) project. The ultimate goal of the project is to develop simulation technology to be able to model fully all relevant physics, from radiative transfer to gas dynamics and star formation, in simulation volumes of over 100 comoving Mpc and with spatial resolution approaching 100 pc in physical units, thus covering the full range of spatial, temporal, and mass scales relevant to modeling reionization.<br/><br/>In terms of broader impacts, this project will serve as a vehicle for professional training of graduate students in high performance computing.  In addition, this project has substantial outreach potential via a strong connection between the University of Chicago and Adler Planetarium."
"1835863","Collaborative Research: HDR Elements: Software for a new machine learning based parameterization of moist convection for improved climate and weather prediction using deep learning","OAC","CLIMATE & LARGE-SCALE DYNAMICS, DATANET","10/01/2018","08/13/2018","Michael Pritchard","CA","University of California-Irvine","Standard Grant","Amy Walton","09/30/2021","$289,409.00","","mspritch@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","5740, 7726","062Z, 077Z, 4444, 7923","$0.00","This project targets a difficult problem in weather and climate prediction -- the representation of convection.  Accurate representation of convection is important, since a majority of current model predictions depend on it.  Unraveling the physics involved in convective conditions, clouds and aerosols may take years of modeling to fully understand; however, a set of machine learning techniques, known as ""neural net techniques"", may provide enhanced predictability in the interim, and this project explores their potential.<br/><br/>The project develops a Python library enabling the use of machine learning (artificial neural networks) in a broad range of science domains. The focus is on integration of convection and cloud formation within larger-scale climate models, with the Community Earth System Model (CESM) as an initial target.  The project develops a new set of machine learning climate model parameterizations to reduce uncertainty in weather and climate predictions.  The neural networks will be trained on high-fidelity simulations that explicitly resolve convection.  Two types of high-resolution simulations will be used for training the neural networks: 1) an augmented super-parameterized simulation, and 2) a full Global Cloud Resolving Model (GCRM) simulation based on the ICOsahedral Non-hydrostatic (ICON) modelling frameworks provided by the Max Planck Institute, using initial 5km horizontal resolution.  The effort has the potential to increase understanding of convection dynamics and processes across scales, and could potentially be implemented to address other scale problems as well, where it is too computationally costly or impractical to represent processes occurring at much finer scales than the main grid resolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835511","Cyberinfrastructure for Sustained Scientific Innovation - Software Elements: Cloud WRF for the Atmospheric Research and Education Communities","OAC","LARS SPECIAL PROGRAMS, Software Institutes, EarthCube","10/01/2018","07/31/2018","Jordan Powers","CO","University Corporation For Atmospheric Res","Standard Grant","Stefan Robila","09/30/2020","$284,201.00","Yuh-Lang Lin, Russ Schumacher","powers@ncar.ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","7790, 8004, 8074","026Z, 062Z, 077Z, 1525, 4444, 7923, 8004","$0.00","This award supports the establishment of an officially-supported version of the Weather and Research Forecast (WRF) model in the cloud environment.  WRF is the world's most popular numerical weather prediction model and is supported by the National Center for Atmospheric Research (NCAR) for a community of users across universities, research labs, and operational weather centers.  This project will address fundamental issues such as modeling system accessibility, improvement of model support to the research and educational user communities, student and scientist training, and facilitation of model development. Given WRF's prominence in both atmospheric research and real-time weather forecasting, this work will not only promote the advancement of science, but also will contribute to the vigor of development and application of one of the nation's cyberinfrastucture (CI) assets, a key weather prediction model used at operational centers and for public purposes.  Furthermore, this project will contribute to education at minority-serving institutions and will yield tools to better the training of new generations of the nation's atmospheric scientists. Lastly, the project will leverage the resources and support of commercial cloud service providers to serve these national interests, in a collaboration of the principal researchers with industry.<br/><br/>The viability of running the WRF in the cloud has been previously demonstrated, and this project would advance the field by configuring and supporting the official version of the WRF in the cloud, with an up-to-date, cloud-configured version of the WRF system code synced to the WRF GitHub repository.  Other materials that will be available include the WRF tutorial materials and system documentation, WRF input and output datasets, and the WRF Testing Framework code analysis package.  The project will produce a configured and documented cloud WRF system that will open a new arena for model use and that will enhance the efficiency of both user support and the integration of contributed model improvements.  This cloud capability will exploit an emerging common cyber-ground to facilitate code development for the target model and allow for critical reproducibility in code analysis and testing.  It will advance the delivery of modeling system tutorials, and thus scientist training and education, through establishing globally-accessible modeling spaces, enhancing instruction portability and access, decreasing costs for host institutions, and improving usability of on-line materials.  These new cloud capabilities will be addressing bottlenecks in model support and development, and once developed, the CI can be adapted for other community models, thus benefiting other scientific disciplines and their tools via its reuse.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1735609","CRISP Type 1: Protecting Coastal Infrastructure in a Changing Climate by Integrating Optimization Modeling and Stakeholder Observations","OAC","CRISP - Critical Resilient Int, CYBERINFRASTRUCTURE","09/01/2017","05/29/2018","Kyle Mandli","NY","Columbia University","Standard Grant","William Miller","08/31/2019","$515,310.00","Rebecca Morss, Heather Lazrus, Daniel Bienstock, George Deodatis","kyle.mandli@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","027Y, 7231","008Z, 062Z, 116E, 7433, 7918, 8004, 9178, 9231, 9251","$0.00","Infrastructure - such as roads, bridges, railways - is the backbone of a functional and healthy community. When parts of this infrastructure are threatened, it is critical for society to respond to that threat or risk loss of life and property. One of the most significant threats to our infrastructure in recent U.S. history has been as the result of hurricanes, most memorably Hurricane Katrina, which devastated New Orleans, and Hurricane Sandy, from which the infrastructure of New York City is still recovering from. Addressing these threats requires a multi-pronged approach that takes into account how infrastructure is connected and how failures in one type of infrastructure can impact the other. Questions then arise as to how we can protect ourselves from these threats. This Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP) project develops a methodology that can answer questions related to protecting infrastructure. For instance, if a community wanted to build a sea-wall, questions such as ""how high should it be?"" and ""where should it be placed?"" must be asked. Other important questions compare a sea-wall with other protective options, such as: ""Is a sea-wall the best protective measure?"", ""What about artificial sand dunes?"", or ""What about raising the infrastructure to a higher elevation?"" And a related critically important question should be ""Can adopting this option for protecting one community be detrimental to the safety of a neighboring community?"" In addition to questions about the efficacy of protective measures, there are questions about how coastal protection may impact other coastal uses including recreational, cultural, and economic activities, and how negatively impacting these uses can be reduced while still maximizing protection. Resource constraints are also important to take into account and the question of how to optimally protect a community given constrained resources is critical. Such questions require the combination of advanced computing, mathematics and social science approaches to design tools to address these complex intersecting problems and placing these tools in the hands of decision makers that need to make these types of critical decisions. This is the goal of this project.<br/><br/>Interdependent critical infrastructure in coastal regions has long been threatened by storm-induced flooding. Events such as Hurricanes Sandy and Katrina punctuated the need for plans to protect our infrastructure, but these events only reflect a possible future threat and do not fully address the unknown probability and impacts of a future threat. This uncertainty is only made more critical by the addition of climate change to exacerbate and amplify impacts, in particular sea-level rise.  The goal of the proposed work is to address the threat from storm-induced flooding to interdependent infrastructure, including transportation and power systems and emergency services, by developing a methodology that can test various adaptation strategies. Strategies in this context include, but are not restricted to, building sea-walls or other physical, protective mechanisms. The proposed methodology would optimize strategies to maximize their protective abilities over time and space constrained by budgetary considerations. To accomplish this the methodology will contain four conceptual steps: (1) formulate a new strategy for adaptation, (2) computationally determine flooding levels given an ensemble of storms representing the likely threat and future sea-level rise, (3) estimate the damage over the ensemble to the infrastructure considered, and (4) using appropriate metrics evaluate the relative suitability of a given strategy including cost and social acceptability. This process would repeat iteratively until a sufficiently optimal strategy is found. Developing such a methodology will be challenging however. The magnitude of the computational effort needed is significant. Using a set of computational models that vary in accuracy and speed, the methodology will swap between models appropriate for the optimization stage. The methodology will also not be successful without stakeholder engagement. For this reason, interviews with key stakeholders will be an important component of the methodology design and implementation. Interviews will inform the identification of critical components of infrastructure and the interdependencies among them that could be affected by coastal flooding, assist in the design of the optimization metrics, and assess how well the output of the methodology matches stakeholder expectations. Community meetings will also be held to introduce and discuss the results of the methodology with local communities who would potentially benefit from the adaptation strategies. Finally, using New York City's complex infrastructure and recent events, the methodology will be validated."
"1839739","Student Travel Support for MVAPICH User Group (MUG) Meeting","OAC","EDUCATION AND WORKFORCE","09/01/2018","08/17/2018","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Sushil Prasad","08/31/2019","$10,000.00","Hari Subramoni","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7361","026Z, 062Z, 7556, 9179","$0.00","Modern High-Performance Computing (HPC) systems are rapidly evolving with respect to processor, networking, and I/O technologies. In such a rapidly changing environment it is critical that the next-generation engineers and scientists get exposed to the modern architectural trends of HPC systems and learn how to use the features of these technologies to design HPC software stacks, learn about the process of open-source software developments and its sustainability. The annual MVAPICH User Group (MUG) meeting provides an open forum to exchange information on the design and usage of MVAPICH2 libraries, which are open-source, high-performance and scalable MPI libraries to take advantage of the RDMA technology. Travel funding from this project will enable a set of students (undergraduates and graduates) to attend the MUG meeting. Their participation will help them to enter the next-generation HPC workforce with increased expertise on software design, reuse, and sustainability. The project thus serves the national interest, as stated by NSF's mission: to promote the progress of science.<br/><br/>The MVAPICH project focuses on the design of high-performance MPI and PGAS runtimes for HPC systems. Over the years, this project has been able to incorporate new designs to leverage novel multi-/many-core platforms like Intel Xeon Phis, NVIDIA GPGPUs, Open POWER, and ARM architectures coupled with Remote Direct Memory Access (RDMA) enabled commodity networking technologies like InfiniBand, RoCE, Omni-Path, and 10/25/40/50/100GigE with iWARP. An annual MVAPICH User Group (MUG) meeting was created five years ago to provide an open forum to exchange information on MVAPICH2 libraries. The funding under this grant aims to achieve increased participation of undergraduate and graduate students working in the HPC area (systems and applications) in the annual MUG event. The requested student travel fund will help in attracting a set of students from a range of US institutions. The participation in an international event such as MUG will enable the students to get a global picture of the developments happening in the rapidly evolving HPC domain and open-source software design. The selection committee will stress on diversity to attract students from minority and under-represented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835769","Collaborative Research: HDR Elements: Software for a new machine learning based parameterization of moist convection for improved climate and weather prediction using deep learning","OAC","DATANET, EarthCube","10/01/2018","08/13/2018","Pierre Gentine","NY","Columbia University","Standard Grant","Amy Walton","09/30/2021","$307,426.00","","pg2328@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7726, 8074","062Z, 077Z, 7923","$0.00","This project targets a difficult problem in weather and climate prediction -- the representation of convection.  Accurate representation of convection is important, since a majority of current model predictions depend on it.  Unraveling the physics involved in convective conditions, clouds and aerosols may take years of modeling to fully understand; however, a set of machine learning techniques, known as ""neural net techniques"", may provide enhanced predictability in the interim, and this project explores their potential.<br/><br/>The project develops a Python library enabling the use of machine learning (artificial neural networks) in a broad range of science domains. The focus is on integration of convection and cloud formation within larger-scale climate models, with the Community Earth System Model (CESM) as an initial target.  The project develops a new set of machine learning climate model parameterizations to reduce uncertainty in weather and climate predictions.  The neural networks will be trained on high-fidelity simulations that explicitly resolve convection.  Two types of high-resolution simulations will be used for training the neural networks: 1) an augmented super-parameterized simulation, and 2) a full Global Cloud Resolving Model (GCRM) simulation based on the ICOsahedral Non-hydrostatic (ICON) modelling frameworks provided by the Max Planck Institute, using initial 5km horizontal resolution.  The effort has the potential to increase understanding of convection dynamics and processes across scales, and could potentially be implemented to address other scale problems as well, where it is too computationally costly or impractical to represent processes occurring at much finer scales than the main grid resolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1253881","CAREER: Algorithmic and Software Foundations for Large-Scale Graph Analysis","OAC","CAREER: FACULTY EARLY CAR DEV","05/01/2013","12/10/2012","Kamesh Madduri","PA","Pennsylvania State Univ University Park","Standard Grant","Sushil K Prasad","06/30/2019","$500,000.00","","madduri@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1045","1045","$0.00","The proposed research aims at designing highly efficient and scalable algorithms for large graph-based computations on modern data sets and emerging parallel platforms. Graph abstractions and graph-theoretic computations play a critical role in the analysis of data from experimental devices, medical data sets, socially-generated data from the web and mobile devices, and scientific simulation data. The proposal aims to provide a fundamental understanding of parallel graph analytics, with the creation of novel algorithmic frameworks. Motivated by current terascale applications in genomics, proteomics, and social network analytics, the project will undertake the clean-slate design of four algorithmic frameworks that capture broad classes of graph-based computations: traversal-based static graph computations, dynamic graph analytics, subgraph enumeration and pattern search computations, and multiscale and multilevel graph computations. This research will lead to the design of new memory-efficient graph representations and data structures, the creation of novel linear time parallel algorithmic strategies based on data partitioning, and a deeper understanding of architectural features that impact graph processing efficiency and scalability. The expected outcome is to enable non-parallel computing experts design graph analytics at orders-of-magnitude higher levels of abstraction and performance than the current state-of-the-art.<br/><br/>The proposed educational activities, closely related to the research goals, attempt to foster an environment of interdisciplinary computational research within Penn State. New graduate classes on parallel graph analysis, parallel algorithms for computational biology, and high-performance social data mining, will facilitate student involvement in current research activities of this project and its collaborators. The identification of new high-level data-centric parallel algorithm and software design principles will be a key broader impacts outcome. The project will actively collaborate with academic, industrial, and government laboratory partners that rely on graph-based analytics, release the algorithmic frameworks under open-source licenses, and train practitioners and interdisciplinary teams through virtual workshops and tutorials."
"1835372","Element: Data: HDR: Enabling data interoperability for NSF archives of high-rate real-time GPS and seismic observations of induced earthquakes and structural damage detection in OK","OAC","DATANET","10/01/2018","07/26/2018","Jennifer Haase","CA","University of California-San Diego Scripps Inst of Oceanography","Standard Grant","Amy Walton","09/30/2021","$355,077.00","","jhaase@ucsd.edu","8602 La Jolla Shores Dr","LA JOLLA","CA","920930210","8585341293","CSE","7726","062Z, 077Z, 7923","$0.00","Recent studies have identified critical differences between earthquakes induced by wastewater injection in tectonically passive regions of oil and gas exploration (such as earthquakes recently experienced in Oklahoma, Texas, and Kansas) and earthquakes in tectonically active environments (such as fault zones in California).  This has significant implications for earthquake engineering in the Midwest, where the building inventory was not designed to withstand large earthquakes or cumulative damage due to successive earthquakes.  Estimating the level of ground shaking at different frequencies is needed to calculate structural response, understand which structures run higher risks of earthquake damage, and establish criteria for building design and real-time decision making. <br/> <br/>This project uses recent breakthroughs in real-time GPS data analysis to address challenges limiting the use of real-time GPS and seismic data by the geoscience and engineering communities.  The effort develops new capabilities to handle data streams in a manner that is independent of the content and formats of the environmental sensor measurements.  Creating these links will have a substantial impact on interoperability among the geodesy, seismology, and earthquake engineering research communities.  The project demonstrates the approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the Magnitude 5.8 September 2016 Pawnee earthquake.  The effort creates new methods for capturing permanent deformation in structures, and a better understanding of building inventory resiliency.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1443061","CIF21 DIBBs: Systematic Data-Driven Analysis and Tools for Spatiotemporal Solar Astronomy Data","OAC","OFFICE OF MULTIDISCIPLINARY AC, , DATANET, EarthCube","11/01/2014","08/14/2014","Rafal Angryk","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Amy Walton","10/31/2019","$1,499,933.00","Petrus Martens, Katharine Reeves","angryk@cs.gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","CSE","1253, 1798, 7726, 8074","7433, 7480, 8048","$0.00","The large quantities of data produced by modern solar telescopes provide a rich and rapidly growing opportunity for discovery.  These large data sets create an unprecedented ability to observe correlations between phenomena that have previously gone unexplored.   However, to harness the power of these large data sets, it is necessary to provide the solar physics and space weather communities with software tools that will rapidly and accurately catalog, explore, track and correlate solar phenomena.<br/><br/>This project develops tools for processing large volumes of spatio-temporal solar data.  Initially, the team enables tracking of all solar events previously identified by an international consortium (the Feature Finding Team of the Solar Dynamics Observatory) and reported to the Heliophysics Event Knowledgebase (HEK).  Next, the team develops systematic spatiotemporal characterizations of solar event types, and identifies spatiotemporal co-occurrence patterns.  Throughout the project, the team disseminates the generated data, discovered patterns, and developed software tools to the community. By adding an ability to track features previously identified by an international consortium, the project has potential to advance knowledge of solar phenomena, and the impact of such phenomena on space weather."
"1350374","CAREER: Integrating Physical Models into Data-Driven Inference","OAC","CAREER: FACULTY EARLY CAR DEV, INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE","06/01/2014","04/19/2018","Linwei Wang","NY","Rochester Institute of Tech","Standard Grant","Sushil K Prasad","05/31/2019","$570,826.00","","linwei.wang4@gmail.com","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","1045, 1640, 7361","019Z, 1045, 7433, 9102, 9179, 9251, CL10","$0.00","Individualized assessment of high-dimensional spatiotemporal systems - such as in-vivo human physiological systems - has been increasingly enabled by paralleled advances in two fields: computer modeling that supports quantitative understanding of the dynamic behavior and mechanism of these systems, and modern sensor technologies that continuously improve the quantity and quality of measurement data available for analysis. There is, however, a gap between the two fields that is ubiquitous in many application domains: the current state of computer modeling is generally decoupled from specific measurements of an individual system, while individualized data-driven analysis often struggles for realistic domain contexts. This project aims to bridge this gap by investigating and developing new methodologies, algorithms, and software that will enable the integration of complex domain knowledge - yielded by computer simulation of domain physical models - into the process of data-driven inference. The overarching theme of this research is flexibility and robustness. Specifically, it addresses the following three challenges: 1) to enable a plug-and-play inclusion of domain physical models catering to different efficiency vs. accuracy needs; 2) to further overcome the lack of measurements and potential errors in domain physical models by exploiting the low-dimensional structure in high-dimensional systems; and 3) to enable a robust adaptation of the time-varying error that potentially exists in domain physical models. The driving application of this project is individualized modeling of in-vivo cardiovascular systems - using noninvasive biomedical and physiological data - for improved prevention, diagnosis, and treatment of heart diseases. <br/><br/>The outcome of this project will contribute theoretically, algorithmically, and computationally to the foundations of statistical inference, and extend to a wide range of applications such as tumor modeling, climate modeling, systems biology, and finance. In addition, this project will deliver publicly-available multicore/GPU software that will encapsulate the most effective algorithms developed. These toolkits will contribute to the national effort toward noninvasive medicine and healthcare, while supporting numerous scientific applications involving data-driven modeling and inference. This project also includes an integrated educational and outreach program to foster interdisciplinary research training and to increase participation of underrepresented groups in STEM disciplines. It includes: 1) development and evaluation of ""learning-by-doing"" concept in graduate and undergraduate education; 2) research training for students from graduate to high-school levels, with a focus on engaging women and underrepresented students at an early stage; and 3) broader outreach activities to area K-12 students and Paramedic communities. The participation of women, underrepresented, K-12, and Paramedic groups are reinforced through continued partnerships between the PI and different programs offered in RIT, local school district, and community college."
"1739032","CICI: CE: Implementing CYBEX-P: Helping Organizations to Share with Privacy Preservation","OAC","Cyber Secur - Cyberinfrastruc","01/01/2018","11/05/2018","Shamik Sengupta","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Micah Beck","12/31/2019","$1,002,067.00","Mehmet Gunes, Nancy LaTourrette, Ming Li, Jeff Springer","ssengupta@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","CSE","8027","9150, 9251","$0.00","In response to the increasing number of attacks on cyberspace, public and private organizations are encouraged to share their cyber-threat information and data with each other. Although there are long-term interests in sharing security related information, it places organizations at risk regarding the protection of their data and exposure of other vulnerabilities. This project designs, develops and implements a CYBersecurity information EXchange with Privacy (CYBEX-P) platform using trusted computing paradigms and privacy-preserving information sharing mechanisms for cybersecurity enhancement and development of a robust cyberinfrastructure. The outcome of this project has a broader impact on the development of a novel cybersecurity information-sharing platform with privacy preservation and a robust governance structure. The project also has direct impact on undergraduate and graduate student education and training, emphasizing the engineering development of minorities and women, by providing a real-world platform for investigation and management of cyber threats.<br/><br/>Envisioning that effective and privacy-preserving threat intelligence sharing can be instrumental for auditing the state of the threat landscape and helping to predict and prevent major cyber-attacks, this project provides a service for structured information exchange. The CYBEX-P platform provides valuable measurable information about the security status of systems and devices together with data about incidents stemming from cyber-attacks. To develop and implement such an environment across statewide organizations, then across the nation, this research project incorporates blind processing, privacy preservation and integrity of shared incident data by ensuring that only trusted processes access the raw data and only anonymized data are shared with other operators. Blind processing enables the advantages of additional information exchange while respecting organizational constraints and trust boundaries. This research also establishes a flexible governance framework that includes both policies and procedures to protect the data and provide all customers with the tools to demonstrate they are complying with both regulatory and internal data governance requirements. Specifically, the outcomes of the project demonstrate: i) CYBEX-P infrastructure development with affordable scalability, secure data exchange, and analytic components, ii) Privacy-preserving information sharing via blind processing and anonymization, and an iii) CYBEX-P governance framework."
"1821926","CICI: Data Provenance:  Collaborative Research: Provenance Assurance Using Currency Primitives","OAC","Cyber Secur - Cyberinfrastruc","10/01/2017","06/28/2018","Anthony Skjellum","TN","University of Tennessee Chattanooga","Standard Grant","Micah Beck","12/31/2019","$248,865.00","","tony-skjellum@utc.edu","615 McCallie Avenue","Chattanooga","TN","374032504","4234254431","CSE","8027","7434, 9150, 9251","$0.00","Data provenance is the ability to track data history including things such as where the data resided, who handled it, and what systems stored, forwarded and processed it. This research builds on the architecture of the digital currency Bitcoin. It develops distributed data ledgers - similar to bookkeeping ledgers - that maintain data history so it can't be manipulated by hackers trying to hide their activities. Data consistency guarantees  that everyone gets the right answers about where, who, and what regardless of which ledger is read.  This software advances the security of computing systems by making data accountable, especially for online commerce and big data (""the cloud'').  It secures forensic information taken from compromised computers  for further analysis. It validates whether privacy requirements are being met for medical records.  The key outcome is a software prototype that implements the complete system and illustrates the ability to store, maintain, and update provenance information for real data. <br/><br/>A data provenance framework will be designed, prototyped, evaluated and then delivered as an Application Programmer Interface, software library, and distributed service. This work will produce a reusable distributed service architecture achieving scalability by using distributed services that maintain ledger information. The system leverages Bitcoin cryptocurrency by building on Bitcoin's block-chain architecture to maintain provenance metadata securely. It leverages existing tools for provenance data exploration and visualization.  Digital signatures from both the server/system as well as the user creates dual information about possession, while distributed ledgers remove control and maintenance of metadata from the user who creates it. The prototype enables research into long-term provenance creation, maintenance, and utilization for workflows in the area of cybersecurity as well  studies of how to integrate and secure provenance into existing file systems and network services.  Opt-in and passive (involuntary) provenance systems will be enabled using the API, library, and distributed ledgers prototyped, enabling data provenance for systems where needed, notably high assurance cloud computing and scientific workflow systems. The tool can be used  to enable reproducibility of published results from archived data and artifacts."
"1840069","CICI: RDP: SAMPRA: Scalable Analysis, Management, and Protection of Research Artifacts","OAC","Cyber Secur - Cyberinfrastruc","09/01/2018","08/27/2018","Patrick Bridges","NM","University of New Mexico","Standard Grant","Micah Beck","08/31/2021","$598,594.00","Vince Calhoun, Vincent Clark, Kevin Comerford","bridges@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","8027","9150","$0.00","Current computing systems that support research on sensitive data, such as personally identifiable information, are frequently single-purpose and rely on ad-hoc approaches to data protection and management.  This project develops system called SAMPRA: Scalable Analysis, Management, and Protection of Research Artifacts.  SAMPRA's goal is to provide a compliant research computing platform that supports diverse, inter-disciplinary, collaborative research on protected data.  SAMPRA leverages modern virtualization technology to enable the decentralized management of protected computing enclaves that can be customized to the needs of each specific research project. In addition, the project trains researchers and students on best practices for managing and analyzing protected data, and technical staff on how to customize environments to the needs of individual research groups.<br/><br/>SAMPRA investigates multiple techniques to meet these goals, with the overall technical goal of understanding the technical and administrative tradeoffs between isolating and sharing protected research infrastructure services. First, SAMPRA systematically virtualizes hardware, software, and network resources to provide a flexible system architecture that supports research computing with varying analysis, management, and protection needs. SAMPRA also provides virtual data transfer nodes to interface protected environments with external data acquisition systems, with the goal of supporting modern data-intensive research projects using central institutional resources. The project develops exemplar computing, data analysis, and data management virtual environments, and integrating these with institutional systems for managing protected data. These exemplar systems are also the examples used in workshops that train researchers on the use of SAMPRA to support research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1837847","Accelerating Public Access to Research Data by Research Universities","OAC","NSF Public Access Initiative","09/01/2018","06/11/2018","Kacy Redd","DC","Association of Public and Land-Grant Universities","Standard Grant","Beth Plale","08/31/2019","$49,687.00","Tobin Smith, James Reecy","kredd@aplu.org","1307 New York Ave NW","Washington","DC","200054722","2024786040","CSE","7414","7556","$0.00","Creating a robust system for public access to research data and findings requires the active engagement of researchers and their institutions, research sponsors, community data repositories, disciplinary societies, and others.  Building on a widely recognized AAU/APLU Public Access Working Group report, this workshop will convene 20-30 teams of university leaders and researchers to learn about current data sharing practices and emerging tools, evaluate options for supporting and promoting public access to research data on their campuses, and begin cross-institutional discussions to create an interoperable system that accelerates progress in supporting public access to research data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835897","ELEMENTS: DATA: HDR: SWIM to a Sustainable Water Future","OAC","DATANET","01/01/2019","09/06/2018","Natalia Villanueva Rosales","TX","University of Texas at El Paso","Standard Grant","Amy Walton","12/31/2021","$599,451.00","Deana Pennington","nvillanuevarosales@utep.edu","ADMIN BLDG RM 209","El Paso","TX","799680001","9157475680","CSE","7726","062Z, 077Z, 7923","$0.00","The project develops a system that automates ingestion of data into models, the integration of decoupled models, and the dynamic generation and interpretation of models.  The focus is on water resources.  The team leverages software development from an ongoing USDA-funded project, and the expertise of an interdisciplinary, international team of scientists and students who are investigating future scenarios of water availability and use in the Middle Rio Grande valley of southern New Mexico, west Texas, and northern Chihuahua (Mexico).<br/><br/>This project advances water sustainability research capabilities by creating a Sustainable Water through Integrated Modeling (SWIM) framework that automates ingestion of data into models, facilitates integration of decoupled models, and supports dynamic generation and interpretation of models. The four objectives are to: <br/>1) foster use of water models by stakeholders (non-modelers) through direct participation enabled by a web-based interface and provenance capture; <br/>2) enable seamless model-to-model integration through service-driven data exchange and transformation; <br/>3) develop data- and technology-enabled approaches for reasoning with biophysical and social models; and <br/>4) engage data providers, modelers and stakeholders in conceiving and testing the framework.<br/>The research and products of this project contribute to advanced research capabilities on water sustainability.  By providing seamless integration of scientific data and models, and generating provenance data to create dynamic user interfaces, the project instills trust in the models generated through participatory analysis.   This approach is built around a strong appreciation of the value of stakeholder engagement and alignment to achieving the described goals.  The research is carried out by a diverse and experienced team, and will contribute to understanding of how to more effectively conduct convergent research with researchers and stakeholders.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835446","Elements: Software: Towards Efficient Embedded Data Processing","OAC","Software Institutes","01/01/2019","08/24/2018","Jignesh Patel","WI","University of Wisconsin-Madison","Standard Grant","Vipin Chaudhary","12/31/2021","$599,800.00","","jignesh@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Embedded databases are ubiquitous, though that fact may not be widely-recognized. Every smart phone has many embedded databases, which implies that billions of people worldwide carry dozens of databases in their phones/pockets every day. Many of these databases are powered by data processing technology that has not kept up with the pace with which the underling hardware in phones have evolved. As a result, data processing is slow, and consumes more energy than needed. The focus of this proposal is on developing new data processing technology for mobile devices that targets a 10X efficiency and performance improvements. The aims of the project go beyond more efficient data processing on phones to also include more efficient processing in embedded environments, which also includes databases running on laptops. Thus, the project aims for a broad impact on database across a spectrum of mobile devices.<br/><br/>The technical contributions of this project are in recognizing that modern hardware, even at the ?low-end? which includes mobile phones and laptops, now have multiple processing cores, relatively large amounts of memory, and flash storage. There is a critical need for a new class of embedded data processing systems that can work efficiently, and effectively on such modern mobile platforms. This project aims to build a system, called Hustle, to address this need. The project will design, develop and implement a range of data processing methods, which include predicate-based concurrency control mechanisms, query processing methods that inherently expose and exploit opportunities for intra and inter-operator parallelism, and query optimization methods that target embedded settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835135","NSCI Elements: Software - PFSTRASE - A Parallel FileSystem TRacing and Analysis SErvice to Enhance Cyberinfrastructure Performance and Reliability","OAC","Software Institutes","10/01/2018","08/13/2018","Richard Evans","TX","University of Texas at Austin","Standard Grant","Vipin Chaudhary","09/30/2021","$385,893.00","","rtevans@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","This project will develop an open-source software service, the Parallel FileSystem TRacing and Analysis SErvice (PFSTRASE), that improves the reliability and performance of data storage systems for the nation?s largest supercomputers.  As simulations and computations represent reality more faithfully they grow commensurately in scale along with the size of the data they consume and generate.  To handle the storage and movement of this data, supercomputing systems are built on the backbone of massively parallel data storage systems.  Due to their parallel nature these storage systems are capable of moving data at hundreds of times the speed of conventional storage systems, enabling otherwise impractical computations.  The performance capabilities these storage systems provide is accompanied by a complexity that results in them often functioning significantly less than optimally and even in some instances failing.  This results in wasted computational time and ultimately lost scientific progress.  The state of development of tools that could cast light on these problems and improve storage system reliability and performance is inadequate for current and future computing systems.  PFSTRASE will fill this gap by continually and automatically monitoring storage system health and performance, providing insights through an easy to use interface that will improve the reliability and performance of storage and supercomputer systems. <br/><br/>Parallel filesystems (PFSs) are the most critical high-availability components of High Performance Computing (HPC) architectures, providing input/output (I/O) services to running computations, the environment that users and system services operate in, and storage for applications and data. Because of this central role, failure or performance degradation events in the PFS impact every user of an HPC resource. PFS events must be dealt with quickly and effectively by system administrators; however, there is typically insufficient information to establish precise causal relationships between PFS activity and events, impeding the implementation of timely and targeted remedies. To fill this information gap, an open-source Parallel FileSystem TRacing and Analysis SErvice (PFSTRASE) that traces and analyzes the requisite data to establish causal relationships between PFS activity and both realized and imminent events will be developed. This project will implement the service for the open-source Lustre filesystem, which is the most commonly used PFS at large-scale HPC sites. Loads for specific PFS directory and file operations will be measured and incorporated into the service to construct authentic server load contributions from every job, process, and user. The service?s infrastructure will continuously monitor the entire PFS and generate a real-time, seamless representation that connects contributions of jobs, processes, and users to storage server loads, network bandwidth, and storage capacities. The infrastructure will provide an easily navigable web interface that presents this data, both real-time and historical, in a visual format.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835371","Element: Data: HDR: Enabling data interoperability for NSF archives of high-rate real-time GPS and seismic observations of induced earthquakes and structural damage detection in OK","OAC","DATANET, EPSCoR Co-Funding","10/01/2018","07/26/2018","Priyank Jaiswal","OK","Oklahoma State University","Standard Grant","Amy Walton","09/30/2021","$244,704.00","Mohamed Soliman","priyank.jaiswal@okstate.edu","101 WHITEHURST HALL","Stillwater","OK","740781011","4057449995","CSE","7726, 9150","062Z, 077Z, 7923, 9150","$0.00","Recent studies have identified critical differences between earthquakes induced by wastewater injection in tectonically passive regions of oil and gas exploration (such as earthquakes recently experienced in Oklahoma, Texas, and Kansas) and earthquakes in tectonically active environments (such as fault zones in California).  This has significant implications for earthquake engineering in the Midwest, where the building inventory was not designed to withstand large earthquakes or cumulative damage due to successive earthquakes.  Estimating the level of ground shaking at different frequencies is needed to calculate structural response, understand which structures run higher risks of earthquake damage, and establish criteria for building design and real-time decision making. <br/> <br/>This project uses recent breakthroughs in real-time GPS data analysis to address challenges limiting the use of real-time GPS and seismic data by the geoscience and engineering communities.  The effort develops new capabilities to handle data streams in a manner that is independent of the content and formats of the environmental sensor measurements.  Creating these links will have a substantial impact on interoperability among the geodesy, seismology, and earthquake engineering research communities.  The project demonstrates the approach using multi-sensor geoscience and engineering datasets recorded on structures on the Oklahoma State University campus and in the field near the location of the September 2016 Pawnee earthquake.  The effort creates new methods for capturing permanent deformation in structures, and a better understanding of building inventory resiliency.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835904","Collaborative Research: Framework: Software: HDR: Reproducible Visual Analysis of Multivariate Networks with MultiNet","OAC","DATANET, Software Institutes","01/01/2019","09/06/2018","Miriah Meyer","UT","University of Utah","Standard Grant","Stefan Robila","12/31/2022","$1,899,694.00","Bryan Jones, Alexander Lex","miriah@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7726, 8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","Multivariate networks -- datasets that link together entities that are associated with multiple different variables -- are a critical data representation for a range of high-impact problems, from understanding how our bodies work to uncovering how social media influences society. These data representations are a rich and complex reflection of the multifaceted relationships that exist in the world. Reasoning about a problem using a multivariate network allows an analyst to ask questions beyond those about explicit connectivity alone: Do groups of social-media influencers have similar backgrounds or experiences? Do species that co-evolve live in similar climates? What patterns of cell-types support different types of brain functions? Questions like these require understanding patterns and trends about entities with respect to both their attributes and their connectivity, leading to inferences about relationships beyond the initial network structure. As data continues to become an increasingly important driver of scientific discovery, datasets of networks have also become increasingly complex. These networks capture information about relationships between entities as well as attributes of the entities and the connections. Tools used in practice today provide very limited support for reasoning about networks and are also limited in the how users can interact with them. This lack of support leaves analysts and scientists to piece together workflows using separate tools, and significant amounts of programming, especially in the data preparation step. This project aims fill this critical gap in the existing cyber-infrastructure ecosystem for reasoning about multivariate networks by developing MultiNet, a robust, flexible, secure, and sustainable open-source visual analysis system.  <br/><br/><br/>MultiNet aims to change the landscape of visual analysis capabilities for reasoning about and analyzing multivariate networks. The web-based tool, along with an underlying plug-in-based framework, will support three core capabilities: (1) interactive, task-driven visualization of both the connectivity and attributes of networks, (2) reshaping the underlying network structure to bring the network into a shape that is well suited to address analysis questions, and (3) leveraging provenance data to support reproducibility, communication, and integration in computational workflows. These capabilities will allow scientists to ask new classes of questions about network datasets, and lead to insights about a wide range of pressing topics. To meet this goal, we will ground the design of MultiNet in four deeply collaborative case studies with domain scientists in biology, neuroscience, sociology, and geology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1450377","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2015","05/08/2018","G.J. Peter Elmer","NJ","Princeton University","Continuing grant","Bogdan Mihaila","04/30/2020","$1,145,564.00","","gelmer@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","1253, 7244, 8004","7433, 8005, 8009, 8084","$0.00","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces.  However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community.  Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program."
"1450319","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2015","05/07/2018","Michael Sokoloff","OH","University of Cincinnati Main Campus","Continuing grant","Bogdan Mihaila","04/30/2020","$1,000,000.00","","mike.sokoloff@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","1253, 7244, 8004","7433, 8009, 8084","$0.00","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces.  However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community.  Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program."
"1842054","EAGER: GreenDataFlow: Minimizing the Energy Footprint of Global Data Movement","OAC","DATANET, Campus Cyberinfrastrc (CC-NIE)","09/01/2018","03/14/2019","Tevfik Kosar","NY","SUNY at Buffalo","Standard Grant","Amy Walton","08/31/2020","$325,595.00","","tkosar@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7726, 8080","062Z, 7916, 9251","$0.00","This project fills an important gap in the understanding of data transfer energy efficiency.  The models, algorithms and tools developed as part of this project will help increase performance and decrease power consumption during end-to-end data transfers, which should save significant quantities of resources (estimated to be gigawatt-hours of energy and millions of dollars in the US economy alone).  The applicability and efficiency of these novel techniques will be evaluated in actual applications, in a collaborative partnership with IBM.<br/><br/>The project explores options for minimizing the energy-use footprint of global data movement. The effort is focused on saving energy at the end systems (sender and receiver nodes) during data transfer.  It explores a novel approach to achieving low-energy end-to-end data transfers, through application-layer energy-aware throughput optimization. The research team investigates and analyzes the factors that affect performance and energy consumption in end-to-end data transfers, such as CPU frequency scaling, multi-core scheduling, I/O block size, TCP buffer size, and the level of parallelism, concurrency, and pipelining, along with the data transfer rates at the network routers, switches, and hubs.   How these parameters decrease energy consumption in the end systems and networking infrastructure, without sacrificing transfer performance, are assessed.  The project will create novel application-layer models, algorithms, and tools for:<br/> - predicting the best combination of end-system and protocol parameters for optimal data transfer throughput with energy-efficiency constraints; <br/> - accurately predicting the network device power consumption due to increased data transfer rate on the active links, and dynamic readjustment of the transfer rate to balance the energy performance ratio; and <br/> - providing service level agreement (SLA) based energy-efficient transfer algorithms to service providers. <br/>The models, algorithms and tools developed as part of this project will help increase performance and decrease power consumption during end-to-end data transfers, saving significant quantities of resources.  Since the tools focus on the application layer, they will not require changes to the existing infrastructure, nor to the low-level networking stack, and wide deployment of the developed system should be readily attainable.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1443037","CIF21 DIBBs: Collaborative Research: Cyberinfrastructure for Interpreting and Archiving U-series Geochronologic Data","OAC","PALEOCLIMATE PROGRAM, PETROLOGY AND GEOCHEMISTRY, MARINE GEOLOGY AND GEOPHYSICS, DATANET, EarthCube, EPSCoR Co-Funding","09/01/2014","08/25/2014","James Bowring","SC","College of Charleston","Standard Grant","Amy Walton","08/31/2019","$579,762.00","","bowringj@cofc.edu","66 GEORGE ST","CHARLESTON","SC","294240001","8439534973","CSE","1530, 1573, 1620, 7726, 8074, 9150","4444, 7433, 8048, 9150","$0.00","Uranium-series geochronology plays a critical role in understanding the time-scales and rates of climate change, sea-level change, and volcanic activity.  There are no standardized data-handling protocols or community-based open data archives for raw isotopic data and reduced results. The U-series geochronology community wants to change this and is encouraged by NSF's vision for 21st century cyberinfrastructure.  In this pilot demonstration project, software engineers and geochronologists collaborate to build open-source cyberinfrastructure that standardizes and facilitates U-series data analysis, reporting, and archiving and analysis and re-processing of the vast amounts of legacy data.  The project uses the NSF-funded EarthChem-Geochron data repository that archives results from many dating schemes, stimulating inter-domain sharing and discovery. This cyberinfrastructure supports teaching and training at all levels and provides non-experts access to new knowledge.  <br/><br/>This collaborative effort applies modern software engineering practices to solving the cyberinfrastructure problems of the U-series geochronology community, making the calculation, archiving, access, and interpretation activities of U-series geochronology as rigorous, seamless, and simple as possible.  Currently, isotopic dates from U-series data are calculated and analyzed using legacy, platform-dependent software, and dates are difficult to synthesize because they have been published with disparate decay constants and reporting norms. This pilot project includes new software to calculate, visualize, and interpret U-series dates from new and legacy data, and new schema for data archiving at Geochron.org. Importantly, this project advances the sustainability of NSF's software ecosystem by building upon the cyberinfrastructure architecture already developed for the U-Pb geochronology community under the EARTHTIME umbrella."
"1636764","BD Spokes: PLANNING: NORTHEAST: Collaborative: Planning for Privacy and Security in Big Data","OAC","Secure &Trustworthy Cyberspace","09/01/2016","07/17/2018","Rebecca Wright","NJ","Rutgers University New Brunswick","Standard Grant","Beth Plale","03/31/2019","$89,963.00","","rwright@barnard.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8060","027Z, 7433, 7434, 8083","$0.00","Security and privacy play a key role in areas such as health, energy, and smart cities, as well as constituting a grand challenge in and of themselves.  Privacy and security are critical for realizing Big Data's promise to advance society.  If data are used without regard to privacy of individuals or protection of the data, then individuals may be hurt.  If the data's authenticity is not guaranteed, or if data are not permitted to be used at all due to privacy and security concerns, then the data's value is not realized. As the information that is collected about us grows in quantity, scientific and commercial value, and sensitivity, addressing these challenges is crucial to accessing the benefits of big data while ensuring that privacy and associated basic rights such as free speech and association are respected.   Through two workshops hosted by DIMACS, this planning project will advance the research agenda on privacy and security for big data, build a community of interested researchers and practitioners, and propose regional activities in security and privacy related to the Northeast Big Data Hub.  The long-term vision of the project is to catalyze both foundational and practical advances in privacy and security for big data that have the ability to positively impact society by (1) expanding the extent to which individuals can have control over protection of their personal data, and (2) enabling data's value to be harnessed for advances in areas including health, energy, smart cities and regions, finance, and education. <br/><br/>The first event, a workshop on ""Overcoming Barriers to Data Sharing including Privacy and Fairness"", will bring together computer scientists, legal scholars, social scientists, and consumers of data to understand the extent to which privacy currently limits the sharing of data, including but not limited to research data, and to develop standards and best practices to enable new information flows in domains from healthcare to energy.  The second event, the ""NE BD Hub Workshop on Privacy and Security for Big Data"", will bring together privacy and security experts as well as experts in a variety of big data application areas to highlight privacy and security issues associated with each of the Northeast Big Data hub spokes.  Expected project outcomes include broad participation, development of new collaborations and partnerships, production and dissemination of tutorial and talk videos, and reports on planned activities and potential best practices.<br/><br/>This award is co-funded by the CISE Division of Computer and Network Systems (CNS) Secure and Trustworthy Computing (SaTC) Program."
"1659174","CC* Integration: Enhancement and deployment of LDM7 for scientific data distribution","OAC","CISE RESEARCH RESOURCES","04/01/2017","04/06/2017","Malathi Veeraraghavan","VA","University of Virginia Main Campus","Standard Grant","Deepankar Medhi","03/31/2020","$1,000,000.00","Steven Emmerson","mv5g@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","2890","","$0.00","The University Corporation for Atmospheric Research (UCAR)  distributes scientific weather<br/>data from instruments and simulations to its 250 consortium member institutions via the Internet<br/>Data Distribution (IDD) system. The software program used for this data distribution is called Local Data Manager<br/>(LDM). LDM6, the current deployed version, has been in use since 2003. This project seeks to<br/>create a higher performing solution for real-time scientific data distribution by leveraging Software Defined<br/>Networking and OpenFlow technologies.<br/><br/>This project seeks to  (i) deploy LDM7 on multiple campuses after provisioning<br/>VLANs across campuses and regionals, and connecting them to a multipoint VLAN across Internet2?s<br/>network, (ii) add new features for security and control-plane to LDM7, (iii) integrate LDM7<br/>with an SDN controller (OESS) client for automated addition/deletion of VLAN segments to a<br/>multipoint VLAN, (iv) collect information, and develop tools for simplifying VLAN provisioning<br/>and testing L2 path connectivity and performance using perfSONAR and other tools, and (v)<br/>run extensive tests of LDM7 for performance and reliability."
"1724898","CIF21 DIBBs: PD: OneDataShare: A Universal Data Sharing Building Block for Data-Intensive Applications","OAC","DATANET","09/01/2017","04/13/2018","Tevfik Kosar","NY","SUNY at Buffalo","Standard Grant","Amy Walton","08/31/2020","$616,469.00","","tkosar@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7726","068P, 7433, 8048, 9251, 9290","$0.00","Applications in scientific, industrial, and personal spaces now generate more data than ever before. As data become more abundant and data resources become more heterogeneous, the accessing, sharing and disseminating of data sets becomes a bigger challenge.  Existing technologies for transferring and sharing data suffer from serious shortcomings, including low transfer performance, inflexibility, restricted protocol support, and poor scalability.  This project develops a universal data sharing building block for data-intensive applications, dubbed OneDataShare, with three major goals: (1) optimization of end-to-end data transfers and reduction of the time to delivery of the data; (2) interoperation across heterogeneous and incompatible data resources; and (3) predicting the data delivery time and decreasing the uncertainty in real-time decision-making processes.<br/><br/>OneDataShare deliverables include: (1) design and implementation of novel algorithms for application-layer optimization of the data transfer protocol parameters to achieve optimal end-to-end data transfer throughput; (2) development of a universal interface specification for heterogeneous data storage endpoints and a framework for on-the-fly data transfer protocol translation; (3) instrumentation of end-to-end data transfer time prediction capability, and feeding it into real-time scheduling and decision-making processes for advanced provisioning, high-level planning, and co-scheduling of resources; (4) deployment of these capabilities as stand-alone OneDataShare cloud-hosted services to end users; and (5) integration of these capabilities with widely used data scheduling and workflow management tools, and validation in specific applications.  OneDataShare services and tools are developed at the application level, and they do not require any changes to the existing infrastructure, nor to the low-level networking stack, although they increase the end-to-end performance of the data movement tasks substantially.  These efficient and high-performance data transfer techniques will help the scientific community, industry, and end-users to save significant time and effort in transferring and sharing data."
"1642133","Collaborative Research: CICI: Secure and Resilient Architecture: Data Integrity Assurance and Privacy Protection Solutions for Secure Interoperability of Cloud Resources","OAC","Secure &Trustworthy Cyberspace, EPSCoR Co-Funding","10/01/2016","12/13/2018","Wei-Shinn Ku","AL","Auburn University","Standard Grant","Micah Beck","09/30/2019","$640,440.00","Anthony Skjellum, Ming-Kuo Lee, Shiwen Mao","weishinn@auburn.edu","310 Samford Hall","Auburn University","AL","368490001","3348444438","CSE","8060, 9150","9150","$0.00","Cloud computing provides many clear benefits for users, including scalability and reduced system acquisition cost. However, data security, integrity and privacy are becoming major concerns for scientific researchers when they access data from the cloud to conduct experiments or analytics. In addition, data owners may not want to reveal their data to cloud service providers either because of the sensitivity of the data (e.g., medical records) or because of its value. Therefore, it is important to create cloud data integrity assurance and privacy protection solutions that help users fully embrace cloud services as well as protect cyberinfrastructure resources. With a cloud database, data owners can store large-scale datasets collected from various sources. Users can then launch queries retrieving the data records for conducting research and experiments. However, there are several possible threats to query result accuracy. For example, a cloud database could be compromised and the stored data could be tampered with. There could be a malfunction in the cloud server, so that the cloud database inadvertently returns incomplete query results. It is unlikely that the client would be aware of such incorrect or incomplete query results. Consequently, erroneous data could be employed in subsequent scientific experiments or analyses, which could lead to false results. Cloud database query integrity assurance is critical issue that underpins a secure and trustworthy end-to-end scientific workflow. <br/><br/>This work approaches these problems in a privacy-friendly manner, building on top of encrypted queries over encrypted data. This is key for achieving both data privacy and data integrity. Data provenance - the history of the data and how its been handled - is also an important aspect of scientific workflows. However, securing the provenance to provide integrity, privacy, and confidentiality guarantees is also challenging, making it hard for many scientific workflows to provide a verifiable provenance history of scientific data and query results. With clouds, providing such guarantees is difficult for both data and provenance. This project enables infrastructural support for secure collection, storage, transmission, and verification of provenance information for all data and results stored and computed in the cloud. The availability of such verifiable provenance offers benefits to scientific workflows, making the process more trustworthy via verifiable history and results. The research team creates a query integrity assurance, data privacy protection, and verifiable provenance framework which provides an array of solutions for supporting secure cloud services. This project contributes to the cybersecurity research community by piloting novel cloud data security approaches that accomplish the following goals: (1) developing Voronoi diagram&#8208;based integrity assurance techniques, (2) designing cloud database data privacy protection methods, (3) modeling the trade off between query integrity assurance and query evaluation costs, (4) realizing secure cloud data provenance mechanisms, and (5) implementing a prototype system, where all the components are integrated for security and performance evaluation."
"1547428","CICI: Secure Data Architecture: STREAMS: Secure Transport and REsearch Architecture for Monitoring Stroke Recovery","OAC","Cyber Secur - Cyberinfrastruc","01/01/2016","07/29/2016","Yan Luo","MA","University of Massachusetts Lowell","Standard Grant","Micah Beck","12/31/2019","$515,858.00","Martin Margala, Xinwen Fu, Yu Cao","Yan_Luo@uml.edu","600 Suffolk Street","Lowell","MA","018543643","9789344170","CSE","8027","7434, 9251","$0.00","The rehabilitation of stroke patients is a long but critical process for their long-term wellness. Monitoring patients with wearable sensors and web cameras can support at-home rehabilitation by reducing the risk of events such as accidental falls and inappropriate dietary intake. Such sensor-generated live data streams about patient status and activities are processed at data centers for real-time analytics, helping healthcare professionals to respond to patients' needs quickly and effectively. Since the data streams may contain electronic Protected Health Information (ePHI), they must be protected so that transmission and usage conform to security and privacy regulations, such as Health Insurance Portability and Accountability Act (HIPAA) and applicable state laws. Therefore, it is important to investigate advanced networking and computing technologies to meet these security requirements, which are critical for bringing sensors and data analytics from research to clinical environments.<br/><br/>This research plans to address the security challenges in transferring and processing patient related sensor data by developing a Secure Transport and REsearch Architecture for Monitoring Stroke Recovery (STREAMS), a technical proof-of-concept implementation, to secure end-to-end sensor data streams using secure software defined networking and elastic compute and storage resources. STREAMS will be the first prototype of a secure network architecture to provide advanced data analytics-based healthcare to stroke patients in a realistic clinical environment. This project addresses issues in securing heterogeneous sensory ePHI patient data. It captures the workflows of patient data analysis and defines a role-based security enforcement framework to apply access policies. A Secure SDN controller will be designed to authenticate, identify, and direct encrypted data streams to ensure the data streaming over the network are HIPAA compliant, provide guidance in provisioning of compute resources at the cloud, and apply the most appropriate decryption algorithms based on the role of users, priority, types and source of the sensor data stream, as well as network conditions.  A generalizable secure hardware and software architecture collects, encrypts, decrypts, stores, transports, analyzes, and maintains the integrity and availability of the data from these multimodal sensors to enable them to be fused using analytics algorithms to learn about patient activities that are highly relevant to stroke recovery. The highly interdisciplinary project team consists of healthcare professionals, medical researchers, computer scientists, IT staff, engineering staff, and industrial partners."
"1443046","CIF21 DIBBs: STORM: Spatio-Temporal Online Reasoning and Management of Large Data","OAC","PHYSICAL & DYNAMIC METEOROLOGY, DATANET, EarthCube","11/01/2014","08/10/2016","Feifei Li","UT","University of Utah","Standard Grant","Amy Walton","10/31/2019","$1,173,975.00","John Horel, Paul Rosen, Jeff Phillips","lifeifei@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","1525, 7726, 8074","4444, 7433, 8048, 9150, 9251","$0.00","A fundamental challenge for many research projects is the ability to handle large quantities of heterogeneous data. Data collected from different sources and time periods can be inconsistent, or stored in different formats and data management systems. Thus, a critical step in many projects is to develop a customized query and analytical engine to translate inputs.  But for each new dataset, or for each new query type or analytic task for an existing dataset, a new query interface or program must be developed, requiring significant investments of time and effort.  This project will develop an automatic engine for searching large, heterogeneous data collections for weather and meteorology, particularly from instruments in the western US, in a regional network called MesoWest. <br/><br/>This project develops an automatic query and analytical engine for large, heterogeneous spatial and temporal data. This capability allows users to automatically deploy a query and analytical engine instance over their large, heterogeneous data with spatial and temporal dimensions.  The system supports a simple search-box and map-like query interface that allows numerous powerful analytical queries.  Techniques to make these queries robust, relevant, and highly scalable will be developed.  The project also enables users to execute queries over multiple data sources simultaneously and seamlessly. The goal of the work is to dramatically simplify the management and analysis of large spatio-temporal data at different institutions, groups, and corporations."
"1547380","CICI: Secure Data Architecture: Collaborative Research: Assured Mission Delivery Network Framework for Secure Scientific Collaboration","OAC","Cyber Secur - Cyberinfrastruc","01/01/2016","08/31/2015","Raj Jain","MO","Washington University","Standard Grant","Micah Beck","12/31/2019","$220,000.00","","Jain@CSE.WUSTL.Edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","8027","7434, 9150","$0.00","Collaborative, multi-disciplinary and multi-institutional research projects require secure and resilient cyberinfrastructure in order to efficiently support data sharing, access to remote scientific instruments, video-conferencing and on-line discussions. The underlying network plays a crucial role in supporting these needs in that it must provide assurance about the security of data and collaborative activities. This project addresses such requirement by designing and developing a network architecture to securely share data among groups of scientists. A community of scientists sharing a common interest and supporting resources is called a mission. This project will design and prototype the architecture of the Assured Mission Delivery Network (AMDN), which will enable collaboration among scientific communities involving multiple independent organizations with varying levels of trust.  <br/><br/>They novelty of AMDN lies in the notion of network-level Mission Assurance Services (MAS); these services allows mission directors to specify actions to be taken by the network to deal with attacks and anomalies and to quickly reconfigure the network to best assure the successful completion of the mission. Security is the key part of the AMDN design, and addresses essential functionality such as authentication, integrity, accountability and privacy. AMDN also includes Collective Anomaly Detection, in which intra- and inter-cloud networking alarms and anomalies indicative of attacks are combined and used for mission assurance strategies. The detected anomalies and alarms are correlated over the whole system in order to detect sophisticated attacks that might be undetectable at the single node level. The security of the entire system is flexible and programmable depending on the nature of collaborations, computing resources needed, and various requirements of scientists. In addition to scientific use, AMDN can be used for commercial applications such as financial data sharing among banks or health data sharing among hospitals and between critical infrastructures such as Smart Grids. Although AMDN is primarily designed for wide-area network usage, it can be used for services and clients residing inside a single cloud or data center."
"1828467","MRI: Development of An Instrument for Secure Cyber Physical Systems Analytics","OAC","MAJOR RESEARCH INSTRUMENTATION","09/01/2018","08/24/2018","Murat Kantarcioglu","TX","University of Texas at Dallas","Standard Grant","Stefan Robila","08/31/2021","$601,797.00","Latifur Khan, Bhavani Thuraisingham, Alvaro Cardenas","muratk@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1189","1189","$0.00","This project aims to develop a secure and scalable instrument that can efficiently perform big data analytics on data collected from Internet of Things (IoT) devices while providing tools for preserving data security and privacy. IoT devices are monitoring and controlling systems that interact with the physical world by collecting, processing and transmitting data using the internet. IoT devices include home automation systems, smart grid, transportation systems, medical devices, building controls, manufacturing and industrial control systems. With the increase in deployment of IoT devices, the amount of data generated by these devices also increases.  There is thus a need for large-scale, and secure data processing systems to process and extract information for efficient and impactful decision-making. The issue of trustworthiness in computation and data security arises when the IoT data contains sensitive information. For example, data collected using the home health devices such as a wireless blood-pressure monitor may need to be analyzed and correlated with other information. In these cases, data owners may need to protect their data and demand guaranties about data security and integrity. Development of this instrument will enable new research projects that require efficient and secure processing of IoT data. Consequently, this may allow the creation of novel IoT data processing tools and services which are not feasible today due to security and privacy concerns. <br/><br/>The proposed instrument will integrate two important components in a novel and unique way. First, an IoT data gathering component that can collect various data from IoT devices including the industrial IoT (IIoT) devices, will be developed. This component will create the necessary data gathering part of the instrument. It will allow researchers to adjust data collection frequency and granularity to enable different types of data collection activities for various research projects. The second component of the instrument will be the secure data analytics layer that can process the potentially sensitive IoT data including the network packets, sensor data, etc. As part of this component, recently developed secure data processing techniques which leverage trusted execution environments (TEEs) will be implemented. In addition, these TEE-based techniques will be tailored for different types of IoT data to increase their efficiency and limit any sensitive data leakage. During this project, these components will be integrated using custom developed software which will be open sourced at the final stage. Furthermore, these components will be integrated to test various research tools with respect to scalability, security and data privacy. The developed instrument will be made available to our collaborators and larger scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1535191","SI2-SSE: Algorithms and Tools for Data-Driven Executable Biology","OAC","ADVANCES IN BIO INFORMATICS, Software Institutes","10/01/2015","11/24/2017","Rastislav Bodik","WA","University of Washington","Standard Grant","Stefan Robila","09/30/2019","$515,784.00","Aditya Virendra Thakur","bodik@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1165, 8004","026Z, 7433, 8004, 8005, 9251","$0.00","This project seeks to understand the signaling mechanisms that control cellular activities such as  cell division, cell growth, and cell differentiation.  Errors in cellular signaling cause diseases such as cancer, autoimmunity, and diabetes.  Accurate models of cellular signalling are thus necessary for rational drug design and other applications central to national health.  This project focuses on inferring models from experimental data. Specifically, it is interested in models of protein signalling because proteins control and mediate the vast majority of biological processes in a living cell. The project follows of the approach of executable biology: models of cell signalling are computer programs, which allows executing the models on the computer and comparing the model behavior against the behavior of the living cell observed in the lab setting.  Most importantly for this project, viewing models as programs will allow the team to harness the recent advances in automatic synthesis of computer programs for synthesis of models from experimental measurements of cells. <br/><br/>The goal of the project is to provide biologists with a tool that synthesizes a variety of executable models from varied types of experimental data.  To facilitate synthesis of mechanistic models from experimental data, the project will develop a family of modeling languages that will capture complex behaviors of biological systems, such as time and concurrency.  The languages will be instances of the more general Boolean-Networks language.  The team will investigate how to adjust the modeling abstraction based on the nature of available experimental data; the abstractions will be instantiated as suitably chosen languages from their language family.  The modeling framework will be built by leveraging techniques from programming languages and formal methods such as meta-programming and constraint solving."
"1642120","CICI: Secure and Resilient Architecture: Campus Infrastructure for Microscale, Privacy-Conscious, Data-Driven Planning","OAC","Cyber Secur - Cyberinfrastruc","01/01/2017","09/15/2016","John Foster","NY","Cornell University","Standard Grant","Micah Beck","12/31/2019","$999,364.00","Fred Schneider, Deborah Estrin, David Shmoys","jnfoster@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","8027","","$0.00","Individuals today generate an immense amount of data as a byproduct of their daily activities through use of mobile phones, wearable devices, and online services. These small bits of data are called microscale data. The availability of microscale data creates new opportunities for solving a variety of complex planning problems at the institutional level, but it also raises concerns about security and privacy for the individual whose data is collected. The ability to realize the beneficial uses of microscale data is threatened by these concerns over privacy. This project develops and evaluates an architecture that allows individuals to monitor and manage sharing of their microscale data in order to maximize individual and institutional utility.<br/><br/>This project develops a software framework to support the implementation of data-driven planning applications where individuals have fine-grained control over use of their data. Work on the project focuses on: (i) creating a campus testbed capable of acquiring microscale data streams from sources such as wireless access points, card readers, room sensors, and point-of-sale systems; (ii) building a data management platform that offers flexible controls for imposing use-based restrictions on queries and transformations of microscale data; (iii) developing applications that use microscale data to solve practical planning problems related to transportation, space, and food in a campus setting. Having an open-source platform that addresses fundamental security and privacy challenges for microscale data has the potential for large impact on real applications and industry. Under the auspices of this funding, the investigators will also develop masters-level projects on microscale data-driven planning, providing the next generation of engineers with training in an emerging interdisciplinary area."
"1547411","CICI: Secure Data Architecture: Collaborative Research: Assured Mission Delivery Network Framework for Secure Scientific Collaboration","OAC","Cyber Secur - Cyberinfrastruc","01/01/2016","08/31/2015","Arjan Durresi","IN","Indiana University","Standard Grant","Micah Beck","12/31/2019","$139,999.00","","durresi@cs.iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8027","7434","$0.00","Collaborative, multi-disciplinary and multi-institutional research projects require secure and resilient cyberinfrastructure in order to efficiently support data sharing, access to remote scientific instruments, video-conferencing and on-line discussions. The underlying network plays a crucial role in supporting these needs in that it must provide assurance about the security of data and collaborative activities. This project addresses such requirement by designing and developing a network architecture to securely share data among groups of scientists. A community of scientists sharing a common interest and supporting resources is called a mission. This project will design and prototype the architecture of the Assured Mission Delivery Network (AMDN), which will enable collaboration among scientific communities involving multiple independent organizations with varying levels of trust.  <br/><br/>They novelty of AMDN lies in the notion of network-level Mission Assurance Services (MAS); these services allows mission directors to specify actions to be taken by the network to deal with attacks and anomalies and to quickly reconfigure the network to best assure the successful completion of the mission. Security is the key part of the AMDN design, and addresses essential functionality such as authentication, integrity, accountability and privacy. AMDN also includes Collective Anomaly Detection, in which intra- and inter-cloud networking alarms and anomalies indicative of attacks are combined and used for mission assurance strategies. The detected anomalies and alarms are correlated over the whole system in order to detect sophisticated attacks that might be undetectable at the single node level. The security of the entire system is flexible and programmable depending on the nature of collaborations, computing resources needed, and various requirements of scientists. In addition to scientific use, AMDN can be used for commercial applications such as financial data sharing among banks or health data sharing among hospitals and between critical infrastructures such as Smart Grids. Although AMDN is primarily designed for wide-area network usage, it can be used for services and clients residing inside a single cloud or data center."
"1724889","CIF21 DIBBs: PD: Enhancing and Personalizing Educational Resources through Tools for Experimentation","OAC","DATANET, Core R&D Programs","07/01/2017","08/30/2018","Neil Heffernan","MA","Worcester Polytechnic Institute","Standard Grant","Amy Walton","06/30/2020","$544,644.00","Joseph Williams, Korinn Ostrow","nth@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","7726, 7980","7218, 7433, 8048, 8083","$0.00","This project would automate the creation and data analysis of randomized controlled experiments (RCEs).   RCEs are question sets designed and delivered to teachers and students in a classroom setting, and can be used to compare alternative educational strategies. This effort builds on an existing educational platform (ASSISTments) developed by the Principal Investigator, and uses a template-based approach to increase the efficiency and reliability of conducting educational research.  The goal is to lower the barriers to creating and learning from randomized controlled experiments. The project has the potential to facilitate large-scale learning in education research, reaching hundreds of schools and thousands of students.<br/><br/>The project builds upon two prior developments by this team.  <br/>  -  ASSISTments is an online learning platform originally designed to provide students with assistance and teachers with assessments (establishing the moniker). The system is used primarily as an online tutoring system for middle or secondary education, supporting the delivery, collection, and grading of classwork and homework, providing immediate feedback for students and explicit reporting for teachers.  To date, 24 randomized controlled experiments comparing educational strategies have been published using this platform.  <br/>  -  In addition, AssistmentsTestBed.org is a testbed developed by the PI and his group under a separate NSF grant (#1440753), to identify best practices in education and allow other researchers to propose and run their own studies leveraging ASSISTments as a shared scientific instrument through this testbed.  Beneficiaries include education researchers, teachers, and students, with the existing tool being used in over 500 schools and in the education of over 50,000 students.  <br/>The current project improves two components of the infrastructure that have been resource-intensive bottlenecks in prior research.    One task automates the process of study creation and data analysis, through development of a Template Tool that enables studies within ASSISTments.  A second task automates statistical analyses and improves usability of the existing data reporting tool (Assessment of Learning Infrastructure, or ALI).  The improvements will be achieved, in part, by applying educational data mining algorithms (i.e., deep knowledge tracing) on student data collected before, during, and after experimentation.  These analytics will provide researchers with covariates that will significantly improve the agenda of personalizing education.  The resulting capability will assist researchers as they design and deliver question sets to teachers and students in a classroom setting, increase the efficiency and reliability of conducting educational research at scale, and streamline the research processes.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Education and Human Resources, Division of Research on Learning in Formal and Informal Settings."
"1349002","RCN: Building the Research Data Alliance Community through US and International Engagement (RDA 2)","OAC","INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE, DATANET","10/01/2013","07/17/2018","Francine Berman","NY","Rensselaer Polytechnic Institute","Continuing grant","Amy Walton","09/30/2020","$6,022,637.00","Beth Plale, Laurence Lannom, Mark Parsons, Inna Kouper, Kathleen Fontaine","bermaf@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1640, 7231, 7726","7433, 8048","$0.00","Data-driven innovation requires a foundation of infrastructure to support the sharing, access, use, re-use, analysis, and stewardship of digital data. Data infrastructure includes cyber-infrastructure tools for data discovery and analysis (and their underlying technical structures and components -- persistent digital identifiers, shared metadata frameworks, etc.), and social infrastructure in the form of community policy and organizational practice (harmonization of standards, approaches for data access and preservation, etc.). To address the growing global need for data infrastructure, the Research Data Alliance (RDA) was planned and launched in FY13 as an international community-driven organization. RDA's mission is to build the social, organizational, and technical infrastructure needed to reduce barriers to data sharing and exchange, and to accelerate data-driven innovation world-wide. In the wake of precipitous growth, RDA is now working to develop a stable support model for at least a five-year period. The RDA 2 project provides support for U.S. participation and leadership within the RDA, as well as strategic expansion of the U.S. community within RDA (RDA/U.S.). RDA 2 integrates a) strategic efforts and pilots to expand, diversify, and strengthen the RDA/U.S. community, and b) support for U.S. contributions to RDA operations and leadership, including U.S. participation in, and hosting of, RDA Plenaries. RDA/U.S. will be led by a Steering Committee consisting of Fran Berman, RPI (Chair), Beth Plale, IU, (Vice Chair, Technical Programs), Larry Lannom, CNRI, (Vice Chair, Outreach and Development) and Mark Parsons, RPI (Managing Director).<br/><br/>The RDA 2 project builds RDA through increasing U.S. engagement and leadership. RDA 2 builds RDA/U.S. through pilots in three strategic areas: Community Engagement and Outreach, Student and Early Career Engagement, and Adoption and Impact Amplification. The pilots are designed to expand and diversify RDA/U.S. data community, increase the impact of RDA deliverables in the U.S., and enhance the benefit of RDA for U.S. institutions, communities, businesses, and individuals. A strong RDA/U.S. can be a vehicle for accelerating U.S. innovation, and positioning the U.S. community for greater competitiveness and leadership. RDA/U.S. can contribute to the data infrastructure needed to make new U.S. policy approaches and initiatives work, and serve as a means of capitalizing"
"1450323","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2015","05/07/2018","Brian Bockelman","NE","University of Nebraska-Lincoln","Continuing grant","Bogdan Mihaila","04/30/2019","$1,001,324.00","","bbockelman@morgridge.org","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","CSE","1253, 7244, 8004","7433, 8009, 8084, 9150","$0.00","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces.  However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community.  Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program."
"1541353","CC*DNI Instrument: High-Bandwidth Network Connectivity for Remote Sensing Research","OAC","Campus Cyberinfrastrc (CC-NIE)","11/01/2015","08/25/2015","Michael Zink","MA","University of Massachusetts Amherst","Standard Grant","Kevin Thompson","10/31/2019","$163,028.00","","zink@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","8080","","$0.00","Scientific instruments can produce vast amounts of data. Besides capturing and storing data, it is important to give the global research community fast access to this data. In the case of remote sensing instruments like Doppler weather radar the challenges such instruments face in terms of connecting them to high-bandwidth networks is the fact that they have to be placed outside the lab at locations that are well suited for their sensing tasks, which are often not close to high-bandwidth networking infrastructure. For example, a radar is ideally placed at a location that is free of any obstructions such that it can scan the atmosphere with as little interference as possible.<br/><br/>The overarching goal of this project is to improve research that focuses on remote sensing by providing researchers the ability to transmit high-bandwidth time-series or spectral data, and analog signals via radio over fiber, in real time from sensors to remote storage and computing. The project improves connectivity from an on-campus tower hosting radar and other atmospheric sensors to the UMass Amherst computer network. A fiber connection is established between the tower and the main campus network with local storage capacity increases to 8 TB at the tower. This integration permits several new modes of real time data analysis for weather system research and allows for both more data to be captured and for more timely access to data.<br/><br/>Improving atmospheric observations has significant impacts on weather modeling in general and weather warning and prediction in particular. Thus this project has the potential to support applications that increase the safety and security of US communities and the nation as a whole. In addition, this campus network infrastructure opens new opportunities for students. For example, students can obtain live analog or high-bandwidth time series radar data to experiment with."
"1547164","CICI: Data Provenance:  Collaborative Research: Provenance Assurance Using Currency Primitives","OAC","Cyber Secur - Cyberinfrastruc","01/01/2016","07/07/2018","Richard Brooks","SC","Clemson University","Standard Grant","Micah Beck","12/31/2019","$321,519.00","","rrb@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","8027","7434, 9150, 9251","$0.00","Data provenance is the ability to track data history including things such as where the data resided, who handled it, and what systems stored, forwarded and processed it. This research builds on the architecture of the digital currency Bitcoin. It develops distributed data ledgers - similar to bookkeeping ledgers - that maintain data history so it can't be manipulated by hackers trying to hide their activities. Data consistency guarantees  that everyone gets the right answers about where, who, and what regardless of which ledger is read.  This software advances the security of computing systems by making data accountable, especially for online commerce and big data (""the cloud'').  It secures forensic information taken from compromised computers  for further analysis. It validates whether privacy requirements are being met for medical records.  The key outcome is a software prototype that implements the complete system and illustrates the ability to store, maintain, and update provenance information for real data. <br/><br/>A data provenance framework will be designed, prototyped, evaluated and then delivered as an Application Programmer Interface, software library, and distributed service. This work will produce a reusable distributed service architecture achieving scalability by using distributed services that maintain ledger information. The system leverages Bitcoin cryptocurrency by building on Bitcoin's block-chain architecture to maintain provenance metadata securely. It leverages existing tools for provenance data exploration and visualization.  Digital signatures from both the server/system as well as the user creates dual information about possession, while distributed ledgers remove control and maintenance of metadata from the user who creates it. The prototype enables research into long-term provenance creation, maintenance, and utilization for workflows in the area of cybersecurity as well  studies of how to integrate and secure provenance into existing file systems and network services.  Opt-in and passive (involuntary) provenance systems will be enabled using the API, library, and distributed ledgers prototyped, enabling data provenance for systems where needed, notably high assurance cloud computing and scientific workflow systems. The tool can be used  to enable reproducibility of published results from archived data and artifacts."
"1835764","Framework: Software: NSCI: Collaborative Research: Hermes: Extending the HDF Library to Support Intelligent I/O Buffering for Deep Memory and Storage Hierarchy Systems","OAC","Software Institutes","11/01/2018","11/20/2018","Xian-He Sun","IL","Illinois Institute of Technology","Standard Grant","Vipin Chaudhary","10/31/2022","$2,850,000.00","Ann Johnson, Elena Pourmal","sun@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Modern high performance computing (HPC) applications generate massive amounts of data. However, the performance improvement of disk based storage systems has been much slower than that of memory, creating a significant Input/Output (I/O) performance gap. To reduce the performance gap, storage subsystems are under extensive changes, adopting new technologies and adding more layers into the memory/storage hierarchy. With a deeper memory hierarchy, the data movement complexity of memory systems is increased significantly, making it harder to utilize the potential of the deep memory and storage hierarchy (DMSH) design. As we move towards the exascale era, I/O bottleneck is a must to solve performance bottleneck facing the HPC community. DMSHs with multiple levels of memory/storage layers offer a feasible solution but are very complex to use  effectively. Ideally, the presence of multiple layers of storage should be transparent to applications without having to sacrifice I/O performance. There is a need  to enhance and extend current software systems to support data access and movement transparently and effectively under DMSHs. Hierarchical Data Format (HDF) technologies are a set of current I/O solutions addressing the problems in organizing, accessing, analyzing, and preserving data. HDF5 library is widely popular within the scientific community. Among the high level I/O libraries used in DOE labs, HDF5 is the undeniable leader with 99% of the share. HDF5 addresses the I/O bottleneck by hiding the complexity of performing coordinated I/O to single, shared files, and by encapsulating general purpose optimizations. While HDF technologies, like other existing I/O middleware, are not designed to support DMSHs, its wide popularity and its middleware nature make HDF5 an ideal candidate to enable, manage, and supervise I/O buffering under DMSHs. This project proposes the development of Hermes, a heterogeneous aware, multi tiered, dynamic, and distributed I/O buffering system that will significantly accelerate I/O performance. <br/><br/>This project  proposes to extend HDF technologies with the Hermes design. Hermes is new,  and the enhancement of HDF5 is new. The deliveries of this research include an enhanced HDF5 library, a set of extended HDF technologies, and a group of general I/O buffering and memory system optimization mechanisms and methods. We believe that the combination of DMSH I/O buffering and HDF technologies is a reachable practical solution that can efficiently support scientific discovery. Hermes will advance HDF5 core technology by developing new buffering algorithms and mechanisms to support 1) vertical and horizontal buffering in DMSHs: here vertical means access data to/from different levels locally and horizontal means spread/gather data across remote compute nodes; 2) selective buffering via HDF5: here selective means some memory layer, e.g. NVMe, only for selected data; 3) dynamic buffering via online system profiling: the buffering schema can be changed dynamically based on messaging traffic; 4) adaptive buffering via Reinforcement Learning: by learning the application's access pattern, we can adapt prefetching algorithms and cache replacement policies at runtime. The development Hermes will be translated into high quality dependable software and will be released with the core HDF5 library.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642078","Collaborative Research: CICI: Secure and Resilient Architecture: Data Integrity Assurance and Privacy Protection Solutions for Secure Interoperability of Cloud Resources","OAC","Cyber Secur - Cyberinfrastruc, Secure &Trustworthy Cyberspace, EPSCoR Co-Funding","10/01/2016","09/12/2016","Ragib Hasan","AL","University of Alabama at Birmingham","Standard Grant","Micah Beck","09/30/2019","$224,557.00","Purushotham Bangalore","ragib@uab.edu","AB 1170","Birmingham","AL","352940001","2059345266","CSE","8027, 8060, 9150","9150","$0.00","Cloud computing provides many clear benefits for users, including scalability and reduced system acquisition cost. However, data security, integrity and privacy are becoming major concerns for scientific researchers when they access data from the cloud to conduct experiments or analytics. In addition, data owners may not want to reveal their data to cloud service providers either because of the sensitivity of the data (e.g., medical records) or because of its value. Therefore, it is important to create cloud data integrity assurance and privacy protection solutions that help users fully embrace cloud services as well as protect cyberinfrastructure resources. With a cloud database, data owners can store large&#8208;scale datasets collected from various sources. Users can then launch queries retrieving the data records for conducting research and experiments. However, there are several possible threats to query result accuracy. For example, a cloud database could be compromised and the stored data could be tampered with. There could be a malfunction in the cloud server, so that the cloud database inadvertently returns incomplete query results. It is unlikely that the client would be aware of such incorrect or incomplete query results. Consequently, erroneous data could be employed in subsequent scientific experiments or analyses, which could lead to false results. Cloud database query integrity assurance is critical issue that underpins a secure and trustworthy end&#8208;to&#8208;end scientific workflow. <br/><br/>This work approaches these problems in a privacy&#8208;friendly manner, building on top of encrypted queries over encrypted data. This is key for achieving both data privacy and data integrity. Data provenance - the history of the data and how its been handled - is also an important aspect of scientific workflows. However, securing the provenance to provide integrity, privacy, and confidentiality guarantees is also challenging, making it hard for many scientific workflows to provide a verifiable provenance history of scientific data and query results. With clouds, providing such guarantees is difficult for both data and provenance. This project enables infrastructural support for secure collection, storage, transmission, and verification of provenance information for all data and results stored and computed in the cloud. The availability of such verifiable provenance offers benefits to scientific workflows, making the process more trustworthy via verifiable history and results. The research team creates a query integrity assurance, data privacy protection, and verifiable provenance framework which provides an array of solutions for supporting secure cloud services. This project contributes to the cybersecurity research community by piloting novel cloud data security approaches that accomplish the following goals: (1) developing Voronoi diagram&#8208;based integrity assurance techniques, (2) designing cloud database data privacy protection methods, (3) modeling the trade off between query integrity assurance and query evaluation costs, (4) realizing secure cloud data provenance mechanisms, and (5) implementing a prototype system, where all the components are integrated for security and performance evaluation."
"1642134","Collaborative Research: CICI: Secure and Resilient Architecture: NetSecOps -- Policy-Driven, Knowledge-Centric, Holistic Network Security Operations Architecture","OAC","Cyber Secur - Cyberinfrastruc","09/01/2016","08/22/2016","James Griffioen","KY","University of Kentucky Research Foundation","Standard Grant","Micah Beck","08/31/2019","$499,925.00","Jane Hayes, Vernon Bumgardner","griff@netlab.uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","8027","9150","$0.00","Network infrastructure at University campuses is complex and sophisticated, often supporting a mix of enterprise, academic, student, research, and healthcare data, each having its own distinct security, privacy, and priority policies.  Securing this complex and highly dynamic environment is extremely challenging, particularly since campus infrastructures are increasingly under attack from malicious actors on the Internet and (often unknowingly) internal campus devices. Different parts of the campus have very different policies and regulations that govern its treatment of sensitive data (e.g., private student/employee information, health care data, financial transactions, etc.). Furthermore, data-intensive scientific research traffic often requires exceptions to normal security policies, resulting in ad-hoc solutions that bypass standard operational procedures and leave both the scientific workflow and the campus as a whole vulnerable to attack. In short, state-of-the-art campus security operations still heavily rely on human domain experts to interpret high level policy documents, implement those policies through low-level mechanisms, create exceptions to accommodate scientific workflows, interpret reports and alerts, and be able to react to security events in near real time on a 24-by-7 basis.<br/><br/>This project addresses these challenges through a collaborative research effort, called NetSecOps (Network Security Operations), that assists information technology (IT) security teams by automating many of the operational tasks that are tedious, error-prone, and otherwise problematic in current campus networks. NetSecOps is policy-driven in that the framework encodes high-level human-readable policies into systematic policy specifications that drive the actual configuration and operation of the infrastructure. NetSecOps is knowledge-centric in that the framework captures data, information, and knowledge about the infrastructure in a central knowledge store that informs and guides IT operational tasks. The proposed NetSecOps architecture has the following unique capabilities: (1) the ability to capture campus network security policies systematically; (2) the ability to create new fine-grained network control abstractions that leverage existing security capabilities and emerging software defined networks (SDN) to implement security policies, including policies related to both scientific workflows and IT domains; (3) the ability to implement policy traceability tools that verify whether these network abstractions maintain the integrity of the high-level policies; (4) the ability to implement knowledge-discovery tools that enable reasoning across data from existing security point-solutions, including security monitoring tools and authentication and authorization frameworks; and (5) the ability to automatically adjust the network's security posture based on detected security events. Research results and tools from the project will be released into the public domain allowing academic institutions to utilize the resources as part of their best-practice IT security operations."
"1738912","CICI: RSARC: DICE - Data Insurance in the Cluster Environment","OAC","Cyber Secur - Cyberinfrastruc","08/15/2017","07/13/2017","Zhi Wang","FL","Florida State University","Standard Grant","Micah Beck","07/31/2020","$590,317.00","Xin Yuan, Paul Van Der Mark, Viet Tung Hoang","zwang@cs.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","8027","","$0.00","High-performance, distributed computing has become indispensable in solving complex scientific, engineering, and business problems. The integrity of the data generated and stored on computer clusters is of undisputed importance to scientific research and business intelligence, as compromised data can lead to incorrect conclusions and decisions. Unfortunately, existing security mechanisms for high-performance and distributed computing systems are complex, inconsistent, insecure, and difficult to deploy. Many systems utilizing the current security mechanisms simply do not provide sufficient protection and remain vulnerable to even trivial attacks. For example, recent studies have found that thousands of unprotected database installations and computer clusters have been hacked. As such, there is a pressing need to improve the security of high-performance and distributed computing systems. <br/><br/>This project develops a security framework for high-performance and distributed computing systems that employs strong modern cryptographic algorithms, and is easy to reason, deploy, and use without lengthy and error-prone configurations.  The project consists of three major components: a container-based virtual cluster, a component to defend against side-channel attacks, and a secure execution ledger for auditing. The first component is the key to enabling authentication, authorization, and data protection for clusters without sacrificing usability or performance. The project team will build the virtual cluster based on the popular Docker container but enhance it with flexible key management, attack surface reduction, and security hardening, including defenses against side-channel attacks. Communications among nodes of a virtual cluster and I/O operations are transparently encrypted to protect the data in transition and at rest. The secure execution ledger provides a global holistic view of program execution in the whole system, allowing auditing the behavior of individual users as well as user groups. By tightly integrating these three components, the project seeks to achieve strong support for the four pillars of the cluster data security ? authentication, authorization, auditing, and data protection."
"1738965","CICI: RSARC: SECTOR: Building a Secure and Compliant Cyberinfrastructure for Translational Research","OAC","Cyber Secur - Cyberinfrastruc","09/01/2017","07/20/2018","Yan Luo","MA","University of Massachusetts Lowell","Standard Grant","Micah Beck","08/31/2020","$999,651.00","Yu Cao, Peilong Li, Silvia Corvera, Jomol Mathew","Yan_Luo@uml.edu","600 Suffolk Street","Lowell","MA","018543643","9789344170","CSE","8027","","$0.00","Translational research has been driven by the increasing amount of heterogeneous data sets collected from medical instruments, sensors, patient diagnosis records, genomic and microbiome samples. The recent advancement in information technology greatly accelerates the innovation in translational research by applying in-depth analysis on these data with large scale computing and storage resources. Clinical researchers now heavily rely on such cyberinfrastructure to understand trends, derive correlations, and/or identify anomalies, which are instrumental to accurate diagnosis, precision drug discovery and effective treatment of diseases.  In this context, the security of the patient data, the efficient sharing of data, and the processing of data in a regulation compliant fashion are critical. This project addresses the resource limitations and security aspects of data-driven translational research. The project will greatly speed up clinical research activities that rely heavily on the analysis of sensitive data. The resulting software defined security infrastructure can be applied to a wide range of cyberinfrastructure that carries sensitive data. The project will strengthen the collaboration among computer scientists, clinical researchers, IT managers and provides a rare opportunity to address the cyberinfrastructure challenges in a holistic way instead of an ad hoc, incremental manner.<br/><br/>This project designs and deploys a Security and compliant Cyberinfrastructure for Translational Research (SECTOR) that enables sharing and computing on sensitive data sets between private compute clusters, a shared HPC facility and/or a HIPAA compliant cloud. Specifically, it (1) leverages the emerging software defined infrastructure (SDI), blockchain and secure domain name system to extend the boundary of computing on sensitive and private data. This is the first project to bring the agility and resilience of SDI to clinical research activities; (2) designs a framework that enables the deployment of a new workflows with fine grained user and access control that are compliant with the HIPAA (Health Insurance Portability and Accountability Act of 1996) technical safeguard; (3) enables the migration of data computation using streaming based data redirection and processing, requiring very little effort in porting existing scientific applications; (4) designs a portal that exploits the underlying SDI resources to help various stakeholders to express their workflow and simplify the management of healthcare resources."
"1642158","Collaborative Research: CICI: Secure and Resilient Architecture: NetSecOps - Policy-Driven, Knowledge-Centric, Holistic Network Security Operations Architecture","OAC","Cyber Secur - Cyberinfrastruc","09/01/2016","08/22/2016","Jacobus VAN DER MERWE","UT","University of Utah","Standard Grant","Micah Beck","08/31/2019","$499,923.00","Joseph Breen III, Corey Roach","kobus@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8027","","$0.00","Network infrastructure at University campuses is complex and sophisticated, often supporting a mix of enterprise, academic, student, research, and healthcare data, each having its own distinct security, privacy, and priority policies.  Securing this complex and highly dynamic environment is extremely challenging, particularly since campus infrastructures are increasingly under attack from malicious actors on the Internet and (often unknowingly) internal campus devices. Different parts of the campus have very different policies and regulations that govern its treatment of sensitive data (e.g., private student/employee information, health care data, financial transactions, etc.). Furthermore, data-intensive scientific research traffic often requires exceptions to normal security policies, resulting in ad-hoc solutions that bypass standard operational procedures and leave both the scientific workflow and the campus as a whole vulnerable to attack. In short, state-of-the-art campus security operations still heavily rely on human domain experts to interpret high level policy documents, implement those policies through low-level mechanisms, create exceptions to accommodate scientific workflows, interpret reports and alerts, and be able to react to security events in near real time on a 24-by-7 basis.<br/><br/>This project addresses these challenges through a collaborative research effort, called NetSecOps (Network Security Operations), that assists information technology (IT) security teams by automating many of the operational tasks that are tedious, error-prone, and otherwise problematic in current campus networks. NetSecOps is policy-driven in that the framework encodes high-level human-readable policies into systematic policy specifications that drive the actual configuration and operation of the infrastructure. NetSecOps is knowledge-centric in that the framework captures data, information, and knowledge about the infrastructure in a central knowledge store that informs and guides IT operational tasks. The proposed NetSecOps architecture has the following unique capabilities: (1) the ability to capture campus network security policies systematically; (2) the ability to create new fine-grained network control abstractions that leverage existing security capabilities and emerging software defined networks (SDN) to implement security policies, including policies related to both scientific workflows and IT domains; (3) the ability to implement policy traceability tools that verify whether these network abstractions maintain the integrity of the high-level policies; (4) the ability to implement knowledge-discovery tools that enable reasoning across data from existing security point-solutions, including security monitoring tools and authentication and authorization frameworks; and (5) the ability to automatically adjust the network?s security posture based on detected security events. Research results and tools from the project will be released into the public domain allowing academic institutions to utilize the resources as part of their best-practice IT security operations."
"1440420","SI2-SSE: Scalable Big Data Clustering by Random Projection Hashing","OAC","SPECIAL PROJECTS - CCF, Software Institutes","09/01/2014","08/08/2014","Philip Wilsey","OH","University of Cincinnati Main Campus","Standard Grant","Bogdan Mihaila","08/31/2019","$498,127.00","","philip.wilsey@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","2878, 8004","2878, 7433, 8004, 8005","$0.00","This project plans to develop a distributed algorithm for secure clustering of high dimensional data sets.  Fields in health and biology are significantly benefited by data clustering scalability.  Bioinformatic problems such as Micro Array clustering, Protein-Protein interaction clustering, medical resource decision making, medical image processing, and clustering of epidemiological events all serve to benefit from larger dataset sizes.  The algorithm under development, called Random Projection Hash or RPHash, utilizes aspects of locality sensitive hashing (LSH) and multi-probe random projection for computational scalability and linear achievable gains from parallel speed.  Furthermore, RPHash provides data anonymization through destructive manipulation of the data preventing de-anonymization attacks beyond standard best practices database security methods.  RPHash will be deployable on commercially available cloud resources running the Hadoop (MRv2) implementation of MapReduce.  The exploitation of general purpose cloud processing solutions allows researchers to scale their processing needs using virtually limitless commercial processing resources.<br/><br/>The RPHash algorithm uses various recent techniques in data mining along with a new approach toward achieving algorithmic scalability on distributed systems.  The basic intuition of RPHash is to combine multi-probe random projection with discrete space quantization.  Regions of high density are then regarded as centroid candidates.  To follow common parameterized, k-means methods, the top k regions will be selected.  The focus on a randomized, and thus non-deterministic, clustering algorithm is somewhat uncommon in computing, but common for ill-posed, combinatorially restrictive problems such as clustering and partitioning.  Despite theoretical results showing that k-means has an exponential worst case complexity, many real world problems tend to fair much better under k-means and other similar algorithms."
"1534872","SI2-SSE: ShareSafe: A Framework for Researchers and Data Owners to Help Facilitate Secure Graph Data Sharing","OAC","Software Institutes","09/01/2015","06/23/2016","Raheem Beyah","GA","Georgia Tech Research Corporation","Standard Grant","Bogdan Mihaila","08/31/2019","$506,000.00","","rbeyah@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8004","7433, 8004, 8005, 9251","$0.00","There is a critical need for information/data sharing to solve some of our most significant academic and societal problems. These data are increasing in size and are becoming much more complex; in many cases, they can be considered structured. An example of structured data is data describing disease propagation in a specific population. Widespread sharing of data can, among many other things, help corporations increase their revenues, help reduce the spread of communicable diseases, accelerate the cure of some of the most significant diseases, and enable reproducible experiments amongst researchers. Although there is little disagreement that sharing data has tremendous benefits, it is still not as widespread as it should be. This is, in part, due to privacy concerns with sharing datasets. This project will develop an open source system (ShareSafe) that allows data owners to evaluate the security (such as resistance to de-anonymization attacks) and utility of their anonymized datasets before release, which will help facilitate the data sharing process.<br/><br/>The overarching goals of this project are to develop a software framework, ShareSafe, that (1) helps structured data owners (e.g., social network researchers, epidemiologists) evaluate the security (against modern de-anonymization attacks) and utility of their datasets when using simple and state-of-the-art anonymization techniques; and (2) to provide structured data security/privacy researchers a uniform platform to comprehensively study, evaluate, and compare existing/newly developed techniques for structured data utility and privacy. ShareSafe is a comprehensive, user-friendly framework with the following capabilities: ShareSafe will enable data owners to: (1) anonymize their datasets with all of the state-of-the art anonymization techniques; (2) measure the utility of anonymized datasets using state-of-the-art utility measurement techniques; (3) evaluate the practical security of their datasets by subjecting them to state-of-the-art de-anonymization attacks; and (4) evaluate the theoretical security of their datasets by subjecting them to state-of-the-art de-anonymization quantification (de-anonymizability analysis) techniques. Understanding the results from (2)-(4) allows data owners to determine which anonymization algorithm suits their needs when sharing datasets.  Finally, the aforementioned techniques will be implemented in a uniform manner as open source software, allowing graph data security/privacy researchers the ability to comprehensively study, evaluate, and compare existing/newly developed techniques for graph data utility and privacy."
"1450310","Collaborative Research: SI2-SSI: Data-Intensive Analysis for High Energy Physics (DIANA/HEP)","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","05/01/2015","05/07/2018","Kyle Cranmer","NY","New York University","Continuing grant","Bogdan Mihaila","04/30/2019","$939,189.00","","kyle.cranmer@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","1253, 7244, 8004","7433, 8009, 8084","$0.00","Advanced software plays a fundamental role for large scientific projects. The primary goal of DIANA/HEP (Data Intensive ANAlysis for High Energy Physics) is developing state-of-the-art tools for experiments that acquire, reduce, and analyze petabytes of data. Improving performance, interoperability, and collaborative tools through modifications and additions to packages broadly used by the community will allow users to more fully exploit the data being acquired at CERN's Large Hadron Collider (LHC) and other facilities. These experiments are addressing questions at the heart of physics: What are the underlying constituents of matter? And how do they interact? With the discovery of the Higgs boson in 2012, the Standard Model of particle physics is complete. It provides an excellent description of known particles and forces.  However, the most interesting questions remain open: What is the dark matter which pervades the universe? Does space-time have additional symmetries or extend beyond the 3 spatial dimensions we know? What is the mechanism stabilizing the Higgs boson mass from enormous quantum corrections? The next generation of experiments will collect exabyte-scale data samples to provide answers. Analyzing this data will require new and better tools. <br/><br/>First, the project will provide the CPU and IO performance needed to reduce the iteration time so crucial to explore new ideas. It will develop software to effectively exploit emerging many- and multi-core hardware. It will establish infrastructure for a higher-level of collaborative analysis, building on the successful patterns used for the Higgs boson discovery and enabling a deeper communication between the theoretical community and the experimental community. DIANA?s products will sit in the ROOT framework, already used by the HEP community of more than 10000 particle and nuclear physicists. By improving interoperability with the larger scientific software ecosystem, DIANA will incorporate best practices and algorithms from other disciplines into HEP. Similarly, the project will make its computing insights, tools, and novel ideas related to collaborative analysis, standards for data preservation, and best practices for treating software as a research product available to the larger scientific community.  Finally, to improve the quality of the next generation of software engineers in HEP, DIANA will host an annual workshop on analysis tools and establish a fellowship program."
"1835669","Framework: Software: NSCI: Collaborative Research: Hermes: Extending the HDF Library to Support Intelligent I/O Buffering for Deep Memory and Storage Hierarchy Systems","OAC","Software Institutes","11/01/2018","09/12/2018","Jian Peng","IL","University of Illinois at Urbana-Champaign","Standard Grant","Vipin Chaudhary","10/31/2022","$150,000.00","","jianpeng@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Modern high performance computing (HPC) applications generate massive amounts of data. However, the performance improvement of disk based storage systems has been much slower than that of memory, creating a significant Input/Output (I/O) performance gap. To reduce the performance gap, storage subsystems are under extensive changes, adopting new technologies and adding more layers into the memory/storage hierarchy. With a deeper memory hierarchy, the data movement complexity of memory systems is increased significantly, making it harder to utilize the potential of the deep memory and storage hierarchy (DMSH) design. As we move towards the exascale era, I/O bottleneck is a must to solve performance bottleneck facing the HPC community. DMSHs with multiple levels of memory/storage layers offer a feasible solution but are very complex to use  effectively. Ideally, the presence of multiple layers of storage should be transparent to applications without having to sacrifice I/O performance. There is a need to enhance and extend current software systems to support data access and movement transparently and effectively under DMSHs. Hierarchical Data Format (HDF) technologies are a set of current I/O solutions addressing the problems in organizing, accessing, analyzing, and preserving data. HDF5 library is widely popular within the scientific community. Among the high level I/O libraries used in DOE labs, HDF5 is the undeniable leader with 99% of the share. HDF5 addresses the I/O bottleneck by hiding the complexity of performing coordinated I/O to single, shared files, and by encapsulating general purpose optimizations. While HDF technologies, like other existing I/O middleware, are not designed to support DMSHs, its wide popularity and its middleware nature make HDF5 an ideal candidate to enable, manage, and supervise I/O buffering under DMSHs. This project proposes the development of Hermes, a heterogeneous aware, multi tiered, dynamic, and distributed I/O buffering system that <br/>will significantly accelerate I/O performance. <br/><br/>This project  proposes to extend HDF technologies with the Hermes design. Hermes is new, and the enhancement of HDF5 is new. The deliveries of this research include an enhanced HDF5 library, a set of extended HDF technologies, and a group of general I/O buffering and memory system optimization mechanisms and methods. We believe that the combination of DMSH I/O buffering and HDF technologies is a reachable practical solution that can efficiently support scientific discovery. Hermes will advance HDF5 core technology by developing new buffering algorithms and mechanisms to support 1) vertical and horizontal buffering in DMSHs: here vertical means access data to/from different levels locally and horizontal means spread/gather data across remote compute nodes; 2) selective buffering via HDF5: here selective means some memory layer, e.g. NVMe, only for selected data; 3) dynamic buffering via online system profiling: the buffering schema can be changed dynamically based on messaging traffic; 4) adaptive buffering via Reinforcement Learning: by learning the application's access pattern, we can adaptprefetching algorithms and cache replacement policies at runtime. The development Hermes will be translated into high quality dependable software and will be released with the core HDF5 library.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1838572","Community Track DCL 18-060: Exploring Roles of Universities, Data Centers, Data Repositories and Publishers in Data Re-Use","OAC","NSF Public Access Initiative","10/01/2018","07/25/2018","Richard Hooper","MA","Tufts University","Standard Grant","Beth Plale","09/30/2019","$50,000.00","","richard.hooper@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","7414","7556","$0.00","Scientists are faced with an ever increasing array of options for data publication and a growing set of requirements to meet grant and journal directives. General purpose data repositories, data archives with journals or university libraries, domain-specific data repositories or broader project management and collaborative environments that provide some data publication services (e.g., issuing DOIs) are just a few of the options available. How should scientists choose among these options? How effective are these alternatives in meeting the multiple objectives for data publication, including discoverability, recording of sufficient metadata for reliable re-use, reproducibility of scientific results, convenience for data provider, and persistence. The investigator proposes to organize a workshop that brings together key stakeholders in data reuse.  The workshop targets research carried out in the geosciences and is an activity connected with the South Big Data Regional Innovation Hub.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1664137","SI2-SSI: FAMII: High Performance and Scalable Fabric Analysis, Monitoring and Introspection Infrastructure for HPC and Big Data","OAC","Software Institutes","07/01/2017","11/21/2017","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Vipin Chaudhary","06/30/2020","$800,000.00","Kevin Manalo, Karen Tomko, Xiaoyi Lu, Hari Subramoni","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8004","026Z, 7433, 7942, 8004, 8009","$0.00","As the computing, networking, heterogeneous hardware, and storage<br/>technologies continue to evolve in High-End Computing (HEC) platforms,<br/>it becomes increasingly essential and challenging to understand the<br/>interactions between time-critical High-Performance Computing (HPC)<br/>and Big Data applications, the software infrastructures upon which<br/>they rely for achieving high-performing portable solutions, the<br/>underlying communication fabric these high-performance middlewares<br/>depend on and the schedulers that manage HPC clusters.  Such<br/>understanding will enable all involved parties (application<br/>developers/users, system administrators, and middleware developers) to<br/>maximize the efficiency and performance of the individual components<br/>that comprise a modern HPC system and solve different grand challenge<br/>problems. There is a clear need and unfortunate lack of a high-performance and<br/>scalable tool that is capable of analyzing and correlating the<br/>communication on the fabric with the behavior of HPC/Big Data<br/>applications, underlying middleware and the job scheduler on existing<br/>large HPC systems.  The proposed synergistic and collaborative effort,<br/>undertaken by a team of computer and computational scientists from OSU<br/>and OSC, aims to create an integrated software infrastructure <br/>for high-performance and scalable Fabric Analysis, Monitoring and<br/>Introspection for HPC and Big Data. This tool will achieve the<br/>following objectives: 1) be portable, easy to use and easy to<br/>understand, 2) have high performance and scalable rendering and<br/>storage techniques and, 3) be applicable to the different<br/>communication fabrics and programming models that are likely to be<br/>used on existing large HPC systems and emerging exascale systems.  The<br/>transformative impact of the proposed research and development effort<br/>is to design a comprehensive analysis and performance monitoring tool<br/>for applications of current and next generation multi<br/>petascale/exascale systems to harness the maximum performance and<br/>scalability.<br/><br/>The proposed research and the associated infrastructure will have a<br/>significant impact on enabling optimizations of HPC and Big Data<br/>applications that have previously been difficult to provide. These<br/>potential outcomes will be demonstrated by using the proposed<br/>framework to validate a variety of HPC and Big Data benchmarks and<br/>applications under multiple scenarios.  The integrated middleware and<br/>tools will be made publicly available to the community through public<br/>repositories and publications in the top forums, enabling other MPI<br/>and Big Data stacks to adopt the designs.  Research results will also<br/>be disseminated to the collaborating organizations of the<br/>investigators to impact their HPC software products and<br/>applications. The proposed research directions and their solutions<br/>will be used in the curriculum of the PIs to train undergraduate and<br/>graduate students, including under-represented minorities and female<br/>students. The technical challenges addressed by the proposal include: 1)<br/>Scalable visualization of large and complex HEC networks so as to<br/>provide a near instant rendering to end users, 2) A generalized data<br/>gathering scheme which is easily portable to multiple communication<br/>fabrics, novel compute architectures and high-performance middleware,<br/>3) Enhanced data storage performance through optimized database<br/>schemas and the use of memory-backed key value stores/databases, 4)<br/>Support in MPI, PGAS, and Big Data libraries to enable the proposed<br/>monitoring, analysis, and introspection framework, and 5) Enabling<br/>deeper introspection of particular regions of application.  The<br/>research will also be driven by a set of HPC and Big Data<br/>applications. The transformative impact of the proposed research and<br/>development effort is to design a comprehensive analysis and<br/>performance monitoring tool for applications of current and next<br/>generation multi petascale/exascale systems to harness the maximum<br/>performance and scalability."
"1516695","Big Data on Small Organisms: Petascale Simulations of Data-Driven, Whole-Cell Microbial Models","OAC","PETASCALE - TRACK 1","08/01/2015","06/23/2015","Ilias Tagkopoulos","CA","University of California-Davis","Standard Grant","Edward Walker","07/31/2019","$40,000.00","","iliast@cs.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7781","","$0.00","This project aims to develop the next-generation of genome-scale, data-driven models for microbial organisms. The project will first focus on the most-studied microbe, the gram-negative bacterium Escherichia coli, due to the availability of high-throughput data, cellular organization, its significance to industry and human health. The project will take advantage of the multi-omics datasets that resulted from advances in parallel high-throughput molecular profiling over the past fifteen years, the emergence of data-driven, integrative, multi-scale models with substantial improvement of their predictive power and new techniques in machine learning, especially those related to deep learning. Accurate prediction of microbial fitness and cellular state can have profound implications to the way we test hypotheses that are directly related to health, social or economic benefits. This award will support the training of multiple undergraduate and graduate students in computational modeling and high-performance simulations of biological systems through undergraduate courses, IGEM teams and other initiatives.<br/><br/>This project will support the generation of knowledge from the largest normalized omics compendia for the most widely used microbe that will be a boon for the development and training of the next generation of data-driven predictive methods in molecular and cellular biology. It will provide the computational resources to evaluate a state-of-the-art multi-scale model with the capacity to predict phenotypic characteristics and environmental conditions from collective omics data. This will be the first systems-level simulator that targets a specific microbe (E. coli) and will be able to simulate populations of cells with a resolution ranging from individual gene concentrations to population dynamics. To achieve that, process migration, load-balancing and strong scaling techniques have to be adopted and applied in this context, which are all novel features for the area of whole-cell modeling. The proposed HPC simulations will be intimately related to hypothesis generation and testing. The simulations will address questions related to what their phenotype and expression profiles of microbial cultures are in complex environments. In the context of systems biology, integration of these techniques has the potential of being transformative, but only if the necessary computational infrastructure able to handle these tasks is available. The Blue Waters Supercomputer with its unique architecture, large-scale simulation capabilities and professional support staff provides the ideal platform to achieve this ambitious goal."
"1429160","MRI: Acquisition of Peta-Scale Data Storage System for Big Data Exploration in STEM Fields","OAC","MAJOR RESEARCH INSTRUMENTATION, EPSCoR Co-Funding","09/01/2014","10/16/2018","Mengjun Xie","AR","University of Arkansas Little Rock","Standard Grant","Edward Walker","08/31/2019","$291,908.00","Nitin Agarwal, Jerome Darsey, John Talburt, Jiang Bian, Marc Seigar","mengjun-xie@utc.edu","2801 South University","Little Rock","AR","722041000","5015698474","CSE","1189, 9150","1189, 9150","$0.00","This award is co-funded by EPSCoR program.<br/>This MRI project is to acquire a petascale data storage system to add to the existing high-performance computing (HPC) systems at the University of Arkansas at Little Rock (UALR). The projects concern analysis of medical images, cosmological simulations, social network analyses and protein structure prediction. A number of well-developed broadening participation activities in education and research are  proposed.<br/>The project augments existing high performance computing capabilities, includes projects in different disciplines to be facilitated by cyber upgrades, provides integration of research and education through training opportunities for students at all levels, and has a potential for broadening participation and development of a diverse scientific workforce."
"1835893","Collaborative Research: Framework: Software: HDR: Reproducible Visual Analysis of Multivariate Networks with MultiNet","OAC","DATANET, Software Institutes","01/01/2019","09/06/2018","Luke Harmon","ID","University of Idaho","Standard Grant","Stefan Robila","12/31/2022","$122,506.00","","lukeh@uidaho.edu","Office of Sponsored Programs","MOSCOW","ID","838443020","2088856651","CSE","7726, 8004","026Z, 062Z, 077Z, 7925, 8004","$0.00","Multivariate networks -- datasets that link together entities that are associated with multiple different variables -- are a critical data representation for a range of high-impact problems, from understanding how our bodies work to uncovering how social media influences society. These data representations are a rich and complex reflection of the multifaceted relationships that exist in the world. Reasoning about a problem using a multivariate network allows an analyst to ask questions beyond those about explicit connectivity alone: Do groups of social-media influencers have similar backgrounds or experiences? Do species that co-evolve live in similar climates? What patterns of cell-types support different types of brain functions? Questions like these require understanding patterns and trends about entities with respect to both their attributes and their connectivity, leading to inferences about relationships beyond the initial network structure. As data continues to become an increasingly important driver of scientific discovery, datasets of networks have also become increasingly complex. These networks capture information about relationships between entities as well as attributes of the entities and the connections. Tools used in practice today provide very limited support for reasoning about networks and are also limited in the how users can interact with them. This lack of support leaves analysts and scientists to piece together workflows using separate tools, and significant amounts of programming, especially in the data preparation step. This project aims fill this critical gap in the existing cyber-infrastructure ecosystem for reasoning about multivariate networks by developing MultiNet, a robust, flexible, secure, and sustainable open-source visual analysis system.  <br/><br/><br/>MultiNet aims to change the landscape of visual analysis capabilities for reasoning about and analyzing multivariate networks. The web-based tool, along with an underlying plug-in-based framework, will support three core capabilities: (1) interactive, task-driven visualization of both the connectivity and attributes of networks, (2) reshaping the underlying network structure to bring the network into a shape that is well suited to address analysis questions, and (3) leveraging provenance data to support reproducibility, communication, and integration in computational workflows. These capabilities will allow scientists to ask new classes of questions about network datasets, and lead to insights about a wide range of pressing topics. To meet this goal, we will ground the design of MultiNet in four deeply collaborative case studies with domain scientists in biology, neuroscience, sociology, and geology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811123","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","03/26/2019","Nancy Ide","NY","Vassar College","Standard Grant","Stefan Robila","12/31/2019","$177,639.00","Anton Nekrutenko","ide@vassar.edu","124 Raymond Avenue","Poughkeepsie","NY","126040657","8454377092","CSE","8004","026Z, 7916, 8004, 9251","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811402","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","05/08/2018","James Pustejovsky","MA","Brandeis University","Standard Grant","Stefan Robila","05/31/2019","$99,344.00","","pustejovsky@gmail.com","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","8004","026Z, 7916, 8004","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840197","CICI: SSC: Horizon: Secure Large-Scale Scientific Cloud Computing","OAC","Cyber Secur - Cyberinfrastruc","09/01/2018","08/17/2018","Anton Burtsev","CA","University of California-Irvine","Standard Grant","Micah Beck","08/31/2021","$999,925.00","Gene Tsudik","aburtsev@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","8027","","$0.00","Over the last decade, public and private clouds emerged as de facto platforms for computationally intensive scientific tasks. Today, huge volumes of many types of scientific data are routinely uploaded to the cloud. A large fraction of this data is privacy and/or security sensitive. Unfortunately, despite numerous advances in network and enterprise security, modern clouds remain inherently insecure. Recent experience shows that well-funded, targeted attacks manage to breach network perimeters of both public and private clouds. <br/><br/>Horizon is a novel cloud architecture aimed at providing data and computation security within a scientific cloud. Horizon builds upon three premises: (1) strong isolation on end-hosts, (2) fine-grained isolation in the cloud network, and (3) cloud-wide information flow control. To protect the end-hosts, Horizon develops a new layered hypervisor, and disaggregated virtualization stack with key features of: language safety, software fault isolation, and integrated software verification. To provide secure cloud network environment, Horizon relies on a new network architecture and implements a distributed network firewall, where all network communication and exchange of rights are mediated and controlled by the rules of the object capability system. To protect the cloud data, Horizon develops a set of abstractions and mechanisms to enforce cloud-wide information flow control. In Horizon all data is labeled. The hypervisor mediates all communication of each virtual machine and enforces propagation of labels and security checks for each cloud computation.<br/><br/>Horizon aims to provide a practical foundation for developing secure cloud infrastructure suitable for large-scale research workflows that require both speed and security. Horizon will be developed using entirely open-source components, and will be openly available to a broad community of scientists in academia and industry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1661663","SI2-SSI: Lidar Radar Open Software Environment (LROSE)","OAC","PHYSICAL & DYNAMIC METEOROLOGY, Software Institutes, EarthCube","08/15/2016","11/28/2016","Michael Bell","CO","Colorado State University","Standard Grant","Stefan Robila","07/31/2020","$2,499,996.00","","mmbell@hawaii.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","1525, 8004, 8074","4444, 7433, 8004, 8009, 9150","$0.00","Modern radars and lidars are a diverse class of instruments, capable of detecting molecules, aerosols, birds, bats and insects, winds, moisture, clouds, and precipitation. Scientists and engineers use them to perform research into air quality and pollution, dangerous biological plumes, cloud physics, cloud extent, climate models, numerical weather prediction, road weather, aviation safety, severe convective storms, tornadoes, hurricanes, floods, and movement patterns of birds, bats and insects. Radars and lidars are critical for protecting society from high impact weather and understanding the atmosphere and biosphere, but they are complex instruments that produce copious quantities of data that pose many challenges for researchers, students, and instrument developers. This project will develop a new set of tools called the Lidar Radar Open Software Environment (LROSE) to meet these challenges and help address the 'big data' problem faced by users in the research and education communities. This project will open new avenues of scientific investigation, including data assimilation to improve weather forecasts, and help to maximize returns on NSF investments in weather and climate research by providing better software tools to researchers, students, and educators. Improving the effectiveness of NSF research will provide significant scientific and societal benefits through an improved understanding of many diverse scientific topics that are relevant to public safety, national defense, and the global economy.<br/><br/>The LROSE project will develop a 'Virtual Toolbox' with a set of software tools needed for a diverse set of scientific applications. LROSE will be packaged so that it can be run on a virtual machine (VM), either locally or in the cloud, and stocked with core algorithm modules for those typical processing steps that are well understood and documented in the peer-reviewed literature. LROSE will enable the user community to use the core toolset to develop new research modules that address the specific needs of the latest scientific research. Through the VM Toolbox and a core software framework, other developers of open-source radar software can then provide their own compatible software tools to the set. By combining the open source approach with recent developments in virtual machines and cloud computing, we will develop a system that is both highly capable and easy to run on virtually any hardware, without the complexity of a compilation environment. The LROSE project will build on existing prototypes and available software elements, while facilitating community development of new techniques and algorithms to distribute a suite of documented software modules for performing radar and lidar analysis. These modules will each implement accredited scientific methods referencing published papers. The infrastructure and modules will allow researchers to run standard procedures, thereby improving the efficiency and reproducibility of the analyses, and encourage researchers to jointly develop new scientific approaches for data analysis. The use of collaborative open source methods will lead to a suite of available algorithmic modules that will allow scientists to explore radar and lidar data in new, innovative ways. Researchers will benefit from the improved toolset for advancing understanding of weather and climate, leading to a positive outcome in the advancement of scientific knowledge and societal benefits."
"1811101","EAGER:  Collaborative Research: Mining Scientific Literature with the LAPPS Grid","OAC","Software Institutes","06/01/2018","05/08/2018","Brent Cochran","MA","Tufts University","Standard Grant","Stefan Robila","05/31/2019","$29,816.00","","Brent.cochran@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","CSE","8004","026Z, 7916, 8004","$0.00","Scientists have become unable to keep up with the ever-expanding  number of scientific publications. The lack of this ability is a fundamental bottleneck to scientific progress. Current search technologies are limited because they are able to find many relevant documents, but cannot extract and organize the information content of these documents or suggest new scientific hypotheses based on the organized content. Natural Language Processing (NLP) based text mining strategies are a recognized means to approach this problem, but most scientists do not have the expertise or time to take use them. In addition, the lack of interoperability among NLP tools as well as the data in repositories scattered around the web are barriers to sharing workflows, resources, and results. This project will identify what analysis features are needed within an easy-to-use platform for mining scientific texts, implement an initial version of such a platform, and make it available to scientists.<br/><br/>There is currently no open, easy-to-use platform for mining scientific texts that provides interoperable access to a wide array of software, computing resources, and publication data. Publicly available software (such as Google) is not geared toward publication data, and in-house tools are fragile and deliver only a fraction of relevant results. The main objective of this project is, therefore, to (1) identify the requirements for an easy-to-use platform for mining information from scientific publications and (2) deploy facilities that meet these needs. To achieve this goal this project will extend the already existing NSF-funded LAPPS Grid to include means to access a broad range of interoperable NLP tools, large bodies of publication data and lexical and ontological resources, and, crucially, to rapidly adapt existing software to new domains and evaluate results. This project will also leverage enhancements to the NSF-funded Galaxy platform for interactive data exploration and extended access to NSF hardware resources (XSEDE machines including Stampede, Bridges, and Jetstream). By providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property concerns, this project provides capabilities that were previously unavailable to scientists. Researchers are able to perform large-scale text mining using an HPC infrastructure through a web-based interface without the need to know about underlying infrastructure. Additionally, providing iterative domain adaptation capabilities enables scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The ability to examine both explicit and implicit information scattered across massive repositories of publications will undoubtedly result in new observations and insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740211","SI2-SSE: Highly Efficient and Scalable Software for Coarse-Grained Molecular Dynamics","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2017","02/20/2018","Gregory Voth","IL","University of Chicago","Standard Grant","Vipin Chaudhary","08/31/2020","$500,000.00","Hakizumwami B. Runesha","gavoth@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9216","$0.00","Molecular simulation provides a powerful complement to conventional experimental techniques, offering both high-resolution information and unusual levels of control over the experimental conditions. Whileatomic-resolution molecular simulations are well established and widely used, it is also possible to remove extraneous detail from molecular representations to create highly efficient ""coarse-grained"" (CG) models.CG approaches can expand the potential applications of molecular simulations far beyond atomic-resolution models: the computational efficiency of CG models allows the scientist to investigate not only significantlylarger systems, but also phenomena that require significantly longer time scales. These CG approaches are of particular interest in the study of systems where key aspects of various processes emerge frominteractions between large numbers of molecules over relatively long distances. CG models can therefore provide crucial insight into the molecular basis of such systems, e.g., new materials. However, CG models can require significant scientific understanding to create and use effectively. To bring cutting-edge CG methodologies into a wider degree of use, this project willimplement key algorithmic advances and associated CG functionalities into the widely-used LAMMPS molecular dynamics simulation code. Furthermore, the project will implement a publically-accessible repository for CGmodel parameters and input files to accelerate the dissemination of exemplar CG models throughout the scientific community.<br/><br/><br/>The project will integrate key functionalities for very large-scale and dynamic CG models into the LAMMPS molecular dynamics package. These functionalities include not only sparse memory optimizations (e.g.,template molecular topology descriptions and spatial data structures for link cell algorithms) but also user-defined transition information for the propagation of ""ultra-coarse-grained"" (UCG) models; parameterizationof the latter can be achieved by using the integrated multi-scale coarse-grained force matching code (MSCGFM). Furthermore, direct incorporation of experimental data into CG models will be assisted byimplementations of the ""experiment directed metadynamics"" (EDM) and ""experiment directed simulation"" (EDS) algorithms. Taken together, these enhancements will provide cutting-edge CG model generation and simulation techniques to a wide user community. To complement the extended functionality of the LAMMPS code, a user-driven data and metadata repository for CG models will be provided to assist with efficient dissemination of model parameters andsimulation/validation data to the scientific community."
"1450959","IRNC: AMI: The InSight Advanced Performance Measurement System","OAC","INTERNATIONAL RES NET CONNECT","08/01/2015","11/02/2017","Greg Cole","TN","University of Tennessee Knoxville","Continuing grant","Kevin Thompson","07/31/2019","$1,000,000.00","Buseung Cho, Carter Bullard, Joe Gipson","gcole@gloriad.org","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","7369","9150","$0.00","The GLORIAD/InSight program is a global, open-source software development effort to research and experimentally deploy advanced flow-level network measurement technologies at various levels of the research and education (R&E) network eco-system.  The tools developed will enable far-reaching research towards better understanding network utilization, identifying network application performance issues while carefully attending to differing community concerns and requirements regarding data privacy and security. Experimental deployments will showcase actionable analytics and visualizations for network operations, new methods and models of data sharing across the global R&E fabric, and thus a better understood, more performant fabric. <br/><br/>Through a global, community-focused, open-source development effort, the project extends the current beta version of InSight - the flow-level passive measurement, analysis and visualization system in use on the GLORIAD network. The InSight tools are based on passive network measurement and monitoring by combining the rich detail of comprehensive, non-sampled, bi-directional, multi-model, multi-layer Argus flow-data with modern big-data analytic and visualization tools. A flexible stream-based method of enriching network flow metadata enables broader, customer-defined analytics.  Working closely with interested large-network providers, the project works toward experimentally deploying InSight on links up to 100 Gbps."
"1541434","CC*DNI Integration: Network Cyberinfrastructure (CI) for Biomedical Informatics Innovation","OAC","CISE RESEARCH RESOURCES, Campus Cyberinfrastrc (CC-NIE)","09/01/2015","05/11/2016","Vinod Vokkarane","MA","University of Massachusetts Lowell","Standard Grant","Kevin L. Thompson","08/31/2019","$1,016,000.00","Yan Luo, Yu Cao","vinod_vokkarane@uml.edu","600 Suffolk Street","Lowell","MA","018543643","9789344170","CSE","2890, 8080","8002, 9251","$0.00","The application of large-scale cyberinfrastructure to applications such as bio-informatics is resulting in the need to transport and store large quantities of data. Due to the large scale and distributed nature of the  biomedical data and health information systems, developing a robust, distributed, and scalable network cyberinfrastructure is a major focus for biomedical informatics research innovation.   Among the issues associated with such an infrastructure is the transport of time or delay sensitive data, what is often described as Quality of Service (QoS).   Software Defined Network (SDN) technology has the potential for providing QoS but it has not been integrated and demonstrated in end-to-end systems.   In addition to health care applications, an integrated system providing QoS is applicable to real time instrument control, transport of real time video and other time-sensitive applications.<br/> <br/>This project is enhancing the current network infrastructure with an intelligent middleware service that operates  as a broker between the SDN controller and the end-user application. The flexible  co-scheduling middleware engine (referred to as Flexware) will provide the capability to dynamically configure a  network to best support the varied nature of biomedical inputs. Flexware combines several emerging intelligent network services, such as anycast, manycast, survivability, and parallel transfers for large-scale biomedical data storage and retrieval under a shared umbrella that provides an application with customized and optimized  network performance. This project will have significant impacts on improving the quality of healthcare applications, providing clinical and scientific researchers with flexible and efficient network resource  allocation for studying patient behavior, and training the next generation workforce in medical and engineering fields."
"1827138","CC* Networking Infrastructure: CyberInfrastructure Technology Advancement for Delaware (CITADel) - 100 Gb/s Connection Upgrade to Internet2","OAC","Campus Cyberinfrastrc (CC-NIE)","07/01/2018","06/18/2018","Jason Cash","DE","University of Delaware","Standard Grant","Kevin L. Thompson","08/31/2019","$447,089.00","Doke Scott, Sharon Pitt, William Totten, Fraser Gutteridge","cash@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","8080","9150","$0.00","The CC* Networking Infrastructure project at the University of Delaware (UD) is constructing a high-bandwidth, low-latency Internet2 connection to support the university's research and education initiatives by fulfilling the need for efficient and timely data transfer to collaborating institutions, national high-performance computing resources, and service for the global research community. This project enables the sharing of large and time-sensitive data sets without impediment. It provides the necessary infrastructure to support major initiatives for the Delaware Biotechnology Institute, department of Chemistry & Biochemistry, department of Mechanical Engineering, Center for Bioinformatics and Computational Biology, Bartol Research Institute, department of Chemical & Biomolecular Engineering, Entomology and Wildlife Ecology, and many more data intensive programs across the university. This is a transformational grant which enables DNA sequence, energy flux, molecule property, molecular dynamics trajectory, protein, solid particle, visualization, weather, and other data to be transferred between HPC clusters and scientists for immediate use with shorter delays between successive simulations and resulting in faster overall research schedule.<br/><br/>This project provides two enhancements to the University of Delaware network infrastructure: (1) creation of a 100Gbps capable network for connectivity to the Internet2 network community, and (2) creation of a ScienceDMZ to enable researchers to take full advantage of the new high-bandwidth capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1713749","More Power to the Many: Scalable Ensemble-based Simulations and Data Analysis","OAC","PETASCALE - TRACK 1","05/01/2017","04/28/2017","Shantenu Jha","NJ","Rutgers University New Brunswick","Standard Grant","Edward Walker","04/30/2019","$29,150.00","","shantenu.jha@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7781","","$0.00","Glutamate receptors, and understanding their binding characteristics, are of fundamental biomedical importance as they mediate neuronal signaling. This project proposes to characterize and understand glutamate binding to the N-methyl-D-aspartate receptor (NMDAr), a member of the glutamate receptor family of proteins, with potential profound consequences for neuroscience and pharmacology.   However, the characterization of the configurational landscape of NMDAr is a High Performance Computing (HPC) problem.  It requires simulations with timescales and system sizes well beyond any that have previously been undertaken.  The project will use the petascale computing capabilities of Blue Waters to study such a system, using new sampling methods and original computing and data processing techniques.<br/><br/>The project will use molecular dynamics (MD) simulations to study this macromolecular system.  However, it remains a challenge to obtain an adequate sampling of the configurational space of complex chemical systems to accurately describe the structural properties of important substates, their relative propensities, and accessible transitions between them.  The project proposes to use a novel software framework that on the right computational resource makes a step-change in our ability to sample the conformational space of macromolecules by MD.  The project will study a protein of great biomedical relevance that exemplifies these issues, namely the ligand binding domain (LBD) of the N-methyl-D-aspartate receptor (NMDAr).  The idea at the core of the software strategy is similar to many other multiscale methods -- such as umbrella sampling, metadynamics, adaptive biasing methods, or transition path sampling: instead of one or a few long MD trajectories being run, many (hundreds or thousands) of short trajectories may be simulated concurrently.  Information is extracted from these very large datasets using sophisticated data reduction and analysis methods, and the coarse-grained information -- which embodies the chemical insight necessary to understand the system, e.g. an approximate free energy -- is used to refine the way in which further trajectories are generated (i.e., how we sample). Results from the analysis of the space sampled are then used in an iterative process to further direct the search of the conformational space (i.e., where we sample). This Blue Waters allocation will allow the project to access a total of 2.7 milliseconds of simulation of the NMDAr LBD system. With the three orders of magnitude (at least) speed-up in sampling allowed by our methodology with respect to plain MD, the project will be able to map the configurational landscape of this protein relevant for conformational dynamics up to a timescale of seconds, that is, to completely characterize the role of the ligand binding domain in the biological function and mechanism of NMDAr."
"1811597","Data-driven, biologically constrained biophysical computational model of the hippocampal network at full scale","OAC","PETASCALE - TRACK 1","05/01/2018","03/26/2019","Ivan Soltesz","CA","Stanford University","Standard Grant","Edward Walker","04/30/2020","$3,340.00","","isoltesz@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7781","","$0.00","This project aims to fundamentally improve our understanding of the brain.  In particular, this project will construct a detailed, biologically realistic, computational model of the brain hippocampus.  The hippocampus is a major component of the brain that plays an important role in memory and other major human functions.  This detailed computational model of the hippocampus will allow researchers to understand the origins of specific behavioral level features of the brain, as well as treatment effectiveness for conditions such as epilepsy.  In addition, the project aims to investigate brain changes under different radiation exposure regimes.  Furthermore, the software infrastructure developed in the project, as well as simulation results, will be publicly released to the wider neuroscience community.<br/><br/>The overarching goal of the research projects is the construction of realistic and biophysically detailed computational models of the three major neuronal circuit layers in the hippocampus: the dentate gyrus (DG), CA3, and CA1. These three regions of the hippocampus are interconnected, and therefore the computational aspects of this research require building a detailed data-driven, full-scale computational model of the entire hippocampal formation and its inputs from the septum and the entorhinal cortex.  Information processing in the brain is organized and facilitated by the complex interactions of intrinsic biophysical properties of distinct neuronal types, neuronal morphology, and network connection topology. These properties give rise to specific types of network oscillations and other dynamic processes that govern neural information encoding and exchange. The hypotheses in this proposal are designed to create a detailed picture at unprecedented scale of how the intrinsic properties of hippocampal principal neurons and interneurons define the network activity under normal conditions, and how pathological changes in those properties under epileptic conditions disrupt hippocampal function.  The project has made public releases of the CA1 model code and simulation management tool (SimTracker), and have also published the CA1 simulation datasets that accompany the publication describing the main results of the CA1 work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1515572","PRAC - Ensembles of Molecular Dynamics Engines for Assessing Force Fields, Conformational Change, and Free Energies of Proteins and Nucleic Acids","OAC","PETASCALE - TRACK 1","08/01/2015","07/17/2015","Thomas Cheatham","UT","University of Utah","Standard Grant","Edward Walker","07/31/2019","$40,000.00","Carlos Simmerling, Adrian Roitberg, David Case","tec3@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7781","9150","$0.00","Using the Blue Waters petascale resource, our research centers on developing and applying accurate methods for the simulation of biomolecules using the AMBER software. Using ensembles of these simulations, we can efficiently model the conformational distributions of biomolecules to better understand their structure, dynamics and interactions, and ultimately this provides detailed information about their function. This information can be used to design drugs to modulate function and to help determine how to alter the biomolecular structure and dynamics to influence function. Over the past few years, using these technologies we have shown the ability to reproducibly converge the conformational distributions (i.e. the set of conformations sampled under a particular set of conditions, such as at a particular temperature, pH, or specific environment) of various RNA molecules. We have also improved the molecular ""force fields"" that allow proper modeling of the structure and dynamics. This generally allows us to better understand how biomolecular machines, including various protein and RNA molecules, function. For example, one of our goals is to understand how riboswitches recognize specific metabolites which in turn leads to conformational changes that alters regulation of those metabolites. Another goal is to better understand the structure and dynamics of biomolecules in the crystal and this will lead to a better understanding of the role of structure and disorder in protein function. The methods and ""force fields"" developed are in wide use by a larger community that is aiming to provide realistic insight into biomolecular structure, dynamics, and function. Although not all research groups have access to the Blue Waters petascale resource at this time, the technology we are developing and applying now will become routinely accessible to the larger community who can then easily apply these methods on future resources.  Since computer power and accessibility continues to grow at a rapid pace, the broader community will see the impact of these technologies within a few years.<br/><br/><br/>Advances in simulation methods and increases in computational power have coupled together over the past few decades to transform our understanding of biological macromolecules; we can watch small proteins fold to their correct structure, we can help design new therapeutics, and improved simulation methods can help refine low resolution or ambiguous experimental data.  The biomolecular simulation methods we apply add to experimental data by exposing information not readily measured about the motions of biomolecules across many size and time scales, ranging from the fastest bond vibrations to slower collective motions. This allows elucidation of the dynamic landscape of proteins and nucleic acids as a function of time, especially at smaller scales.  Access to petascale computational resources allows us for the first time to fully explore the structural, dynamic and energetic landscape of complex biomolecules.  In our current PRAC, we have been able to fully converge the conformational ensemble of DNA helices, RNA tetranucleotides and tetraloops, we assessed and improved the force fields, and we also developed novel multi-dimensional replica-exchange methods and analysis tools.  With further collaboration of an experienced team of AMBER developer?s, we aim to decipher the full landscape of protein and nucleic acid structure and function, with a heavy focus on RNA, DNA and proteins and their complexes. On Blue Waters we will continue to hierarchically and tightly couple ensembles of highly GPU optimized molecular dynamics engines to fully map out the conformational, energetic and chemical landscape of biomolecules.  This will be done not only to assess, validate and improve currently available biomolecular force fields, but also to provide novel insight into DNA structure in the crystal and in solution, RNA riboswitch dynamics and function, and also protein-nucleic acid interactions.  Additional aims focus on the development of new analysis methods and dissemination of the simulation data to the larger community for deeper and broader inspection."
"1547457","CICI: Secure Data Architecture: CapNet: Secure Scientific Workloads with Capability Enabled Networks","OAC","Cyber Secur - Cyberinfrastruc","10/01/2015","07/19/2016","Anton Burtsev","UT","University of Utah","Standard Grant","Micah Beck","03/31/2019","$499,999.00","Jacobus VAN DER MERWE, Anton Burtsev","aburtsev@uci.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8027","7434, 9150","$0.00","Modern scientific experiments have outgrown the capacity of a single lab.  <br/>They require the storage and processing power of a datacenter, involve <br/>cross-institutional access to sensitive data, and span multiple domains of <br/>administrative trust. In such a setting, security is fragile. In the face of <br/>steady growth of sophisticated cyber-attack tools, modern server and desktop <br/>machines are fundamentally insecure. Over a hundred critical <br/>vulnerabilities that allow unrestricted access to the entire system are <br/>discovered in the Linux kernel each year. Lacking flexibility to express <br/>fine-grained access control policies, modern networks often give vulnerable <br/>hosts excessive or even unrestricted connectivity to the rest of the network.  <br/>An exploit of any host enables attackers to explore, exploit and take control <br/>over an entire cyber facility. Without support from the network, scientific <br/>facilities will remain vulnerable. <br/><br/>CapNet is a network architecture that enables secure, least privilege <br/>collaboration in the cross-institutional environment of a modern research <br/>facility. Building on the principles of capability access control, this <br/>research develops key elements needed to secure a network of a modern <br/>scientific infrastructure: 1) ""off by default"" behavior, with connectivity <br/>granted on as-needed basis; 2) mechanisms for decentralized, <br/>application-driven dynamic management of connectivity; and 3) a formal <br/>foundation enabling secure collaboration of fine-grained, dynamic, <br/>multi-institutional principals. The basis for CapNet's design is strong <br/>isolation of network activities with the mechanisms of software defined <br/>networks (SDN) and mediation of all communication between network hosts by <br/>a capability access control model. CapNet represents the network as an access <br/>control graph. Nodes are network hosts, edges (or ""capabilities"") are <br/>pointers to other hosts allowing communication and further exchange of rights.  <br/>By controlling the initial distribution of capabilities and their flow, CapNet <br/>governs network interactions through fine-grained, application-driven policies <br/>that enable safe collaboration among multiple institutions and third-party <br/>services. Finally, while taking a holistic approach to network access control, <br/>CapNet remains practical: it retains compatibility with unmodified network <br/>network stacks, integrates with existing datacenter and cloud management <br/>stacks, enables incremental adoption, and is fast and scalable."
"1642380","Collaborative Research: SI2-SSE: High-Performance Workflow Primitives for Image Registration and Segmentation","OAC","Software Institutes","10/01/2016","09/08/2016","James Shackleford","PA","Drexel University","Standard Grant","Stefan Robila","09/30/2019","$390,000.00","Nagarajan Kandasamy","shack@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Image registration and segmentation are vital enabling technologies for addressing many complex, data driven problems. Examples include individualized medical treatment where disease progression is monitored by analyzing MRI, CT, or ultrasound images over time; identifying anatomical structures in medical images; recognizing objects and people in video footage; and extracting imageable biometrics such as fingerprints, faces, and the iris. Images and videos can now be easily acquired at a rate that far surpasses our capacity to perform advanced image analysis.  For this reason, advanced registration and segmentation algorithms are not routinely used for many large-scale and time sensitive applications because they require more processing time than is available. This project will remedy this situation by developing a high-performance software package for image registration and segmentation, suitable to be run on massively parallel processors, and building a strong user and developer base around it. All software developed through the project will be open source and licensed under the MIT License. Improvements in processing speed achieved by the proposed platform will have significant impact in disciplines such as computer vision, digital forensics, and biomedical image analysis. Finally, the project team is committed to the diversity mission of Drexel University and will reach out to under-represented groups when recruiting graduate students for this project. Selected research tasks will be integrated within existing courses and curriculum will be developed for new experiential programs stemming from this effort.<br/><br/>The overall goal of this project is to develop a high-performance, many-core CPU and GPU accelerated algorithmic software package for attacking classes of problems that depend on solutions to data-dense inverse problems such as registration, segmentation, tomography, and parameter estimation. The specific technical approach involves developing algorithmic primitives required by a broad class of inference and analysis based workflows. Probabilistic primitives for building generative, discriminative, and conditional random field classification models will be implemented with emphasis on object segmentation.  Specialized registration operators will be developed for spline and voxel-driven algorithms. These primitives will be developed within the single instruction multiple data paradigm which utilizes many-core processing architectures via OpenMP, CUDA, and OpenCL. The workflow will be supplemented by a graphical user interface (GUI), providing a feature rich studio of tools that expose high-performance primitives to scientists visually and intuitively. The platform architecture will be designed as a distributed system service targeting locally administered scientific computing clusters where the number of compute nodes will be able to scale with load requirements. The GUI and the computational core may either run in a distributed client-server configuration or together locally on a single high performance workstation. Emphasis will be placed on documentation and video/written tutorials necessary for adoption. The project team will use an open software development model to build a strong user base comprising both novice users as well as researchers with the need to implement new algorithms on top of a stable software infrastructure. It is expected that the availability of this tool and its source code will catalyze an increase in quantitative image analysis spanning across research disciplines."
"1339765","SI2-SSI: Collaborative Research: Building Sustainable Tools and Collaboration for Volcanic and Related Hazards","OAC","PETROLOGY AND GEOCHEMISTRY, DEEP EARTH PROCESSES SECTION, Software Institutes, Front in Earth Sys Dynamics, EarthCube","10/01/2013","07/02/2018","Matthew Jones","NY","SUNY at Buffalo","Continuing grant","Micah Beck","09/30/2019","$1,416,491.00","Marcus Bursik, Abani Patra, Matthew Jones, Matthew Jones, Greg Valentine, Tevfik Kosar","jonesm@ccr.buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1573, 7571, 8004, 8016, 8074","019Z, 7433, 8009, 9102, 9179","$0.00","This project is focused on creating and upgrading software infrastructure for a large community of scientists engaged in volcanology research and associated hazard analysis.  Specifically, the project will reengineer three widely used tools (TITAN2D - block and ash flows, TEPHRA and Puff - ash transport and dispersal) and develop support for workflows that use these tools to analyze risk from volcanic hazards. Reengineering will encompass modularization so researchers may easily experiment with different modeling approaches, incorporation of techniques to make the tools efficient on new computing architectures like GPUs and many-core chips. The workflows are intended to tackle the challenges of managing complex and often large data flows associated with these tools in validation processes and in probabilistic inference based on the outcomes of the modeling. The tools and workflows will be made available using the popular vhub.org platform. This project will help provide a standard well managed hardware/software platform and approaches to standardize the documentation associated with input data, source code, and output data. This will ensure that model calculations are reproducible.<br/><br/>The wider use of these high fidelity tools and their use in mitigating hazards is likely to have a significant effect on hazard analysis and management. The project will also engage in several major workshops and in training activities. Project personnel will also engage in the Earthcube initiative - popularizing computational methodologies, online access and dissemination mechanisms through the VHub platform."
"1541346","CC*DNI Engineer: Cyber Infrastructure Engineer to Improve Research Effectiveness Across the University of Maine System","OAC","Campus Cyberinfrastrc (CC-NIE)","06/01/2016","08/31/2016","Bruce Segee","ME","University of Maine","Continuing grant","Kevin Thompson","05/31/2019","$399,994.00","John Koskie, Jeffrey Letourneau","segee@maine.edu","5717 Corbett Hall","ORONO","ME","044695717","2075811484","CSE","8080","9150","$0.00","The Cyberinfrastructure (CI) Engineer at the University of Maine System improves the utilization of existing computational infrastructure provided by the Advanced Computing Group (ACG).  This position promotes the use of advanced computing across all campuses of the University of Maine System through live seminars at each campus, short courses live-streamed to other campuses, and virtual office hours.  The CI Engineer assists with data management throughout the data lifecycle, from proposal preparation through long term archiving.  This includes best practices for data security and integrity as well as the utilization of different storage platforms for different performance needs. In addition, the CI engineer assists individual researchers in optimizing codes supporting individual research and inter-institutional collaborations. <br/><br/>The position supports the single university initiative by helping faculty, staff and students use existing cyberinfrastructure to collaborate across campuses.  These activities provide the CI engineer with a better understanding of research needs, so he/she also provides a voice for users in the design of future ACG infrastructure.  He/she also improves the efficiency of the University of Maine's K-12 outreach by providing special technical support for the ACG Outreach Coordinator."
"1642345","Collaborative Research: SI2-SSE: High-Performance Workflow Primitives for Image Registration and Segmentation","OAC","Software Institutes","10/01/2016","09/08/2016","Gregory Sharp","MA","Massachusetts General Hospital","Standard Grant","Stefan Robila","09/30/2019","$110,000.00","","gcsharp@partners.org","Research Management","Somerville","MA","021451446","8572821670","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Image registration and segmentation are vital enabling technologies for addressing many complex, data driven problems. Examples include individualized medical treatment where disease progression is monitored by analyzing MRI, CT, or ultrasound images over time; identifying anatomical structures in medical images; recognizing objects and people in video footage; and extracting imageable biometrics such as fingerprints, faces, and the iris. Images and videos can now be easily acquired at a rate that far surpasses our capacity to perform advanced image analysis.  For this reason, advanced registration and segmentation algorithms are not routinely used for many large-scale and time sensitive applications because they require more processing time than is available. This project will remedy this situation by developing a high-performance software package for image registration and segmentation, suitable to be run on massively parallel processors, and building a strong user and developer base around it. All software developed through the project will be open source and licensed under the MIT License. Improvements in processing speed achieved by the proposed platform will have significant impact in disciplines such as computer vision, digital forensics, and biomedical image analysis. Finally, the project team is committed to the diversity mission of Drexel University and will reach out to under-represented groups when recruiting graduate students for this project. Selected research tasks will be integrated within existing courses and curriculum will be developed for new experiential programs stemming from this effort.<br/><br/>The overall goal of this project is to develop a high-performance, many-core CPU and GPU accelerated algorithmic software package for attacking classes of problems that depend on solutions to data-dense inverse problems such as registration, segmentation, tomography, and parameter estimation. The specific technical approach involves developing algorithmic primitives required by a broad class of inference and analysis based workflows. Probabilistic primitives for building generative, discriminative, and conditional random field classification models will be implemented with emphasis on object segmentation.  Specialized registration operators will be developed for spline and voxel-driven algorithms. These primitives will be developed within the single instruction multiple data paradigm which utilizes many-core processing architectures via OpenMP, CUDA, and OpenCL. The workflow will be supplemented by a graphical user interface (GUI), providing a feature rich studio of tools that expose high-performance primitives to scientists visually and intuitively. The platform architecture will be designed as a distributed system service targeting locally administered scientific computing clusters where the number of compute nodes will be able to scale with load requirements. The GUI and the computational core may either run in a distributed client-server configuration or together locally on a single high performance workstation. Emphasis will be placed on documentation and video/written tutorials necessary for adoption. The project team will use an open software development model to build a strong user base comprising both novice users as well as researchers with the need to implement new algorithms on top of a stable software infrastructure. It is expected that the availability of this tool and its source code will catalyze an increase in quantitative image analysis spanning across research disciplines."
"1547350","CICI: Secure Data Architecture: Improving the Security and Usability of Two-Factor Authentication for Cyberinfrastructure","OAC","Cyber Secur - Cyberinfrastruc","01/01/2016","09/04/2015","Nitesh Saxena","AL","University of Alabama at Birmingham","Standard Grant","Micah Beck","12/31/2019","$249,719.00","","saxena@uab.edu","AB 1170","Birmingham","AL","352940001","2059345266","CSE","8027","7434, 9150","$0.00","Password authentication is a critical vulnerability in cyberinfrastructure because typical passwords are memorable and easily guessed, leaving them vulnerable to malicious actors. One well-recognized method for strengthening the password security is Two-Factor Authentication (TFA), in which the password is complemented by an additional authentication factor such as a mobile phone or a dedicated token (e.g., a USB dongle). However, current TFA mechanisms do not offer sufficient security and usability. This project breaks new ground towards improving both of these aspects. It designs, implements and evaluates TFA schemes that not only protect against on-line guessing attacks, but also against off-line dictionary attacks in case of server or mobile device compromise. Moreover, the project aims to do so without degrading usability compared to password-only authentication. The creation of formal security models for TFA schemes allow for better understanding of TFA security in general. The resulting research prototypes will be of immense value in future research on building resilient and usable authentication services. The project integrates research into educational activities in the form of advanced curriculum development as well as high school and K-12 student mentoring in the area of Identity and Access Management.<br/><br/>The design of new TFA protocols offers security against on-line guessing and offline dictionary attacks. The project formally proves the security of these protocols in a strong security model for TFA protocols that is being introduced as an extension to well-established password-authenticated key exchange (PAKE) models. The goal is to design the TFA protocols in a modular way, allowing for the use of independent device and server components, and enabling the use of the developed schemes with existing password protocols and without the need to modify the server software. Moreover, the research involves developing and testing TFA systems which will instantiate the proposed protocols.  The goal is a TFA systems design that utilizes automated and user-transparent data channel between the mobile device and the client, falling back to localized wireless radio communication only when such a channel is unavailable. Such construction would provide high usability since the user experience of the login process would be almost equivalent to password-only authentication. Finally, the project involves conducting rigorous usability studies in the lab environment and field settings to evaluate the performance, usability, and adoption potential of the proposed approaches."
"1710371","CDS&E: SuperSTARLU - STacked, AcceleRated Algorithms for Sparse Linear Systems","OAC","CDS&E","08/01/2017","08/02/2017","Jeffrey Young","GA","Georgia Tech Research Corporation","Standard Grant","Vipin Chaudhary","07/31/2020","$500,000.00","Richard Vuduc, Edward Riedy","jyoung9@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8084","026Z, 8084","$0.00","Computing systems and associated software have long had to make trade-offs in terms of performance due to an imbalance between fast processors and their slower memory hierarchies. Newly released technologies for 3D stacked memories provide an opportunity to reduce this imbalance by providing higher memory bandwidths and novel ways for accessing memory. One of the most promising techniques for using 3D stacked memory involves ""memory-centric"" computation that moves computation as close as possible to main memory. However, there is little understanding of how to best use these new memory technologies in libraries and applications, even as this hardware is slated to be integrated into near-term exascale supercomputing systems. The goal of this project is to understand the techniques and approaches that are needed to fully utilize 3D stacked memories and to demonstrate a useful set of computational primitives that can serve as a template for accelerating large scientific codes with these new memory components.<br/><br/>This research considers the specific problem of using ""memory-centric"" processors effectively to implement sparse primitives as part of a library that supports a number of key scientific applications including radiation transport, fluid flow, and fusion simulations. This library, SuperLU_DIST, is a sparse direct solver library designed for distributed memory multicore systems that has previously been accelerated on both NVIDIA?s graphics co-processors and Intel?s Xeon Phi co-processor.  While this prior work has thus far yielded promising speedups, it has also revealed critical and fundamental algorithmic performance bottlenecks related to memory data transfers. This research project will investigate whether these bottlenecks may be mitigated by using emerging memory-centric co-processors. Such co-processors, which include Micron?s Hybrid Memory Cube (HMC) and High-Bandwidth Memory (HBM), combine 3-D stacked memories and FPGAs to provide lower latency, higher bandwidth data transfer, and support for near-memory data processing. The project will use high-level languages like OpenCL to take advantage of such technologies and will utilize a mix of algorithmic advances and software library development to improve application performance. Additionally, this work will lead to a new, open-source release of SuperLU called Super Stacked, Accelerated LU (SuperSTARLU), which will be made available to application developers and will be demonstrated on one of the next-generation systems with memory-centric co-processors, such as NERSC?s Cori."
"1810584","Probing the Fossils of the Local Group using Petascale Adaptive Mesh Galaxy Simulations","OAC","PETASCALE - TRACK 1","08/01/2018","03/22/2018","Brian O'Shea","MI","Michigan State University","Standard Grant","Edward Walker","07/31/2019","$13,950.00","Britton Smith, John Wise","oshea@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7781","","$0.00","The goal of this project is to unlock fundamental breakthroughs in galaxy formation with large-scale numerical simulations on the Blue Waters supercomputer. These simulations will resolve the smallest building blocks of galaxies and their star formation histories, allowing for direct measurements of the consequences of the numerical calculations. The complexity of the included physics will lead to the creation of rich data sets that address many key issues relating to star formation histories, chemical evolution, early galaxy assembly, and the observable properties of galaxies. Furthermore, the simulation data produced during the course of this project, as well as a wide range of other data products, will be made publicly available for use by the broader astrophysical community.<br/><br/>The project will answer several pressing, observational motivated questions about low-mass and metal-poor galaxies by using the Blue Waters supercomputer to perform a suite of sophisticated, high dynamic range adaptive mesh simulations of cosmological structure formation. These simulations will probe the early history of the Local Group of galaxies in great detail, and will directly connect the largest of these early galaxies to their relics, which can be found among the present-day Local Group dwarf galaxies.  The project will address four specific questions about galaxy formation: 1. What are the key physical mechanisms that control galaxy formation at the earliest epochs, and how do they differ from the ones in larger galaxies forming at later times? 2. If the seeds of supermassive black holes form from the first generation of stars, how do they grow over the first billion years of cosmic evolution? 3. What are the physical characteristics of the remnants of early galaxy formation in the Local Group, and what information about their formation is retained by the stellar populations of those galaxies? 4. What are the unique observational signatures of the earliest galaxies, both at high redshift and the present day? The team contains experts in galaxy formation and high performance computing, as well as the use of a sophisticated numerical tool, specifically the Enzo code.  The Enzo code that has been demonstrated to scale and perform well on the Blue Waters supercomputer. The project will apply the simulations to the interpretation of measurements of both local and distant galaxies from current astronomical surveys, and to motivate future observational campaigns.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1614804","Ion channels response in physiological conditions: toward a computational framework for nociception","OAC","PETASCALE - TRACK 1","05/01/2016","04/29/2016","Vincenzo Carnevale","PA","Temple University","Standard Grant","Edward Walker","04/30/2019","$8,000.00","Michael Klein, Giacomo Fiorin, Christopher MacDermaid","vincenzo.carnevale@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7781","","$0.00","The extraordinary successes achieved in the last years by bioinformatics, structural biology,<br/>and functional studies of proteins have produced an unprecedented amount of data that holds<br/>promise to shed light on the workings of these molecular machines. Paradoxically these successes<br/>highlighted the fact that sequence, structure and function are linked together in a more subtle<br/>and complex way than previously thought. Specifically, protein families with recognizable<br/>sequence homology and strictly conserved architecture can show a huge functional heterogeneity,<br/>as in the case of voltage-gated-like ion channels (VGLCs).  The project's goal is to define a coherent theoretical framework to analyse both<br/>ensembles of structures and sequences and apply it to the study of ion channel communications. <br/><br/>The project proposes to use the Blue Waters system to address this problem using a combination of  accurate free-energy<br/>MD calculations and statistical inference analysis of both trajectories and large multiple<br/>sequence alignment. The microscopic determinants of mechanical coupling will be identified<br/>with the set of residue-residue interactions that are conserved along evolution and are responsible<br/>for a specific, functionally oriented structural dynamics. The study will focus on two specific<br/>ion channels families gated by different stimuli: transient receptor potential channels and<br/>voltage gated cation channels.<br/><br/>The project, if successful, will bridge the gap in our understanding of molecular processes and neuronal responses, with the potential of significantly advancing large-scale initiatives like the BRAIN in the US and the HUMAN BRAIN in EU.  Additionally, the project provides opportunity to involve graduate and undergraduate students in a research<br/>activity lying at the interface between the fields of data analytics and bio-molecular physical<br/>chemistry. Given the increased relevance of Big Data approaches to drug discovery and the<br/>emergence of precision medicine, the skillset and expertise acquired are expected to match<br/>the future needs of industry and academia. Furthermore, student recruitment in the project will take advantage of two<br/>initiatives. The first is a Professional Masters in Bioinformatics, where the PI is involved in<br/>all organizational aspects, from the steering committee to the design of Structural Bioinformatics<br/>classes. The second is the MARC U-STAR program designed to mentor underrepresented groups<br/>in research, providing financial assistance to prepare students for competitive graduate programs."
"1811600","Molecular and Coarse-Grained Simulations of Biomolecular Processes at the Petascale","OAC","PETASCALE - TRACK 1","04/01/2018","03/22/2018","Gregory Voth","IL","University of Chicago","Standard Grant","Edward Walker","03/31/2019","$8,100.00","","gavoth@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7781","","$0.00","Computer simulations of biological systems can offer insights that are difficult or impossible to access with conventional experimental techniques, providing significant benefits for basic scientific research.  However, the large range of characteristic time and length scales observed in biological processes makes the use of any single computational technique difficult.  While atomic-resolution simulations can furnish a scientist with exquisite levels of detail, the sheer computational expense of these simulations sometimes presents a significant barrier for their application to large-scale biological problems.  This project proposes to us coarse-grained (CG) molecular models to expand the reach of computer simulations to cellular scales. The project propose to integrate atomic-resolution and CG simulations to study a range of biologically relevant systems, in close collaboration with an international cohort of scientists from various experimental fields. This work will involve not only elucidating and explaining biomolecular processes, but also the development and dissemination of cutting-edge simulation software to the wider scientific community.<br/><br/>The project aims to combine experimental data with cutting-edge computer simulations to investigate a number of important biomolecular systems.   The systems of interest can be grouped into two main categories: critical stages of the viral lifecycles of HIV-1 and influenza, and studies of the actin filaments and microtubules of the cellular cytoskeleton.   The project will develop an integrated pipeline which allows scientists to convert experimental data into computer models capable of investigating biomolecular processes at scales that are inaccessible to other approaches. While the results of atomic-scale simulations will clearly be important in and of themselves, they will also, in combination with experimental data, form the basis for generating and parameterizing rigorous UCG models. Results and predictions made by these models will be validated by close collaboration with experimental scientists, and used to suggest new directions in both the theoretical and experimental field.  In addition to the development and deployment of advanced biomolecular simulation techniques, this proposal will also assist in the dissemination of the advances to the wider research community by the integration of the UCG model generation and simulation algorithms with the popular LAMMPS software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1757924","Education Innovation Initiative - Summer Undergraduate Research Institute EI2 - SURI","OAC","RSCH EXPER FOR UNDERGRAD SITES","03/01/2018","12/15/2017","Guy-Alain Amoussou","MD","Bowie State University","Standard Grant","Sushil Prasad","02/28/2021","$323,997.00","Azene Zenebe","aamoussou@bowiestate.edu","14000 Jericho Park Road","BOWIE","MD","207159465","3018604399","CSE","1139","9102, 9250","$0.00","The Education Innovation Initiative (EI2) - Summer Undergraduate Research Institute (SURI) in Cybersecurity (EI2-SURI) is designed to implement, in part, Bowie State University's first signature program - Education Innovation Initiative - to develop students' identities as scientists/researchers with the integration of carefully selected support activities that enable students to feel and become part of an active research community. EI2-SURI is significant because it supports two critical needs in cybersecurity and workforce development in relation to securing the national defense.As the various types of cyber threats continue to increase, so has the demand for trained cybersecurity professionals. This has resulted in a serious shortage of qualified professionals in the private and public sectors. EI2-SURI proposes to build the capacity for ten traditionally underrepresented minorities annually. The fellows are recruited from the Mid-Atlantic region, the District of Columbia, Maryland and Virginia (DMV) and Prince George's County where Bowie State University resides. Fellows are provided with real-world research explorations and experiences in cybersecurity. The results of planned projects will be enhanced student competency in research-related activities, advanced academic skills and increased diversity in STEM fields. The success of the proposed efforts is sustained by the integration of evidenced-based strategies related to establishing communities of learning. These communities include faculty mentors, graduate and undergraduate fellows and cybersecurity alumni, who are actively engaged in research.<br/><br/>EI2-SURI is actively engaging a diverse community in the investigation of cybersecurity issues related to vulnerability tracking in industrial control systems, analysis of software vulnerability; secure software design and testing, malware detection, and network security. Specifically, the following interdisciplinary cyber-related investigations optimize the best solutions to prevent and detect vulnerabilities: (1) Vulnerability Tracking in industrial control systems for Supervisory, Control, and Data Acquisition Systems designed to mitigate the risk to critical control systems, by demonstrating the feasibility of monitoring the execution of embedded Linux Kernel modules; (2) Analysis of Software Vulnerability to use visual analytics to deliver actionable security intelligence to defend software systems; (3) Security of Blockchain to compare various hardware, encryption algorithms, and implementation approaches; and (4) Network Systems' Data breaches to assess current vulnerability management protocols. The method used will include approaches such as data analytics and event log mining. Additionally, project-based learning will be used to actively engage the research community. To support the research and successfully build the community, the program planning and organization is structured around key activities including the program start-up package and orientation; research project development and implementation, weekly start up, weekly research forum; various seminars and workshops such as attending graduate school and monetizing intellectual property, industry and agency visits; andthe program culminating research symposium. One expected outcome is that fellows publish their research findings and explore ways to patent their work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640771","A PENULTIMATE PETASCALE COMPUTATIONAL LABORATORY FOR TURBULENCE AND MAGNETOHYDRODYNAMIC TURBULENCE","OAC","PETASCALE - TRACK 1","09/01/2016","08/22/2016","Pui-Kuen Yeung","GA","Georgia Tech Research Corporation","Standard Grant","Edward Walker","08/31/2019","$16,702.00","Katepalli Sreenivasan","pk.yeung@ae.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7781","","$0.00","The objective of this project is to use the Blue Waters supercomputer to enable lasting contributions to the study of scale similarity in turbulent flows at high Reynolds number, and of the behavior of turbulent flows subjected to an external magnetic field and solid-body rotation. Advanced post and on-the-fly processing techniques are to be applied to extract unique physical insights from the largest turbulence simulation to date (over half a trillion grid pints) aimed at fine-scale structure, made possible through a prior<br/>PRAC allocation. A careful choice of parameters and initial conditions will allow the first numerical simulation of the response of turbulence with a classical inertial range to the forces of electromagnetic induction as well as uniform solid-body rotation.  The project is expected to be of great impact for many fields of research such as meteorology, combustion, wind power, where intermittent outbursts of strong turbulence must be considered. Additionally, studying the combined effects of magnetic fields and solid-body rotation on a turbulent flow will provide crucial steps for our understanding how the Earth's magnetic field arises. Furthermore, by providing new data generated from this project, the impact on fundamental turbulence research is likely to be transformative.<br/><br/>The search for conclusive results on the problem of refined scale similarity applicable to high Reynolds number turbulent flows in diverse geometries has long been a grand challenge in turbulence theory. The intellectual merit of the proposed research lies in part in the rigorous analysis of high-resolution simulation data that has finally shown promise of enabling a penultimate description in this pursuit. Time-resolved dynamical information on extreme events in connection to vortical structures will in particular be new especially at the high Reynolds number possible on Blue Waters. For magnetohydrodynamic (MHD) turbulence the new simulations are expected to meet the challenge of understanding the flow physics while meeting competing demands for higher Reynolds number, domain size especially along the direction of an imposed magnetic field, and different time scales associated with the strength of the latter. Consideration of the coupling MHD turbulence with solid-body rotation also raises many further complexities, for which the proposed simulations will provide a basic characterization previously not available in the literature. Finally, sustained access to Blue Waters will provide doctoral students in this project with opportunities towards becoming leaders in the next generation of computational scientists."
"1811176","Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with Multi-Scale Fluid-Kinetic Simulation Suite","OAC","PETASCALE - TRACK 1","04/01/2018","03/27/2018","Nikolai Pogorelov","AL","University of Alabama in Huntsville","Standard Grant","Edward Walker","03/31/2020","$10,501.00","Jacob Heerikhuisen","np0002@uah.edu","301 Sparkman Drive","Huntsville","AL","358051911","2568242657","CSE","7781","9150","$0.00","The objective of this proposal is to use the possibilities provided by the Blue Waters supercomputer to model fundamental and challenging space physics problems.  The heliosphere is the sphere like region of space dominated by the Sun, which extends far beyond the orbit of Pluto.  The Solar Wind (SW) consists of ionized atoms from the Sun.  The project will model the SW flows in the inner and outer heliosphere, and compare the results with observational data.  It is anticipated that the project will provide a leap forward in the simulation of complex charged and neutral gas systems. Furthermore, the proposed approach to computational resource management for complex codes utilizing multiple algorithm technologies is expected to be a major advance to current approaches. The development of resource management technologies will be essential for all future modeling efforts that incorporate a wide diversity of scales and physical processes.<br/><br/>The analysis of flows of partially ionized plasma that are characterized by multiple or highly localized scales and multiple processes, will have a transformative impact for heliophysics. The project will address a variety of physical phenomena occurring throughout the solar system, such as the charge exchange processes between neutral and charged particles, the birth of pick-up ions (PUIs), the origin of energetic neutral atoms (ENAs), turbulence, the interplay of the heliopause instability and magnetic reconnection at the SW and local interstellar medium (LISM) interface, plasma wave generation in the LISM, the effect of the heliosphere on the TeV cosmic ray anisotropy, and more.  The project will also fit simulation results with observational data to constrain the properties of the LISM and refine time-dependent SW models. The project will incorporate the direct measurements from the New Parker Solar Probe and Solar Orbiter missions, as well as the in situ measurements of the SW from the Sun to Earth and further to the heliospheric boundary, from the New Horizons, Voyager, IBEX, and air shower observations. This project will extract the fundamental physics of plasma-neutral flows accompanied by the interaction with energetic particles. The goal of the modeling activities is to understand and interpret observations in a way previously unthinkable because of the limitations in both physical models and computing power. Components of the physical model and corresponding code routines will be made available in a publicly accessible simulation suite.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1147944","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse","OAC","LINGUISTICS, METHOD, MEASURE & STATS, SPECIAL PROJECTS - CISE, COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, SOFTWARE & HARDWARE FOUNDATION, Software Institutes, ","08/01/2012","05/31/2018","Nancy Ide","NY","Vassar College","Standard Grant","Bogdan Mihaila","07/31/2019","$994,057.00","James Pustejovsky, Eric Nyberg, Christopher Cieri","ide@vassar.edu","124 Raymond Avenue","Poughkeepsie","NY","126040657","8454377092","CSE","1311, 1333, 1714, 7298, 7484, 7798, 8004, O422","1311, 1333, 5983, 7433, 7944, 8004, 8009, 9251","$0.00","The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. <br/><br/>The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. <br/><br/>By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages."
"1147912","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse","OAC","SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF, COLLABORATIVE RESEARCH, Computer Systems Research (CSR, IIS SPECIAL PROJECTS, ROBUST INTELLIGENCE, SOFTWARE & HARDWARE FOUNDATION, Software Institutes, ","08/01/2012","08/24/2018","James Pustejovsky","MA","Brandeis University","Standard Grant","Bogdan Mihaila","07/31/2019","$1,764,929.00","Eric Nyberg, Christopher Cieri, Marc Verhagen","pustejovsky@gmail.com","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","1714, 2878, 7298, 7354, 7484, 7495, 7798, 8004, O422","1714, 2878, 5983, 7433, 7484, 7944, 8004, 8009","$0.00","The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. <br/><br/>The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. <br/><br/>By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages."
"1149591","CAREER: A Parallel Computational Framework of Multiscale Geometric Modeling and Mesh Generation for Cardiac Biomechanics Application","OAC","CAREER: FACULTY EARLY CAR DEV, INFORMATION TECHNOLOGY RESEARC, Biomechanics & Mechanobiology","08/01/2012","08/24/2017","Yongjie Zhang","PA","Carnegie-Mellon University","Standard Grant","Sushil K Prasad","07/31/2019","$428,000.00","","jessicaz@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1045, 1640, 7479","019Z, 1045, 7433, 7479, 9102, 9179, 9251","$0.00","The overall goal of this CAREER project is to establish a novel parallel computational framework of multiscale geometric modeling and mesh generation that can be applied to cardiac biomechanics applications.  This capability will enable accurate, stable, efficient simulations of many biological processes such as calcium (Ca2+) mediated signaling, excitation-contraction coupling and energy metabolism in cardiac muscle cells, and lead to great advances in cardiac biomechanics.  In cardiac muscle cells, Ca2+ is best known for its role in contraction activation.  Alterations in Ca2+ distributions are now recognized to be the primary mechanisms of cardiac dysfunction in a diverse range of common pathologies including cardiac arrhythmias and hypertrophy.  To predict and analyze how Ca2+ dynamics and cardiac excitation-contraction coupling are regulated, modeling of realistic geometries of large, complicated t-tubule network and associated protein complexes is needed.  However, previous studies have been limited to simplified geometries.  In this project, the PI focuses on 1) multiscale geometric modeling for protein complexes starting from atomic resolution data in the Protein Data Bank; 2) parallel mesh generation with topology ambiguity resolved and curvature-driven quality improvement; and 3) model validation in adaptive finite element analysis of Ca2+ signaling in ventricular myocytes with complicated realistic geometry.  To handle such large, complicated systems, multicore parallel meshing toolkits will be developed and encapsulated with the simulation software. <br/><br/>The proposed research will attain the highest degree of accuracy, efficiency and robustness in model development and simulation.  It will significantly advance predictive capability in cardiac applications, and the understanding of anatomical and physiological properties at molecular and cellular scales.  This parallel computational infrastructure can also be used for other complicated systems, providing engineers and scientists with novel technologies to construct accurate computer models.  Furthermore, this interdisciplinary project will integrate research and education via novel educational tool and curriculum development as well as outreach activities.  Students will interact with collaborative institutions to gain firsthand experience of real issues.  Women, minority groups and high school students will be included in the proposed research and education activities through CMU's K-12 Programs.  Education activities will be assessed in conjunction with CMU's Eberly Center for Teaching Excellence."
"1550126","US-EA CENTRA: US - East Asia Collaborations to Enable Transnational Cyberinfrastructure Applications","OAC","S&CC: Smart & Connected Commun, COLLABORATIVE RESEARCH, INTERNATIONAL RES NET CONNECT","10/01/2015","12/14/2017","Jose Fortes","FL","University of Florida","Continuing grant","Kevin Thompson","09/30/2019","$519,575.00","","fortes@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","033Y, 7298, 7369","022Z, 5921, 5924, 5983, 7369","$0.00","This project supports collaborative cyberinfrastructure (CI) research activities designed to educate a new generation of researchers who are technically and culturally competent to engage with international scientific networks. The goal of this project is to advance the scientific understanding of distributed software-defined cyberinfrastructures and the ability to run scientific applications using data and tools in different countries. Specifically, the targeted applications are in environmental modeling, disaster management and smart cities, focusing on how these domains impact each other. The related middleware research seeks solutions for software-defined data sharing, middleware interoperability via software-definition, and coordinated software-definition of distributed IT systems.  This project pursues rigorous understanding and solution of the scientific problems via international collaborations which bring out real-world contexts of transnational applications and create transnational cyberinfrastructure instances where researchers get practical insight. <br/><br/>Junior U.S. Ph.D. candidate researchers are expected to have short stays at collaborating sites in different countries working on project-related topics and have direct access to leading-edge facilities, local instances of global problems and top scientists working on these problems.  At the same time they are immersed in international team activities. Annual workshops help conceptualize, frame, advance and report on collaborative research projects and contribute to establishing the coordination framework. Key institutional partners in East Asia include the National Center for High-Performance Computing of the National Applied Research Laboratories of Taiwan and the National Institute of Information and Communication Technologies of Japan. This project is to contribute towards a framework to coordinate CI-based and CI-enabled research with East Asian partners in order to achieve scientific progress and engage junior researchers in international activities."
"1450122","SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)","OAC","Software Institutes","09/01/2015","05/04/2016","Vincent Weaver","ME","University of Maine","Standard Grant","Stefan Robila","08/31/2019","$282,828.00","","vincent.weaver@maine.edu","5717 Corbett Hall","ORONO","ME","044695717","2075811484","CSE","8004","7433, 8004, 8009, 9150, 9251","$0.00","Modern High Performance Computing (HPC) systems continue to increase in size and complexity. Tools to measure application performance in these increasingly complex environments must also increase the richness of their measurements to provide insights into the increasingly intricate ways in which software and hardware interact. The PAPI performance-monitoring library has provided a clear, portable interface to the hardware performance counters available on all modern CPUs and some other components of interest (scattered across the chip and system). Widely deployed and widely used, PAPI has established itself as fundamental software infrastructure, not only for the scientific computing community, but for many industry users of HPC as well.  But the radical changes in processor and system design that have occurred over the past several years pose new challenges to PAPI and the HPC software infrastructure as a whole.  The PAPI-EX project integrates critical PAPI enhancements that flow from both governmental and industry research investments, focusing on processor and system design changes that are expected to be present in every extreme scale platform on the path to exascale computing.<br/><br/>The primary impact of PAPI-EX is a direct function of the importance of the PAPI library. PAPI has been in predominant use by tool developers, major national HPC centers, system vendors, and application developers for over 15 years. PAPI-EX builds on that foundation. As important research infrastructure, the PAPI-EX project allows PAPI to continue to play its essential role in the face of the revolutionary changes in the design and scale of new systems. In terms of enhancing discovery and education, the list of partners working with PAPI-EX includes NSF computing centers, major tool developers, major system vendors, and individual community leaders, and this diverse group will help facilitate training sessions, targeted workshops, and mini-symposia at national and international meetings. Finally, the active promotion of PAPI by many major system vendors means that PAPI, and therefore PAPI-EX, will continue to deliver major benefits for government and industry in many domains.<br/><br/>PAPI-EX addresses a hardware environment in which the cores of current and future multicore CPUs share various performance-critical resources (a.k.a., 'inter-core' resources), including power management, on-chip networks, the memory hierarchy, and memory controllers between cores. Failure to manage contention for these 'inter-core' resources has already become a major drag on overall application performance. Consequently, the lack of ability to reveal the actual behavior of these resources at a low level, has become very problematic for the users of the many performance tools (e.g., TAU, HPCToolkit, Open|SpeedShop, Vampir, Scalasca, CrayPat, Active Harmony, etc.). PAPI-EX enhances and extends PAPI to solve this critical problem and prepare it to play its well-established role in HPC performance optimization. Accordingly, PAPI-EX targets the following objectives: (1) Develop shared hardware counter support that includes system-wide and inter-core measurements; (2) Provide support for data-flow based runtime systems; (3) Create a sampling interface to record streams of performance data with relevant context; (4) Combine an easy-to-use tool for text-based application performance analysis with updates to PAPI?s high-level API to create a basic, ?out of the box? instrumentation API."
"1450429","SI2-SSI: Collaborative Proposal: Performance Application Programming Interface for Extreme-Scale Environments (PAPI-EX)","OAC","Software Institutes, CDS&E","09/01/2015","08/27/2015","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Stefan Robila","08/31/2019","$2,126,446.00","Anthony Danalis, Heike Jagode","dongarra@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","8004, 8084","7433, 8009, 8084, 9150","$0.00","Modern High Performance Computing (HPC) systems continue to increase in size and complexity. Tools to measure application performance in these increasingly complex environments must also increase the richness of their measurements to provide insights into the increasingly intricate ways in which software and hardware interact. The PAPI performance-monitoring library has provided a clear, portable interface to the hardware performance counters available on all modern CPUs and some other components of interest (scattered across the chip and system). Widely deployed and widely used, PAPI has established itself as fundamental software infrastructure, not only for the scientific computing community, but for many industry users of HPC as well.  But the radical changes in processor and system design that have occurred over the past several years pose new challenges to PAPI and the HPC software infrastructure as a whole.  The PAPI-EX project integrates critical PAPI enhancements that flow from both governmental and industry research investments, focusing on processor and system design changes that are expected to be present in every extreme scale platform on the path to exascale computing.<br/><br/>The primary impact of PAPI-EX is a direct function of the importance of the PAPI library. PAPI has been in predominant use by tool developers, major national HPC centers, system vendors, and application developers for over 15 years. PAPI-EX builds on that foundation. As important research infrastructure, the PAPI-EX project allows PAPI to continue to play its essential role in the face of the revolutionary changes in the design and scale of new systems. In terms of enhancing discovery and education, the list of partners working with PAPI-EX includes NSF computing centers, major tool developers, major system vendors, and individual community leaders, and this diverse group will help facilitate training sessions, targeted workshops, and mini-symposia at national and international meetings. Finally, the active promotion of PAPI by many major system vendors means that PAPI, and therefore PAPI-EX, will continue to deliver major benefits for government and industry in many domains.<br/><br/>PAPI-EX addresses a hardware environment in which the cores of current and future multicore CPUs share various performance-critical resources (a.k.a., 'inter-core' resources), including power management, on-chip networks, the memory hierarchy, and memory controllers between cores. Failure to manage contention for these 'inter-core' resources has already become a major drag on overall application performance. Consequently, the lack of ability to reveal the actual behavior of these resources at a low level, has become very problematic for the users of the many performance tools (e.g., TAU, HPCToolkit, Open|SpeedShop, Vampir, Scalasca, CrayPat, Active Harmony, etc.). PAPI-EX enhances and extends PAPI to solve this critical problem and prepare it to play its well-established role in HPC performance optimization. Accordingly, PAPI-EX targets the following objectives: (1) Develop shared hardware counter support that includes system-wide and inter-core measurements; (2) Provide support for data-flow based runtime systems; (3) Create a sampling interface to record streams of performance data with relevant context; (4) Combine an easy-to-use tool for text-based application performance analysis with updates to PAPI?s high-level API to create a basic, ?out of the box? instrumentation API."
"1849113","FCTaaS: Federated Cybersecurity Testbed as a Service","OAC","Cyber Secur - Cyberinfrastruc","01/01/2019","12/17/2018","Salim Hariri","AZ","University of Arizona","Standard Grant","Kevin Thompson","12/31/2020","$300,000.00","Nizar Al-Holou, Utayba Mohammad, Cihan Tunc","hariri@ece.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","8027","7916","$0.00","With the advent of smart infrastructures, smart buildings, smart grids, smart industry and manufacturing, and smart cities and governments have created more new vulnerabilities than would exist if they were isolated from one another. Sophisticated cyberattacks can exploit these vulnerabilities to disrupt or even completely disable the operations of these infrastructures and consequently can severely impact our national security and all aspects of our life and economy. To overcome the cybersecurity challenges introduced by smart infrastructures, researchers and educators need to better understand the interdependencies among these infrastructures, their implications on cybersecurity issues and how to develop effective defense and protective protection solutions.<br/><br/>The main goal of this project is to explore innovative algorithms to allow the investigators and students to access resources across multiple and heterogeneous testbeds. This approach has the potential to provide new capabilities to conduct  important research such as:<br/><br/>1. How to model, and predict operations and interactions among complex, large, heterogeneous, and dynamic federation of cybersecurity and cyberphysical testbeds; <br/>2. How to secure and protect smart infrastructure resources and services and their interactions under normal and abnormal situations that may be caused by nature, accident, or malicious actions; and <br/>3. How to develop an innovative teaching and training experiments to provide hands-on experiences on how to discover existing or newly created vulnerabilities within an infrastructure or caused by the interactions with other infrastructures, detect and protect their operations against malicious attacks. <br/><br/>Service Oriented Architecture (SOA) are adopted to develop the federated smart infrastructure testbed in order to enable researchers and educators to publish/discover  testbeds that are needed for their research and educational programs. Initially, the Ford  Breadboard Smart Car testbed available at the University of Detroit-Mercy, and  UA testbeds including IoT Testbed, Virtual Cybersecurity Testbed that is currently hosted on Amazon public cloud, and Wireless Security Testbed are used for federation, experimentation and evaluation.  Open communication standards and security tools that are developed at the NSF Center for Cloud and Autonomic Computing are used to maintain the security and privacy of the federated security testbed. These services allow heterogeneous testbeds to communicate their data syntactically and semantically, enabling accurate interpretation of the semantics of data received and the dependencies among these testbeds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1614664","Dissipation and Kinetic Physics of Astrophysical Turbulence","OAC","PETASCALE - TRACK 1","09/01/2016","08/19/2016","Vadim Roytershteyn","CO","SPACE SCIENCE INSTITUTE","Standard Grant","Edward Walker","08/31/2019","$12,008.00","Kai Germaschewski, Yuri Omelchenko","vroytershteyn@spacescience.org","4750 Walnut Street","Boulder","CO","803012532","7209745888","CSE","7781","","$0.00","Turbulence is one of the fundamental physical phenomena associated with fluids, gasses, and plasmas.  In astrophysical phenomena, turbulence provides a mechanism of efficient energy, momentum, and particle transport thus playing a key role in the dynamics of astrophysical objects. The solar wind has served as a unique laboratory for studies of turbulence due to the availability of measurements from spacecraft.   Accordingly, considerable effort has been made to study solar wind turbulence using a combination of observational, theoretical, and computational tools. While understanding of the basic physics of this phenomenon has advanced greatly, a significant number of key questions remain unanswered. This project proposes to use the Blue Waters leadership class computing system to execute ground-breaking large scale simulations to investigate how energy cascades and is ultimately dissipated in the solar wind. This study will significantly advance the state of knowledge of turbulence and will represent a very important milestone in the efforts to perform realistic modeling of solar wind turbulence. Additionally, the project results will be of great interest to a wide community of researchers working on solar wind plasma, coronal plasma, and plasma astrophysics. Overall, this project is part of a larger stream of fundamental scientific research, which is central to the mission of the NSF.<br/><br/>One particularly challenging question, which is the focus of this proposal, is what happens to the energy that is cascaded to kinetic scales by magnetohydrodynamic (MHD) turbulence. This question is not only of basic plasma physics interest, but is also directly related to the very important problem of solar wind and solar corona heating that is crucial to understanding of the Sun-Earth interaction, but has so far eluded a conclusive solution.  This study will overcome many of the limitations of existing approaches and will provide complementary information covering effects and parameter regimes inaccessible to other techniques. The planned 3D simulations on Blue Waters will reach into the MHD scales while retaining kinetic effects and will provide a glimpse into many of the crucial questions posed by the research community.  Additionally, the simulations will enable detailed comparison with spacecraft observations and exploration of the role of competing processes that are simultaneously present in the simulations. The wealth of information in these simulations is akin to data collected from many spacecraft missions and warrants analysis from the community at large. As such, the simulation data resulting from the project will be made available to the community for analysis."
"1642404","SI2- SSE:  Symbolic Toolboxes for Differential Geometry and Mathematical Physics","OAC","GEOMETRIC ANALYSIS, Software Institutes","10/01/2016","09/06/2016","Ian Anderson","UT","Utah State University","Standard Grant","Micah Beck","09/30/2019","$314,161.00","Charles Torre","ian.anderson@usu.edu","Sponsored Programs Office","Logan","UT","843221415","4357971226","CSE","1265, 8004","7433, 8004, 8005","$0.00","This project develops the DifferentialGeometry (DG) software for research and educational use across a broad spectrum of disciplines, from mathematics to physics and engineering. With this software many pencil and paper calculations in differential geometry and its applications, calculations which were previously intractable, can now be performed quickly, reliably, and with relative ease.  DG provides extensive mathematical infrastructure which supports the formulation of new conjectures, the creation of examples and application of theoretical results, the ability to easily verify many results in the existing scientific literature,  and the ability to effortlessly share complex calculations with collaborators, colleagues, and students. DG libraries also provide access - for both experts and non-experts - to large tracts of scientific and mathematical knowledge. A number of undergraduate and graduate students will participate in this project, performing software development and exploring applications of DG to research problems in mathematics and physics.  In particular, DG provides an excellent means to get undergraduates involved in advanced research projects which normally would be accessible only to graduate students.<br/><br/>This project creates symbolic computational toolboxes and libraries to support research needs in differential geometry, relativity and field theory, differential equations and integrable systems, and Lie theory. These toolboxes and libraries will provide new infrastructure for symbolic computing in differential geometry and its applications; meet specific user community demands; and explore new areas where symbolic methods have heretofore been unused. Project highlights include new objects and environments for working with submanifolds, general connections, differential operators, and constrained jet spaces. Tools for analyzing asymptotic structure of spacetimes represent an innovative use of computer algebra. A new toolbox will be created which incorporates much of the extensive mathematical literature on the classification of Lie subalgebras. This project will provide, for the first time, a comprehensive symbolic toolkit for investigations of integrable Partial Differential Equations (PDE). New libraries of symbolic data include symmetric and isotropy irreducible homogeneous spaces, solutions of relativistic field equations and their properties, integrable PDE and their properties. As libraries of symbolic data are created, DG is used to validate and correct results in the literature. Software development and community engagement projects which will ensure sustainability are included."
"1535032","SI2-SSE: Scalable Multifaceted Graphical Processing Unit (GPU) Program Debugging","OAC","INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CCF, Software Institutes","09/01/2015","09/14/2016","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Bogdan Mihaila","03/31/2019","$435,482.00","","ganesh@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","1640, 2878, 8004","019Z, 7433, 8004, 8005, 9150, 9179, 9251","$0.00","Modern scientific research crucially depends on software simulations that help model scientific phenomena, and accelerate the process of discoveries, and communal result sharing. With the availability of affordable computational accelerators known as GPUs, the scientific community has begun migrating their existing CPU codes as well as creating new codes targeting GPUs. Unfortunately, this has resulted in a situation where the generated scientific results do not often agree across CPUs and GPUs. This exacerbates the danger of drawing wrong conclusions in crucial areas such as physics, weather simulations, drug discovery, and engineering computations. This project offers a combination of existing and new techniques in dissecting scientific experiments conducted through simulations, obtaining believable results, finding the root causes of varying results, and developing best practices to ensure higher result fidelity. Its techniques have special emphasis on GPUs, given their often poorly specified and evolving nature.<br/><br/>Result variability has many causes, including evolving, incorrect, or ambiguous specifications of computer hardware and software, racing data accesses, varying floating point precision standards, and incorrect result association within compound computational steps. This project develops methods that help a scientist systematically search through and eliminate these causes, thus accelerating the process of debugging result variability. The produced tools and exemplars of known erroneous behaviors allow a scientist to avoid the use of incorrect specifications, isolate and eliminate data races, and isolate and eliminate unreliable numerical steps. It also develops methods that help a scientist maintain focus on their basic scientific pursuits while still keeping up with technology evolution. It trains students in critical software engineering techniques that help the nation build the talent pool necessary for the extreme scale computing era.<br/><br/>The project will combine six research thrusts (GPU concurrency; challenge problems and develop user interfaces; pedagogy for domain scientists; improved GPU concurrency debugging tool support; more reproducible simulation results; and evolving and scaling tools with standards) to build and deliver open source software that incorporates proven stress-testing methods into tools; builds challenge problems, supports formalization support, and designs the user interface; delivers demos, books, and tutorials that help illustrate concurrency nuances; exploits symbolic analysis for input generation in mixed formal and GPU runs; develops stress testing inputs for round-off errors and separable verification to root-cause roundoff; and componentizes the symbolic verifier to enable parallelism, targeting from new APIs."
"1515969","Collaborative Research: Predicting the Transient Signals from Galactic Centers: Circumbinary Disks and Tidal Disruptions around Black Holes","OAC","PETASCALE - TRACK 1","08/01/2015","07/14/2015","Scott Noble","OK","University of Tulsa","Standard Grant","Edward Walker","07/31/2019","$12,683.00","","scott-noble@utulsa.edu","800 S. Tucker Drive","Tulsa","OK","741049700","9186312192","CSE","7781","9150","$0.00","With masses millions to billions times that of our Sun, the strong gravitational influence of supermassive black holes can heat nearby gas so that it outshines an entire galaxy of stars, and slingshot material outward at relativistic speeds. They exist at the centers of almost every massive galaxy, and they may play a significant role in regulating rates of star formation. Our project aims to produce the most realistic simulations to-date of two of the most dramatic events they can make: the inspiral and merger of two orbiting supermassive black holes and the tidal disruption of stars. Both were once thought to be unobservable, but the advent of very large astronomical surveys sensitive to transients have led to the discovery of numerous tidal disruptions and promise the discovery of many mergers. The prospect of extensive observational data make both very hot topics. The great advantage of this program is the opportunity it provides to integrate many existing research programs into a larger community of scientists, students, and the general public. It will foster crossdisciplinary collaborations among undergraduate students, graduate students, postdoctoral researchers and faculty at the Univ. of Tulsa, Johns Hopkins Univ., NASA/GSFC, and Rochester Institute of Technology. Through such a network, our previous highly successful outreach programs will be able to reach an even larger and more diverse audience. For instance, S. Noble (PI) will continue to speak about hot physics topics with the 100-200 high school students that attend the monthly ""Journal Club"" event sponsored by his department, and will work with his collaborators on building similar activities at their institutions.<br/><br/>The major goal of this project is to make specific predictions of electromagnetic signatures of circumbinary accretion and tidal disruption events that will allow astronomers to identify, based solely on EM observations, these spectacular events using HST (optical), JWST (IR), Chandra (X-ray), and ISS-Lobster (X-ray), as well as ground based instrumentation such as the Panoramic Survey Telescope and Rapid Response System, which is already in operation, or the planned Large Synoptic Survey Telescope. Such identifications will be a tremendous aid in refining our estimates of the population of merging supermassive black holes, both for estimating the numbers of potential gravitational wave targets and for evaluating the impact of black hole mergers on galaxy evolution. Our simulations on Blue Waters will employ a number of new computational developments needed to achieve the most realistic simulations of these systems to date. They include an adaptive load balancer to evenly distribute runs with nonuniform costs, and a multi-patch system for evolving the magnetohydrodynamics equations on different coordinate domains. It is our intent to make the latter tool publicly available for use in other magnetohydrodynamics codes. Our scientific results will also have a wide-reaching impact on the astrophysical community, both theoretically and observationally. We will release detailed spectra and simulation data for use by observers and future mission development teams. This work comes at a particularly opportune time, as the fields of time-domain and multi-messenger astronomy are growing rapidly."
"1740309","NSCI SI2-SSE: Multiscale Software for Quantum Simulations of Nanostructured Materials and Devices","OAC","DMR SHORT TERM SUPPORT, CI REUSE, Software Institutes, DMREF","09/01/2017","08/29/2017","Jerzy Bernholc","NC","North Carolina State University","Standard Grant","Vipin Chaudhary","08/31/2020","$500,000.00","Carl Kelley, Wenchang Lu, Emil Briggs","bernholc@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","1712, 6892, 8004, 8292","026Z, 6863, 6893, 7237, 7433, 8004, 8005, 9216","$0.00","Computational science is firmly established as a pillar of scientific discovery and technology, promising unprecedented new capabilities. The National Strategic Computing Initiative (NSCI) establishes an ambitious roadmap to advance Science and Technology (S&T) through support for sustained innovations in high performance computing and its use. Harnessing the power of millions of computer cores and/or compute accelerators (also called graphical processing units, GPUs) foreseen in future high performance computers requires a new generation of application software and algorithms, able to effectively utilize such resources and create the revolutionary S&T advances that underpin the nation's economic competitiveness. The proposed work will develop high-performance, scalable quantum simulation software for complex materials and devices, which will be portable and tunable across alternative exascale architectures. It will be able to address grand challenges in the design of quantum materials and devices, responding to one of the five strategic objectives of NSCI. Quantum materials and processes also underpin one of NSF's 10 Big Ideas for Future NSF Investments, The Quantum Leap: Leading the Next Quantum Revolution. Another important program in which materials simulation plays a key role is the Materials Genome Initiative. It seeks to ""deploy advanced materials at least twice as fast at a fraction of the cost"" and relies on computational materials design as the critical aspect, with computation guiding experiments. The goals of this project are to refactor and extend the open-source RMG software suite to future computer architectures at exascale, to enable transformational research on the design of quantum materials and devices from fundamental quantum-mechanical level. The RMG software will extend from desktops to the largest supercomputer systems, and will also perform well on a multitude of other systems, such as parallel computing clusters of various sizes, including those with GPUs. At the highest level of performance, it will enable predictive simulations at unprecedented scale, impact several areas of science and engineering and become a source of new discoveries and economic growth. RMG, already highly parallel and capable of multi-petaflops speeds, can provide a pathway towards reaching key NSCI goals. RMG has already been included in a benchmark suite which will be used to help select future supercomputers. At the same time, it's scalability means that it will be useful in classroom education running on students' laptops, to help individual researchers perform significant scientific or technological research on their accelerator- or GPU-equipped workstations, and, to run larger problems on a multitude of computer clusters with varying capabilities.<br/><br/>The goals of this project are to refactor and extend the open-source RMG software suite to exascale architectures, to enable transformational research on the design of quantum materials and devices from fundamental quantum-mechanical level. The RMG software will extend from desktops to the largest supercomputer systems, and will also perform well on a multitude of other systems, such as parallel clusters of various sizes, including those with GPUs. At the highest level of performance, it will enable predictive simulations at unprecedented scale, impact several areas of science and engineering and become a source of new discoveries and economic growth. RMG, already highly parallel and capable of multi-petaflops speeds, can provide a pathway towards reaching some of key NSCI goals. It has been included just as a part of NSF's Sustained Petascale Performance Benchmarks, which will be used to select NSF's future Leadership Class supercomputers. However, it will also be useful in classroom education, running on individual students' laptops, help individual researchers perform significant scientific or technological research on their accelerator- or GPU-equipped workstations, and also run on a multitude of clusters with varying capabilities. The extensible and portable exascale-capable software tools for simulations of complex quantum materials and devices will enable many scientific and technological endeavors that are currently too difficult to pursue, including dramatically accelerated discovery and design of complex quantum materials structures, such as nanostructured energy storage materials; nanoscale biosensors for electrical sequencing of DNA and nanoscale ""laboratories on a chip"" for monitoring health; as well as addressing fundamental questions about quantum behavior and the manipulation of quantum systems. Analogous accelerated progress is expected in other areas of science and technology that depend on nano and meso scales that are intermediate between those of molecules and bulk solids. Medium-size simulations will be enabled on local computing platforms, with an easy migration pathway to national facilities with the same input GUI. The exascale quantum simulation software will thus become a major resource to the national community. The easy availability of desktop binaries, supported source code, and optimized binaries at national facilities will lead to a major increase in high-end usage, dramatically enlarging the number and quality of simulations. The increase in users at all levels will stimulate their contributions both by new development and though incorporation of existing code elements into various materials frameworks. The national Cyberinfrastructure Community will be engaged through SI2 Software Institutes, Blue Waters and XSEDE projects, including live tutorials at workshops, as well tutorial sessions at conferences. STEM education and interests will be addressed by recruitment of undergraduate students, visually attractive presentations at libraries and science museums, and web-based presentation modules.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1560410","REU Site: Interdisciplinary Research Experience in Computational Sciences","OAC","RSCH EXPER FOR UNDERGRAD SITES","05/01/2016","03/17/2016","Juana Moreno","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Sushil K Prasad","04/30/2020","$346,641.00","Jesse Allison","moreno@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1139","9102, 9150, 9250","$0.00","Non-technical:<br/>This award funds a Research Experiences for Undergraduates (REU) Site at the Center for Computation and Technology (CCT) at Louisiana State University (LSU). The recent, rapid development of high performance computer platforms, together with a similar emergence of highly accurate algorithms, allow the treatment and modeling of complex systems which were intractable just a few years ago. However, Computational Science (CSci) will only fulfill its full potential if early exposure of undergraduate students to CSci accompany the advances in hardware. Currently, the majority of students learn little CSci in the classroom and are not prepared for CSci research. This REU site targets these issues and prepares the next generation of students for CSci research. A multi-disciplinary team of Computational Science faculty from several LSU departments will collaborate to provide participating students with research-learning experiences in the computational aspects of multiple scientific disciplines, including Chemistry, Computer Science, Digital Media, Mathematics, Physics, Astronomy, Biological Sciences, and Civil, Environmental, Petroleum, Electrical, and Computer Engineering. This project serves the national interest and NSF's mission by promoting early engagement of undergraduate students in research, exposing students to high-performance computing and other scientific cyberinfrastructure resources, fostering active learning through hands-on experiences in a collaborative research environment, and inspiring students to pursue advanced STEM education and STEM research careers.<br/><br/>Technical:<br/>The goals of this project are to engage students in a set of carefully and extensively planned research projects that pose a wide range of scientific and technological challenges. With research groups working on problems such as gravitational waves, complex emergent phenomena in material science, and computational arts, the participants will be working on cutting edge research in CSci. An extensive training component will remedy the weak preparation of most students in CSci, providing knowledge on how to leverage state-of-the-art cyberinfrastructure tools. The program offers a variety of activities designed to improve students' computing, reasoning, problem-solving, research, and communication skills. The 30 undergraduate students participating in this REU, as well as the 6 high school students supported by IBM, will be engaged with authentic computational science projects, learn how to use state-of-the-art cyberinfrastructure tools, experience activities that characterize research careers, and work in interdisciplinary research teams. This participating faculty have found that the combination of individual training with student immersion in a multidisciplinary research group has been successful in engaging a diverse body of students to explore CSci. Faculty mentors provide activities that help students appreciate the nature of multidisciplinary research and the value of working as a team. These skills have the potential to be transformative in both the students' education as well as their future careers."
"1904444","Exploring Clouds for Acceleration of Science (E-CAS)","OAC","CYBERINFRASTRUCTURE","11/15/2018","11/15/2018","Howard Pfeffer","DC","INTERNET2","Cooperative Agreement","Kevin Thompson","10/31/2021","$931,842.00","Ana Hunsinger, James Bottum","hpfeffer@internet2.edu","1150 18th St NW","Washington","DC","200363825","7349134264","CSE","7231","","$0.00","Internet2 leads the ""Exploring Clouds for Acceleration of Science (E-CAS)"" project in partnership with representative commercial cloud providers to accelerate scientific discoveries. The effort seek to demonstrate the effectiveness of commercial cloud platforms and services in supporting applications that are critical to growing academic and research computing and computational science communities, and seeks to illustrate the viability of these services as an option for leading-edge research across a broad scope of science. The project helps researchers understand the potential benefit of larger-scale commercial platforms for scientific application workflows such as those currently using NSF's High-Performance Computing (HPC). It also explores how scientific workflows can innovatively leverage advancements provided by commercial cloud providers.  The project aims to accelerate scientific discovery through integration and optimization of commercial cloud service advancements; identify gaps between cloud provider capabilities and their potential for enhancing academic research; and provide initial steps in documenting emerging tools and leading deployment practices to share with the community.<br/><br/>Cloud computing has revolutionized enterprise computing over the past decade and it has the potential to provide similar impact for campus-based scientific workloads. The E-CAS project explores this potential by providing two phases of funded campus-based projects addressing acceleration of science. Each phase is followed by a community-led workshop to assess lessons learned and to define leading practices. Projects are selected from two categories; time-to-science (to achieve the best time-to-solution for scientific application/workflows that may be time or situation sensitive) and innovation (to explore innovative use of heterogeneous hardware resources, serverless applications and/or machine learning to support and extend application workflows). The project is guided by an external advisory board including leading academic experts in computational science and other fields, commercial cloud representatives, NSF program officers, and others. It leverages prior and concurrent NSF investments while creating a new model of scalable cloud service partnerships to enhance science in a broad spectrum of disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1445344","EAGER: Collaborative Research: Making Software Engineering Work for Computational Science and Engineering: An Integrated Approach","OAC","Software Institutes","08/01/2014","05/03/2018","Jeffrey Carver","AL","University of Alabama Tuscaloosa","Standard Grant","Bogdan Mihaila","07/31/2019","$250,410.00","","carver@cs.ua.edu","801 University Blvd.","Tuscaloosa","AL","354870005","2053485152","CSE","8004","026Z, 7433, 7916, 8004, 8005, 9150","$0.00","Scientific and engineering advances are increasingly dependent upon software, and this overall field is often called Computational Science & Engineering (CSE). With the increase in the reliance on software for many critical decisions and advances, it is of utmost importance for that software to be correct and easy to maintain. As a result, scientists and engineers are devoting a larger percentage of their time to the development and maintenance of code, leaving less time for the underlying science or engineering. The discipline of software engineering (SE) is focused on creating techniques and tools to help developers work more effectively and efficiently. Historically, the use of SE practices for science and engineering software (in CSE) has been relatively low. Modern SE has a number of light-weight practices that would likely fit well into the workflow of science and engineering software projects. The low use of SE in CSE appears to be the result, at least partially, of the lack of properly-tailored, lightweight, practices that have been shown effective in science/engineering domains.<br/><br/>This project has three main activities:<br/>(1) Continuing the Software Engineering for Computational Science & Engineering Workshop (SE-CSE) series.  This will bring together members of both communities and inform them about the problems and solutions each community has, and will lead to collaborations between the communities.<br/>(2) Evaluating the usefulness of peer code reviews on scientific and engineering software. By building and validating a set of checklists to support scientific/engineering peer code review, a deeper understanding of the types of defects commonly made by CSE developers will be obtained. This knowledge will not just motivate checklist items, but it will inform the development of additional SE methodologies.<br/>(3) Developing and evaluating whether properly chosen and clearly presented metrics can be beneficial for scientific and engineering developers.  A community survey concerning metrics will provide unique insight into the type of information the CSE developers are currently using and would be interested in using if provided properly. This type of knowledge does not currently exist and will be quite valuable to others in the community wishing to support the use of metrics. The project will build an initial version of a metrics dashboard. This dashboard will act as a proof-of-concept to illustrate that a standard SE methodology (i.e. using metrics to provide insight into the software development process) can be used effectively by CSE developers.<br/><br/>In addition to the results of the individual activities, all of them together will help advance scientific/engineering progress by helping developers build higher quality software. The project will provide examples of how appropriate SE practices can be used effectively within the CSE domain. Dissemination of these results into the appropriate SE, CSE, and additional venues will allow them to have broad impacts."
"1637094","VOSS: Flexible Research Infrastructure: A Comparative Study","OAC","VIRTUAL ORGANIZATIONS","02/04/2016","09/15/2018","David Ribes","WA","University of Washington","Standard Grant","Vipin Chaudhary","08/31/2019","$211,051.00","","dribes@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7642","7642, 9179","$0.00","Using ethnographic and archival methods, this study will develop a comparative case analysis of two successful long-term cyberinfrastructures that have been supporting scientific research for nearly thirty years, in ecology and medical science: the Long-term Ecological Research Network (LTER) and the Multi-Center AIDS Cohort Study (MACS). Archival research will provide a backdrop to current events that will be investigated ethnographically.  The project addresses a significant gap in comparative studies of infrastructure, refocusing attention on the implications of the always-changing technologies, sociotechnical-organization, and institutional environments that make up contemporary research infrastructures.  The central research questions is: How are ""old organizations"" renewing themselves to sustain value for ""new science""? By investigating the past and present of cyberinfrastructures that have weathered many transformations, this research seeks to inform future cyberinfrastructure development efforts.  The findings will identify successful ""strategies of the long-term"" -- organizational forms and methods of design with a track-record of facilitating responsiveness to change.  These insights will be a contribution to the fields of Science and Technology Studies (STS), science policy, organization science, and the sociotechnical design of research infrastructure.<br/><br/>The purpose of this research is to understand the challenges that long-term scientific organizations face over time, and the strategies they employ to manage these challenges. The development of scientific research infrastructure is central to the NSF?s vision of science; however there has as yet been little or sporadic empirical attention to the dynamics of flexible long-term infrastructure in the face of changing social organization, information technology (IT) and scientific interests. This research will fill that gap, contributing to new, practical, and boundary-spanning knowledge about the characteristics of infrastructure in the making and over the long-term.  This project will inform efforts to build more open, effective, and sustainable cyberinfrastructure in the sciences, leading to smarter and more sustainable investment and design choices on the part of cyberinfrastructure project leaders, participants, tool builders, and funders.  This research will also inform science policy and regulatory environments, to help foster a sustainable and productive research infrastructure across multiple fields of inquiry."
"1912166","EAGER: A Framework For Economical Cyber Security Inspection and Assurance","OAC","Cyber Secur - Cyberinfrastruc","03/15/2019","02/21/2019","Theodore Allen","OH","Ohio State University","Standard Grant","Kevin Thompson","02/28/2021","$300,000.00","Laura Albert, Rajiv Ramnath","allen.515@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8027","7916","$0.00","This project seeks to develop and apply smart inspection methods to keep the costs of cyber security low, while ensuring the security of scientific results. Many other industries use statistical sampling and sequential inspection methods for quality assurance. Yet, at present much of cyber security relies on attempted inspection of either 100% of items of a given type (such as top priority domain name server logs) or 0% (such as many types of possible hardware inspections). Working across two major universities, Ohio State University and the University of Wisconsin, the project develops new inspection methods, tailored to cyber security objectives. Specific objectives relate to hardware, cyber software vulnerabilities, log inspection, and emergency management and related scientific research. The project explores the usefulness of these new methods to senior quality professionals to support widespread adoption. By working smarter and not harder, scientists using this approach can feel more confident that their results are trustworthy, and, at the same time, the cost curve can be bent so that research can remain cost-competitive.<br/><br/>Specifically, the project models security metrics probabilistically and develops patterns of testing to estimate these metrics. The research addresses a variety of domains and cyber security challenges: (1) supercomputing log and hardware inspections, (2) ordinary department vulnerability sampling and estimation and improved logging and sampling (focusing on scientific infrastructure), (3) highly resourced department stratified sampling hardware, improved vulnerability decision-making, and strategic log generation and sampling, and (4) the UW Emergency Management system center. Metrics from attack-defend and standard quality assurance will be explored and serve as validation outputs for the techniques. This project seeks to meet the vital security needs by formulating an assurance framework that adapts and extends existing methods in quality control and assurance to the cyber domain. The assurance framework offers many advantages: it can address sequential decision-making, it can seek solutions that are robust to uncertain data, and it balances cost with threat reduction. The inspection methods to be studied include Multiple Complete Inspection & Inspector Inspection, Attempted 100% Inspection with Optimal Supplementation, Stratified Sampling, Sequential Sampling, and Optimal Multi-Fidelity Inspection. In each case, operating characteristic curves and other estimates of system integrity will be developed and applied. A key and unique element of the proposed approach is the combination of multiple inspection methods, including new methods, to provide a comprehensive framework for cost-effectively enhancing integrity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740330","SI2-SSE: Software Elements to Enable Immersive Simulation","OAC","Software Institutes, CDS&E","09/01/2017","08/28/2017","Kenneth Jansen","CO","University of Colorado at Boulder","Standard Grant","Stefan Robila","08/31/2020","$499,997.00","Kurt Maute, Alireza Doostan, John Evans","jansenke@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8004, 8084","029E, 030E, 036E, 067E, 7433, 8004, 8005, 9263","$0.00","Parallel computers have grown so powerful that they are now able to solve extremely complex fluid flow or structures problems in seconds.  Unfortunately, it may take a researcher many hours or even days to set up a complex problem before it can be solved.  Furthermore it may take hours or often weeks to extract insight from the volume of data the simulation produces, if using standard techniques. For discovery and design questions, where the next variant of the problem requires a change to the problem definition, these delays disrupt the flow of experimentation and the associated intuition and learning about how the change in the problem definition relates to a change in the solution. To address this issue, a paradigm shift, referred to here as ""immersive simulation"", is planned to enable new approaches to problem definition editing that allow practitioners to interact with the simulations (visual model iteration) in a manner where they can dynamically experience the influence of parameter variations from a single, live, and ongoing simulation.  Examples include a surgeon virtually altering the shape of a bypass graft on one computer monitor and then virtually observing the change in the blood flow patterns not only within the bypass but throughout the vascular system. Likewise, an engineer altering the shape of a virtual car to see if the flow pattern improves or worsens. These applied research examples have parallels in fundamental research where live insight into the flow physics of unsteady, turbulent flows and their sensitivity to live parameter changes will be made available to researchers for the first time. Visually connecting the solution change to the visually iterated geometry and/or parameter change will enable a new age of intuition-driven discovery and design. This paradigm shift will also be incorporated into foundational undergraduate and graduate courses to enable deeper, experiential-based learning.  <br/><br/>The central goal of this project is to advance state-of-the-art tools into generic components that, when integrated, will make the following capabilities available to any partial differential equation solver: 1) live, reconfigurable visualization of ongoing simulations, 2) live, reconfigurable problem definition to allow the dynamic solution insight to guide the choice of key problem parameters, 3) real-time parameter sensitivity feedback, 4) adaptive simulation control to account for discretization errors and geometry changes, and 5) integration and demonstration of reliable, immersive simulation. The first communities that these software components will be developed with include cardiovascular flow and aerodynamic flow control. They have already articulated a need for software to more rapidly explore the performance of their systems under a broad parameter space with intuitive and quantitative parameter sensitivity. This software will enable not only design (applied research e.g., exploring bypass vs. stent type and placement for a particular patient's diseased vasculature or flow control actuator placement), but also discovery (fundamental research e.g., explore physics of flow response to discover completely new surgical procedures and flow control processes and devices). This twofold and complementary software application will have a similar impact on education, where foundational courses will use the integrated software modules to create immersive simulations that build intuition about flow physics, and then reinforce that learning in an applied nature in capstone design courses.  While the ideas will be prototyped and proven within the field of fluid dynamics, they will be developed generally, with sustainable software engineering, for easy adoption by other fields that make use of simulation. The successful development, integration, and demonstration of these tools at scale will transform massively parallel simulation from a series of I/O-intensive steps to live, reconfigurable discovery using carefully designed interfaces that blaze the trail for all simulation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642124","CICI: Regional: New England Cybersecurity Operation and Research Center (CORE)","OAC","Cyber Secur - Cyberinfrastruc","01/01/2017","06/20/2017","Xinwen Fu","MA","University of Massachusetts Lowell","Standard Grant","Micah Beck","12/31/2019","$508,480.00","Benyuan Liu, Yan Luo, Lawrence Wilson","xinwenfu@ucf.edu","600 Suffolk Street","Lowell","MA","018543643","9789344170","CSE","8027","9251","$0.00","The New England Cybersecurity Operation and Research Center (CORE) is a collaboration between cybersecurity researchers and networking experts from the University of Massachusetts Lowell, and Information Technology (IT) support personnel and leadership from the Office of the President of University of Massachusetts (UMass), who work together to improve the security of under-resourced institutions in New England and providing a model of a regional approach to cybersecurity. The project leaders have built long term partnerships with local IT and cybersecurity-related organizations and consortia, which enable the project to reach beyond academia and affect the cybersecurity of the general public. Finding qualified cybersecurity personnel is a challenge faced by campuses and businesses across the nation.  One major goal of the CORE center is development of the cybersecurity workforce in New England.  The project directly addresses this need, providing a template for other regions to follow. The project offers internships, assistantships, and co-ops for students to work in the security operations and research center and obtain hands-on training in cybersecurity operation and research. The researchers incorporate the outcomes of the project into courses on computer and network security and privacy, mobile computing, wireless networks and digital forensics. They have a track record of accepting female and minority students into research groups, and consciously and actively encourage students from traditionally underrepresented groups to join this project.<br/><br/>The researchers have established an open cybersecurity program at UMass, which guides customers through a sequence of steps and selects security controls and technologies from both proprietary solutions and free open source solutions, considering the budget of the institution or enterprise that wants to protect their assets. In the UMass cybersecurity program, these assets go through the UMass controls ""factory"". The input of the factory is unmanaged assets with weak security controls or without any controls. The output is a suite of managed assets with strong security controls, thus ensuring campus environments have robust cybersecurity protection. The researchers have applied the UMass open cybersecurity program to the UMass network and assets, which are monitored 24 hours a day, 7 days a week. This project expands the services including security consulting, security operations and security training/education to other local institutions and companies, and performs research on emerging threats, trends and defense based on the collected data. To sustain the operation, the CORE center provides services at affordable rates."
"1730137","CyberTraining: CIP - Professional Skills for CyberAmbassadors","OAC","CyberTraining - Training-based","11/01/2017","06/28/2017","Dirk Colbry","MI","Michigan State University","Standard Grant","Sushil K Prasad","10/31/2020","$498,330.00","Kathleen Colbry","colbrydi@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","044Y","7361","$0.00","Scientists and engineers use advanced cyberinfrastructure (CI) to conduct research that benefits society. For instance, engineers use CI to build faster airplanes; doctors use CI to discover new medicines; and scientists use CI to develop safer materials. CI includes hardware (like supercomputers, high-speed networks, digital cameras, and cloud-based storage), as well as the software and tools used to collect, organize and analyze information. CI Professionals are experts at developing and using CI, and scientists and engineers from many disciplines ask CI Professionals to help them use cyberinfrastructure in their research. In order to work effectively with these disciplinary experts, CI Professionals need to be able to communicate across disciplines, work in diverse teams, and serve as collaborative leaders and mentors. This project develops a training program to help CI Professionals build these professional skills such as communication, teamwork and leadership so that they can work more effectively with scientists and engineers and help them use CI to improve research in many areas. <br/><br/>This project provides professional skills training for technically proficient CI Professionals, with the goal of developing CyberAmbassadors who are prepared to lead multidisciplinary, computationally-intensive research at their home institutions. CyberAmbassadors will also be prepared to help mentor the next generation of CI Professionals and CI Users, who will become a sustaining source of new CyberAmbassadors. The first objective is to develop curriculum that builds professional skills (communications, teamwork, leadership) within the context of large scale, multi-disciplinary computational research. The curriculum will be developed with input from an External Advisory Board of CI Professionals and CI Users from academia, industry and national laboratories. The pedagogical approach is grounded in constructivism and socioculturism, and will combine in-person training with examples from real, multi-disciplinary research. The second objective is to pilot, evaluate and revise the curriculum. Pilot trainings will be held at Michigan State University (MSU), at appropriate CI conferences, and at other institutions and laboratories. During the pilot process, approximately 75 individuals will be trained as CyberAmbassadors and the curriculum will be evaluated and refined based on these experiences. The third objective is to ""train the trainers"" by collaborating with external partners (XSEDE, Blue Waters, Software/Data Carpentry, Tau Beta Pi) to prepare a cohort of at least 20 facilitators who can offer the CyberAmbassadors training through regional or national events. 100-150 additional participants will complete the CyberAmbassadors program during the ""train the trainers"" process. The curriculum developed as part of this project will be offered on a free, open-source basis, with the longer-term goal of making the CyberAmbassadors training regularly available at academic and research institutions nationwide."
"1640865","DNS of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers","OAC","PETASCALE - TRACK 1","09/01/2016","08/19/2016","Lian Duan","MO","Missouri University of Science and Technology","Standard Grant","Edward Walker","08/31/2019","$6,326.00","","duanl@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","7781","9150","$0.00","Understanding the physics of the pressure fluctuations induced by turbulent boundary layers are of major theoretical and practical importance. From a practical point of view, the fluctuating pressure on aerodynamic surfaces of flight vehicles plays an important role in vibrational loading and often leads to damaging effects as fatigue and flutter.  An in-depth knowledge of the nature of boundary-induced pressure fluctuations is thus essential to the structural design of launch vehicles and other important applications. The proposed work aims to use the computational power of the Blue Waters to provide the basis for an in-depth understanding of the global pressure field induced by turbulent boundary layers at supersonic speeds and the dependence of the fluctuating pressure field at different flow conditions. Such an understanding will advance the state of the art knowledge of wall-bounded turbulence and contribute to the modeling and control of wall-bounded turbulence.  Additionally, the broader impact of the project will be to enable valuable energy savings of transport vehicles. Overcoming turbulent friction consumes a large fraction of the United States energy budget. In 2012, vehicles that transport people/goods on ground, air or water throughout the U.S. consumed more than a quarter of the annual energy expenditure to overcome turbulent friction. The study of wall-bounded turbulent flows will contribute to better vehicle designs and the deployment of advanced flow control techniques for transport vehicles. Furthermore, the research activities of the project fully integrates with an education and outreach programs to inspire and meet the ever-increasing educational demands of next-generation engineers.<br/><br/>The research objective of this project is to investigate the structure of the turbulentpressure field in compressible boundary layers. This will be achieved by carrying out high-Reynolds-number direct numerical simulations (DNS) of compressible turbulent boundary layers using Blue Waters. Many existing analysis of the boundary-layer-induced pressure fluctuations is based on the Poisson equation in the context of incompressible flows, but for supersonic flows the pressure field is governed by the acoustic wave equation and is significantly less understood. In this work, the PI will generate high-Reynolds-number DNS data of compressible turbulent boundary layers to fundamentally understand the non-local pressure-related coupling between different regions of the boundary layer. In particular, this work will focus on studying pressure statistics and their scaling and  developing a high-fidelity understanding of the origin of the pressure fluctuations in various frequency/wave-number ranges."
"1810774","Realistic Simulations of the Intergalactic Medium: The Search for Missing Physics - Part 2","OAC","PETASCALE - TRACK 1","05/01/2018","03/22/2018","Michael Norman","CA","University of California-San Diego","Standard Grant","Edward Walker","04/30/2019","$7,750.00","","mlnorman@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7781","","$0.00","The intergalactic medium (IGM) is the hot, X-ray emitting gas that permeates the space between galaxies.  Currently, standard numerical simulation cannot match several key observational features of the IGM.  The mismatch between the simulations and observation data is much too large to be caused by observational errors, and is seen in multiple comparisons using different statistics, and by different groups. This project proposes to resolve this open question in astrophysics by running very high resolution simulations on the Blue Waters supercomputer to make more accurate and robust estimates of the IGM.  Furthermore, the simulations and codes will be publicly available to allow others to make their own estimates from existing or new observations.<br/><br/>The project will simulate the hydrogen Lyman alpha forest (LYAF) using the newly developed extreme scale branch of the Enzo called Enzo-P. Enzo-P implements the rich suite of physics models from the widely used Enzo community code on top of a new, highly scalable AMR infrastructure called Cello. Both Enzo-P and Cello have been developed by the PIs group at UCSD. The combination of Enzo-P and Cello will permit for the first time the use of AMR throughout a large cosmological volume, removing the existing tradeoff between large volume statistics and high local resolution that characterized previous Enzo simulations. The project will apply this new capability to carry out simulations of the LAF with sufficiently high resolution to capture the gaseous halos of galaxies which contribute significantly to intergalactic absorption, while at the same time surveying a statistically significant volume of the universe.  Moreover, the project will construct synthetic LYAF spectra and analyze their statistical properties to see whether the inclusion of high column density absorbers in the model improves agreement observations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1638186","CRISP Type 2: Interdependencies in Community Resilience (ICoR): A Simulation Framework","OAC","CRISP - Critical Resilient Int","09/01/2016","09/13/2016","Sherif El-Tawil","MI","University of Michigan Ann Arbor","Standard Grant","William Miller","08/31/2020","$2,499,945.00","Benigno Aguirre, Seymour Spence, Vineet Kamat, Jason McCormick","eltawil@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","027Y","008Z, 029E, 036E, 039E","$0.00","Research in natural hazards engineering, and, more broadly, disaster science, seeks to develop a science behind mitigating the effects of natural hazards. However, this research is being done by a multitude of highly specialized disciplines, each dedicated to handling a subset of the overall challenge. There is now an urgent need for researchers across disciplines to collaborate, so that the research done is holistic in nature, so as to find comprehensive, complete solutions to the problems in disaster science. Computation is widely used in disaster-science research across all the disciplines. Thus computational modeling may be used as a common language to link the disciplines. This project's planned integrative, computational platform will serve as this link. Users will be able to connect individual computational models and simulations from multiple disciplines to the platform and simultaneously run them to explore the complex interactions that take place between the different systems of society during and after natural hazard disasters. The ability to seamlessly interface with other models with minimal effort will foster entirely new collaborations between researchers who do not traditionally work together, enabling new studies within the natural hazards engineering and disaster science fields, leading to new contributions. Specifically in this project, new understanding will result of the complex interactions that take place between policy, casualty rates and community resilience. This will help policy makers determine what policy changes are needed in order to significantly influence a community's level of resilience to natural disasters. This project will also contribute to a better-skilled workforce. Students who will work on this project will attain a truly multi-disciplinary education at the intersection of civil engineering, social science and computer science. The unique skills that these students will acquire will allow them to make significant contributions to the future of natural hazards engineering and disaster science and position them as thought leaders in these fields. Thus, this project serves both the NSF's science mission as well as its mission to develop a science-aware workforce.<br/><br/>Extreme natural hazards, such as earthquakes and hurricanes, can trigger intricate inter-dependencies between the critical infrastructure systems of society, including the built environment (e.g., buildings and bridges), elements of social organization (e.g., social power and cohesion), and institutional arrangements (e.g., policies, politics, economics, and disaster mitigation). By employing an established set of standards for software interoperability, a simulation framework will be developed to allow researchers from different natural hazards research sub-fields to link their models together to study the effects of infrastructure interdependencies on community resilience. These interdependencies are complex and dynamic; e.g. in a hurricane, each building of the community shelters people while being a potential target of and source for wind-borne missiles. The interdependencies have not been adequately studied in the past because of the broadly interdisciplinary nature of the problem and the lack of tools to study them in an integrated manner. This project will address this issue. In addition, community resilience will be assessed in terms of the interactions that arise between infrastructure robustness, social organization, and policy. Infrastructure robustness directly influences casualty rates. Casualty rates are a direct function of social organization, and while they depend on the policies in effect prior to the event, they also influence future policy. By applying the tools developed in this research to seismic and hurricane scenarios as case studies, interactions between policies (especially as they have evolved over the past decades), cost, casualty rates, and community resilience will be modeled with the objective of seeking new insights into their complex interactions. The studies will address the extent to which policy changes need to be implemented to significantly influence a community's level of resilience. Quantifying these values will allow the most cost-effective changes to be pin-pointed and therefore help to direct future changes in policy targeting resilience. They will also allow the disciplined study of emergence in the complex community resilience problem, an interdisciplinary topic recognized as extremely important to all branches of science."
"1445347","EAGER: Collaborative Research: Making Software Engineering Work for Computational Science and Engineering: An Integrated Approach","OAC","Software Institutes","08/01/2014","06/27/2018","George Thiruvathukal","IL","Loyola University of Chicago","Standard Grant","Bogdan Mihaila","07/31/2019","$109,589.00","","gkt@cs.luc.edu","1032 W. Sheridan Road","CHICAGO","IL","606601537","7735082471","CSE","8004","7433, 7916, 8005","$0.00","Scientific and engineering advances are increasingly dependent upon software, and this overall field is often called Computational Science & Engineering (CSE). With the increase in the reliance on software for many critical decisions and advances, it is of utmost importance for that software to be correct and easy to maintain. As a result, scientists and engineers are devoting a larger percentage of their time to the development and maintenance of code, leaving less time for the underlying science or engineering. The discipline of software engineering (SE) is focused on creating techniques and tools to help developers work more effectively and efficiently. Historically, the use of SE practices for science and engineering software (in CSE) has been relatively low. Modern SE has a number of light-weight practices that would likely fit well into the workflow of science and engineering software projects. The low use of SE in CSE appears to be the result, at least partially, of the lack of properly-tailored, lightweight, practices that have been shown effective in science/engineering domains.<br/><br/>This project has three main activities:<br/>(1) Continuing the Software Engineering for Computational Science & Engineering Workshop (SE-CSE) series.  This will bring together members of both communities and inform them about the problems and solutions each community has, and will lead to collaborations between the communities.<br/>(2) Evaluating the usefulness of peer code reviews on scientific and engineering software. By building and validating a set of checklists to support scientific/engineering peer code review, a deeper understanding of the types of defects commonly made by CSE developers will be obtained. This knowledge will not just motivate checklist items, but it will inform the development of additional SE methodologies.<br/>(3) Developing and evaluating whether properly chosen and clearly presented metrics can be beneficial for scientific and engineering developers.  A community survey concerning metrics will provide unique insight into the type of information the CSE developers are currently using and would be interested in using if provided properly. This type of knowledge does not currently exist and will be quite valuable to others in the community wishing to support the use of metrics. The project will build an initial version of a metrics dashboard. This dashboard will act as a proof-of-concept to illustrate that a standard SE methodology (i.e. using metrics to provide insight into the software development process) can be used effectively by CSE developers.<br/><br/>In addition to the results of the individual activities, all of them together will help advance scientific/engineering progress by helping developers build higher quality software. The project will provide examples of how appropriate SE practices can be used effectively within the CSE domain. Dissemination of these results into the appropriate SE, CSE, and additional venues will allow them to have broad impacts."
"1713678","Collabortive Research: Petascale Simulations of Merging Black Holes and Neutron Stars","OAC","PETASCALE - TRACK 1","06/01/2017","04/27/2017","Saul Teukolsky","NY","Cornell University","Standard Grant","Edward Walker","05/31/2019","$18,899.00","Lawrence Kidder","saul@astro.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7781","","$0.00","The primary scientific objective of the research is to use Blue Waters to theoretically underpin and improve the ability of NSF's LIGO experiment to extract from observed gravitational waves the rich information that the waves carry.  More general objectives are to contribute to our understanding of relativistic astrophysical systems, and contribute to the development of numerical methods and computer codes capable of carrying out robust and accurate simulations of highly dynamical curved space-time.  This program will also serve as a training ground for young physicists and astrophysicists. It will teach them a wide variety of computational techniques. The research will include developing a next-generation supercomputer code called SpECTRE that will use radically new techniques. SpECTRE will be released as open source and simulation outputs will be made publicly available. This will enable other research to benefit from the work. Group members will vigorously pursue a wide range of activities that reach out to the broader scientific community and the general public.<br/><br/>The research will consist of two components. In the first, the researchers will run numerical simulations on Blue Waters using the Spectral Einstein Code (SpEC) to produce gravitational waveforms for binary black holes. These waveforms, which take weeks to months to produce, will then be used to develop a numerical surrogate model that can evaluate a single waveform in milliseconds while retaining the accuracy of full numerical simulations; this surrogate model will be suitable for direct use in LIGO data analysis.  The second component will develop, and run on Blue Waters, the new code SpECTRE for numerical relativity simulations with matter and radiation.  SpECTRE uses Discontinuous Galerkin finite element methods. It is designed for high accuracy and high scalability on current and future supercomputers by following a novel task-based parallelization paradigm.  SpECTRE's initial version already implements general-relativistic magnetohydrodynamics.  It will be upgraded to handle the dynamical spacetimes of neutron star-neutron star and black hole-neutron star mergers and stellar collapse, including nuclear-theory based hot equations of state and neutrinos.  SpECTRE will be applied to compute high-accuracy ultra-long neutron star inspiral simulations on Blue Waters to predict gravitational wave signals and help LIGO constrain the nuclear equation of state."
"1713694","Collaborative Research: Petascale Simulations of Merging Black Holes and Neutron Stars","OAC","PETASCALE - TRACK 1","06/01/2017","04/27/2017","Mark Scheel","CA","California Institute of Technology","Standard Grant","Edward Walker","05/31/2019","$5,016.00","","scheel@tapir.caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7781","","$0.00","The primary scientific objective of the research is to use Blue Waters to theoretically underpin and improve the ability of NSF's LIGO experiment to extract from observed gravitational waves the rich information that the waves carry.  More general objectives are to contribute to our understanding of relativistic astrophysical systems, and contribute to the development of numerical methods and computer codes capable of carrying out robust and accurate simulations of highly dynamical curved space-time.  This program will also serve as a training ground for young physicists and astrophysicists. It will teach them a wide variety of computational techniques. The research will include developing a next-generation supercomputer code called SpECTRE that will use radically new techniques. SpECTRE will be released as open source and simulation outputs will be made publicly available. This will enable other research to benefit from the work. Group members will vigorously pursue a wide range of activities that reach out to the broader scientific community and the general public.<br/><br/>The research will consist of two components. In the first, the researchers will run numerical simulations on Blue Waters using the Spectral Einstein Code (SpEC) to produce gravitational waveforms for binary black holes. These waveforms, which take weeks to months to produce, will then be used to develop a numerical surrogate model that can evaluate a single waveform in milliseconds while retaining the accuracy of full numerical simulations; this surrogate model will be suitable for direct use in LIGO data analysis.  The second component will develop, and run on Blue Waters, the new code SpECTRE for numerical relativity simulations with matter and radiation.  SpECTRE uses Discontinuous Galerkin finite element methods. It is designed for high accuracy and high scalability on current and future supercomputers by following a novel task-based parallelization paradigm.  SpECTRE's initial version already implements general-relativistic magnetohydrodynamics.  It will be upgraded to handle the dynamical spacetimes of neutron star-neutron star and black hole-neutron star mergers and stellar collapse, including nuclear-theory based hot equations of state and neutrinos.  SpECTRE will be applied to compute high-accuracy ultra-long neutron star inspiral simulations on Blue Waters to predict gravitational wave signals and help LIGO constrain the nuclear equation of state."
"1811236","Collaborative Research: Petascale Simulations of Binary Neutron Star Mergers","OAC","PETASCALE - TRACK 1","04/01/2018","03/27/2018","David Radice","NJ","Princeton University","Standard Grant","Edward Walker","03/31/2020","$7,500.00","","dradice@astro.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7781","","$0.00","The era of multimessenger astronomy has been inaugurated with the extraordinary detection of the collision of two neutron stars (NSs) by the Laser Interferometer Gravitational-Wave Observatory (LIGO), and the subsequent observations by X-ray, optical, infrared, and radio facilities. These observations have started to revolutionize our understanding of many areas in physics, including the origins of the heavy elements, like silver, gold, and platinum. However, they also pose many pressing open questions.  This project will make use of the Blue Waters supercomputer to address these open questions.  State-of-the-art NS merger simulations will be performed on the Blue Waters supercomputer to examine all the different possible NS collision scenarios to understand the fundamental physical processes that generated the observation data. This will be followed by extremely high-resolution simulations considering the post-merger evolution.  Simulation results will be made publicly available, and dissemination via movies, public lectures and school visits are planned.<br/><br/>A systematic study of the evolution of NS binary systems compatible with the observed NS collision, GW170817, will be performed by means of merger simulations on Blue Waters, employing sophisticated microphysics and neutrino treatment.  These simulations will ascertain the viability of tidal torques and shocks as mechanisms for the ejection of matter during mergers, which in turn powers the observed optical and infrared transients and synthesizes heavy elements. The impact of the NS equation of state (EOS) will be evaluated by considering a set of 3 EOSs spanning the range of the current nuclear uncertainties. The range of possible outcomes of the merger as a function of the binary parameters and EOSs will be assessed. High-resolution general-relativistic magnetohydrodynamics simulations of the merger remnant will be performed, with sufficient resolution to determine the magnetorotational instability (MRI) and the angular-momentum redistribution in the remnant while employing a microphysical treatment of the NS matter. These simulation results will ascertain the role of magnetohydrodynamics processes in determining the lifetime of the remnant and the observational signatures of an early or delayed black-hole formation. The role played by magnetized winds in the powering of the optical and infrared emissions and their nucleosynthetic yields will be assessed.   In addition, this project will develop significant improvements to current open-source codes, and all improvements will be released to the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1834740","Elements: NSCI-Software -- A General and Effective B-Spline R-Matrix Package for Charged-Particle and Photon Collisions with Atoms, Ions, and Molecules","OAC","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL PHYSICS, Software Institutes","03/01/2019","02/15/2019","Oleg Zatsarinny","IA","Drake University","Standard Grant","Bogdan Mihaila","02/28/2022","$476,284.00","Klaus Bartschat","oleg.zatsarinny@drake.edu","2507 University Avenue","Des Moines","IA","503114505","5152713788","CSE","1253, 7244, 8004","026Z, 077Z, 7569, 7923, 8004","$0.00","This project concerns the development and subsequent distribution of a suite of computer codes that can accurately describe the interaction of charged particles (mostly electrons) and light (mostly lasers and synchrotrons) with atoms and ions. The results are of importance for the understanding of fundamental collision dynamics, and they also fulfill the urgent practical need for accurate atomic data to model the physics of stars, plasmas, lasers, and planetary atmospheres. With the rapid advances currently seen in computational resources, such studies can now be conducted for realistic systems, as opposed to idealized models. In particular, it has become possible to describe very complex targets, such as transition metals and other open-shell systems. Examples include the excited states of the inert gases beyond helium, as well as neutral and lowly-ionized iron.  These systems are of significant importance for plasma diagnostics and astrophysics, respectively.  The source code will be made publicly available. The project will support a post-doctoral researcher. A website devoted to user-developer interaction will be developed and maintained together with the neccessary code documentation and training materials.<br/><br/>The numerical calculations will be based upon the non-perturbative R-matrix (close-coupling) method.  A particular strength of the implementation pursued in this project is the use of a highly flexible B-spline basis with non-orthogonal orbital sets.  The major advantage of the approach compared to traditional methods is the fact that an accurate target description can be achieved with a much-reduced configuration-interaction (CI) expansion if the orthogonality requirements on the individual orbitals are relaxed.  This is critical for complex targets, where the valence orbitals in particular are known to be strongly term-dependent. Using a sufficiently small but still accurate enough CI expansion for the target states is essential for the feasibility and quality of the subsequent collision calculation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1713784","The Computational Microscope","OAC","PETASCALE - TRACK 1","06/01/2017","09/29/2017","Emad Tajkhorshid","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","05/31/2019","$20,000.00","John Stone, James Phillips, Juan Perilla, Rafael Bernardi","tajkhors@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","","$0.00","Cells are the building blocks of life, yet they are themselves a collection of proteins, small molecules, and solvent, none of which, when taken in isolation, would be considered alive. How living things can arise from the ""behavior"" of molecules, which are simply obeying the laws of physics, is the essential conundrum of modern biology. The rise of scientific supercomputing has offered the chance to study living systems at the level of atoms, cells, and all levels in between.  Now, in the era of petascale computing, with computational resources like Blue Waters, it is becoming possible to take the most critical step from inanimate to animate matter by describing assembly and cooperation of thousands of macromolecules made of billions of atoms.<br/><br/>The projects in this proposal will use Blue Waters to study the chemo-mechanical properties of motor proteins, and the structure and function of the macromolecular complexes central to bacterial chemotaxis and plant fiber metabolism. Notably, all the projects build on extensive previous work. The first project will study the mechanisms of two motor systems: protein unfolding by ClpX unfoldase and cytoskeletal cargo transport by cytoplasmic dynein. The chemosensory array project seeks to answer how input from many chemical sensors on the bacterial surface are transduced across hundreds of nanometers in the array, leading the cell to decide if it should continue swimming or change direction, to adapt to changing environments. The cellulosome project aims to answer why human gut bacteria produces such an elaborate cellulosome when compared to the cellulosomes produced by other bacteria.  All the projects will leverage computational methods to combine structural data from multiple sources of differing resolutions (X-ray crystallography of individual proteins, medium-resolution cryo-EM of multi-protein systems, and low-resolution cryo-EM tomography of subcellular organelles) yielding atomic-resolution structural models of structures on the order of 100 nm in size, containing 10-100 million atoms. <br/><br/>The broader impacts of the project is the ongoing development of the hugely popular (over 300,000 registered users) NAMD and VMD software, which, combined with the longstanding training efforts of the PIs, will enable researchers worldwide to address key health and energy needs of mankind."
"1740210","SI2-SSE: C11Tester: Scaling Testing of C/C++11 Atomics to Real-World Systems","OAC","Software Institutes","10/01/2017","08/30/2017","Brian Demsky","CA","University of California-Irvine","Standard Grant","Stefan Robila","09/30/2020","$399,999.00","","bdemsky@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","8004","7433, 8004, 8005","$0.00","We have long relied on increased raw computing power to drive technological progress. However, processors are now reaching their limits in terms of raw computing power, and continuing progress will require increased productivity in developing parallel software. Fully leveraging the performance of multi-core processors will in many cases require developers to make use of low-level ""atomic"" (or indivisible) operations such as those provided by the C11 and C++11 languages, so that can make very fine-grained optimizations to their code, and take full advantage of the computing power these processors offer them. Unfortunately, using C/C++ atomics is extremely difficult to do correctly and it is very easy to introduce subtle bugs in the use of these constructs. Testing for concurrency bugs in code that uses C/C++11 atomics can be extremely difficult as a bug can depend on the schedule, the state of the processor's memory subsystem, the specific processor, and the compiler. The C11Tester project will develop tools for testing concurrent code that makes use of C/C++11 atomics and make these tools available to both researchers and practitioners.<br/><br/>The C/C++11 standard introduced a relaxed memory model with atomic operations into the C and C++ languages. While C/C++11 atomics can provide significant performance benefits, using C/C++11 atomics correctly is extremely difficult. Existing tools such as CDSChecker can only find bugs in small unit tests of concurrent data structures.  Bugs can also arise due to the interaction of subtle memory model semantics and the composition of software components. The C11Tester project will develop new techniques for testing and debugging complete concurrent applications that make use of C/C++11 atomics. The C11Tester project will make the following contributions: (1) it will develop new approaches for testing the correctness of concurrent applications, (2) it will develop new approaches for debugging concurrent applications, and (3) it will develop and make available a robust implementation of the approach in the C11Tester tool."
"1533581","SI2-SSE: Fast Dynamic Load Balancing Tools for Extreme Scale Systems","OAC","Software Institutes","10/01/2015","07/08/2015","Mark Shephard","NY","Rensselaer Polytechnic Institute","Standard Grant","Stefan Robila","09/30/2019","$500,000.00","Cameron Smith","shephard@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8004","7433, 8005","$0.00","Massively parallel computing combined with scalable simulation workflows that can reliably model systems of interest are central to the continued quest of scientists, engineers, and other practitioners to address advances in scientific discovery, engineering design, and medical treatment. However, to meet their potential, these methods must be able to operate efficiently and scale on massively parallel computers executing millions of processes. Reaching the goal of millions of parallel processes requires new methods in which the computational workload is extremely well balanced and interprocessor communications overheads are minimized. Attaining such parallel performance is greatly complicated in realistic simulation workflows where the models and their discrete computer representation must evolve to ensure simulation reliability, or to account for changing input streams. To address the need to obtain workload balance with controlled communications, various algorithms and associated software, referred to as load balancing procedures, have been, and continue to be, developed. To be effective in the execution of simulation workflows in which the workload evolves, the load balancing procedures must be applied dynamically at multiple points in the simulation. Current load balancing techniques demonstrate two deficiencies when applied as dynamic load balancing procedures at very large numbers of compute cores (e.g., greater than 100,000 cores): They become a major fraction of the total parallel computation (in some cases never finishing within an allocation) and they do not maintain good load balance for simulation steps that must balance based on multiple criteria.  Building on initial efforts to improve dynamic load balancing methods for adaptive unstructured mesh applications, the goal of the proposed research is to develop fast multicriteria dynamic load balancing methods that are capable of quickly producing well balanced computations, with well controlled communications, for a wide variety of applications. <br/><br/>An important characteristic of the dynamic load balancing procedures to be developed is generalizing the graph to account for multiple types of computational entities and interactions. The initial ideas for supporting multiple entity types came from consideration balancing finite element calculations that must consider multiple orders of mesh entities. These concepts will be refined and generalized to support multiple applications areas. An additional development will be fast hybrid dynamic load balancing methods that are combinations of ""geometric"", standard graph, and multicriteria graph methods in which the individual methods can be executed globally of at a more local level (such as at the node level). The dynamic load balancing method to be developed will be demonstrated on three applications in which the workload, and its distribution, is changing as the simulation proceeds. The applications will be adaptive mesh simulations, adaptive multiscale modeling, and massive scale free graphs. These applications will be carried out on available massively parallel computers where examples on >1 million cores will be demonstrated. A goal of the dynamic load balancing methods to be developed will be to attain scalability, and do so with controlled data movement such that the wall clock time and energy used is substantially less than that required for an equivalent accuracy non-adaptive calculation.<br/><br/>The software produced by this project will be made available as open source components. These developments coupled with efforts to support users in applying them in the development of new simulation tools will impact many research communities. Based on past and present efforts, the PIs fully expect that technologies developed in this project will also be integrated into future industrial software systems."
"1726534","MRI: Acquisition of Thorny Flat Next Generation Cluster for High-Performance Computing in West Virginia","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC","10/01/2017","09/15/2017","Blake Mertz","WV","West Virginia University Research Corporation","Standard Grant","Stefan Robila","09/30/2020","$989,408.00","George Spirou, Stephen DiFazio, Zachariah Etienne, Aldo Romero","blake.mertz@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","1189, 1640","1189, 9150","$0.00","This project will enable West Virginia University (WVU), the flagship Land Grant University for the state of West Virginia (WV) with over 31,000 students, to acquire a High Performance Computing (HPC) cluster called Thorny Flat.  The project will use the resource to support a broad spectrum of research efforts from cosmology to neuroscience, as well as expand utilization of the resource to non-traditional disciplines like systems ecology and biometrics.  In addition, an important goal of the project is to nurture a cohort of new HPC users across West Virginia, a region in acute need of economic diversification. Moreover, the project plans to use the HPC cluster to establish collaborations that will serve to expand and transition the WV economy toward a focus on the high-level technology sector, a critical need in this disadvantaged region.<br/><br/>Specifically, the new HPC cluster will accelerate research projects in biophysical chemistry studies with implications for drug delivery and diagnostic imaging; state-of-the-art code development for materials optimization; and simulations of binary neutron star mergers for future LIGO gravity wave observations.  As well, the resource will catalyze new efforts in Appalachian watershed dynamics, business analytics, and the university's burgeoning quantitative neuroscience program, with six new hires planned for 2017.  With established Centers of Research Excellence in gravitational wave astronomy, STEM education, water security, regional health disparities, and responsible natural gas utilization, the university is devoting new resources to leverage existing strengths in neuroscience and cybersecurity. The availability of high quality infrastructure, such as Thorny Flat, will be essential to the success of many of these computationally intense efforts, led by 35 faculty in conjunction with 180 postdocs, graduate students, and undergrads.<br/><br/>Moreover, the project is committed to using the HPC cluster for training and support for both the university and state-wide research communities.  For example, courses from a diverse set of disciplines will utilize the cluster as a vital training resource, and the project plans to integrate with an annual week-long HPC Summer Institute providing hands-on training to users from across the state.  Furthermore, the project will broaden demand for HPC resources from less traditionally computational areas to address critical big data issues in water security, opiate addiction, and health care disparities. In parallel, the project will be cultivating the next generation of HPC coders and diversifying the pool with 7 female faculty mentors.  In addition, the expanded infrastructure and development of a well-qualified HPC workforce will promote additional university partnerships with regional federal labs, as well as with regional industrial partners."
"1840034","CICI: CSRC: Research Security Operations Center (ResearchSOC)","OAC","Cyber Secur - Cyberinfrastruc","10/01/2018","08/24/2018","Von Welch","IN","Indiana University","Standard Grant","Kevin Thompson","09/30/2021","$4,933,641.00","James Marsteller, Inna Kouper, Richard Biever, Susan Sons","vwelch@iu.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","8027","","$0.00","The Research Security Operations Center (ResearchSOC) provides the research and education (R&E) community with cybersecurity services, training, and information sharing needed to make scientific computing resilient to cyberattacks and capable of supporting trustworthy, productive research. The R&E community has particular challenges regarding cybersecurity: a large span of size and autonomy, the use of diverse infrastructure (scientific instruments, sensor networks, sequencers, etc.), the highly collaborative and dynamic nature of scientific work, and acquiring the scarce domain expertise expertise needed to support cybersecurity in the research context.<br/><br/>To address these challenges, the ResearchSOC leverages existing cybersecurity services from Indiana University, Duke University, the Pittsburgh Supercomputing Center, and the University of California San Diego. It tailors these service to the needs of the R&E community and combines these operational services with the establishment of a community of practice for sharing knowledge and expertise, and operational intelligence. The ResearchSOC also offers outreach and training targeted at  educating research project teams and the higher education information security community regarding information security for research. The ResearchSOC is engaging with the cybersecurity research community to discern how it can advance research activities and make data available for research purposes in order to improve the overall state of the art in information security.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840191","CICI: SSC: TrOnto - A Community-Based Ontology for a Trustworthy and ResiliCent Scientific Cyberspace","OAC","Cyber Secur - Cyberinfrastruc","09/01/2018","08/27/2018","Raul Aranovich","CA","University of California-Davis","Standard Grant","Micah Beck","08/31/2020","$640,000.00","Matt Bishop, Premkumar Devanbu, Vladimir Filkov, Kenji Sagae","raranovich@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8027","","$0.00","Scientific research relies on an infrastructure of networked computers (cyberinfrastructure), which need to be secured against malicious threats and intrusions. For instance, the privacy of clinical trial subjects, the integrity of medical data, and the safety of tissue samples and cultures can be compromised if the cyberinfrastructure of a university hospital is breached. Information Security Officers (ISOs) are in charge of maintaining a secure cyberinfrastructure for research, but their task is complicated by the speed at which new threats emerge, and by the proliferation of software (often written in different languages) and obsolete or specialized hardware among labs and research centers. ISOs look for answers in their quest to fight intrusions and vulnerabilities in documentation and on-line discussion forums, but this is a time-consuming process. To streamline the process, an interdisciplinary team of researchers are developing a computer system to automatically and continuously harvest all that knowledge from online sources, organizing it into a network of concepts and relations that can be queried and reasoned upon to keep ISOs a step ahead of malicious attacks. <br/><br/>The project develops a number of interconnected modules: 1) A sub-system that identifies concepts and relations in software documentation, advisory bulletins, and on-line technical forums, and then retrieves that information using state-of-the-art natural language processing techniques; 2) An ontology for cybersecurity, which is a knowledge representation system that organizes the retrieved concepts and relations into a logical network, allowing for implicit knowledge to be extracted by means of automatic reasoning algorithms, and;  3) A querying interface, which allows ISO staff to access the knowledge represented in the ontology to find answers to their questions about cybersecurity. This innovative approach to cybersecurity extends the use of ontologies in the biomedical field, leveraging the metaphor of vulnerabilities in information systems as viruses or infections. Even though the initial stages in the creation of the ontology will involve curation by human experts, the researchers expect that the system can itself automatically thanks to the use of information retrieval techniques, therefore overcoming one of the known bottlenecks in the usefulness of ontologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1738902","CICI: RSARC: Secure Time for Cyberinfrastructure Security","OAC","Cyber Secur - Cyberinfrastruc, EPSCoR Co-Funding","08/01/2017","04/13/2018","Yongqiang Wang","SC","Clemson University","Standard Grant","Micah Beck","07/31/2020","$1,015,464.00","Ganesh Venayagamoorthy, Christopher Post, Kuang-Ching Wang","yongqiw@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","8027, 9150","8027, 9150, 9251","$0.00","The accurate marking and tracking of time plays a key role in scientific discovery and technology advancement. Scientific discovery is becoming increasingly reliant on more precise and reliable timekeeping. For example, the precise synchronization of time enables GPS (Global Positioning System) systems to accurately measure distances between distributed components of scientific instruments. Furthermore, tightly synchronized clocks between the base stations of mobile phone and data networks enable mobile handsets to share limited radio spectrum more efficiently. In fact, many modern critical infrastructures including computer networks, communication systems, electrical power grids, and financial networks rely on precise time and time synchronization for efficient and safe operations. However, existing commonly used time synchronization approaches, including both GPS and the Network Time Protocol (NTP), are extremely vulnerable to attacks. As modern research cyberinfrastructure quickly evolves towards a distributed architecture with the incorporation of cloud computing resources and new forms of sensing and computing infrastructures, and as attacks on the time infrastructure have increased, developing secure time synchronization approaches for resilient time service is of paramount importance.<br/><br/>The focus of this project is to provide novel and holistic approaches to enable time security for cyberinfrastructures that employ the two most commonly used time synchronization mechanisms, i.e., GPS based and NTP based synchronization. More specifically, the project seeks to provide a security solution as a national service for all civil GPS time users. It will also develop and prototype a redundancy based security solution to NTP synchronization with minimal extra communication overhead. The main thrusts are to: i) establish a reference GPS receiver and server at Clemson University that can be used for GPS receivers across the US to detect spoofing attacks by cross checking encrypted military code in GPS signals; ii) develop a pulse based redundant synchronization approach to verify the reliability of network time provided by NTP; iii) experimentally prototype the two secure time approaches on a flood monitoring platform and a weather station platform, and use them to secure Clemson's campus NTP timing network in collaboration with the Information Technology (IT) department."
"1535177","Collaborative Research:  Personalized Benchmarks for High Performance Computing Applications","OAC","CESER-Cyberinfrastructure for","09/01/2015","11/06/2015","Marianne Winslett","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","08/31/2019","$330,000.00","","winslett@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7684","7684, 9251","$0.00","As high-performance computing applications target ever-larger problems, data input and output (I/O) takes up more and more run time. Users, software developers, and platform administrators often find it difficult to understand what an application's I/O code is doing, why it is slow, how it might be improved, or how well it would perform on a different platform. I/O benchmarks help address this problem, but they are expensive to produce and thus are not available for most applications. This project is providing user-friendly personalized I/O benchmarks for all applications, by leveraging existing lightweight I/O profilers that already monitor the behavior of applications on high-performance computing platforms. The resulting personalized benchmarks will help researchers, developers, and purchasers in evaluating potential new storage system architectures, evaluating existing or new versions of storage systems and I/O libraries, planning for purchases, comparing performance of application clusters or workloads across platforms, and improving the performance of parallel I/O libraries and applications. The analytics and benchmark generation software, and example benchmarks, will be publicly released.<br/> <br/>This project uses two methods to construct personalized I/O benchmarks. First, the project is making existing applications self-benchmarking across all of their runs, by providing analytics and visualization facilities to convey to stakeholders the information already automatically captured by lightweight I/O profilers such as Darshan during each run. Second, the project is creating platform-customized benchmark suites that represent the mix of application-level workloads observed on a given platform. To accomplish this, the project is clustering observed production jobs based on their I/O behavior and using both new and existing I/O kernel generation techniques to generate a compact benchmark for each cluster. The resulting benchmark suite will advance the state of the art by serving as a proxy for real-world, platform-specific production I/O workloads, and by providing previously unavailable insight into how prevalent those workloads are at a given facility."
"1713695","Petascale Integrative Approaches to Protein Structure Prediction","OAC","PETASCALE - TRACK 1","08/01/2017","05/11/2017","Ken Dill","NY","SUNY at Stony Brook","Standard Grant","Edward Walker","07/31/2019","$15,566.00","Alberto Perez","dill@laufercenter.org","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7781","","$0.00","This project addresses a central challenge of biology: the prediction of protein structure from sequence. Accurate determination of protein structure is essential to understanding biological mechanism and designing new drugs. The purpose of this work is to use the petascale capabilities of Blue Waters to overcome limitations of current experimental and computational tools of structure determination.  The project will do this by running a physics-based simulation method, called MELD (Modeling Employing Limited Data) that rationally incorporates sparse, noisy, and ambiguous experimental or heuristic information. Currently around 15% of protein targets cannot be tackled by known methods. The transformative goal of this work is to use MELD, running on Blue Waters, to break substantial barriers in size and complexity for computational structure prediction.<br/><br/>This work has three major goals: (1) use MELD to provide structures for the known, small proteins were bioinformatics methods fail and where databases are sparse (e.g. membrane proteins), (2) provide kinetic and pathway information for protein folding by seeding Markov State Models (MSM) with MELD intermediate states, and (3) use accurate relative free energies of folding upon binding for more accurate peptide design. The project will use the the petascale capabilities of Blue Waters to push the size boundaries of full atomistic physics based predictions.  Both the software and the results obtained from this project will be freely accessible to the larger scientific community."
"1713765","Collaborative Research: Simulating Two-Fluid MHD Turbulence in Star Forming Molecular Clouds on the Blue Waters System","OAC","PETASCALE - TRACK 1","06/01/2017","05/16/2017","Dinshaw Balsara","IN","University of Notre Dame","Standard Grant","Edward Walker","05/31/2019","$20,000.00","","dbalsara@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7781","","$0.00","Astrophysics is at the threshold of a new data-rich and simulation-rich era in star-formation studies. The question of how stars form is fascinating in itself and has a great impact on several other areas of astrophysics. There is a general consensus that a predominant amount of star formation in our Galaxy takes place in molecular clouds, and specifically in giant molecular clouds (GMC). Consequently, NASA has made multi-million dollar investments in instruments such as HAWC+ on the SOFIA airborne observatory with the specific goal of understanding the turbulent nature of star forming clouds. This project on Blue Waters will use the petascale capabilities of the system to validate theories of star formation by simulating the same processes the NASA's instruments are currently measuring.<br/><br/>This project will carry out simulations on Blue Waters that integrate the ions and the neutrals to reproduce the turbulent, partially ionized gas in which stars form.  Since neutral gas does not couple to the magnetic field, but ionized gas do, the project has built frontline simulation capabilities for simulating two-fluid plasmas. Simultaneously, the project has built theoretical and numerical analysis tools that will enable cross-comparison of simulations with observations. This cross-comparison provides important verification of theories of star formation. Further diagnostics that emerge from the simulations will also be used to motivate more detailed observations. In this fashion, Blue Waters will enable theory, computation and detailed observations to support one another.  In addition, the project intends to combine education in computational astrophysics with code releases of their simulation tools to the greater astrophysics community."
"1713782","Collaborative Research:  Simulating Two-Fluid MHD Turbulence in Star Forming Molecular Clouds on the Blue Waters System","OAC","PETASCALE - TRACK 1","06/01/2017","05/16/2017","Alexandre Lazarian","WI","University of Wisconsin-Madison","Standard Grant","Edward Walker","05/31/2019","$20,000.00","","lazarian@astro.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7781","","$0.00","Astrophysics is at the threshold of a new data-rich and simulation-rich era in star-formation studies. The question of how stars form is fascinating in itself and has a great impact on several other areas of astrophysics. There is a general consensus that a predominant amount of star formation in our Galaxy takes place in molecular clouds, and specifically in giant molecular clouds (GMC). Consequently, NASA has made multi-million dollar investments in instruments such as HAWC+ on the SOFIA airborne observatory with the specific goal of understanding the turbulent nature of star forming clouds. This project on Blue Waters will use the petascale capabilities of the system to validate theories of star formation by simulating the same processes the NASA's instruments are currently measuring.<br/><br/>This project will carry out simulations on Blue Waters that integrate the ions and the neutrals to reproduce the turbulent, partially ionized gas in which stars form.  Since neutral gas does not couple to the magnetic field, but ionized gas do, the project has built frontline simulation capabilities for simulating two-fluid plasmas. Simultaneously, the project has built theoretical and numerical analysis tools that will enable cross-comparison of simulations with observations. This cross-comparison provides important verification of theories of star formation. Further diagnostics that emerge from the simulations will also be used to motivate more detailed observations. In this fashion, Blue Waters will enable theory, computation and detailed observations to support one another.  In addition, the project intends to combine education in computational astrophysics with code releases of their simulation tools to the greater astrophysics community."
"1811352","Collaborative Research: Petascale Simulations of Binary Neutron Star Mergers","OAC","PETASCALE - TRACK 1","04/01/2018","03/27/2018","Philipp Moesta","CA","University of California-Berkeley","Standard Grant","Edward Walker","03/31/2019","$5,495.00","","pmoesta@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","7781","","$0.00","The era of multimessenger astronomy has been inaugurated with the extraordinary detection of the collision of two neutron stars (NSs) by the Laser Interferometer Gravitational-Wave Observatory (LIGO), and the subsequent observations by X-ray, optical, infrared, and radio facilities. These observations have started to revolutionize our understanding of many areas in physics, including the origins of the heavy elements, like silver, gold, and platinum. However, they also pose many pressing open questions.  This project will make use of the Blue Waters supercomputer to address these open questions.  State-of-the-art NS merger simulations will be performed on the Blue Waters supercomputer to examine all the different possible NS collision scenarios to understand the fundamental physical processes that generated the observation data. This will be followed by extremely high-resolution simulations considering the post-merger evolution.  Simulation results will be made publicly available, and dissemination via movies, public lectures and school visits are planned.<br/><br/>A systematic study of the evolution of NS binary systems compatible with the observed NS collision, GW170817, will be performed by means of merger simulations on Blue Waters, employing sophisticated microphysics and neutrino treatment.  These simulations will ascertain the viability of tidal torques and shocks as mechanisms for the ejection of matter during mergers, which in turn powers the observed optical and infrared transients and synthesizes heavy elements. The impact of the NS equation of state (EOS) will be evaluated by considering a set of 3 EOSs spanning the range of the current nuclear uncertainties. The range of possible outcomes of the merger as a function of the binary parameters and EOSs will be assessed. High-resolution general-relativistic magnetohydrodynamics simulations of the merger remnant will be performed, with sufficient resolution to determine the magnetorotational instability (MRI) and the angular-momentum redistribution in the remnant while employing a microphysical treatment of the NS matter. These simulation results will ascertain the role of magnetohydrodynamics processes in determining the lifetime of the remnant and the observational signatures of an early or delayed black-hole formation. The role played by magnetized winds in the powering of the optical and infrared emissions and their nucleosynthetic yields will be assessed.   In addition, this project will develop significant improvements to current open-source codes, and all improvements will be released to the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1339723","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","OAC","PHYSICAL OCEANOGRAPHY, SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF, Software Institutes, EarthCube","10/01/2014","08/26/2014","Richard Luettich","NC","University of North Carolina at Chapel Hill","Standard Grant","Bogdan Mihaila","09/30/2019","$759,047.00","","rick_luettich@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1610, 1714, 2878, 8004, 8074","7433, 8009","$0.00","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience."
"1902308","Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building","OAC","Software Institutes","10/01/2018","02/01/2019","Timo Heister","UT","University of Utah","Standard Grant","Stefan Robila","09/30/2023","$700,000.00","","heister@sci.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.<br/><br/>Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835673","Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building","OAC","Software Institutes","10/01/2018","09/26/2018","Wolfgang Bangerth","CO","Colorado State University","Standard Grant","Stefan Robila","09/30/2023","$1,000,000.00","","bangerth@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.<br/><br/>Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740130","SI2-SSE: Entangled Quantum Dynamics in Closed and Open Systems, an Open Source Software Package for Quantum Simulator Development and Exploration of Synthetic Quantum Matter","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, COMPUTATIONAL PHYSICS, Software Institutes","09/01/2017","08/29/2017","Lincoln Carr","CO","Colorado School of Mines","Standard Grant","Vipin Chaudhary","08/31/2020","$499,978.00","","lcarr@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","1253, 1712, 7244, 8004","026Z, 057Z, 7203, 7433, 7569, 8004, 8005, 8084, 9216","$0.00","On the way to universal quantum computing, quantum simulators, literally ""analog"" quantum computers, have already been a huge success and are fulfilling Feynman's original 1982 vision of quantum computing. Each such simulator requires a dedicated experimental platform, and costs on the order of several million dollars to build. Such experiments have many interacting parts often requiring a complex rearrangement and months of work in order to perform a specified quantum computation. A widely accessible and easy to use software tool to shortcut design considerations for quantum simulator experimentalists is much needed. This project will create such a tool. Initially, the project researchers will design and release on SourceForge an open source software package centered around 1D matrix product state (MPS) and matrix product density operator (MPDO) methods, for both closed and open quantum systems, which any experimentalist can download and easily use locally to design and benchmark their quantum simulator architecture of choice. The software elements will include (i) prebuilt generalized Ising, Hubbard, and other models, (ii) multi-legged ladders to extend into quasi-1D, (iii) different time propagation methods for short and long-range interactions, and (iv) supplemental exact diagonalization and quantum trajectory methods. Secondly, the project members will create an even simpler graphical version via a web interface in collaboration with the Science Gateways Community Institute which allows any experimentalist to run quick simulations and tests on local dedicated high-performance computing resources at the Colorado School of Mines in a secure and user-friendly format. Finally, the project team will develop a key new software element in terms of a series of increasingly accurate discretization schemes to model continuum quantum simulators and mesoscopic limits, as well as control systematic error in experiments.<br/><br/><br/>MPS/MPDO methods enable the treatment of outstanding quantum problems for design of new materials dubbed synthetic quantum matter, push the boundaries of quantum mechanics into strongly correlated physics, where particles and even quasiparticles lose meaning, and allow exploration of totally new realms of entangled dynamics. The software package proposed here will make these extraordinary capacities accessible to the over 150 quantum simulator experimental groups, as well as the many theoretical/computational groups investigating entangled quantum dynamics, thus greatly speeding up exploration of quantum simulator physics. The software package will provide a platform for exploring far-from-equilibrium open quantum system dynamical models, an important topic in the theoretical and computational physics community at present due to the new and expanded capacity offered by quantum simulators; and it will provide an educational venue for new graduate students entering research groups and faculty developing courses in computational physics and related areas. Broader impact activities include integration of computation across the Colorado School of Mines undergraduate physics curriculum as a model for departments across the country to achieve this AAPT/APS goal, e.g. redesigning the mathematical physics course to cover equal parts analytical and computational methods, including specific research skills for summer REUs and internships. Finally, graduate students will be trained in critical analysis for scientific problem solving and rigorous numerical techniques for high-performance computing, including open source science via a science gateway, an approach key to success in a number of arenas in society, from the materials genome initiative to the space program.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Physics Division, Materials Research Division, and Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences."
"1339804","SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CYBERINFRASTRUCTURE, Software Institutes, CDS&E","10/01/2013","09/04/2014","Sohrab Ismail-Beigi","CT","Yale University","Continuing grant","Bogdan Mihaila","09/30/2019","$1,380,247.00","","sohrab.ismail-beigi@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","1253, 1712, 7231, 8004, 8084","7433, 7569, 8009, 8084, 9216, 9263","$0.00","Computer simulation plays a central role in helping us understand, predict, and engineer the physical and chemical properties of technological materials systems such as semiconductor devices, photovoltaic systems, chemical reactions and catalytic behavior. Despite significant progress in performing realistic simulations in full microscopic detail, some problems are currently out of reach: two examples are the modeling of electronic devices with multiple functional parts based on new materials such as novel low power computer switches that would revolutionize the Information Technology industry, and the photovoltaic activity of complex interfaces between polymers and inorganic nanostructures that would enhance US energy self-reliance.  The research program of this collaborative software institute aims to create an open and effective scientific software package that can make efficient use of cutting-edge high performance computers (HPC) to solve challenging problems involving the physics and chemistry of materials.  By having such software available, this software initiative will have multiple broad impacts.  First, the community of materials scientists will be able to study next-generation problems in materials physics and chemistry, and computer science advances that enable the software will be demonstrated and made accessible for both communities which will help cross-fertilize further such collaborative efforts.  Second, the capability of simulating and engineering more complex materials systems and technological devices could play a role in helping the US continue is competitive edge in science, technology, and education. Third, through training of young scientists, direct outreach to the broader scientific community through workshops and conferences, and educational programs ranging from secondary to graduate levels, the power, importance, and capabilities of computational modeling, materials science, and computer science methodologies that enable the science will be communicated to a broad audience.  Finally, by enabling the refinement of existing materials systems as well as discovery of new materials systems, the resulting scientific advances can help broadly impact society via technological improvements: in terms of the two examples provided above, (a) the successful design of new electronic device paradigms helps significantly advance the digital revolution by permitting the introduction of smaller, more efficient, and more capable electronic circuits and information processing systems, and (b) successful creation of inexpensive, easy-to-fabricate, and durable photovoltaic materials and devices can lead to cleaner forms of energy production while reducing reliance on fossil fuels.<br/><br/>The technical goal is to greatly enhance the open software tool OPENATOM to advance discovery in nanoscience and technology. OPENATOM will be delivered as a open, robust and validated software package capable of utilizing HPC architectures efficiently to describe the electronic structure of complex materials systems from first principles.  In terms of describing electronic ground-states, OPENATOM will be enhanced by features such as improved configurational sampling methods, hybrid density functionals, and incorporation of fast super-soft pseudopotential techniques. In addition, the team will incorporate the many-body GW-BSE approach for electronic excitations that permits accurate computation of electronic energy levels, optical absorption and emission, and luminescence.  Ultimately, such an extensible software framework will permit accurate electronic structure computations to employ effectively future HPC platforms with 10,000,000 cores."
"1339715","SI2-SSI: Collaborative Research: Scalable, Extensible, and Open Framework for Ground and Excited State Properties of Complex Systems","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, SPECIAL PROJECTS - CCF, CYBERINFRASTRUCTURE, Software Institutes, CDS&E","10/01/2013","09/10/2014","Laxmikant Kale","IL","University of Illinois at Urbana-Champaign","Continuing grant","Bogdan Mihaila","09/30/2019","$2,383,226.00","","kale@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1253, 1712, 2878, 7231, 8004, 8084","7433, 7569, 8009, 8084, 9216, 9263","$0.00","Computer simulation plays a central role in helping us understand, predict, and engineer the physical and chemical properties of technological materials systems such as semiconductor devices, photovoltaic systems, chemical reactions and catalytic behavior. Despite significant progress in performing realistic simulations in full microscopic detail, some problems are currently out of reach: two examples are the modeling of electronic devices with multiple functional parts based on new materials such as novel low power computer switches that would revolutionize the Information Technology industry, and the photovoltaic activity of complex interfaces between polymers and inorganic nanostructures that would enhance US energy self-reliance.  The research program of this collaborative software institute aims to create an open and effective scientific software package that can make efficient use of cutting-edge high performance computers (HPC) to solve challenging problems involving the physics and chemistry of materials.  By having such software available, this software initiative will have multiple broad impacts.  First, the community of materials scientists will be able to study next-generation problems in materials physics and chemistry, and computer science advances that enable the software will be demonstrated and made accessible for both communities which will help cross-fertilize further such collaborative efforts.  Second, the capability of simulating and engineering more complex materials systems and technological devices could play a role in helping the US continue is competitive edge in science, technology, and education. Third, through training of young scientists, direct outreach to the broader scientific community through workshops and conferences, and educational programs ranging from secondary to graduate levels, the power, importance, and capabilities of computational modeling, materials science, and computer science methodologies that enable the science will be communicated to a broad audience.  Finally, by enabling the refinement of existing materials systems as well as discovery of new materials systems, the resulting scientific advances can help broadly impact society via technological improvements: in terms of the two examples provided above, (a) the successful design of new electronic device paradigms helps significantly advance the digital revolution by permitting the introduction of smaller, more efficient, and more capable electronic circuits and information processing systems, and (b) successful creation of inexpensive, easy-to-fabricate, and durable photovoltaic materials and devices can lead to cleaner forms of energy production while reducing reliance on fossil fuels.<br/><br/>The technical goal is to greatly enhance the open software tool OPENATOM to advance discovery in nanoscience and technology. OPENATOM will be delivered as a open, robust and validated software package capable of utilizing HPC architectures efficiently to describe the electronic structure of complex materials systems from first principles.  In terms of describing electronic ground-states, OPENATOM will be enhanced by features such as improved configurational sampling methods, hybrid density functionals, and incorporation of fast super-soft pseudopotential techniques. In addition, the team will incorporate the many-body GW-BSE approach for electronic excitations that permits accurate computation of electronic energy levels, optical absorption and emission, and luminescence.  Ultimately, such an extensible software framework will permit accurate electronic structure computations to employ effectively future HPC platforms with 10,000,000 cores."
"1339782","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","OAC","PHYSICAL OCEANOGRAPHY, SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF, Software Institutes, EarthCube","10/01/2014","08/26/2014","Hartmut Kaiser","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Bogdan Mihaila","09/30/2019","$970,835.00","Robert Twilley","hkaiser@cct.lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1610, 1714, 2878, 8004, 8074","7433, 8009, 9150","$0.00","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience."
"1339801","SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling","OAC","OFFICE OF MULTIDISCIPLINARY AC, PHYSICAL OCEANOGRAPHY, SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF, Software Institutes, CDS&E-MSS, EarthCube","10/01/2014","02/27/2015","Clinton Dawson","TX","University of Texas at Austin","Standard Grant","Bogdan Mihaila","09/30/2019","$540,012.00","Craig Michoski","clint@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1253, 1610, 1714, 2878, 8004, 8069, 8074","7433, 8009, 8251","$0.00","The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.<br/>The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience."
"1835452","Collaborative Research: Frameworks: Software: Future Proofing the Finite Element Library Deal.II -- Development and Community Building","OAC","Software Institutes","10/01/2018","09/26/2018","Timo Heister","SC","Clemson University","Standard Grant","Stefan Robila","03/31/2019","$700,000.00","","heister@sci.utah.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","8004","026Z, 077Z, 7925, 8004","$0.00","Partial differential equations (PDEs) are used as mathematical models throughout the natural sciences, engineering, and more recently also in the biomedical and social sciences as well as in finance. Their numerical solution is, consequently, of great relevance in understanding, accurately simulating, and optimizing natural, human, and engineered systems. In many applications, finite element methods (FEM) are the method of choice converting the PDE into finite dimensional, computationally solvable problems. The deal.II project is an open source FEM software library that enables scientists to solve PDEs across many disciplines, that supports simulation and computational discovery in virtually all parts of the sciences and engineering by providing tools to solve essentially all PDEs amenable to the FEM. In this project new capabilities will be added and the user and contributor community expanded to include additional science domains.<br/><br/>Deal.II is a project with a thriving, world-wide user and developer community. This project will further enable its community of users and developers, by undertaking specifically for work that can either not be expected of volunteers, or that is necessary to strengthen the long-term independent sustainability of the project. Based on a recent user survey, the following work items in the following four categories will be addressed: 1. Foundational features too large or complicated to be tackled by volunteers: the team will research and implement efficient and scalable approaches to support parallel, adaptive multigrid and hp FEM. 2. Expand documentation and training modules through more tutorial programs and YouTube-hosted video lectures: This will further broaden the reach of the project and extend the education for the computational science community. 3. Continuous integration and packaging infrastructure to better support the pace of development. 4. Support and expand deal.II's thriving communities through a summer school, workshops, hackathons, and careful mentoring of newcomers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836997","Workshop Proposal:  Rethinking NSF's Computational Ecosystem for 21st Century Science and Engineering","OAC","ETF","05/15/2018","05/15/2018","Daniel Reed","IA","University of Iowa","Standard Grant","Robert Chadduck","05/31/2019","$26,645.00","","dan-reed@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7476","7556","$0.00","For nearly four decades, NSF has effectively supported the broad availability and innovative uses of leading edge computational resources to accelerate advances in science and engineering. These NSF investments have spanned discipline-specific instruments and facilities; computational systems of varying capabilities and architectures optimized for different applications; virtual organizations for allocating resources and interfacing with users; and network backbones connecting and providing access to these resources. Recent profound advances across science and engineering interests and priorities, as well as in enabling technologies, contribute to the current view of cyberinfrastructure supporting research as appropriately embodying a model of an ""ecosystem"" constituting interconnected, appropriately synergistically evolving, elements.<br/><br/>The objective of this workshop is to further foster and promote the NSF supported cyberinfrastructure ecosystem, including its makeup of interconnected, appropriately evolving, elements, to continue to be both motivated by scientific interests and priorities, as well as to evolve responsively to an increasing pace of technologies developments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1560385","REU SIte: Engineering Modeling and Computational Research","OAC","RSCH EXPER FOR UNDERGRAD SITES, HUMAN RESOURCES DEVELOPMENT","04/01/2016","03/09/2016","Bala Ram","NC","North Carolina Agricultural & Technical State University","Standard Grant","Sushil K Prasad","12/31/2019","$359,979.00","Marcia Williams","ram@ncat.edu","1601 E. Market Street","Greensboro","NC","274110001","3363347995","CSE","1139, 1360","116E, 2886, 9102, 9178, 9250","$0.00","Non-Technical:<br/>This project will offer a ten-week summer program focused on computational models in engineering for ten undergraduate students in each year of the project. Computer-based models or computational models are used to understand and analyze natural and engineered systems, and are indispensable in modern science and engineering. Computational models are now widely-accepted, along with theory and experiment, as a crucial third mode of scientific research and engineering design. The undergraduate research experience outlined in this project provides the participants a skill set in contemporary engineering research that is sought after. A significant part of the skills acquired will be applicable to interdisciplinary research in academic, corporate, and federal laboratories. This skills-transferability, along with the opportunity for the participants to observe disparate research among their cohort will be a unique aspect of this REU. A large proportion of the students will be drawn from universities without graduate programs in engineering. Due to the broad applicability of computational modeling, the research experience gained, computing skills developed, and professional development provided during this research experience will prepare the students for a variety of paths to graduate education. This project will serve the mission of the National Science Foundation by training a diverse scientific workforce in the vital area of computational modeling. <br/><br/>Technical:<br/>The College of Engineering at North Carolina A&T State University houses a Department of Computation Science and Engineering that offers only graduate programs and has affiliate faculty from several departments in the campus including engineering, math, and sciences. The department has extensive computational facilities that will be available to this REU program. This summer REU program will begin with a comprehensive exposure in engineering modeling methods, MATLAB computing software, and computational environments including visualization tools. Following the introductory short course, students will work on a chosen project with one of seven mentors. The research experience will involve modeling and computational tools and techniques applied to architectural, bio-, civil, industrial, and mechanical engineering problem domains. Examples of research topics are: shock motion in supersonic vehicles, hydrologic simulation, biomechanics, humanitarian logistics, analysis of soft gels, energy efficiency of buildings, and non-destructive testing. The program will offer career development workshops, visits to corporate and federal computational laboratories, and humanitarian outreach events that will provide key enriching experiences to the participants. The key broader impact of the project will be the mentored training of students in regional institutions without graduate programs in computational modeling."
"1341698","Gateways to Discovery: Cyberinfrastructure for the Long Tail of Science","OAC","ETF, EQUIPMENT ACQUISITIONS","10/01/2013","08/30/2018","Michael Norman","CA","University of California-San Diego","Cooperative Agreement","Edward Walker","03/31/2021","$27,313,477.00","Chaitanya Baru, Philip Papadopoulos, Amitava Majumdar, Richard Moore, Nancy Wilkins-Diehr, Robert Sinkovits, Shawn Strande","mlnorman@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7476, 7619","7619, 9251","$0.00","The University of California at San Diego will provide a ground-breaking new computing facility, Wildfire, that will be made available to the research community to both well established users of high end computing (HEC) and especially to new user communities that are less familiar with how HEC can advance their scientific and engineering goals.  <br/>The distinguishing features of Wildfire are:<br/>(i) Deliver 1.8-2.0 Petaflop/s of long sought capacity for the 98% of XSEDE jobs (50% of XSEDE core hours) that use fewer than 1,000 cores and also support larger jobs.  The exact number will depend on the speed of the processor being delivered by Intel but cannot be less that 1.8 Petaflop/s.<br/>(ii) Provide 7 PB of Lustre-based Performance Storage at 200 GB/s bandwidth for both scratch and allocated storage as well as 6 PB of Durable Storage<br/>(iii) Ensure high throughput and responsiveness using allocation/scheduling using proven policies on earlier deployed systems such as Trestles and Gordon<br/>(iv) Establish a rapid-access queue to provide new accounts within one day of the request<br/>(v) Enable community-supported custom software stacks via virtualization for communities that are unfamiliar with HPC environments.  These virtual clusters will be able to perform at or near native InfinBand bandwidth/latency<br/><br/>Wildfire will provide novel approaches for resource allocation, scheduling, and user support, queues with quicker response for high-throughput computing, medium-term storage allocations, virtualized environments with customized software stack, dedicated allocations of physical/virtual machines, support for Science Gateways and bandwidth reservations on high-speed networks.  Wildfire has been designed to efficiently serve the 98% of XSEDE jobs that need fewer than 1,000 cores, while also supporting larger jobs. The award leverages but also enhances the services available through the XSEDE project. <br/><br/>The Wildfire acquisition will work to increase the diversity of researchers able to effectively make use of advanced computational resources and establish a pipeline of potential users through virtualization, science gateways and educational activities focused on the undergraduate, graduate and post-graduate levels."
"1835909","Elements: libkrylov, a Modular Open-Source Software Library for Extremely Large Eigenvalue and Linear Problems","OAC","Software Institutes","09/01/2018","08/28/2018","Filipp Furche","CA","University of California-Irvine","Standard Grant","Bogdan Mihaila","08/31/2021","$599,852.00","","filipp.furche@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","8004","026Z, 077Z, 7923, 8004, 8005, 9216","$0.00","Strongly coupled linear equation systems or eigenvalue problems with extremely large numbers of unknowns are a critical bottleneck for computational solutions to many grand challenges in science and engineering. For example, computational design of light emitting or photovoltaic materials from first principles requires the solution of tens of millions of strongly coupled linear equations within minutes to be practical. This project aims to develop, implement, test, and deploy libkrylov, a robust, efficient, and general open-source library of ""on-the-fly"" Krylov space methods suitable for solving such extremely large, dense problems. libkrylov will deliver the latest innovations in Krylov-space methods to the scientific and engineering communities by providing a uniform, reproducible, and user-friendly software standard. Coupled with electronic structure codes, the library will enable large-scale simulations of molecular time-dependent X-ray absorption spectra of organometallic and bio-inorganic systems. This project will promote computational literacy through student training and workforce education at University of California, Irvine and San Diego State University, and enhance national software infrastructure through collaboration with the NSF-funded Molecular Sciences Software Institute (MolSSI) in Blacksburg, VA.<br/><br/>The PI and his group have recently developed nonorthonormal Krylov space methods for solving extremely large dense eigenvalue and linear problems ""on-the-fly"", i.e., without explicit storage or access of coefficient matrices, with demonstrated efficiency and stability. This project aims to transform this methodology into robust, efficient, and sustainable software infrastructure freely accessible to the public. Key features include (i) unique capability to solve extremely large problems, (ii) a highly flexible interface to matrix-vector multiplication ""engines"" (iii) ultrahigh efficiency by minimizing the number and cost of matrix-vector multiplications, (iv) outstanding robustness by dynamic control of errors and condition, and stabilization methods, (v) versatility by exploiting symmetry and special structure, real and complex arithmetic, (vi) configurable precision, convergence control, preconditioning, memory and disk usage, (vii) portability to broad range of platforms, environments, and languages, (viii) flexible and user-friendly generic interfaces, documentation, and testing capabilities, (ix) extensibility through object orientation and modularity, (x) reproducibility through a dedicated test suite, (xi) community involvement and sustainability by collaboration with MolSSI and deployment of a public issue and feature request tracker.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1638283","CRISP Type 1/Collaborative Research: A Human-Centered Computational Framework for Urban and Community Design of Resilient Coastal Cities","OAC","CRISP - Critical Resilient Int","01/01/2017","08/29/2016","Walid Saad","VA","Virginia Polytechnic Institute and State University","Standard Grant","William Miller","12/31/2019","$199,994.00","Anamaria Bukvic","walids@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","027Y","008Z, 029E, 036E, 039E","$0.00","Coastal cities play a critical role in the global economy. However, they are being increasingly exposed to natural hazards and disasters, such as hurricanes, and recurrent flooding due to the rise of sea-levels caused by climate change. These disasters directly impact critical coastal infrastructure such as the energy, transportation, water, and sewer systems as well as streets, buildings and houses of coastal cities, thus adversely affecting the safety and well-being of their residents. The goal of this research is to create new paradigms for the resilient design of urban communities, and uniquely tailored toward the design of coastal cities, thus contributing to NSF's science and engineering mission. Results from this research will help make critical coastal infrastructures more tolerant to damage. The in turn will foster socio-economic resilience by enabling anticipatory interventions. The developed techniques and simulation models will redefine traditional urban design strategies through the integration of architecture, urban design, land-use planning, civil engineering, and advanced computational methods that explicitly consider socio-economic drivers. This project will be conducted in close collaboration with the cities of Miami and Miami Beach. In addition to these collaborations serving as as case studies for the proposed research, the research will directly and tangibly benefit high-risk coastal urban centers by providing them with clear, context-specific recommendations with respect to implementing resiliency. Broad dissemination efforts will be undertaken via a series of seminars for decision-makers and practitioners within the cities of Miami and Miami Beach. An exposition at the Miami Museum of Science will be organized to raise awareness and promote research on resiliency. The project will involve students via direct engagement in the research as well as via new learning modules that will integrate research findings into the existing curriculum. The proposed educational plan will thus help train a new workforce that is skilled in STEM disciplines, in general, and adept in resiliency planning of coastal cities, in particular. In addition to serving NSF's science mission, therefore, this project also serves its education mission.<br/><br/>This transformative research will introduce a novel methodological approach that symbiotically integrates urban design and socio-economic considerations into an advanced simulation and optimization framework to enhance the resilience of a coastal city's critical infrastructure. This human-centered computational framework will help identify key resilient infrastructures, and design and land use patterns that will increase the damage tolerance of coastal cities while reducing the socio-economic impacts of coastal hazards and disasters. The proposed approach will bring together an interdisciplinary set of collaborators from engineering, architecture, and social sciences, to yield several key innovations: 1) a holistic human-centered computational framework for the design of resilient cities; 2) identification of key typologies, morphologies and their interdependencies by analyzing the urban design and its infrastructure networks; 3) an innovative flexible modeling and computational framework that integrate socio-economic characteristics for simulation and resilience optimization (damage tolerance) of the critical infrastructure; 4) a novel optimization framework that will facilitate making damage tolerance decisions that can achieve anticipatory resilience in face of disaster uncertainty; and 5) new identified interdependences, trends, and typologies of socio-economic system of highly-urbanized coastal communities based on the cities of Miami and Miami Beach in Florida. In summary, the proposed research will lay the scientific foundation for envisioning and redesigning resilient coastal cities making them ready to meet anticipated future challenges."
"1638336","CRISP Type 1/Collaborative Research: A Human-Centered Computational Framework for Urban and Community Design of Resilient Coastal Cities","OAC","CRISP - Critical Resilient Int","01/01/2017","08/29/2016","Landolf Rhode-Barbarigos","FL","University of Miami","Standard Grant","William Miller","12/31/2019","$299,579.00","Wangda Zuo, Sonia Chao","landolfrb@miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","027Y","008Z, 029E, 036E, 039E","$0.00","Coastal cities play a critical role in the global economy. However, they are being increasingly exposed to natural hazards and disasters, such as hurricanes, and recurrent flooding due to the rise of sea-levels caused by climate change. These disasters directly impact critical coastal infrastructure such as the energy, transportation, water, and sewer systems as well as streets, buildings and houses of coastal cities, thus adversely affecting the safety and well-being of their residents. The goal of this research is to create new paradigms for the resilient design of urban communities, and uniquely tailored toward the design of coastal cities, thus contributing to NSF's science and engineering mission. Results from this research will help make critical coastal infrastructures more tolerant to damage. The in turn will foster socio-economic resilience by enabling anticipatory interventions. The developed techniques and simulation models will redefine traditional urban design strategies through the integration of architecture, urban design, land-use planning, civil engineering, and advanced computational methods that explicitly consider socio-economic drivers. This project will be conducted in close collaboration with the cities of Miami and Miami Beach. In addition to these collaborations serving as as case studies for the proposed research, the research will directly and tangibly benefit high-risk coastal urban centers by providing them with clear, context-specific recommendations with respect to implementing resiliency. Broad dissemination efforts will be undertaken via a series of seminars for decision-makers and practitioners within the cities of Miami and Miami Beach. An exposition at the Miami Museum of Science will be organized to raise awareness and promote research on resiliency. The project will involve students via direct engagement in the research as well as via new learning modules that will integrate research findings into the existing curriculum. The proposed educational plan will thus help train a new workforce that is skilled in STEM disciplines, in general, and adept in resiliency planning of coastal cities, in particular. In addition to serving NSF's science mission, therefore, this project also serves its education mission.<br/><br/>This transformative research will introduce a novel methodological approach that symbiotically integrates urban design and socio-economic considerations into an advanced simulation and optimization framework to enhance the resilience of a coastal city's critical infrastructure. This human-centered computational framework will help identify key resilient infrastructures, and design and land use patterns that will increase the damage tolerance of coastal cities while reducing the socio-economic impacts of coastal hazards and disasters. The proposed approach will bring together an interdisciplinary set of collaborators from engineering, architecture, and social sciences, to yield several key innovations: 1) a holistic human-centered computational framework for the design of resilient cities; 2) identification of key typologies, morphologies and their interdependencies by analyzing the urban design and its infrastructure networks; 3) an innovative flexible modeling and computational framework that integrate socio-economic characteristics for simulation and resilience optimization (damage tolerance) of the critical infrastructure; 4) a novel optimization framework that will facilitate making damage tolerance decisions that can achieve anticipatory resilience in face of disaster uncertainty; and 5) new identified interdependences, trends, and typologies of socio-economic system of highly-urbanized coastal communities based on the cities of Miami and Miami Beach in Florida. In summary, the proposed research will lay the scientific foundation for envisioning and redesigning resilient coastal cities making them ready to meet anticipated future challenges."
"1740300","Collaborative Research:   SI2-SSE:   An open source multi-physics platform to advance fundamental understanding of plasma physics and enable impactful application of plasma systems","OAC","PLASMA PHYSICS, OFFICE OF MULTIDISCIPLINARY AC, SPECIAL INITIATIVES, Software Institutes","09/01/2017","08/30/2017","Steven Shannon","NC","North Carolina State University","Standard Grant","Vipin Chaudhary","08/31/2019","$160,000.00","","scshanno@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","1242, 1253, 1642, 8004","004Z, 026Z, 1062, 7433, 7569, 8004, 8005, 8396","$0.00","As the world moves toward a more sustainable life cycle for vital resources, new techniques for the synthesis, modification, or remediation of materials will be needed. Techniques that utilize plasma discharges will make significant contributions to a more sustainable nexus spanning food, water, and energy. To advance the fundamental understanding of these plasma-based systems and how they interact with the materials that will drive this higher level of sustainability, the ability to simulate both the complex interactions within the plasma itself and the complex interaction of the plasma with surrounding materials is needed. This project will provide a powerful simulation platform to the scientific community that will enable the study of plasma chemistry formation and plasma material interaction with a level of fidelity that is not currently available to researchers around the world. The open-source framework for this platform will enable researchers from institutions around the world to contribute to the capabilities of this framework and advance the underlying science of these systems to move toward a more sustainable food, energy, and water nexus.<br/><br/><br/>To advance plasma-based technology that will enable greater sustainability in the future food, energy, and water nexus, there exists an overarching need for advances in simulation capability that address four unifying research challenges, 1.) Plasma Produced Selectivity in Reaction Mechanisms in the Volume and on Surfaces, 2.) Interfacial Plasma Phenomena, 3.) Multiscale, Non-Equilibrium Chemical Physics, and 4.) Synergy and Complexity in Plasmas. This research effort will expand, deploy, and support a powerful open-source multi-physics platform that will enable advanced simulation in these unifying research areas. A plasma science simulation application will be expanded to include complex multi-phase chemistries, multiple-domain simulation of the interface between plasmas and other material phases, and fully coupled electro-magnetic treatment of plasma systems that will link plasma formation mechanisms with underlying chemical and electrical multi-phase interactions. Zapdos will be supported on the existing multi-physics Object Oriented Simulation Environment (MOOSE) and will leverage the existing support, verification, revision tracking, and training infrastructure and best known methods employed by both MOOSE and the 22 developed applications (including Zapdos) that currently reside on the MOOSE framework. This proposal will leverage collaboration not only between the two partnering universities, but with framework developers (Idaho National Laboratory), existing users (Oak Ridge National Laboratory), and the broader plasma community (APS Topical Meeting on Gaseous Electronics) to develop efficient development, deployment, support, and training of this impactful simulation tool.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Physics Division and the Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences, and the Division of Chemical, Bioengineering, Environmental, and Transport Systems in the Directorate of Engineering."
"1835267","Elements: Software: NSCI: A Quantum Electromagnetics Simulation Toolbox (QuEST) for Active Heterogeneous Media by Design","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/15/2018","09/06/2018","Carlo Piermarocchi","MI","Michigan State University","Standard Grant","Bogdan Mihaila","08/31/2021","$563,340.00","Shanker Balasubramaniam","carlo@pa.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1712, 8004","026Z, 053Z, 077Z, 7923, 7926, 8004, 8990, 9216, 9263","$0.00","Designing novel optical materials with enhanced properties would impact many areas of science and technology, leading to new lasers, better components for photonics, and to a deeper understanding of how light interacts with matter. This project will develop software that simulates how light would propagate in yet to be made complex optical materials. The final product will be a software toolbox that computes the dynamics of each individual light emitter in the materials rather than calculating an average macroscopic field. This toolbox will permit the engineering and optimization of optical properties by combining heterogeneous components at the nanoscale. The software will be disseminated widely to enable scientists worldwide to conduct research on this area and will provide a blueprint for broader applications to magnetic materials and ultrasound acoustics. Two graduate students will be engaged in this research and will be trained in interdisciplinary topics encompassing fundamental physics, mathematics, materials science, and software engineering.<br/><br/>Three innovative modules will be implemented and tested in the software toolbox: (i) A module based on Time Domain Accelerated Integrated Methods. These methods rely on the separation into near field and far field terms in the interaction between optically active centers and introduce a hierarchical structure that can be computationally exploited. This module will dramatically reduce computational costs, leading to simulations of realistic systems with millions of optical emitters. (ii) A stochastic optimization module that maximizes materials functionalities based on geometrical and compositional distribution of the emitters in the medium. This optimization module will simulate in parallel several virtual samples and will guide the computational effort towards optimal materials. (iii) A rationalized representation of electromagnetic field localization based on the novel mathematical concept of landscape functions, which effectively reduces the eigenvalue problem associated to localization into a static boundary condition problem. This computationally efficient approach provides approximated eigenvalues and quickly identifies the sub-regions of the system that support electromagnetic field localization.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1415399","SEES Fellows: Sustainable Infrastructure in a Changing Climate","OAC","EDUCATION AND WORKFORCE, SEES Fellows","09/01/2014","08/25/2014","Anne Marie Stoner","TX","Texas Tech University","Standard Grant","Sushil K Prasad","08/31/2019","$419,446.00","","anne.stoner@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","7361, 8055","7231, 8232","$0.00","The project is supported under the NSF Science, Engineering and Education for Sustainability Fellows (SEES Fellows) program, with the goal of helping to enable discoveries needed to inform actions that lead to environmental, energy and societal sustainability while creating the necessary workforce to address these challenges. Sustainability science is an emerging field that addresses the challenges of meeting human needs without harm to the environment, and without sacrificing the ability of future generations to meet their needs. A strong scientific workforce requires individuals educated and trained in interdisciplinary research and thinking, especially in the area of sustainability science. With the SEES Fellowship support, this project will enable a promising early career researcher to establish herself in an independent research career related to sustainability.<br/> <br/>This project addresses the need of incorporating climate trends and projections to ensure the sustainability of long-term planning in the infrastructure sector. For infrastructure in particular (e.g., roads, bridges, culverts, and storm drainage systems), future change may shift many of these risks beyond historical and even current design standards. Modifying existing infrastructure and ensuring new construction will be adapted to future change is a central and critical concern surrounding the issues of infrastructure resiliency and sustainable infrastructure design. The proposed work will combine state-of-the-art models and tools in climate change research with engineering know-how to help design stronger and longer-lasting infrastructure for the future. Advances in both high-resolution climate modeling and implementation of climate projections in infrastructure engineering will enhance collaboration efforts within the ICNet RCN and significantly contribute towards the science of sustainable engineering under changing environmental conditions. This proposal aims to reduce the knowledge gap between climate scientists and engineers and to address specific challenges that climate change poses to infrastructure engineering. It will explore the ways in which climate information can be used to inform design standards to build better bridges and roads that will last longer in the future and require less money and energy to maintain."
"1620896","Scientific Software Days Conference","OAC","INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE, Software Institutes","02/15/2016","02/11/2016","Damon McDougall","TX","University of Texas at Austin","Standard Grant","Stefan Robila","01/31/2020","$36,000.00","","damon@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1640, 7361, 8004","7433, 7556","$0.00","Technology and science have a wide influence on our lives. From the discovery of new medical treatments to the simulation of weather and climate systems, novel scientific research directly impacts the health, well-being and security of the nation. Computer systems, and the software that runs on them, serve as a vessel to execute this research. As a result, software is now a fundamental research tool that advances science. Software must be developed sustainably with adequate maintenance, testing and growth for the purposes of ensuring the reproducibility of scientific results and to promote the growth of science. This conference brings together experts across varying scientific disciplines to share best practices for the development of scientific software. Furthermore, this conference aims to reach the next generation of scientists and researchers that develop software, serving not only as an educational experience but also as an opportunity to see how to leverage open source contributions for the purposes of career development. Since the practice of developing software sustainably is applicable to all scientific domains, this conference will have a broader impact on science as a whole. Moreover, reaching an audience of earlier-career researchers has the twofold benefit of a) being an educational experience; and b) helping solidify and promote sustainable software development for the future.<br/><br/>21st century research increasingly relies on the development of ever more sophisticated scientific software. Projects such as R, Trilinos, PETSc, and IPython have been adopted in a manifold number of disciplines and now are indispensable tool sets. The goals of the Scientific Software Days (SSD) conference are twofold: (1) To bring together scientific software communities to share best practices; (2) Share information on the latest technologies of interest to communities. For both graduate students and early career academics interested in scientific software, SSD provides a natural venue to interact with scientific software leaders from three core areas: universities, research labs, and industry. In all three areas, the combination of deep domain expertise and advanced software development skills is in high demand. This conference will be held in the Peter O'Donnell Jr. Building (POB) at the University of Texas at Austin, February 25-26, 2016. The meeting will include invited talks from both young and established developers and scientists, a poster session to showcase the work of junior researchers, as well as a panel discussion on the challenges and opportunities for developing and supporting scientific software in each of the three main career paths. Funding is included in the budget for travel and accommodations for poster session participants to support those who may not have another source of funding, and an effort will also be made to advertise to and promote the inclusion of members of underrepresented groups in these participants."
"1740310","Collaborative Proposal:  SI2-SSE:  An open source multi-physics platform to advance fundamental understanding of plasma physics and enable impactful application of plasma systems","OAC","PLASMA PHYSICS, OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2017","08/30/2017","Davide Curreli","IL","University of Illinois at Urbana-Champaign","Standard Grant","Vipin Chaudhary","08/31/2019","$160,001.00","","dcurreli@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1242, 1253, 8004","004Z, 026Z, 1062, 7433, 7569, 8004, 8005, 8396","$0.00","As the world moves toward a more sustainable life cycle for vital resources, new techniques for the synthesis, modification, or remediation of materials will be needed. Techniques that utilize plasma discharges will make significant contributions to a more sustainable nexus spanning food, water, and energy. To advance the fundamental understanding of these plasma-based systems and how they interact with the materials that will drive this higher level of sustainability, the ability to simulate both the complex interactions within the plasma itself and the complex interaction of the plasma with surrounding materials is needed. This project will provide a powerful simulation platform to the scientific community that will enable the study of plasma chemistry formation and plasma material interaction with a level of fidelity that is not currently available to researchers around the world. The open-source framework for this platform will enable researchers from institutions around the world to contribute to the capabilities of this framework and advance the underlying science of these systems to move toward a more sustainable food, energy, and water nexus.<br/><br/><br/>To advance plasma-based technology that will enable greater sustainability in the future food, energy, and water nexus, there exists an overarching need for advances in simulation capability that address four unifying research challenges, 1.) Plasma Produced Selectivity in Reaction Mechanisms in the Volume and on Surfaces, 2.) Interfacial Plasma Phenomena, 3.) Multiscale, Non-Equilibrium Chemical Physics, and 4.) Synergy and Complexity in Plasmas. This research effort will expand, deploy, and support a powerful open-source multi-physics platform that will enable advanced simulation in these unifying research areas. A plasma science simulation application will be expanded to include complex multi-phase chemistries, multiple-domain simulation of the interface between plasmas and other material phases, and fully coupled electro-magnetic treatment of plasma systems that will link plasma formation mechanisms with underlying chemical and electrical multi-phase interactions. Zapdos will be supported on the existing multi-physics Object Oriented Simulation Environment (MOOSE) and will leverage the existing support, verification, revision tracking, and training infrastructure and best known methods employed by both MOOSE and the 22 developed applications (including Zapdos) that currently reside on the MOOSE framework. This proposal will leverage collaboration not only between the two partnering universities, but with framework developers (Idaho National Laboratory), existing users (Oak Ridge National Laboratory), and the broader plasma community (APS Topical Meeting on Gaseous Electronics) to develop efficient development, deployment, support, and training of this impactful simulation tool.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering, the Physics Division and the Office of Multidisciplinary Activities in the Directorate of Mathematical and Physical Sciences, and the Division of Chemical, Bioengineering, Environmental, and Transport Systems in the Directorate of Engineering."
"1726023","MRI:  Acquisition of Cutting-Edge GPU and Phi Nodes for the Interdisciplinary UMBC High Performance Computing Facility","OAC","MAJOR RESEARCH INSTRUMENTATION","09/01/2017","03/15/2019","Meilin Yu","MD","University of Maryland Baltimore County","Standard Grant","Stefan Robila","08/31/2020","$552,353.00","Marc Olano, Jianwu Wang, Meilin Yu, Meilin Yu, Daniel Lobo","mlyu@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","1189","1189","$0.00","This project will expand the interdisciplinary University of Maryland Baltimore County (UMBC) High Performance Computing Facility (HPCF), the community-based, interdisciplinary core facility for scientific computing and research on parallel algorithms at UMBC.  The expansion will support the research projects of 51 researchers from 13 academic departments and research centers across the entire campus, including the areas of Computer Science, Information Systems, Mathematics, Statistics, Physics, Biology, Chemistry, Marine Biotechnology, Environmental Systems, Engineering (Computer, Electrical, Mechanical, Chemical, and Environmental), and research centers focused on environmental research, earth sciences, and imaging research.<br/><br/>Specifically, the expanded computational facility will comprise a total of 84 compute nodes including cutting-edge NVIDIA GPU accelerators and Intel Xeon Phi KNL processors. The availability of the new resource will give researchers at UMBC the opportunity to increase scientific discovery significantly through the dramatic speedup in their simulation and modeling activities from state-of-the-art CPUs and cutting-edge GPUs and Phi KNL processors. An existing cluster at HPCF has already attracted a broad user base through a winning combination of sufficient hardware, tight integration of student education, freely available user support, and an appropriate usage policy. <br/><br/>Moreover, the new expanded resources of HPCF will enabled UMBC to develop a powerful synergy between research and education at all levels. Through the project's consulting approach to user support, application researchers and their post-docs, graduate students, and undergraduate students will be exposed to the power of state-of-the-art computing software and hardware, a crucial experience for the future workforce. Synergistic integration of education and research is concretely exemplified by current NSF-funded initiatives at UMBC, including an REU Site on high performance computing, a proposed REU Site in quantitative biology, proposed CyberTraining initiatives, and a growing number of courses that use HPCF. HPCF also actively partners with other efforts on campus, such as the UMBC Meyerhoff Scholarship and the NIH-funded MARC programs, two nationally recognized programs that attract substantial numbers of students from underrepresented groups into the sciences."
"1559978","REU Site: High Performance Computing in STEM Disciplines at South Dakota State University","OAC","RSCH EXPER FOR UNDERGRAD SITES, WORKFORCE IN THE MATHEMAT SCI","03/01/2016","03/09/2016","Stephen Gent","SD","South Dakota State University","Standard Grant","Sushil K Prasad","02/29/2020","$356,290.00","Jung-Han Kimn","stephen.gent@sdstate.edu","1015 Campanile Ave","Brookings","SD","570070001","6056886696","CSE","1139, 7335","116E, 1800, 9102, 9150, 9178, 9250","$0.00","Part 1:<br/>The health of our nation's economy depends on the continued growth of a STEM workforce. The effective application of high performance computing (HPC) is a skill increasingly demanded in a widening variety of fields including engineering, biology, plant science, biochemistry, and many others. This REU program will equip underrepresented students to pursue STEM careers by training them to use HPC in mathematics and statistics, engineering, and biological sciences. Participating undergraduate students will engage in cutting-edge research to learn how high fidelity models represent physical phenomena; how these models are implemented within HPC; and how results are analyzed, interpreted, and used to resolve theoretical and applied problems. In addition, a variety of personal, interpersonal, and leadership skills (including technical communication, time management, networking, and continuous improvement) will be cultivated through complementary hands-on/minds-on activities. This project serves the national interest, as stated by NSF's mission: to promote the progress of science by engaging undergraduate students in state-of-the art research in engineering applications and computation; and to advance the national health, prosperity, and welfare by strengthening the STEM workforce and increasing the participation of underrepresented groups and veterans returning from active duty. <br/><br/>Part 2:<br/>The goals of this REU program are to enhance student capacity to: 1) conduct innovative and meaningful research using HPC, 2) learn and think independently within an interdisciplinary collaborative environment, 3) conduct research with the utmost integrity, 4) prepare for professional careers, and 5) pursue graduate degrees in STEM programs. This program will recruit ten students per year from a diverse pool of first- and second-year students from smaller colleges, tribal colleges, and community colleges, with an emphasis on both minority and traditionally underrepresented students, as well as returning veterans. These students will participate in an intensive ten-week summer program and will work closely with faculty mentors on research projects that involve 1) simulation tools in the context of engineering design and analysis; 2) state-of-the-art research tools in engineering applications and computation; 3) statistical analysis based on real datasets and simulations; and 4) advanced numerical methods including parallel algorithms in HPC. The tangible outcomes will include student-authored journal articles, conference proceedings, and technical presentations. Program success will be assessed by an external evaluator through formative and summative evaluations, daily student reflections, student interviews, and student enrollment in STEM graduate programs."
"1450871","IRNC: RXP: StarLight SDX A Software Defined Networking Exchange for Global Science Research and Education","OAC","INTERNATIONAL RES NET CONNECT","04/01/2015","03/23/2015","Joel Mambretti","IL","Northwestern University","Cooperative Agreement","Kevin Thompson","03/31/2020","$2,500,000.00","Maxine Brown, Thomas DeFanti, Jim Hao Chen","j-mambretti@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7369","5912, 5921, 5927, 7369, 7561","$0.00","The StarLight Software Defined Networking Exchange (SDX) project focuses on research, development, and deployment of services, architectures, and technologies designed to provide scientists, engineers, and educators with highly advanced, diverse, reliable, persistent, and secure networking services, enabling them to optimally access resources in North America, South America, Asia, South Asia (including India), Australia, New Zealand, Europe, Africa, and other sites around the world. The StarLight SDX ensures continued innovation and development of advanced networking services and technologies. The facility supports continued research, development, experimental deployment, and trials of multi-domain SDXs and serves as an innovation platform on which to introduce next-generation services, architectures, policies, processes, and technologies, supported by persistent experimental research networking.<br/><br/>The project focuses on three main activities: (1) development of an SDN Development Environment - the StarAX (StarLight Advanced eXchange) Innovation Engine - that enables facilities around the world to deploy SDXs that are compatible and interoperable with others; (2) deployment of an instance of this system at the StarLight facility; and, (3) coordination and collaboration with others around the world to deploy such systems at their exchange points. This project establishes an SDN Innovation Platform (software closely integrated with select hardware) to provide multiple network functions and features quickly and reliably. This StarLight initiative builds on prior successes of the StarLight International/National Communication Exchange Facility,  and operates a wide range of emerging services and capabilities, including those based on 100 Gbps channels, programmable networking, SDN, SDX techniques, and related innovations. Northwestern University, the University of California at San Diego, and the University of Illinois at Chicago are undertaking this project in partnership with other research organizations, advanced R&E networks, and science communities around the world."
"1451045","IRNC: ENgage: Building Network Expertise and Capacity for International Science Collaboration","OAC","INTERNATIONAL RES NET CONNECT","10/01/2014","08/03/2017","Steven Huter","OR","University of Oregon Eugene","Standard Grant","Kevin L. Thompson","09/30/2019","$4,359,093.00","Dale Smith, William Allen","sghuter@nsrc.org","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","7369","7369","$0.00","The Network Startup Resource Center (NSRC) develops network communications infrastructure and local engineering capacity in areas of the world where inadequate research and education network connectivity poses a significant barrier to collaborations with US scientists and educators. By helping to establish and improve underlying cyberinfrastructure, both physical (network connectivity) and human (technical capacity), NSRC plays an effective role for the US science community by helping to incubate and build sustainable Internet infrastructure to enable international scientific research.<br/> <br/>The broader impacts of NSRC's activities are global in scope. NSRC's focus on teaching and training about network design and operations, combined with technically supporting international colleagues, results in the development of stable computer networks, managed by local hands, in many countries all over the world. NSRC achieves this through targeted capacity building activities and partnerships with universities, Internet service providers, industry, government and supranational agencies in Africa, Asia-Pacific, the Middle East, Latin America-Caribbean, and North America.<br/> <br/>NSRC exemplifies the NSF's stated strategic goal of ""encouraging collaborative research and education across organizations, disciplines, sectors, and international boundaries."" Through coordinated training programs, NSRC builds institutional capacity to support the research community, and leverages US research infrastructure to bring value to international researchers and educators. Through hands-on, lab-based curricula and a train-the-trainers approach, NSRC provides technical capacity development to thousands of network engineers working in hundreds of R&E institutions to augment networking expertise in regions of interest to NSF and the International Research Network Connections (IRNC) community."
"1450280","Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","06/15/2015","06/11/2015","Volker Blum","NC","Duke University","Standard Grant","Bogdan Mihaila","05/31/2019","$1,358,608.00","Jianfeng Lu","volker.blum@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1253, 1712, 8004","7433, 8009, 8084, 9216","$0.00","Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (""in silico"") design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the ""cubic scaling wall"" of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure ""ELSI"" that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.<br/><br/>ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the ""scaling wall"" of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches."
"1440443","SI2-SSE: A Next-Generation Open-Source Computational Fluid Dynamic Code for Polydisperse Multiphase Flows in Science and Engineering","OAC","FLUID DYNAMICS, Software Institutes","10/01/2014","08/21/2014","Alberto Passalacqua","IA","Iowa State University","Standard Grant","Bogdan Mihaila","09/30/2019","$499,551.00","Rodney Fox, Simanta Mitra","albertop@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","1443, 8004","058E, 1443, 7433, 8005, 9150","$0.00","Many processes for the production of drugs, fuels and plastic materials, as well as energy from coal or biomasses, involve multiphase flows, which are composed by a combination of a fluid, either liquid or gas, and particles, droplets or bubbles. This type of flow is also naturally present in the environment. Examples are the formation of a mixture of air and solid particles due to volcanic eruptions, and particles of sand and other materials transported by the wind. Scientists and engineers use software to study how these flows behave in order to improve the yield of industrial processes, reduce their environmental impact, and energy consumption. The computer programs used to perform these studies solve complex mathematical problems, and require powerful computers to be able to obtain the results in a useful time. This project focuses on developing the next generation of computer software for the simulation of multiphase flows, enabling it to use the latest generation of computers which combine traditional and graphical processors for improved performance. This software will be released to the public and will enable, scientists and engineers from different research areas to tackle real-world problems by taking advantage of the latest developments in multiphase flow science, combined with the benefit of being able to use the software on powerful computer infrastructures. Students and educators will be able to use the software and learn about multiphase flows through the examples and the documentation that will be provided.<br/><br/>The objectives of the project will be achieved by first developing computational models to describe turbulent flows in the framework of quadrature-based moment methods, an efficient and accurate approach to describe this type of flows. These computational models will then be implemented, together with appropriate numerical methods that will ensure the accuracy of the computational codes, in the open-source framework OpenFOAM. Three representative problems of typical multiphase flows will be considered: a population balance equation for particles with negligible inertia, such as in the formation of nanoparticles in a fluid flow; the description of gas-liquid flows, where the bubble inertia is small but not zero; and the most complex case of gas-solid flows, where particle inertia is large. The possibility of using graphical processing units will be added to the OpenFOAM framework, to enable it to run on hybrid computational systems involving traditional processors and graphical processing units. The source code and its documentation will be made available to the public at an early stage of their development, under the GNU GPL 3 license, in order to disseminate the results of the research and gather feedback. Detailed code documentation, verification and validation cases, and tutorials will also be created to favor external contributions to the software."
"1835885","Elements: Software: Multidimensional Fast Fourier Transforms on the Path to Exascale","OAC","Software Institutes","10/01/2018","08/13/2018","Dmitry Pekurovsky","CA","University of California-San Diego","Standard Grant","Vipin Chaudhary","09/30/2021","$477,460.00","","dmitry@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","This project will contribute to software available freely to the public, providing solution of an important class of problems in computational science, namely the multidimensional Fast Fourier transforms (FFTs). This software will enable scientists to run certain classes of computer experiments on powerful state-of-the-art supercomputers, known as Exascale machines. A wide range of science fields will benefit from such public software; thus, it will contribute to innumerable discoveries related to fundamental understanding of nature, protecting the environment, efficient energy and transportation, designing new materials and improving medicine. It will also contribute to the computational ecosystem by virtue of being a building block that can be used and reused by other programs. This project also includes an educational effort in training a wide range of supercomputer users, from undergraduate interns to graduate/postdoctoral researchers and faculty. Effort will be made to reach out to underrepresented groups when recruiting the interns.<br/><br/>The National Strategic Computing Initiative (NSCI), in particular Strategic Objective 4 ""An Enduring National HPC Ecosystem"", outlines the crucial need for development of foundational algorithms and software for next generation supercomputers, and for easing access to the next-generation compute resources to wide classes of users. Fast Fourier Transforms (FFT) is a ubiquitous tool in scientific simulations, from Computational Fluid Dynamics to plasma physics, astrophysics, ocean modeling, materials research, medical imaging, molecular dynamics and many others. This project will fill the gap in highly efficient software for multidimensional FFTs for use on Exascale platforms. While running full FFT at exa-scale appears prohibitive due to strong interconnect bandwidth dependence, steps must be taken towards this goal due to the importance of the algorithm. Building on the previous work with P3DFFT, an open-source numerical library used by many applications in diverse science fields, this proposal aims to push the envelope in terms of adapting multidimensional FFT to new architectures and aggressively scale its performance, without losing the portability and the practical ease of use. The project will employ both novel and proven state-of-the-art tools, such as auto-tuning, overlap of communication with computation and GPU implementation, which will help reduce the bandwidth bottleneck. In addition, features will be added that will make the software appealing to a wider user base. The software will become an integral part of the National Cyberinfrastructure and will aid in numerous scientific and engineering advances, including in areas such as environmental research, energy, efficient transportation, new drugs and new materials. This project includes training of a new generation of scientific software developers through XSEDE training webinars, conference training workshops and supervision of undergraduate interns. The project plan includes reaching out to underrepresented groups when recruiting undergraduate interns.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1814225","Student Support: IEEE Cluster 2018 Conference","OAC","INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE","05/15/2018","05/10/2018","Amanda Randles","NC","Duke University","Standard Grant","Sushil Prasad","04/30/2019","$20,000.00","","amanda.randles@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1640, 7361","7556, 9102","$0.00","It is important that the STEM community identifies and develops the next generation of scientists through engagement in premier scientific conferences. The IEEE Cluster conference series, an international event for presenting the research results, problem solutions, and insights on new challenges in high performance computing in general and cluster computing in particular, has been<br/>pursuing this mission by seeking to increase student participation in the conference and the cluster computing field. To this end, the student program at the Cluster conference has been established to provide a comprehensive means for students to improve their overall research skills and planning rather than just presenting posters and papers. By attending the student program, students can now obtain unique experiences and interactions with academic and industry researchers in the cluster computing community. The overall exposure to research through the IEEE Cluster student program promotes foundational understanding of scientific methods, which will serve the students well throughout their careers, and is aligned with NSF?s mission of promoting progress of science.<br/><br/>This award supports the travel of up to 20 students from US-based institutions to participate in the student program at IEEE Cluster 2018. The student program at IEEE Cluster 2018 includes multiple sessions scheduled during the lunch breaks and after the end of the regular sessions. The sessions target research presentation training, research experience and career guidance, industry interaction, and feedback from the best paper candidates. Travel grants particularly encourage the research interests and the involvement of students in the field who are not well funded and those who are just beginning their participation in the field or are interested in entering it.<br/>Particular effort is made to solicit applications for the travel support from female students and students from under-represented communities by reaching out to graduate programs of computer science and engineering departments at universities who are members of the National Consortium for Graduate Degrees for Minorities in Engineering and Science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1842088","2018 NSF Campus Cyberinfrastructure and Cybersecurity Innovation for  Cyberinfrastructure PI Workshop","OAC","Campus Cyberinfrastrc (CC-NIE)","09/01/2018","08/22/2018","Gwendolyn Huntoon","PA","Keystone Initiative for Network Based Education and Research","Standard Grant","Kevin Thompson","08/31/2019","$90,000.00","Jennifer Leasure","huntoon@kinber.org","5775 Allentown Blvd","Harrisburg","PA","171124051","7179637490","CSE","8080","","$0.00","The 2018 Campus Cyberinfrastructure and Cybersecurity Innovation for Cyberinfrastructure PI Workshop builds upon the success of the previous Campus Cyberinfrastructure Workshops providing an opportunity for recipients of all active NSF Campus Cyberinfrastructure (CC*) and Cybersecurity Innovation for Cyberinfrastructure (CICI) awards to meet in person, exchange project findings, interact with national cyberinfrastructure experts and collaborate across project areas and project regions. By again co-locating with the Quilt Fall Member Meeting, the broader scope encourages relationships between campus cyberinfrastructure, science driven applications, cybersecurity, and regional and national cyberinfrastructure resources. Based on last year's experience, we have updated the meeting schedule to provide additional opportunities for interaction between PIs themselves as well as between the three groups participating in the meetings. New to the 2018 PI Workshop is an expanded pre-workshop set of breakout sessions half-day breakout sessions on topics relevant to academic research networking challenges facing campuses and CC* and CICI PI teams. <br/><br/>The intellectual merit of this project is the exchange of project findings, interaction between national cyberinfrastructure experts, collaboration across project areas and project regions that will lead to new ideas, relationships and collaborations associated with an in person workshop. The half-day cyberinfrastructure engineering session will provide timely insights into growing practice of cyberinfrastructure facilitation in the campus environment that can be shared with the co-located meeting attendees as well as the community in general. The broader impact associated with the workshop includes the dissemination of workshop information, including the workshop presentations. By co-locating with the Quilt meeting the workshop continues to bring together stakeholders with a vested interest in leveraging campus cyberinfrastructure investments to provide the opportunity to develop stronger ties between campus cyberinfrastructure, science driven applications and regional and national cyberinfrastructure resources, leading to a broader set of collaborations and lasting impact on national campus cyberinfrastructure and science driven applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1550486","Collaborative Research:  SI2-SSI: Sustaining Innovation in the Linear Algebra Software Stack for Computational Chemistry and Other Sciences","OAC","Software Institutes","07/15/2016","07/26/2016","Tze Meng Low","PA","Carnegie-Mellon University","Standard Grant","Micah Beck","06/30/2019","$265,000.00","","lowt@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8004","7433, 8004","$0.00","Scientific discovery now often involves computer simulation in addition to, or instead of, laboratory experimentation. This can accelerate, improve, and/or expand scientific insight, often at a great reduction in cost. Many such computer simulations spend much or most of their time solving linear algebra (matrix) problems. For these simulations, linear algebra problems constitute the most basic building blocks of the computation. As a result, software libraries (bundles of specialized code) that efficiently solve linear algebra problems fundamentally support sustained innovation in science. The project aims to create a next generation of software libraries for this domain and will make these libraries available to the scientific community as open source software that can be easily ported to current and future computer architectures. This will directly and indirectly impact discovery in academia, at the national labs, and in industry. The project will also impact affordable education through open course ware that is expected to reach a broad audience. The involvement of undergraduate and graduate students will strengthen the pool of qualified individuals trained to support scientific computing.  The project involves research staff and students who are members of traditionally underrepresented groups.<br/><br/><br/>The BLAS (Basic Linear Algebra Subprograms) are well-known routines that provide standard building blocks for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations. Because the BLAS are efficient, portable, and widely available, they are commonly used in the development of high quality linear algebra software, such as the well-known Linear Algebra PACKage (LAPACK), as an example. However, the BLAS libraries that exist today have not evolved to new computing architectures, and hence do not perform as well as they could. The technical goal and scope of this project, therefore, is to develop a new high-performance dense linear algebra library with broad functionality that can be easily ported to current and future multi-core and many-core processors. The project builds on the BLAS-like Library Instantiation Software (BLIS) effort that has exposed low-level primitives that facilitate the high-performance implementation of BLAS. By implementing the higher-level dense linear algebra functionality in terms of these low-level primitives, portable high performance will be achieved for higher-level functionality needed by many scientific computing applications. Contributions will include the to-be developed techniques for implementing such software, the resulting open source software, and pedagogical artifacts that will include open course ware."
"1540990","CC*DNI Engineer: An Engagement Model for Accelerating use and Knowledge of Cyberinfrastructure in Virtual Science Organizations at the University of Pittsburgh","OAC","Campus Cyberinfrastrc (CC-NIE)","05/15/2016","07/07/2018","Brian Stengel","PA","University of Pittsburgh","Standard Grant","Kevin Thompson","05/31/2019","$391,872.00","Christopher Keslar","bstengel@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","CSE","8080","","$0.00","The project embeds a CI Engineer into four University of Pittsburgh research centers providing a resource for cyberinfrastructure engineering on existing projects and consulting on the use of campus and national cyberinfrastructures to advance center missions and objectives.  Research centers are the nexus for campus collaborations and serve as ""virtual science organizations"" in the University.   These centers provide a rich locus for engagement and development of cyberinfrastructure use cases and best practices for the use of shared facilities and information sharing among large collaboration teams.  The selection of centers with different missions and objectives provides an excellent test bed from which to develop the role of the CI Engineer and influence future support models.   The project integrates the knowledge and experiences derived from the center engagements into a CI-Consultant curriculum for use in workforce training and student education.  Sharing of the CI-Consultant curriculum with peer institutions contributes to the development and maturation of the CI Engineer role at large."
"1550493","Collaborative Research: SI2-SSI: Sustaining Innovation in the Linear Algebra Software Stack for Computational Chemistry and Other Sciences","OAC","Software Institutes","07/15/2016","03/29/2017","Robert van de Geijn","TX","University of Texas at Austin","Standard Grant","Micah Beck","12/31/2019","$899,902.00","Don Batory, John Stanton, Victor Eijkhout, Margaret Myers","rvdg@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","7433, 8004, 8009","$0.00","Scientific discovery now often involves computer simulation in addition to, or instead of, laboratory experimentation. This can accelerate, improve, and/or expand scientific insight, often at a great reduction in cost. Many such computer simulations spend much or most of their time solving linear algebra (matrix) problems. For these simulations, linear algebra problems constitute the most basic building blocks of the computation. As a result, software libraries (bundles of specialized code) that efficiently solve linear algebra problems fundamentally support sustained innovation in science. The project aims to create a next generation of software libraries for this domain and will make these libraries available to the scientific community as open source software that can be easily ported to current and future computer architectures. This will directly and indirectly impact discovery in academia, at the national labs, and in industry. The project will also impact affordable education through open course ware that is expected to reach a broad audience. The involvement of undergraduate and graduate students will strengthen the pool of qualified individuals trained to support scientific computing.  The project involves research staff and students who are members of traditionally underrepresented groups.<br/><br/><br/>The BLAS (Basic Linear Algebra Subprograms) are well-known routines that provide standard building blocks for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations. Because the BLAS are efficient, portable, and widely available, they are commonly used in the development of high quality linear algebra software, such as the well-known Linear Algebra PACKage (LAPACK), as an example. However, the BLAS libraries that exist today have not evolved to new computing architectures, and hence do not perform as well as they could. The technical goal and scope of this project, therefore, is to develop a new high-performance dense linear algebra library with broad functionality that can be easily ported to current and future multi-core and many-core processors. The project builds on the BLAS-like Library Instantiation Software (BLIS) effort that has exposed low-level primitives that facilitate the high-performance implementation of BLAS. By implementing the higher-level dense linear algebra functionality in terms of these low-level primitives, portable high performance will be achieved for higher-level functionality needed by many scientific computing applications. Contributions will include the to-be developed techniques for implementing such software, the resulting open source software, and pedagogical artifacts that will include open course ware."
"1450372","Collaborative Research: SI2-SSI: ELSI-Infrastructure for Scalable Electronic Structure Theory","OAC","OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes","06/15/2015","06/11/2015","Lin Lin","CA","University of California-Berkeley","Standard Grant","Bogdan Mihaila","05/31/2019","$504,016.00","","linlin@math.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","1253, 1712, 8004","7433, 8009, 8084, 9216","$0.00","Predictive, so-called ab initio electronic structure calculations, particularly those based on the Kohn-Sham density functional theory (DFT) are now a widely used scientific workhorse with applications in virtually all sciences, and increasingly in engineering and industry. In materials science, they enable the computational (""in silico"") design of new materials with improved properties. In biological or pharmacological research, they provide molecular-level insights into the function of macromolecules or drugs. In the search for new energy solutions, they give molecular-level insights into new solar cell designs, catalytic processes, and many others. A key bottleneck in many applications and calculations is the ""cubic scaling wall"" of the so-called Kohn-Sham eigenvalue problem with system size (i.e., the effort increases by a factor of 1,000 if the model size increases by a factor of 10). This project will establish an open source software infrastructure ""ELSI"" that offers a common, practical interface to initially three complementary solution strategies to alleviate or overcome the difficulty associated with solving the Kohn-Sham eigenvalue problem. ELSI will enable a broad range of end user communities, centered around different codes with, often, unique features that tie a specialized group of scientists to that particular solution, to easily incorporate state-of-the-art solution strategies for a key problem they all share. By providing these effective, accessible solution strategies, we will open up major areas for electronic structure theory where DFT based predictive methodologies are not applicable today. This will in turn open doors for new development in materials science, chemistry, and all related areas. Commitments to support ELSI exist from some of the most important electronic structure developer communities, as well as from industry and government leaders in high-performance computing. Thus, we will create a strong U.S. based infrastructure that leverages the large user and developer base from a globally active community developing DFT methods for materials research.<br/><br/>ELSI will support and enhance three state-of-the-art approaches, each best suited for a specific problem range: (i) The ELPA (EigensoLvers for Petascale Applications) library, a leading library for efficient, massively parallel solution of eigenvalue problems (for small- and mid-sized problems up to several 1,000s of atoms), (ii) the OMM (Orbital Minimization Method) in a recent re-implementation, which circumvents the eigenvalue problem by focusing on a reduced, auxiliary problem (for systems in the several 1,000s of atoms range), and (iii) the PEXSI (Pole EXpansion and Selective Inversion) library, a proven reduced scaling (at most quadratic scaling) solution for general systems (for problems with 1,000s of atoms and beyond). By establishing standardized interfaces in a style already familiar to many electronic structure developers, ELSI will enable production electronic structure codes that use it to significantly reduce the ""scaling wall"" of the eigenvalue problem. First, ELSI will help them make efficient use of the most powerful computational platforms available. The target platforms are current massively parallel computers and multicore architectures, GPU based systems and future manycore processors. Second, the project will make targeted methodological improvements to ELPA, OMM, and PEXSI, e.g., a more effective use of matrix sparsity towards very large systems. The focus on similar computational architectures and similar methodological enhancements will lead to significant cross-fertilization and synergy between these approaches."
"1642443","SI2-SSE: PERTURBO: A Software for Accelerated Discovery of Microscopic Electronic Processes in Materials","OAC","DMR SHORT TERM SUPPORT, CDS&E, DMREF","10/01/2016","09/06/2016","Marco Bernardi","CA","California Institute of Technology","Standard Grant","Micah Beck","09/30/2019","$500,000.00","","bmarco@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1712, 8084, 8292","7433, 8004, 8005, 8084, 8396, 8400, 8607, 8990, 9215, 9216","$0.00","The electronic, optical, and thermal properties of materials are determined by microscopic electron collisions that take place inside materials on a time scale of a trillionth of a second (a picosecond), and are thus very hard to study experimentally. This project supports the development of a computer program, called PERTURBO, which will be used to study the dynamics of electrons in materials, with a view to understanding electron collisions. The only input needed by PERTURBO to solve the equations of quantum mechanics that control electron motion is the atomic structure of the material under study. In addition, PETURBO use innovative computational techniques to solve these equations efficiently. Thus PERTURBO may be used not just to study existing materials but also to study the next generation of more complex materials and devices. The application of PERTURBO to materials will enable disruptive advances in electronics, lighting, energy, and other technologies. Further, experimental facilities of national interest, such as the free-electron laser and other ambitious experiments to understand materials on a fundamental level, are in critical need of theoretical and computational tools to understand electron dynamics at microscopic levels. PERTURBO fills an important gap in the scientific software needed for the future development of science and technology in the United States, thus serving NSF's science mission, and places the country at the leading edge of materials and device research. Finally, the code will be freely available, user-friendly, and widely usable; users will include academic research groups, national laboratories, and industry.<br/><br/>This award supports the development of PERTURBO, a robust software platform to compute from first principles the scattering processes among electrons, phonons, defects, photons, and excitons in materials. The modular architecture of PERTURBO revolves around fast parallel routines to compute electron-phonon, electron-electron, electron-defect, and electron-photon scattering processes. The code employs novel techniques to compute and interpolate the matrix elements and converge the timescale for these scattering processes. PERTURBO imports inputs from density functional theory, density functional perturbation theory, and GW-BSE calculations, and is interfaced with routines to compute and output transport properties and ultrafast electron dynamics. For enhanced usability, a Python driver will be developed to automatically run the calculations and analyze the results. PERTURBO allows the users to quantitatively study charge, heat, and energy transport in novel materials, including semiconductors, insulators, metals, nanostructures, and interfaces, as well as ultrafast electron and excited state dynamics. The code fills&#8203; a major void in the current software ecosystem. In particular, it addresses the need to more deeply understand electron dynamics in the post-Moore's law era of electronics, as well as study ultrafast electronic processes of key importance for advanced ultrafast time-resolved spectroscopies."
"1756005","CRII:OAC: Novel techniques for improving convergence and scalability of a Monte Carlo radiation solver for large-scale combustion simulations","OAC","CRII CISE Research Initiation","03/01/2018","02/07/2018","Somesh Roy","WI","Marquette University","Standard Grant","Sushil K Prasad","02/29/2020","$174,951.00","","somesh.roy@marquette.edu","P.O. Box 1881","Milwaukee","WI","532011881","4142887200","CSE","026Y","026Z, 8228","$0.00","Combustion has been an important source of energy for ages and will continue to be so forconsiderable future.  With thehelpof high-performance computing (HPC),predictiveand accuratecombustion simulations have a tremendous potential to emerge asacost-effective andreliable design, assessment, and decision-making tool for practicalsystems (e.g., gas turbines, internal combustion engines, furnaces, etc.). Detailed predictive modeling of combustion system requires, among other things, detailed and accurate modeling of thermal radiation.  However, models for thermalradiationused in combustion simulations are usually over-simplified.  The main bottlenecks in usingdetailedradiation modelare itshighcomputational cost and poorparallel efficiencyinHPC. This project explores several novel ideas to increase efficiency and robustness of a high-fidelityradiation solverinHPCcombustionsimulations, leading to the possibility of performingpredictiveand accurate simulations ofpracticalcombustion systems in a realistic time-frame. Theability to performsuchlarge-scalereliablepredictive simulationisnot onlyimportantin the design process ofrealcombustion devicesbutalsoessential tofurther our understanding of fundamentals of combustion processes. Considering the ever-increasing need for cleaner combustion devices, this predictive capability can potentially have a significanteffectin academic research,as well asin energyandtransportation industry.  Theproject will also have an impact in popularizing computer programming in undergraduate students.  Therefore, this research aligns with the NSF'smission to promote the progress of science and to advance the national health, prosperity, and welfare.<br/><br/>The radiation solver of choice in this project is a Monte-Carlo ray tracing-based (MCRT) solver.  It is one of the most accurate radiation solver available, and typically outshines all other radiation solvers as the complexity of the problem increases. To achieve improvements inefficiency and scalabilityoftheMCRTsolverin HPCsimulationsof large-scale combustionsystems,thisresearchbrings together ideas from different disciplines of mathematics, statistical theory, and computer science and applies them to solve an engineering problem. Considering the fact thattheperformance of an MCRT solver in HPC primarily depends onthe underlying statistical algorithm andcomputationalload-balancing,thecurrent researchis divided into three primary tasks.  First, the projectisdeveloping new algorithms for improved convergence using special statistical distributions with low discrepancy.  Second, novel strategies for MCRT load management, both in terms of computational time and memory utilization, are beingexplored toimprove scalability of the solverin HPCsimulations. Third, the improved MCRT solver are planned to be created as a modular, platform-independentsolvermodulewithstandardized interfaces so that it can be used with any combustion and/or CFD solver withoutsignificant sacrifice of its performance. By enhancing efficiency and scalability of MCRT, this work aims to enable more accurate predictive HPC simulations of large-scale combustion systems in a realistic timescale.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1755779","CRII: OAC: A Hybrid Finite Element and Molecular Dynamics Simulation Approach for Modeling Nanoparticle Transport in Human Vasculature","OAC","CRII CISE Research Initiation, CAREER: FACULTY EARLY CAR DEV","03/01/2018","05/15/2018","Ying Li","CT","University of Connecticut","Standard Grant","Sushil K Prasad","02/28/2021","$182,999.00","","yingli@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","026Y, 1045","026Z, 8228, 9251","$0.00","Through nanomedicine significant methods are emerging to deliver drug molecules directly to diseased areas for cancer treatment.  Targeted drug delivery is one of the most promising approaches which relies on nanoparticles (NPs) that carry and release drugs.  The therapeutic efficacy of NP-based drug carriers is determined by the proper concentration of drug molecules at the lesion site.  NPs need to be delivered directly to the diseased tissues while minimizing their uptake by other tissues, thereby reducing the potential harm to healthy tissue.  Therefore, the design of these NPs and hence the efficacy of the targeted drug delivery could be significantly improved by understanding how the drugs carried by NPs are transported and dispersed in human body.  This project proposes a set of computational tools to model and investigate the transport and dispersion of NPs in human vasculature.  This, in turn, can provide better imaging sensitivity, therapeutic efficacy and lower toxicity of NP-based drug carriers.  The multidisciplinary nature of the project also brings together concepts from biology, engineering and computer science to educate the next generation of computational biologists, scientists and engineers. This research, thus, aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare. <br/><br/>The technical objective of this project is to create a hybrid finite element and molecular dynamics computational approach for modeling NP transport and adhesion in human vasculature.  The realistic geometry of vascular network and fluid dynamics of blood flow are accurately captured through the finite element model.  The microscopic interactions between NPs and red blood cells within blood flow and adhesion of NPs to vessel wall are resolved through the molecular dynamics simulation.  A robust and efficient coupling interface is built to couple the finite element and molecular dynamics solvers.  Specifically, this project aims to 1) create a multiscale and multiphysics computational model for predicting the vascular dynamics of NPs under the influence of realistic geometrical and physiochemical features of human vasculature; 2) craft an interface coupling technique that enhances computational accuracy and predictability by coupling the finite element and molecular dynamics solvers; 3) build testsuits for multiscale and multiphysics simulations for coupled solution error and convergence analysis; and 4) advance the current cyberinfrastructure to accelerate the material design process and enrich the cyber-enabled materials design community.  Such a computational method can be used to explore how the vascular dynamics of NPs will be affected by their size, shape, surface and stiffness properties, as well as complex geometry of human vasculature.  The simulation results can further guide experimentalists to design NP-mediated drug delivery platforms that optimally accumulate within diseased tissue to provide better imaging sensitivity, therapeutic efficacy and lower toxicity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1614853","The First Billion Years: a Petascale Universe of Galaxies and Quasars","OAC","PETASCALE - TRACK 1","05/01/2016","04/29/2016","Tiziana Di Matteo","PA","Carnegie-Mellon University","Standard Grant","Edward Walker","04/30/2019","$40,000.00","Rupert Croft","tiziana@phys.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7781","","$0.00","The Hubble Deep/Ultra Deep fields are iconic, and almost certainly the<br/>most studied space telescope observations. Upcoming observations from an enormous range of<br/>current and planned instruments, from the successor to Hubble, The James Webb Telescope, to<br/>the Large Synoptic Survey Telescope (LSST), to the WFIRST Satellite will also have the<br/>opportunity to reach galaxies and black holes at the same extreme magnitudes or deeper, but<br/>over unprecedentedly wide fields. This will open up the study of the highest redshift galaxies<br/>and quasars to the type of statistical studies that have made modern cosmology a precision<br/>science. This project aims to compute theoretical predictions to make contact with current<br/>and upcoming observations of the high-redshift universe using the Blue Waters supercomputer. <br/><br/>The project will extend the BlueTides simulation, with an unprecedented<br/>volume and resolution, to cover the evolution of the first billion years of cosmic history.<br/>The goal is to significantly increase the scientific impact of this calculation to the community.<br/>Importantly, the project will attempt to make contact<br/>with observations of quasars, which have not been discovered at redshift greater than 7 (while<br/>the simulation now has run to redshift equal 8). In addition the project will be using BlueTides as<br/>a path-finder for developing methods/calculations for future cosmological hydrodynamical simulations<br/>of galaxy formation with volumes and resolutions suitable for creating mocks for next generation<br/>surveys. The impact of the proposed work will extend way beyond BlueTides. Large hydrodynamical simulations<br/>will be more and more useful in all stages of major observational projects in astrophysics<br/>and cosmology. For example, a simulation that covers a significant fraction of the entire<br/>observable universe with BlueTides resolution and runs to the present day (an epoch which<br/>is fully dominated by the hydrodynamic computations) will be needed for LSST.<br/><br/>The project will establish a theoretical framework for understanding the role of galaxies<br/>in the evolution of the universe at high redshifts. Different communities<br/>of scientists are interested in the behavior history of quasars and galaxy assembly, including<br/>cosmologists, the galaxy evolution community and high energy astrophysicists, so the results<br/>would have a wide impact across many different scientific communities. <br/><br/>Additionally, the image and catalog generation, and<br/>database techniques developed by the project will<br/>strengthen the project already on-going synergistic activities with computer science, machine learning<br/>and statistics.  Furthermore, the project will have a strong education component by involving undergraduate<br/>and graduate students in this research.  Finally, the project propose to perform outreach using<br/>the visualization and interactive Gigapan software."
"1730417","CyberTraining: CDL: iPDC - Summer Institute for Integrating Parallel and Distributed Computing in Introductory Programming Classes","OAC","CyberTraining - Training-based","09/01/2017","07/12/2017","Sheikh Ghafoor","TN","Tennessee Technological University","Standard Grant","Sushil K Prasad","08/31/2020","$499,988.00","Michael Rogers, David Brown","sghafoor@tntech.edu","Dixie Avenue","Cookeville","TN","385050001","9313723374","CSE","044Y","7361","$0.00","The computing landscape has been changed in recent years due to the pervasive shift to multicore and GPU-based computer architectures. The current and future generations of our computing workforce will require the acquisition of a broad parallel and distributed computing (PDC) skill set, to enable the effective utilization of existing and emerging computing devices and cyber infrastructures. The current undergraduate computer science (CS) and computing engineering (CE) curricula, in most cases, do not integrate PDC concepts as required topics. The overall goal of the project is to prepare CS, CE, and Engineering undergraduate students for their future careers in light of this technological shift towards parallelism by improving faculty expertise in PDC through a series of week-long training workshops and by providing them with the resources to integrate PDC topics into introductory programming classes. This will enable the future workforce to contribute effectively in the emerging technological ecosystem, promoting the progress of science and advancing the national health, prosperity and welfare, which serves the national interest, as stated by NSF's mission. <br/>Based on the training of the trainer model, this project will be conducting a series of week-long workshops for non PDC CS1 and CS2 instructors through whom the project will be able to reach a larger audience of CS and CE undergraduates. The project will leverage existing PDC educational resources to enact a two-phase plan.  The first phase is to conduct a preliminary planning workshop to plan and develop a faculty development workshop and the IPDC toolkit. The IPDC toolkit will be an all-inclusive resource repository for the easy integration of PDC topics in introductory programming class without much effort required from the instructor. In the second phase a series five training workshops will be conducted over three years. These intensive faculty development workshops will include: 1) introduction to PDC concepts, 2) developing parallel programs using libraries and tools like Open MP, Java, and Mat lab, 3) PDC concepts through unplugged activities, 4) hands-on programming modules, 5) access to tools and information beyond the scope of IPDC. Continuous improvement through assessment and evaluation will be an integral part of the project. The projects goal is to reach seventy-five faculty over 3 years period who in turn will introduce PDC concepts and topics to over four thousand undergraduates per year. The participants selected for the project will come from institutions primarily involved in teaching but lack research infrastructure and PDC resources. Beyond the NSF funding cycle, the project will continue to provide online faculty development workshops and mini-workshops, using the IPDC toolkit, at CS and CE education conferences."
"1642406","SSE: Development of a High-Performance Parallel Gibbs Ensemble Monte Carlo Simulation Engine","OAC","Proc Sys, Reac Eng & Mol Therm, DMR SHORT TERM SUPPORT, Software Institutes","05/01/2017","04/28/2017","Jeffrey Potoff","MI","Wayne State University","Standard Grant","Stefan Robila","04/30/2020","$499,886.00","Loren Schwiebert","jpotoff@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","1403, 1712, 8004","7237, 7433, 7569, 8004, 8005, 8249","$0.00","The use of molecular simulation to study complex physical phenomena at the atomic level has grown exponentially over the last decade with increasing CPU power and the development of parallel molecular dynamics codes that scale efficiently over thousands of processors.  Molecular dynamics codes that utilize parallel computation on CPUs and GPUs are relatively well developed, however, there are a number of problems that cannot be simulated with this methodology. Specifically, problems that require the simulation of an open system, such as adsorption in porous materials, require an alternative methodology that allows for fluctuation in the number of molecules in the system.  In addition, there are a number of systems where the presence of large free energy barriers and slow diffusion preclude the use of standard molecular dynamics.  Notable examples include the prediction of phase equilibria in multi-component lipid bilayers, or polymers.  For these types of problems, Monte Carlo or hybrid Monte Carlo/molecular dynamics simulations have the potential to significantly improve computational efficiency.  This project is focused on the development of the open-source Monte Carlo simulation engine, GOMC, which is able to use low cost graphics processing units (GPUs) and multi-core processors (CPUs) to significantly reduce computational time.  This effort will enable Monte Carlo simulations to be performed with higher fidelity and for larger systems than is currently accessible with standard Monte Carlo simulation codes, enabling the accelerated development of new materials by domain scientists. In addition, this project will provide training for graduate and undergraduate students in Monte Carlo simulation, design of efficient algorithms for parallel computation on a variety of hardware architectures, and software development.  Tutorials and other educational materials will be created to support the use of GOMC for teaching Monte Carlo simulation of molecular systems to students in undergraduate and graduate courses at Wayne State as well as other universities. The free distribution of GOMC, along with the tutorials for using the software, will enable other research groups to solve important research problems quickly and accurately. This project, supported by the Office of Advanced Cyberinfrastructure (OAC), and the divisions of Material Research and Chemistry in the Directorate of Mathematical and Physical Sciences, and the Division of Chemical, Bioengineering, Environmental and Transport Systems (CBET) in the Directorate of Engneering, will result in software that enables new and better science. It also serves the educational mission of the National, through its active involvement of graduate and undergraduate students.<br/><br/>Parallelization of Monte Carlo is complicated by the inherently sequential nature of the algorithm, which limits the reuse of code from molecular dynamics, and necessitates the development of new approaches.  The team's previous efforts have shown that despite the sequential nature of Monte Carlo, graphics processors (GPU) and multi-core CPUs can be used to yield significant reductions in wall-clock time required for a given calculation compared to a traditional serial CPU Monte Carlo code.  This effort led to the creation of the open-source Monte Carlo simulation engine GPU Optimized Monte Carlo (GOMC).  This work will add significant functional and computational enhancements to be added to GOMC.  These enhancements include: (1) support for polarizable force fields based on the Drude oscillator and AMOEBA models, (2) advanced configurational bias moves, such as concerted rotation, double bridging, and aggregation-volume bias (3) multi-molecule moves (4) hybrid Monte Carlo/molecular dynamics simulations (5) new optimizations for multi-core and GPU architectures. The project will enable the simulation of large systems (>100,000 atoms) at constant chemical potential, providing insight into a broad array of problems such as polymer, lipid and ionic liquid phase behavior, molecular self-assembly, the stabilization of nano and micro particle dispersions for drug delivery, and membrane fusion under physiologically relevant conditions."
"1626516","MRI: Acquisition of the Lawrence Supercomputer to Advance Multidisciplinary Research in South Dakota","OAC","MAJOR RESEARCH INSTRUMENTATION, ETF","10/01/2016","08/30/2018","Douglas Jennewein","SD","University of South Dakota Main Campus","Standard Grant","Stefan Robila","09/30/2019","$504,911.00","Christina Keller, Paul May, Cynthia Anderson, Erliang Zeng, Cheryl Tiahrt","Doug.Jennewein@usd.edu","414 E CLARK ST","Vermillion","SD","570692307","6056775370","CSE","1189, 7476","1189, 9150","$0.00","The University of South Dakota (USD) will acquire, deploy, and maintain a cluster supercomputer to be named after Nobel Laureate and USD alumnus E. O. Lawrence. As a campus-wide resource available to all USD faculty, staff, postdocs, graduate students, and undergraduates as well as researchers across South Dakota, the Lawrence Cluster's key objectives are to 1) Accelerate scientific progress and reduce time to discovery, 2) Enable and accelerate scientific results not previously possible, and 3) Increase student engagement in computationally assisted research. <br/><br/>Initially, the Lawrence Cluster will support 12 STEM projects across 7 departments at 3 institutions in North and South Dakota, including 21 faculty, 26 postdocs, and 307 students. The system will support multidisciplinary research and research training in scientific domains such as high energy physics, the human brain, renewable energy, and materials science. It will help answer questions of considerable public interest and societal value, including the nature of dark matter, and the elusive links between the human brain and human behavior. Moreover, advances in both optical properties of nanomaterials and in design of organic functional materials are relevant to a broad range of material science and engineering problems. Results will help lay the foundation for the efficient fabrication of novel materials.<br/><br/>The Lawrence Cluster will have a peak theoretical performance of more than 60 TFLOPS. The system architecture includes an XSEDE-compatible software stack, general-purpose compute nodes, large memory nodes, GPU-accelerated nodes, interactive visualization nodes, a high speed InfiniBand interconnect, and a high-capacity parallel filesystem. In additional to a traditional command line interface, the Lawrence Cluster will also include a browser-based user portal for job submission and management."
"1642433","SI2-SSE:  Automated Statistical Mechanics for the First-Principles Prediction of Finite Temperature Properties in Hybrid Organic-Inorganic Crystals","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2016","09/06/2016","Anton Van der Ven","CA","University of California-Santa Barbara","Standard Grant","Micah Beck","09/30/2019","$402,095.00","","avdv@engineering.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","1712, 8004","7433, 8004, 8005, 8396, 8400, 8607, 9215, 9216","$0.00","This project seeks to advance computational capabilities in materials science by developing new theoretical and computational tools to predict temperature dependent properties of complex crystalline materials containing organic molecules. The recent discovery that hybrid organic-inorganic compounds can achieve remarkable photovoltaic conversion efficiencies has led to the recognition that a fundamental understanding of these complex compounds is urgently needed and that first-principles computational tools are necessary to enable a prediction of their intrinsic materials properties. The room temperature properties of hybrid organic-inorganic compounds are strongly affected by thermal excitations. Important electronic, thermodynamic and kinetic properties of these compounds therefore cannot be predicted directly with quantum mechanical approaches alone, but require statistical mechanics tools that account for the effects of temperature. A major objective of this project is the development of highly automated statistical mechanics software tools to predict materials properties where disorder due to alloying, atomic vibrations and molecular rotations are rigorously accounted for. These tools will greatly enhance the ability to predict the properties of complex materials from first principles, thereby enabling the directed design of a broad class of new materials with applications in a wide variety of technologies, including energy conversion and storage, carbon capture and organic electronics. The fundamental scientific insights to be generated by this study on hybrid organic/inorganic compounds will lead to invaluable design principles to enable the further improvement of these compounds for photovoltaic applications. The proposed activity will also educate and train graduate students in computational materials science, a field that is increasingly recognized as invaluable in the design and rapid implementation of new materials.<br/><br/>Modern first-principles electronic structure methods have reached a remarkable level of accuracy and ease of use, making them invaluable tools in the design of new materials. Electronic structure methods by themselves, however, do not explicitly account for the role of temperature on thermodynamic and kinetic properties. The properties of many promising materials for energy storage and conversion applications and for transportation applications depend sensitively on temperature due to large entropic contributions arising from atomic-scale excitations and disorder. Most materials of technological relevance are characterized by configurational disorder due to alloying and many high temperature phases are dynamically stabilized by large anharmonic vibrational excitations. Entropic contributions to equilibrium and non-equilibrium properties are especially important in a new class of hybrid organic-inorganic perovskites that show great promise as photovoltaic materials. These compounds belong to a class of crystalline materials that can host molecular species in large interstitial cages and exhibit a wide range of atomic and molecular excitations already at room temperature. Optimal photovoltaic properties are achieved by alloying on all three sublattices of the ABX3 perovskite crystal, leading to configurational disorder in addition to molecular and vibrational excitations. A statistical mechanics approach is therefore essential to accurately predict the electronic, thermodynamic and kinetic properties of these materials. The aim of this project is to develop a statistical mechanics framework and an accompanying highly automated software infrastructure that rigorously accounts for all relevant configurational, vibrational and molecular degrees of freedom in crystalline solids containing interstitial molecular species. The prediction of finite temperature thermodynamic and kinetic properties will rely on effective Hamiltonians that serve to extrapolate highly accurate first-principles electronic structure calculations within Monte Carlo simulations. A major activity of the project is the creation of a highly automated statistical mechanics software package called a Clusters Approach to Statistical Mechanics (CASM) to predict the finite temperature properties of multicomponent crystalline materials from first principles. The application of these tools in a first-principles study of alloyed hybrid organic-inorganic perovskites will generate a fundamental scientific understanding of the relative importance of the various atomic and molecular excitations on electronic structure, phase stability and ionic transport properties."
"1740111","SI2-SSE: Collaborative Research: Software Framework for Strongly Correlated Materials: from DFT to DMFT","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2017","08/30/2017","Aldo Romero","WV","West Virginia University Research Corporation","Standard Grant","Vipin Chaudhary","09/30/2020","$249,947.00","","Aldo.Romero@mail.wvu.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9102, 9150, 9216","$0.00","The main objective of this project is to develop advanced computational (ab-initio) tools that bridge the gap between the existing complex theories that describe the behavior of strongly-correlated electron materials, and the scientists working in other diverse fields who want to investigate the physical properties of the strongly correlated materials using modernstate-of-the-art computational methodologies. These strongly-correlated materials show a large set of interesting properties that can impact different fields as in opto-catalysis, magneto-optics, magneto-transport, high temperature superconductivity and magneto-electricity. The intriguing properties of such strongly-correlated materials includes unconventional superconductivity, complex charge spin and orbital ordering, metal-to-insulator transitions, and excellent thermoelectricity that have promising applications in modern technology. The existence of strong electron-electron interactions limits the use of existing Density Functional Theory (DFT) to understand the electronic structure of the strongly-correlated materials. However, recent developments of a new theory, named Dynamical Mean Field Theory (DMFT), has enabled researchers to correctly describe the electronic structure of the strongly correlated materials. In this project, the PIs will develop advanced Python-based computational research tools that will enable the researchers from diverse fields to investigate the properties of the strongly-correlated materials using DMFT. The specific applications include -- correct prediction of the electronic structure, vibrational properties and elastic properties of the strongly-correlated materials. The developed software tools will be freely available and open source and a user-manual will be made available for training purposes.<br/><br/><br/>The main goal of this project is to provide end users of various electronic structure codes with a flexible Python-based interface that does not rely on the extensive user experience or specific parameters to perform calculations for strongly-correlated materials and to develop new software to calculate electronic, vibrational, and elastic properties of strongly-correlated materials by using Dynamical Mean Field Theory (DMFT) methods starting from a Density Functional Theory (DFT) calculation. The developed software tools will be powerful enough to allow scientists in different fields to calculate the diverse electronic properties of a wide range of strongly-correlated materials with the state-of-the-art computational methodologies. Furthermore, these software packages will allow the correct electronic structure calculations in a minimal set of parameters, by offering to the end user the possibility of using three different methodologies to describe basic physics of strongly-correlated materials.All the developed computer software will be designed to enable the non-expert materials scientists and engineers to investigate the novel properties of the strongly-correlated materials. The scientific aim of this project also concerns the evolution of electronic correlations for several complex oxinitrides and Heusler alloys, in particular the dependence of several physical observables with respect to external fields such as pressure and strain. Targeted physical properties include electronic, vibrational, and elastic. The technical goal consists of the development of an open-source software that will address the scientific issues raised by the research on calculating properties of the strongly-correlated materials.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1808553","CDS&E: Collaborative Research: A Computational Framework for Reconstructing and Visualizing Myocardial Active Stresses","OAC","CDS&E-MSS, CDS&E","10/01/2018","09/06/2018","Suzanne Shontz","KS","University of Kansas Center for Research Inc","Standard Grant","Vipin Chaudhary","09/30/2021","$331,047.00","","shontz@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457568","7858643441","CSE","8069, 8084","026Z, 028E, 8084, 9263","$0.00","The normal heart functions by contracting and pushing the blood from the left ventricle into the rest of the body. Due to various diseases, the contraction capabilities of the heart become diminished in certain regions of the heart chamber wall, compromising the overall function of the heart. In order to identify and select optimal treatment, it is critical to identify the regions of the heart wall that exhibit reduced contractions. Unfortunately, contractions cannot be easily measured. This project will estimate the stress (contraction power) developed within the heart muscle by combining medical imaging and mechanical modeling of the heart. These stresses will serve as a quantitative measure of the contractile function of the heart and help detect and localize disease. Therefore, this research has the potential to evolve into a future tool to diagnose cardiac function. This project will also feature a synergistically integrated education and outreach program. We will foster research opportunities for graduate and undergraduate students in computer science, biomedical engineering, mathematics, and imaging science at Rochester Institute of Technology and the University of Kansas. The PIs will develop innovative hands-on workshops to inspire and educate K-12 students from underrepresented groups on biomedical computing and medicine.<br/> <br/>This project proposes to develop a method that enables non-invasive appraisal and visualization of the active stresses developed in the myocardium to serve as a direct means to assess the bio-mechanical function of the heart. The PIs will develop open-source cyberinfrastructure and integrate it into a novel computational framework for cardiac biomechanics that will reconstruct the active stresses from cardiac deformations. The PIs will accomplish this goal by developing and integrating techniques for medical image computing, high-order meshing, and inverse-problem bio-mechanical modeling. This research will address a currently unexplored niche in the cardiac modeling field, specifically the reconstruction and visualization of myocardial active stresses, to enable direct appraisal of cardiac function. This research will contribute to medical image computing through the development of algorithms for medical image processing and visualization. The PIs will develop and implement novel high-order meshing techniques and integrate them with the fiber architecture to enable accurate and efficient scientific modeling and computing. The project will contribute new knowledge in mathematical modeling and simulation by implementing efficient nonlinear least-squares solutions for inverse cardiac biomechanics. Lastly, the PIs will release the resulting cyberinfrastructure to the scientific computing community for research and education use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1740219","Collaborative Research: NSCI: SI2-SSE: Time Stepping and Exchange-Correlation Modules for Massively Parallel Real-Time Time-Dependent DFT","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2017","08/24/2017","Andre Schleife","IL","University of Illinois at Urbana-Champaign","Standard Grant","Vipin Chaudhary","08/31/2020","$249,940.00","","schleife@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9216","$0.00","Recent advances in high-performance (HPC) computing allow simulations of quantum dynamics of electrons in complex materials, and such simulations are central to advancing various medical and semiconductor technologies, ranging from proton beam cancer therapy to fabricating faster and smaller electronics. At the same time, the increasing scale and complexity of modern high-performance computers exposed a need for development of scientific software that is tailored for computers with large numbers of processors so that simulations can efficiently take advantage of increasing computing power. This project advances scientific software for simulating quantum dynamics of electrons for high-performance computers with tens and hundreds of thousands of processors that are becoming widely available. This work builds the HPC academic research community around the proposed software by extending the existing software available for quantum dynamics simulation with better user-friendly features and analysis techniques. In the process, this project engages graduate students and early-career researchers to use and further develop scientific software for high-performance computers in general. Additionally, a summer school for hands-on training will be conducted. The open source software will be made available to the community on Github (public repository). <br/><br/>Real-time propagation in time-dependent density functional theory (RT-TDDFT) is becoming increasingly popular for studying non-equilibrium electronic dynamics both in the linear regime and beyond linear response. RT-TDDFT can be combined to study coupled dynamics of quantum-mechanical electrons with the movement of classical ions within Ehrenfest dynamics. In spite of its great promise, RT-TDDFT is computationally very demanding, especially for studying large condensed-matter systems. The large cost arises from small time steps of numerical integration of the electron dynamics, rendering accurate (hybrid) exchange-correlation (XC) functionals unfeasible, despite their clear benefits. In addition, while modern high-performance computing (HPC) helps tackling great scientific questions, massively parallel, hybrid-paradigm architectures present new challenges. Theoretical and algorithmic methods need to be developed in order to take full advantage of modern massively parallel HPC. This work builds new modules for the RT-TDDFT software component of the Qb@ll code, that enables a large community of researchers to perform advanced first-principles simulations of non-equilibrium electron dynamics in complex condensed-phase systems, using massively parallel HPC. This is done through developing (1) new modules for numerical integration that propagate the underlying non-linear partial differential equations in real time with high efficiency and accuracy, and (2) new modules for improved approximations of the underlying electronic structure, using a modern meta-generalized-gradient XC functional. Furthermore, the work builds the HPC academic research community around RT-TDDFT within the Qb@ll code through (1) development of user-friendly features that interface Qb@ll with other code and analysis techniques and (2) engagement of early-career scientists by incorporating hands-on training on RT-TDDFT using the Qb@ll code in TDDFT summer school.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, the Materials Research Division and Chemistry Division in the Directorate of Mathematical and Physical Sciences."
"1730105","CyberTraining: CIP: North Central Region Cyber Training Center for Cybersecurity at Dakota State University","OAC","CyberTraining - Training-based, EPSCoR Co-Funding","09/01/2017","07/25/2017","Wayne Pauli","SD","Dakota State University","Standard Grant","Sushil K Prasad","08/31/2020","$479,658.00","Yong Wang","Wayne.Pauli@dsu.edu","820 N. Washington Avenue","Madison","SD","570421799","6052565112","CSE","044Y, 9150","7361, 9150","$0.00","The gap between necessary cybersecurity skills and the demands placed on cybersecurity professionals is a challenge for the scientific research enterprise of the nation.  This project will create a scalable online training program and develop specific training materials in cybersecurity for the advanced scientific cyberinfrastructure to address the skillset gap.  Investigators will collaborate with their academic and industry partners to identify cybersecurity skill gaps specifically for advanced scientific cyberinfrastructure and to develop corresponding training materials for the online program.  The project will engage a wide ranging audience from the scientific research community and help them develop and expand their expertise in cybersecurity for advanced scientific cyberinfrastructure.  Thus, as stated by NSF's mission, the project serves the national interest to promote the progress of science; to advance national; and to secure the national defense.<br/><br/>The project will establish a North Central Region (NCR) Cyber Training Center (CTC) for cybersecurity at Dakota State University.  The goal of CTC is to create a scalable online cyber training program to assist with the workforce development of scientific cyberinfrastructure professionals in cybersecurity.  The target participants of the online training program include post-docs, research scientists and faculty researchers and educators from 2-year and 4-year colleges, as well as undergraduate and graduate students who want to develop specialty in cybersecurity within the NCR and beyond.  The CyberTraining Professionals program is included in the project to increase the impact of the program and establish a joint task force with industry to address the cybersecurity skills gap challenge.  Highlights of this online training program are cutting-edge cybersecurity curriculum and hands-on labs and assignments to promote and support the rich learning environment. A new online training web site and an AWS Cybersecurity Lab will be created in the project to facilitate the training activities.  The project will also develop specific training materials which are relevant to advanced scientific cyberinfrastructure and provided online for public access.  Examples of the training materials to be included in this program include modules on compliance issues, identity management on different campuses, science DMZ, and Internet of Things.  The CTC program will bring unique learning experiences to the project participants.  Dakota State University will leverage its NSA designation as a Center of Academic Excellence in Research (CAE-R) and the already developed and functioning Information Security Research and Education consortium (INSuRE) to further promote and expand scientific research in the cyber world."
"1740112","SI2-SSE: Collaborative Research: Software Framework for Strongly Correlated Materials: from DFT to DMFT","OAC","DMR SHORT TERM SUPPORT, Software Institutes","10/01/2017","08/30/2017","Hyowon Park","IL","University of Illinois at Chicago","Standard Grant","Vipin Chaudhary","09/30/2020","$250,001.00","","hyowon@uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9102, 9150, 9216","$0.00","The main objective of this project is to develop advanced computational (ab-initio) tools that bridge the gap between the existing complex theories that describe the behavior of strongly-correlated electron materials, and the scientists working in other diverse fields who want to investigate the physical properties of the strongly correlated materials using modernstate-of-the-art computational methodologies. These strongly-correlated materials show a large set of interesting properties that can impact different fields as in opto-catalysis, magneto-optics, magneto-transport, high temperature superconductivity and magneto-electricity. The intriguing properties of such strongly-correlated materials includes unconventional superconductivity, complex charge spin and orbital ordering, metal-to-insulator transitions, and excellent thermoelectricity that have promising applications in modern technology. The existence of strong electron-electron interactions limits the use of existing Density Functional Theory (DFT) to understand the electronic structure of the strongly-correlated materials. However, recent developments of a new theory, named Dynamical Mean Field Theory (DMFT), has enabled researchers to correctly describe the electronic structure of the strongly correlated materials. In this project, the PIs will develop advanced Python-based computational research tools that will enable the researchers from diverse fields to investigate the properties of the strongly-correlated materials using DMFT. The specific applications include -- correct prediction of the electronic structure, vibrational properties and elastic properties of the strongly-correlated materials. The developed software tools will be freely available and open source and a user-manual will be made available for training purposes.<br/><br/><br/>The main goal of this project is to provide end users of various electronic structure codes with a flexible Python-based interface that does not rely on the extensive user experience or specific parameters to perform calculations for strongly-correlated materials and to develop new software to calculate electronic, vibrational, and elastic properties of strongly-correlated materials by using Dynamical Mean Field Theory (DMFT) methods starting from a Density Functional Theory (DFT) calculation. The developed software tools will be powerful enough to allow scientists in different fields to calculate the diverse electronic properties of a wide range of strongly-correlated materials with the state-of-the-art computational methodologies. Furthermore, these software packages will allow the correct electronic structure calculations in a minimal set of parameters, by offering to the end user the possibility of using three different methodologies to describe basic physics of strongly-correlated materials.All the developed computer software will be designed to enable the non-expert materials scientists and engineers to investigate the novel properties of the strongly-correlated materials. The scientific aim of this project also concerns the evolution of electronic correlations for several complex oxinitrides and Heusler alloys, in particular the dependence of several physical observables with respect to external fields such as pressure and strain. Targeted physical properties include electronic, vibrational, and elastic. The technical goal consists of the development of an open-source software that will address the scientific issues raised by the research on calculating properties of the strongly-correlated materials.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1657507","CRII: ACI: Unveiling the Origin of the Highest Energy Particles in the Universe with Large-Scale First-Principle Fully-Kinetic Simulations","OAC","CRII CISE Research Initiation","07/01/2017","12/21/2016","Lorenzo Sironi","NY","Columbia University","Standard Grant","Sushil Prasad","06/30/2019","$170,680.00","","lsironi@astro.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","026Y","8228","$0.00","The long-term objective of this project is to unveil the origin of the highest energy particles in the Universe - the so-called ultra-high-energy cosmic rays (UHECRs) --- charged ions whose energies can exceed the energy of a tennis ball. Since the highest energy ions are extremely rare, their origin still remains  elusive. One of the leading candidates for UHECR production are blazars, a class of galaxies with relativistic jets  emerging from supermassive black holes. However, the processes that can accelerate UHECRs in blazar jets are not fully understood. Most models attempt to infer the properties of the accelerated particles by fitting the observed emission from blazars. Due to the large number of free parameters, the models are not uniquely constrained by the observations and, therefore, have little predictive power. In fact, there is no reliable theory built from first principles for the  mechanism that transfers the jet energy to the highest energy particles, and so there is no definite answer to whether UHECRs originate from blazar jets. This proposal aims to address this fundamental problem - thus, serving NSF's mission to promote the progress of science - by studying  the physics of particle acceleration in blazar jets from first principles. The proposed program will also create research opportunities for undergraduate students of Astronomy, Computer Science and Physics departments. In the process, the students will be exposed to an active research environment and trained in the scientific method. An interactive website will be developed with the goal of providing the public an easy access to this exciting field of multidisciplinary research (at the interface between computing, physics and astronomy). The website will also provide a portal for high school teachers interested in updating their lectures with current scientific findings. <br/><br/>The proposed project will investigate the origin of UHECRs by studying self-consistently the physics of particle acceleration in the relativistic jets of blazars.  The research will explore particle acceleration in magnetic reconnection - a process by which magnetic field lines of opposite polarity annihilate, releasing their energy to the particles. The physics of particle acceleration is highly non-linear - the reconnection process affects the overall jet dynamics, which in turn changes the efficiency of energy dissipation via reconnection - and therefore hard to model with analytical tools. The project takes advantage of the enormous growth of computing power in the last few years and the development of powerful Particle-In-Cell (PIC) codes that can model collisionless plasmas from first principles. The investigation of particle acceleration will be performed via a suite of 2D and 3D PIC simulations in unprecedentedly large domains, so that the results can be properly extrapolated from the microscopic scales of PIC simulations to the macroscopic scales of astrophysical accelerators. As the plasma composition in blazar jets is not well constrained, project will investigate the efficiency of proton acceleration in both electron-proton and electron-positron-proton plasmas. A novel cooling module for the particles will be implemented in the PIC code, to account self-consistently for the effect of radiative losses on the electron and proton dynamics (i.e., synchrotron radiation and photohadronic interactions with radiation fields  in the jet). This will be of significant importance to assess whether blazar jets can accelerate ions up to ultra-high energies."
"1730519","CyberTraining: CIP: NCSA Internship Program for CI Professionals","OAC","CyberTraining - Training-based","08/01/2017","06/28/2017","Daniel Lapine","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sushil Prasad","07/31/2020","$499,999.00","Volodymyr Kindratenko","lapine@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","044Y","7361","$0.00","This project is NCSA's pilot internship program for cyberinfrastructure (CI) professionals, designed to address the shortage of a workforce with the specialized skills needed to support advanced CI operations.  The program both provides internship opportunities for individuals who want to gain first-hand experience in the CI operations of a supercomputing center, and develops and refines instructional materials to serve as templates that are openly distributed for use by other centers and institutions to train CI professionals for scalability.  Program interns work directly with a group of engineers in one of the areas of CI focus to gain hands-on experience in the deployment and operation of cutting-edge high-performance computing (HPC) infrastructure at a leading HPC center.  Graduates of this internship program will enter a workforce that will develop, deploy, manage and support advanced CI at other universities, centers, and industry to meet the needs of the national computational science research community.  By doing so, the project serves the national interest by promoting the progress of science and advancing the nationalprosperity and welfare.<br/><br/>Managing modern CI that supports scientific research is a challenging task which requires deep and broad knowledge and experience to plan, acquire, deploy, and operate complex HPC resources.  This project plans to help prepare the next generation of system engineers and administrators knowledgeable about computer architecture, storage technology, interconnect, operating system, cluster software stack, user management, job management, resources monitoring, networking, and security, as well as best practices in the field.  The goals for this internship program include expanding the talent pool by attracting and supporting professional development of minority and underrepresented groups in the field of CI, providing motivated individuals the opportunity to obtain real-world CI operational experience through a short, full-time program in an area already supported by NSF for its programmatic needs, and developing training materials and identifying best practices that will be contributed back to the community to be used stand-alone or in combination with an internship program to train next-generation CI professionals."
"1808530","CDS&E: Collaborative Research: A Computational Framework for Reconstructing and Visualizing Myocardial Active Stresses","OAC","CDS&E-MSS, CDS&E","10/01/2018","09/06/2018","Cristian Linte","NY","Rochester Institute of Tech","Standard Grant","Vipin Chaudhary","09/30/2021","$523,295.00","Niels Otani","clinte@mail.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","8069, 8084","026Z, 028E, 8084, 9263","$0.00","The normal heart functions by contracting and pushing the blood from the left ventricle into the rest of the body. Due to various diseases, the contraction capabilities of the heart become diminished in certain regions of the heart chamber wall, compromising the overall function of the heart. In order to identify and select optimal treatment, it is critical to identify the regions of the heart wall that exhibit reduced contractions. Unfortunately, contractions cannot be easily measured. This project will estimate the stress (contraction power) developed within the heart muscle by combining medical imaging and mechanical modeling of the heart. These stresses will serve as a quantitative measure of the contractile function of the heart and help detect and localize disease. Therefore, this research has the potential to evolve into a future tool to diagnose cardiac function. This project will also feature a synergistically integrated education and outreach program. We will foster research opportunities for graduate and undergraduate students in computer science, biomedical engineering, mathematics, and imaging science at Rochester Institute of Technology and the University of Kansas. The PIs will develop innovative hands-on workshops to inspire and educate K-12 students from underrepresented groups on biomedical computing and medicine.<br/> <br/>This project proposes to develop a method that enables non-invasive appraisal and visualization of the active stresses developed in the myocardium to serve as a direct means to assess the bio-mechanical function of the heart. The PIs will develop open-source cyberinfrastructure and integrate it into a novel computational framework for cardiac biomechanics that will reconstruct the active stresses from cardiac deformations. The PIs will accomplish this goal by developing and integrating techniques for medical image computing, high-order meshing, and inverse-problem bio-mechanical modeling. This research will address a currently unexplored niche in the cardiac modeling field, specifically the reconstruction and visualization of myocardial active stresses, to enable direct appraisal of cardiac function. This research will contribute to medical image computing through the development of algorithms for medical image processing and visualization. The PIs will develop and implement novel high-order meshing techniques and integrate them with the fiber architecture to enable accurate and efficient scientific modeling and computing. The project will contribute new knowledge in mathematical modeling and simulation by implementing efficient nonlinear least-squares solutions for inverse cardiac biomechanics. Lastly, the PIs will release the resulting cyberinfrastructure to the scientific computing community for research and education use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1453548","CAREER: Sustaining Scientific Infrastructure:  Researching Transition from Grants to Peer Production","OAC","CAREER: FACULTY EARLY CAR DEV, SCIENCE OF SCIENCE POLICY, Software Institutes","07/01/2015","04/09/2015","James Howison","TX","University of Texas at Austin","Standard Grant","Sushil Prasad","06/30/2020","$535,349.00","","jhowison@ischool.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1045, 7626, 8004","1045, 7433, 8004","$0.00","Software is becoming increasingly critical to the activities of science, yet securing long term support for this infrastructure is becoming more challenging. Given constrained budgets, the options for effectively and efficiently supporting sustainable software infrastructure are of growing importance. This project examines the transition of research infrastructure from grant-based funding to long term sustainable models, by examining scientific software projects as they transition to an open source peer production model.  The project tracks software packages as they attempt to transition, and develops a theoretical framework for understanding this process and for explaining their success or failure. The outcome of this research will inform science policy by shaping how the community funds software infrastructure projects in their initial phases, and how to encourage the transition to long-term models thereafter.<br/> <br/>Inspired by the success of open source software, funding agencies are encouraging transitions to forms of organization that organizational researchers call 'peer production.'  Yet peer production research has shown that simply making code available under an open source license is insufficient to build a motivated and productive community. Successful transition includes changes in team structure (from local to distributed), collaboration technologies (from controlled to open), governance (from hierarchical to shared), and management of participants (from predictable to unpredictable commitment). This project examines successful and unsuccessful transitions from initial grant funding to peer production in scientific software development. Detailed case study histories are developed, by interviewing project participants, funders, and users of scientific software, and by analyzing project archives and software repositories. The research includes six transitioning projects: yt, Enzo, Eclipse PTP, IRODs, Apache OODT and Airavata. In addition, a panel of projects funded by the NSF Software Infrastructure for Sustained Innovation program (which requires plans for sustainability) will be tracked over time to study successful transitions, unsuccessful attempts, and cases where transition was judged inappropriate.  The research is combined with an integrated education plan designed to build organizational skills in the scientific community to enable successful transitions to peer production."
"1833170","Ph.D. Forum at the International Conference on Parallel Processing (ICPP)","OAC","INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE","06/01/2018","05/08/2018","Allen Malony","OR","University of Oregon Eugene","Standard Grant","Sushil Prasad","05/31/2019","$20,600.00","","malony@cs.uoregon.edu","5219 UNIVERSITY OF OREGON","Eugene","OR","974035219","5413465131","CSE","1640, 7361","7556, 9102","$0.00","Now in its 47th year, the International Conference on Parallel Processing (ICPP) is one of the longest continuously running conferences in parallel processing, and it is a rewarding event for Ph.D. graduate students to connect with and learn from the parallel computing community.  The Ph.D. Forum at ICPP promotes the progress of science as stated by NSF's mission by creating a rich educational and learning experience where Ph.D. graduate students (i) present and discuss their research in parallel computing with conference attendees, (ii) expand their understanding of the parallel computing by attending technical sessions, (iii) explore potential research projects with new colleagues, and (iv) receive guidance from leaders in the field.  By assisting in the training, development, and mentorship of the next-generation of parallel computing researchers, the ICPP Ph.D. Forum is contributing to the NSF's mission for advancements in science, support for education and diversity, and broader benefits to society.<br/>    <br/>This award will support the travel of up to 24 eligible students from US-based institutions to participate in the student program at ICPP 2018 and to organize a PhD Forum. There are two main objectives of the Ph.D. Forum at ICPP.  First, it provides Ph.D. students with intellectual opportunities to discuss their research work in a constructive and supportive environment where leading parallel computing topics are presented and debated.  Second, it provides mentoring resources to help graduate students organize their academic research work and plan for their future careers.  The Ph.D. Forum makes it possible for graduate students to (i) present their research during the poster session, (ii) give a 2-3 minute introduction during a Ph.D. mentor luncheon, (iii) attend a special parallel computing career session, (iv) meet and interact with designated faculty and industry mentors during the conference, and (v) distribute their research materials (poster, abstract) and CV to other ICPP participants.  A Ph.D. Forum award acknowledges best overall poster, introduction, and participation.  An associated objective is to lay the groundwork for continuation of the Ph.D. Forum in future years of ICPP.  For this purpose, Ph.D. students provide feedback on their experience at ICPP and thoughts for improvements in the Ph.D. Forum.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1640987","Women in IT Networking at SC (WINS)","OAC","Campus Cyberinfrastrc (CC-NIE)","07/15/2016","02/22/2019","Marla Meehl","CO","University Corporation For Atmospheric Res","Standard Grant","Kevin Thompson","06/30/2019","$162,000.00","Gwendolyn Huntoon","marla@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","CSE","8080","","$0.00","The Women in IT (Information Technology) Networking at SuperComputing (SC) (WINS) program seeks to address the prevalent gender gap that exists in IT, particularly in the fields of network engineering and high performance computing (HPC). The project's goal is to establish a sustainable workforce development program giving U.S. women professionals in the network and software engineering fields the opportunity to gain substantial hands-on expertise while developing business and mentoring relationships with global leaders in the field. The project provides travel support for a total of fifteen qualified female U.S. candidates - five early to mid-career participants per year for three years. The support allows the participants to join the SCinet workforce for the conference years SC16, SC17, and SC18. SCinet is a world-class research and education enterprise network engineered and built exclusively for annual Supercomputing (SC) events. SCinet pushes the boundaries of both production and experimental networking, offering its designers and engineers a unique challenge and experience.  Support is also provided to enable participants to attend a follow-on meeting to further expand their professional network, raise awareness of the WINS program and diversity issues, and report on their experiences resulting from participating in the program. The program builds upon the success of the Women in IT Networking at SC (WINS) pilot program introduced in November 2015 at the SC15 conference in Austin, Texas. WINS is a joint effort between the Energy Sciences Network (ESnet), the Keystone Initiative for Network Based Education and Research (KINBER), and University Corporation for Atmospheric Research (UCAR). <br/><br/>From the intellectual merit perspective, the WINS program is a concentrated effort to create a highly impactful training and solution for female participants who have the desire to build expertise and continue their education and careers in network and computer systems. By working with SCinet and the SC conference, WINS is taking advantage of a well-established community event with a 25+ year track record of helping networking and HPC professionals build their careers. WINS allows more women to tap into this deep well of resources in a very deliberate, outcome-oriented fashion. <br/><br/>The broader impact of the program is to help build a more diverse workforce by providing growth opportunities for women in Science, Technology, Engineering, and Mathematics (STEM) career areas and to retaining qualified women in the IT field while growing mentors and role models for future generations. The cross-organizational collaboration provides a broad and expansive platform for a national impact. This program is a critical component to providing a strong and diverse IT workforce to bolster U.S. international competitiveness."
"1739423","SI2-SSE: A parallel computing framework for large-scale real-space and real-time TDDFT excited-states calculations","OAC","DMR SHORT TERM SUPPORT, Software Institutes","02/15/2018","02/02/2018","Eric Polizzi","MA","University of Massachusetts Amherst","Standard Grant","Vipin Chaudhary","01/31/2021","$485,854.00","","polizzi@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","1712, 8004","026Z, 7237, 7569, 8004, 8005","$0.00","The ability to control electronic materials and understand their properties has been a driving force for technological breakthroughs. The technology for electronic devices has been on a rapidly rising trajectory since the 1960s with the ability to fabricate ever smaller silicon transistors (`Moore's Law'), with today's device sizes in the nanometer range. With the rise of nanotechnology, atom-by-atom quantum simulations of emerging materials are becoming increasingly important to reliably supplement the current experimental investigations. Modeling and simulations of atomic systems are essential to assist the everyday work of numerous engineers and scientists and can universally impact a wide range of disciplines (engineering, physics, chemistry, and biology) spanning the technological fields of computing, sensing and energy. This project will accelerate the development of quantum technologies and their impacts in the global economy. A new software will be produced to help capture many fundamental quantum effects which are increasingly important in nanotechnology for exploring and prototyping new revolutionary electronic materials and devices.<br/><br/>This project aims at developing and offering a new open source software, NESSIE, that can address the modern challenges encountered in material and device nano-engineering applications.  NESSIE will use the most cost-effective method to perform excited states calculations, the time-dependent density functional theory (TDDFT), in conjunction with a novel combination of numerical algorithms and physical and mathematical modeling techniques.  NESSIE will be capable of performing excited-state TDDFT calculations using full-potential (all-electron) in real-space (using finite element) and real-time. A new hierarchical parallelization strategy will allow NESSIE to tackle unprecedented atomistic finite size systems at this level of theory. The outcome of this project will open new perspectives for addressing the numerical challenges in real-time TDDFT excited-states calculations to operate the full range of electronic spectroscopy, and study the nanoscopic many-body effects in arbitrary complex molecules and very large-scale finite-size nanostructures. It is expected that the NESSIE software and associated numerical components will become a new valuable new tool for the scientific community, that could be applied to investigate the fundamental electronic properties of numerous nanostructured materials.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science and Engineering and the Division of Materials Research in the Directorate of Mathematical and Physical Sciences."
"1730170","CyberTraining: DSE. The Code Maker: Computational Thinking for Engineers with Interactive, Contextual Learning","OAC","CyberTraining - Training-based, CDS&E","09/01/2017","07/13/2017","Lorena Barba","DC","George Washington University","Standard Grant","Sushil K Prasad","08/31/2020","$499,965.00","Ryan Watkins, Adam Wickenheiser","labarba@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","044Y, 8084","026Z, 7361, 9102, 9261, 9263","$0.00","The Code Maker is a new vision for educating engineers who can apply computational thinking to many endeavors.  It is a program that embeds computational skills in the curriculum using learner-centered design, informed by the latest research in how people learn.  The project will develop the curriculum and materials for interactive learning of computing, in context.  The philosophy is to move from ""learning to code"" toward ""coding to learn,"" so that computing becomes a natural tool for the new engineer to solve problems, investigate nature, design and build projects.  The Code Maker serves the national interest by training engineers that are effective users of cyberinfrastructure.  It serves NSF's mission - to promote the progress of science; to advance the national health, prosperity and welfare; to secure the national defense - by delivering needed intellectual infrastructure.  The project is open source and open access.  Locally, it builds community via maker-inspired activities and student support via learning assistants at the new George Washington (GW) STEM Works Lab.  Outward-looking, the project will use an online platform to share the training widely, and will coach a close group of collaborators who bring the program to their respective institutions.  The NSF funding will also support a thorough assessment of the program, continuous improvement, and dissemination of the results.  The Code Maker will train computationally skilled engineers who are prepared to enter the workforce competitively, and ready to use computing effectively as a research tool if joining a graduate program in computational science and engineering.<br/><br/>The Code Maker project will deliver eight or more learning modules, each consisting of a series of four or more lessons, written as a Jupyter Notebook.  The modules will be available online and can be completed asynchronously or assigned as a graded course component.  They will embed the learning in the existing courses of the engineering curriculum: mechanics, statistics, heat and mass transfer, and so on.  Short term, the program will train 50 to 100 students at GW, impact similar numbers at partner institutions, and potentially reach hundreds via the online dissemination.  The modules adopt a mastery-learning approach.  The program will be supported by learning assistants and a program of maker-inspired events at a newly created space in the GW Library, the STEM Works Lab.  It will use cloud infrastructure, both public and private: an instance of the Open edX learning platform on Amazon AWS that effectively allows running the program publicly as a MOOC; and a local JupyterHub server to eliminate installation friction and ensure a consistent compute environment for local students.  The evaluation will apply a combination of 4-level training evaluation and a Technology Acceptance model."
"1554005","CAREER: A Novel and Fast Open-Source Code for Global Simulation of Stratified Convection and Magnetohydrodynamics of the Sun","OAC","CAREER: FACULTY EARLY CAR DEV, COMPUTATIONAL MATHEMATICS, FLUID DYNAMICS, SOLAR-TERRESTRIAL, INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE","05/15/2016","08/16/2016","Chunlei Liang","DC","George Washington University","Standard Grant","Sushil K Prasad","04/30/2021","$507,159.00","","chliang@gwu.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","1045, 1271, 1443, 1523, 1640, 7361","019Z, 1045, 4444, 9179, 9263","$0.00","Non-technical: <br/>The goal of this project is to create a unique capability for predicting density-stratified magnetohydrodynamics of the Sun. This research is expected to lay a foundation for developing methods for predicting extreme space weather, e.g. the event of a ""super solar flare"" followed by an extreme geomagnetic storm. Scientific results of this research can help resolve several contradictory predictions from previous studies of the solar convection zone. The Principal Investigator (PI) will develop and disseminate a powerful open-source software package to the space weather and solar physics communities. The success of predicting severe space weather events has significant societal and economic impacts. PI will design high-order accurate computational algorithms suitable for exascale simulations that can perform a billion billion calculations per second. This software will run on massively parallel distributed-memory computers to predict coupled global and local dynamics of the sun. PI will reach out to K-12 students and demonstrate that science of the sun and high-performance computing are exciting and important to society.  Furthermore, PI will leverage outreach efforts with the High Altitude Observatory of the National Center for Atmospheric Research and other research centers.  This project, thus, serves the national interest as stated by NSF's mission: to promote the progress of science and to advance the national welfare.<br/><br/>Technical: <br/>The goal of this research program is to develop a novel, fully compressible model and an open-source community code for global simulations of the solar convection zone that includes the top near surface shear layer of the Sun. Current leading global simulations use an elastic approximation whose computational domains extend from the base of the solar convection zone and must stop at about 0.96 solar radius, stopping short of the top near surface shear layer where Mach number could reach unity. This research program will create a powerful open-source community code CHORUS++ to simulate magnetohydrodynamics of the solar convection zone.  CHORUS stands for Compressible High-ORder Unstructured-grid Spectral difference code which has been co-developed by the PI for hydrodynamics of the solar convection zone.  CHORUS++ will be equipped with variable mesh resolution capability to focus on targeted regions of interests. A fast local time-stepping algorithm will be designed and equipped for CHORUS++ for long-period time integration on massively parallel computers. These technical accomplishments can accelerate the original CHORUS code by a factor over 100.  The PI will conduct a series of global simulations of magnetohydrodynamics of the solar convection zone with unprecedented resolutions for predicting the differential rotation, meridional circulation, giant cells, and super-granulation of the sun."
"1740204","Collaborative Research: NSCI: SI2-SSE: Time Stepping and Exchange-Correlation Modules for Massively Parallel Real-Time Time-Dependent DFT","OAC","DMR SHORT TERM SUPPORT, Software Institutes","09/01/2017","08/24/2017","Yosuke Kanai","NC","University of North Carolina at Chapel Hill","Standard Grant","Vipin Chaudhary","08/31/2020","$250,000.00","","ykanai@UNC.EDU","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1712, 8004","026Z, 054Z, 7433, 8004, 8005, 9216","$0.00","Recent advances in high-performance (HPC) computing allow simulations of quantum dynamics of electrons in complex materials, and such simulations are central to advancing various medical and semiconductor technologies, ranging from proton beam cancer therapy to fabricating faster and smaller electronics. At the same time, the increasing scale and complexity of modern high-performance computers exposed a need for development of scientific software that is tailored for computers with large numbers of processors so that simulations can efficiently take advantage of increasing computing power. This project advances scientific software for simulating quantum dynamics of electrons for high-performance computers with tens and hundreds of thousands of processors that are becoming widely available. This work builds the HPC academic research community around the proposed software by extending the existing software available for quantum dynamics simulation with better user-friendly features and analysis techniques. In the process, this project engages graduate students and early-career researchers to use and further develop scientific software for high-performance computers in general. Additionally, a summer school for hands-on training will be conducted. The open source software will be made available to the community on Github (public repository). <br/><br/>Real-time propagation in time-dependent density functional theory (RT-TDDFT) is becoming increasingly popular for studying non-equilibrium electronic dynamics both in the linear regime and beyond linear response. RT-TDDFT can be combined to study coupled dynamics of quantum-mechanical electrons with the movement of classical ions within Ehrenfest dynamics. In spite of its great promise, RT-TDDFT is computationally very demanding, especially for studying large condensed-matter systems. The large cost arises from small time steps of numerical integration of the electron dynamics, rendering accurate (hybrid) exchange-correlation (XC) functionals unfeasible, despite their clear benefits. In addition, while modern high-performance computing (HPC) helps tackling great scientific questions, massively parallel, hybrid-paradigm architectures present new challenges. Theoretical and algorithmic methods need to be developed in order to take full advantage of modern massively parallel HPC. This work builds new modules for the RT-TDDFT software component of the Qb@ll code, that enables a large community of researchers to perform advanced first-principles simulations of non-equilibrium electron dynamics in complex condensed-phase systems, using massively parallel HPC. This is done through developing (1) new modules for numerical integration that propagate the underlying non-linear partial differential equations in real time with high efficiency and accuracy, and (2) new modules for improved approximations of the underlying electronic structure, using a modern meta-generalized-gradient XC functional. Furthermore, the work builds the HPC academic research community around RT-TDDFT within the Qb@ll code through (1) development of user-friendly features that interface Qb@ll with other code and analysis techniques and (2) engagement of early-career scientists by incorporating hands-on training on RT-TDDFT using the Qb@ll code in TDDFT summer school.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering, the Materials Research Division and Chemistry Division in the Directorate of Mathematical and Physical Sciences."
"1535130","SI2-SSE: A Software Element for Neutrino Radiation Hydrodynamics in GenASiS","OAC","COMPUTATIONAL PHYSICS, Software Institutes","09/01/2015","08/12/2015","Reuben Budiardja","TN","University of Tennessee Knoxville","Standard Grant","Bogdan Mihaila","08/31/2019","$432,488.00","Anthony Mezzacappa, Christian Cardall, Eirik Endeve","reubendb@utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","1798, 7244, 8004","1206, 7433, 8005, 8084, 9150","$0.00","Multi-physics computational modeling is an integral part of the scientific study of many complex natural phenomena. These phenomena often involve the physics of radiation transport. For example, neutrino radiation hydrodynamics is a key element of the physics governing environments with hot and dense nuclear matter. Such extreme environments include the Early Universe, during primordial nucleosynthesis of light nuclei such as hydrogen through lithium just after the Big Bang; the merger through inspiraling of neutron star-neutron star or neutron star-black hole binaries; and the death throes of massive stars, more than ten times the mass of the Sun, in stellar explosions known as core-collapse supernovae, which are responsible for elements such as oxygen and calcium without which life as we know it would not exist. Radiation transport and kinetic theory of particles besides neutrinos---photons, electrons, or neutrons---are also relevant to many areas of astrophysics, as well as a broad range of other science applications, including materials science, plasma physics, neutron transport, multiphase flows, and high-energy-density physics. As such, the availability of a software element to solve radiation transport problems is highly valuable to researchers.<br/><br/>This project will create and deploy a software element to solve radiation hydrodynamics problems on modern supercomputers featuring ""hybrid"" architectures that include traditional CPUs plus ""accelerators"" or ""coprocessors,"" such as GPUs or Intel Many Integrated Core processors, respectively. This radiation hydrodynamics functionality will be developed within GenASiS (General Astrophysical Simulation System), a new framework being developed to facilitate the simulation of astrophysical phenomena on the world's leading capability supercomputers. In particular, the radiation transport solver will utilize the extant capabilities of GenASiS for adaptive computational ""mesh refinement,"" whereby the representation of the natural continuum is captured adaptively on a mesh of points foundational to any computational model in order to maximize the fidelity of the computational model for a given computational cost. We will use the so-called M1 approach, solving directly for the zeroth and first angular moments (energy density and momentum) of the radiation field, with higher-order moments given by ""closure relations,"" expressing them in terms of the zeroth and first moments. The energy dependence of the radiation field will be retained, with the zeroth and first angular moments discretized into ""energy bins."" Our computational approach to neutrino radiation transport will be an ""implicit-explicit"" (IMEX) scheme. Interactions between radiation and matter will be handled with a time-implicit subsolver, which will involve the inversion of dense matrices local to each node of the machine to exploit all available hardware in the node, including accelerators and coprocessors when available.  Algorithms and software resulting from this project will be made available to the community. GenASiS, as an extensible, object-oriented simulation framework, will be valuable to researchers seeking to experiment with and implement different kinds of solvers for multi-physics problems. In particular, the neutrino hydrodynamics solver developed in this project is of high interest to astrophysics modelers."
"1613674","Unified Modeling of Galaxy Populations in Clusters","OAC","PETASCALE - TRACK 1","09/01/2016","08/18/2016","Thomas Quinn","WA","University of Washington","Standard Grant","Edward Walker","08/31/2019","$20,036.00","Fabio Governato","TRQ@astro.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7781","","$0.00","Understanding the history of galaxy and cluster formation fundamentally affects our knowledge<br/>of the Universe as a whole, and sets the context for our place within the Universe. <br/>This project will use Blue Waters to model the formation and evolution <br/>of a population of galaxies in a Coma-sized galaxy cluster, including their contribution <br/>to and interaction with the Intra-Cluster Medium (ICM).  Modeling galaxies and the ICM in galaxy <br/>clusters is a formidable challenge.  The morphology of the galaxies, and the energy and <br/>metal content of the gas is ultimately controlled by star formation processes that happen <br/>on molecular cloud scales of less than a million solar masses. On the other hand, <br/>total cluster masses exceed 10^15 solar masses; hence a dynamic range in mass of over a billion is <br/>necessary for consistently modeling galaxies within this context.   The investigation of such <br/>issues is appealing to many interested in science and even in society at large. In particular, <br/>the results of the project will be used in a program that will introduce the use of computer <br/>simulations in astrophysics to science pre-majors, in the hope of ultimately attracting <br/>underrepresented students into a STEM major.<br/><br/>Several advancements have now enabled the modeling of these systems. First is the availability<br/>of supercomputers like Blue Waters that can sustained petaflop calculations on real world<br/>problems. Second is the improvement of hydrodynamic modeling in Lagrangian codes. These improvements<br/>include better modeling of multiphase medium and the resulting instabilities, more accurate<br/>handling of high Mach number shocks, and better modeling of the subgrid physics including<br/>growth and feedback from supermassive black holes. Finally these algorithm improvements have<br/>been implemented in codes that can scale to a large fraction of Blue Waters even with very<br/>clustered datasets.  This project will use the highly scalable N-body/hydrodynamics code, ChaNGa, to model the<br/>formation and evolution of a population of galaxies in a Coma-sized galaxy cluster, including<br/>their contribution to and interaction with the ICM. This model will be compared to observations<br/>of cluster galaxies to understand the physical and temporal origin of their morphologies.<br/>The model ICM will be compared to X-ray and microwave (via the Sunyaev-Zeldovich effect) to<br/>understand the relation between these observables and the underlying gas properties. Finally,<br/>the overall mass distribution will be used to better understand how these clusters gravitationally<br/>lens background galaxies."
"1713353","Probing New Physics in Galaxy Formation at Ultra-High Resolution","OAC","PETASCALE - TRACK 1","06/01/2017","04/27/2017","Philip Hopkins","CA","California Institute of Technology","Standard Grant","Edward Walker","05/31/2019","$20,064.00","","phopkins@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7781","","$0.00","A wealth of exciting new observational projects promise to revolutionize our understanding of galaxy and star formation: from the LSST and Gaia measuring Milky Way stellar populations in game-changing detail, to the James Webb Space Telescope probing galaxies during cosmic ""first light"", while the Hubble telescope identifies the long-""missing"" mass in the medium around galaxies.   This project intends to run large-scale cosmological hydrodynamic simulations on Blue Waters to make detailed predictions and leverage these transformative observations. The simulations will support the Feedback In Realistic Environments (FIRE) project, a network of theorists at 13 institutions, including several NSF postdoctoral and graduate student fellows: this collaboration has developed new, fully-cosmological simulations of galaxy formation that explicitly follow a diverse range of physics. <br/><br/>This project will carry out novel studies of galaxy formation by running cosmological simulations on Blue Waters with unprecedented resolution and physics. The project run a large suite of cosmological simulations, targeting galaxies from the faintest dwarfs through the Milky Way, at the ultra-high resolution and realism required to leverage the next-generation of observations. The petascale resources of Blue Waters will allow the project to resolve each galaxy with ~1 billion particles and follow them self-consistently over their entire history in live cosmological settings. These simulations will model the physics of galaxy formation with unprecedented realism, uniquely incorporating not only all of the important stellar feedback mechanisms (radiation pressure, photo-heating, stellar winds, supernovae), but also magnetic fields, physical (anisotropic) Braginskii conduction and viscosity, passive scalar (metal) diffusion, and explicit, multi-wavelength radiation hydrodynamics. This represents the culmination of several years of work supported by NSF, and will be critical to enable the science of the FIRE project.  The project will support an outreach component involving high school students and teachers, and undergraduate students, as well as a large science team using these simulations. The simulations will be used to make predictions specifically for next-generation observatories including (but not limited to): JWST, LSST, Gaia, and HST, in order to test theories of galaxy and star formation, constrain the origin of the heavy elements in the Universe, the re-ionization history of the early Universe, the effects of fundamental plasma physics in the circum and inter-galactic medium, and the nature of cold dark matter."
"1835144","Elements: Software: NSCI: Efficient GPU Enabled QM/MM Calculations: AMBER Coupled with QUICK","OAC","Software Institutes","09/01/2018","08/28/2018","Kenneth Merz","MI","Michigan State University","Standard Grant","Bogdan Mihaila","08/31/2021","$600,000.00","Andreas Goetz","merzjrke@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","8004","026Z, 077Z, 7923, 8004, 8005, 9216","$0.00","The over-arching goal of this project is to develop a software cyberinfrastructure aimed at solving important molecular-level problems in catalysis, drug design, and energy conversion. The PI and his collaborators will develop open source software that will enhance the ability to tackle chemical and biological problems using a sustainable model. The PI and his collaborators are collaborating with NVIDIA on this project to help accelerate the development efforts. Finally, the projects undertaken here will train students in formal theory, computer programming, computational chemistry and biology, and manuscript preparation/publication further enhancing the technical workforce in the USA.<br/><br/>Combined quantum mechanical/molecular mechanical (QM/MM) models have enabled significant advances in the understanding of chemical reactivity and intermolecular interactions. This approach allows regions of a system where bonds are to be broken and formed to be modeled using accurate QM methods, while the surrounding environment is treated using classical models. The most widely used QM models in QM/MM studies are generally semiempirical, but the most accurate employ density functional theory (DFT), Hartree-Fock (HF) or post HF methods. The shortcoming when using the more accurate methods is the computational expense, which limits the extent of QM/MM molecular dynamics simulations. The performance of QM methods has been greatly improved over the years through algorithmic and hardware improvements. This project will focus on both: for the former the PI will add the ability to handle long-range interactions in QM/MM calculations, add GPU enabled correlated methods and create an electron repulsion interaction (ERI) engine for general use, while for the latter the PI will integrate the GPU enabled Quantum Interaction Computational Kernel (QUICK) program with the Sander and PMEMD molecular dynamics (MD) engines from the AMBER suite of programs. AMBER is one of the most popular simulations packages and has been supported and sustained by the AMBER developer community for approximately 30 years. The developments proposed here will be fully available to the community via AMBERTools, which is released using an open source model (see http://ambermd.org/AmberTools.php). <br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Chemistry in the Directorate of Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1849519","Promoting International Collaboration on Developing Scalable, Portable & Efficient HPC Software for Modern HPC Platforms","OAC","INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE","10/01/2018","09/15/2018","Amitava Majumdar","CA","University of California-San Diego","Standard Grant","Sushil Prasad","09/30/2019","$27,750.00","","majumdar@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","1640, 7361","026Z, 7556, 9179","$0.00","Supercomputers are used to power discoveries and to reduce the time-to-results in a wide variety of disciplines such as engineering, physical sciences, and healthcare. Scalable and efficient software is required for optimally using the large-scale supercomputing platforms, and thereby, effectively leveraging NSF investments in the nation's advanced CyberInfrastructure (CI). With the rapid advancement in the computer architecture discipline, the complexity of the processors that are used in the supercomputers is also increasing, and, in turn, the task of developing efficient software for supercomputers is further becoming challenging and complex. To mitigate such challenges, there is a need for forums that brings together different stakeholders - the researchers and practitioners from the areas of software engineering and supercomputing. To provide such a platform, the second workshop on ""Software Challenges to Exascale Computing (SCEC)"" is being organized in India. This project funds the participation of ten US students from diverse backgrounds in the workshop and will provide them the opportunities to develop their skills in the area of ""software for supercomputing platforms"". The SCEC workshop will not only inform the participants about the challenges in large-scale software development for supercomputers but will also steer them in the direction of building international collaborations for finding solutions to those challenges. The project provides opportunities for workforce development, and serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense. <br/><br/>The workshop participants will learn about the two NSF funded supercomputers - Stampede2 and Comet - and will get practical experience at developing scalable, efficient, and portable software on those systems through tutorials and ""bring your own code"" sessions. The SCEC workshop will provide a forum through which hardware vendors and software developers can communicate with each other and influence the architecture of the next generation supercomputing systems and the supporting software stack. By fostering cross-disciplinary associations, the SCEC workshop will serve as a stepping-stone towards innovations in the future. The workshop will benefit researchers, students, and practitioners in supercomputing by providing them an opportunity to disseminate their results to the public, and find potential collaborators. The students funded through this project will get an opportunity to network with professionals, faculties and researchers working in the areas of supercomputing and software engineering, and will get to learn about the opportunities for internships, jobs, and higher education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1659377","CC* Cyber Team: Improving Access to Regional and National Cyberinfrastructure for Small and Mid-Sized Institutions","OAC","Campus Cyberinfrastrc (CC-NIE)","05/01/2017","03/15/2019","John Goodhue","MA","The Massachusetts Green High Performance Computing Center, Inc.","Standard Grant","Kevin Thompson","04/30/2020","$1,104,455.00","Bruce Segee, Stephen Everse, Scott Valcourt, Adrian Delmaestro","jtgoodhue@mghpcc.org","100 Bigelow St.","Holyoke","MA","010405984","4135524902","CSE","8080","","$0.00","Cyberinfrastructure is as important for research in the 21st century as test tubes and microscopes were in the 20th century. Familiarity with and effective use of cyberinfrastructure at small and mid-sized institutions is essential if their faculty and students are to remain competitive. This regional initiative enables effective use of cyberinfrastructure by researchers and educators at small and mid-sized institutions in Northern New England (Maine, Massachusetts, New Hampshire, Vermont) by making it easier to obtain support from Research Computing Facilitators. Research Computing Facilitators combine technical knowledge and strong interpersonal skills with a service mindset, and use their connections with cyberinfrastructure providers to ensure that researchers and educators have access to the best available resources.  It is widely recognized that Research Computing Facilitators are critical to successful utilization of cyberinfrastructure, but in very short supply.<br/><br/>Since most small and mid-sized institutions cannot individually support more than one or two Research Computing Facilitators, the project is developing a sustainable pool of experts who can work across institutions in the region. The project is investing further upstream, pairing mentors with students to both accelerate research and education projects at small institutions and develop a pipeline of new talent to meet growing academic and industry demand. In addition to supporting science, technology and education goals that are vital to the economic future of the Northeast, the project is testing and refining a new model for delivering support from Research Computing Facilitators that can benefit small and mid-sized institutions in other regions."
"1827199","Nilch Bee Naa Alkaa Go Ohooa Doo Eidii Tii  (Using Air (Technology) to Learn and Understand New Things)","OAC","Campus Cyberinfrastrc (CC-NIE)","09/01/2018","08/18/2018","Jason Arviso","NM","Navajo Technical University","Standard Grant","Kevin Thompson","08/31/2020","$667,909.00","Marla Meehl, Jared Ribble, John Hernandez","jarviso@navajotech.edu","LOWER POINT ROAD","Crownpoint","NM","873130849","5057864112","CSE","8080","","$0.00","Navajo Technical University (NTU) is one of the nation's largest tribal colleges and a leader in delivering academic and research programs for Native Americans.  NTU students have access to a plethora of academic programs including strong programs in Science, Technology, Engineering, and Math (STEM).  However, NTU and the residents of the Navajo Nation are not well connected to the Internet and to the larger research and education community.  Connectivity limitations, especially at Navajo community centers and at their homes, restrict NTU's ability to collaborate and contribute in the ever-growing integrated global research and education environment.  There is a fundamental lack of Internet connectivity with sufficient bandwidth to successfully participate in the ever-increasing distance or online learning courses/programs. <br/><br/>This proposal will increase Wide Area Network connectivity by connecting NTU to the Front Range GigaPoP (FRGP) regional network at much higher network speeds with dedicated bandwidth for NTU research and academic projects.  The proposal addresses distance education challenges by implementing an advanced wireless test bed to deliver NTU distance education courses to Chapter Houses, tribal libraries, and other community anchor locations.  This proposal engages the country's largest tribal university and is a collaboration with New Mexico and Arizona Tribal Colleges and Universities.  It leverages a strong existing regional relationship with the FRGP, and it provides an organizational model for other tribal colleges to adopt a similar technology and associated collaborations.  The proposal emphasizes needs and requirements-gathering meetings, followed by design and training workshops, which will benefit regional Native American community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1451024","IRNC: RXP: AtlanticWave-Software Defined Exchange: A Distributed Intercontinental Experimental Software Defined Exchange (SDX)","OAC","INTERNATIONAL RES NET CONNECT","04/01/2015","02/15/2019","Julio Ibarra","FL","Florida International University","Cooperative Agreement","Kevin Thompson","03/31/2020","$3,658,020.00","Russell Clark, Heidi L. Morgan","julio@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7369","5913, 5921, 5974, 7369","$0.00","Demand is growing to develop the capability to support end-to-end services, capable of spanning multiple Software Defined Networking (SDN) domains.  SDN deployments that cross multiple domains continue to be constructed manually, involving significant coordination and effort by network operators. Moreover, the demand for more intelligent network services to support the evolving science research and education activities between the U.S. and South America are increasing; these network services, which include dynamic provisioning of end-to-end multi-domain layer2 circuits, and network programmability, are needed to foster innovation for application developers, and to increase efficiency for network operators. AtlanticWave-SDX is a response to the demand for more intelligent network services to foster innovation and to increase network efficiency.<br/><br/>Florida International University (FIU) and the Georgia Institute of Technology (GT) are implementing AtlanticWave-SDX: a distributed experimental Software-Defined Exchange (SDX), supporting research, experimental deployments, prototyping and interoperability testing, on national and international scales.  A Software-Defined Exchange (SDX) will provide a capability to prototype an OpenFlow network where members of each Internet peering fabric could exchange traffic based in different layers of abstraction.  <br/><br/>AtlanticWave-SDX is comprised of two components: (1) a network infrastructure development component to bridge 100G of network capacity between Research and Education (R&E) backbone networks in the U.S. and South America; and (2) an innovation component to build a distributed intercontinental experimental SDX between the U.S. and South America, by leveraging open exchange point resources at SoX (Atlanta), AMPATH (Miami), and Southern Light (Sa&#771;o Paulo, Brazil)."
"1642388","SI2-SSE: Collaborative Research: Extending the Practicality and Scalability of LibMesh-Based Unstructured, Adaptive Finite Element Computations","OAC","Software Institutes, CDS&E","09/01/2016","08/22/2016","Paul Bauman","NY","SUNY at Buffalo","Standard Grant","Stefan Robila","08/31/2019","$350,065.00","","pbauman@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","8004, 8084","026Z, 7433, 8004, 8005, 9263","$0.00","The development and deployment of cyberinfrastructure focused on scientific and engineering simulation has been, and continues to be, essential to the progress of science and engineering in the U.S. This is particularly true for software used in large scale supercomputing environments. Thus, for the U.S. to continue leadership and advancement in scientific computing, it is crucial that software infrastructure advance to enable modern computational and software engineering strategies for simulating complex scientific and engineering systems. Once such piece of software is the libMesh finite element library. libMesh is used by hundreds of research groups in the U.S. and around the world. Critically, libMesh can utilize large scale supercomputing infrastructure for simulating scientific and engineering systems. This work will update the libMesh software library to use state-of-the-art algorithms that will enable robust simulations on the largest supercomputers in the world and further advance the complexity of systems that can be successfully modeled using libMesh. Furthermore, the library will be enhanced to support user applications to leverage modern computer architectures, including emerging many-core architectures. This will enable the continued use of libMesh as both a fundamental tool of scientific and engineering simulation and as an educational tool for computational algorithms.<br/><br/>The libMesh finite element library is a prominent example of an open-source tool supporting adaptive mesh refinement, interfaces to preeminent solver packages, and solutions on large parallel supercomputers of complex finite element models. libMesh supports hundreds of users and many applications in solving partial differential equations across a variety of disciplines including solid mechanics, fluids mechanics, magnetohydrodynamics, hypersonics, nuclear engineering, combustion, and acoustics, to name a few examples. Following over a decade of successful collaborative open-source development, the library is poised to maintain its place as a prominent open-source finite element package. To do so, libMesh must be made to support emerging many core architectures, leverage the most advanced scalable algorithms, and interface with geometry underlying the complex meshes used in engineering analysis. The work addresses these issues directly by extending and enhancing the libMesh finite element library. The extensions will seamlessly make available modern solution algorithms through interfaces to world class solver libraries, facilitate the interaction with underlying geometric representations using openly available software libraries, and efficiently utilize modern computing hardware through cutting-edge software engineering principles and designs. Simultaneously, the developed interfaces will allow for flexibility of development of modeling kernels and maintain the low the barrier of entry that libMesh has always had for both the libMesh community as well as the scientific community in general. Such lofty goals will be attained by designing usable interfaces that hide the complexity of the underlying algorithms and extensive testing on modern computing architectures to ensure performance and scalability is delivered to the libMesh community."
"1852102","REU Site: High Performance Computing with Engineering Applications","OAC","RSCH EXPER FOR UNDERGRAD SITES","02/15/2019","02/04/2019","Daqing Hou","NY","Clarkson University","Standard Grant","Sushil K Prasad","01/31/2022","$359,931.00","Yu Liu","dhou@clarkson.edu","8 Clarkson Avenue","Potsdam","NY","136761401","3152686475","CSE","1139","9250","$0.00","This award provides support for a three-year REU Site at Clarkson University, Potsdam, New York, in High Performance Computing (HPC), a field critical to national security, scientific discovery, and technological innovation. The goal of this project is to encourage 30 talented undergraduate students to pursue graduate study and careers in HPC by engaging them in exciting, ongoing research projects and by cultivating their talents during ten-week summer research experiences and beyond. Sixteen ongoing research projects available for students are innovative and share the common theme of societally relevant engineering applications that originate from disciplines as diverse as aeronautical engineering, chemical engineering, civil engineering, electrical and computer engineering, software engineering, and applied mathematics.  The project thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense.<br/><br/><br/>Students are recruited from targeted institutions that offer limited or no research opportunities in HPC, such as historically minority colleges and universities. Emphasis is placed on attracting underrepresented minority and female students.  Faculty with expertise in HPC applications mentor the students to be researchers and provide them with specialized training in the design and development of HPC applications that are rooted in and motivated by engaging engineering innovations. This specialized training provides the students with technical and analytical skills in mathematical modeling, algorithmic development, and parallel and distributed programming, which benefit them in future pursuits such as graduate study or industry work. The quality of this training is further reinforced by additional professional development activities, including an HPC crash course, field trips, invited speakers, weekly group meetings, and a mentor training workshop for faculty.  Seminars in career development, literature search, technical writing, and graduate school advisement, are also integrated into the program, and all student participants are given the opportunity to attend and present in professional conferences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1642398","SI2-SSE: Collaborative Research: Extending the Practicality and Scalability of LibMesh-Based Unstructured, Adaptive Finite Element Computations","OAC","Software Institutes","09/01/2016","08/22/2016","Roy Stogner","TX","University of Texas at Austin","Standard Grant","Stefan Robila","08/31/2019","$144,815.00","","roystgnr@ices.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8004","026Z, 7433, 8004, 8005","$0.00","The development and deployment of cyberinfrastructure focused on scientific and engineering simulation has been, and continues to be, essential to the progress of science and engineering in the U.S. This is particularly true for software used in large scale supercomputing environments. Thus, for the U.S. to continue leadership and advancement in scientific computing, it is crucial that software infrastructure advance to enable modern computational and software engineering strategies for simulating complex scientific and engineering systems. Once such piece of software is the libMesh finite element library. libMesh is used by hundreds of research groups in the U.S. and around the world. Critically, libMesh can utilize large scale supercomputing infrastructure for simulating scientific and engineering systems. This work will update the libMesh software library to use state-of-the-art algorithms that will enable robust simulations on the largest supercomputers in the world and further advance the complexity of systems that can be successfully modeled using libMesh. Furthermore, the library will be enhanced to support user applications to leverage modern computer architectures, including emerging many-core architectures. This will enable the continued use of libMesh as both a fundamental tool of scientific and engineering simulation and as an educational tool for computational algorithms.<br/><br/>The libMesh finite element library is a prominent example of an open-source tool supporting adaptive mesh refinement, interfaces to preeminent solver packages, and solutions on large parallel supercomputers of complex finite element models. libMesh supports hundreds of users and many applications in solving partial differential equations across a variety of disciplines including solid mechanics, fluids mechanics, magnetohydrodynamics, hypersonics, nuclear engineering, combustion, and acoustics, to name a few examples. Following over a decade of successful collaborative open-source development, the library is poised to maintain its place as a prominent open-source finite element package. To do so, libMesh must be made to support emerging many core architectures, leverage the most advanced scalable algorithms, and interface with geometry underlying the complex meshes used in engineering analysis. The work addresses these issues directly by extending and enhancing the libMesh finite element library. The extensions will seamlessly make available modern solution algorithms through interfaces to world class solver libraries, facilitate the interaction with underlying geometric representations using openly available software libraries, and efficiently utilize modern computing hardware through cutting-edge software engineering principles and designs. Simultaneously, the developed interfaces will allow for flexibility of development of modeling kernels and maintain the low the barrier of entry that libMesh has always had for both the libMesh community as well as the scientific community in general. Such lofty goals will be attained by designing usable interfaces that hide the complexity of the underlying algorithms and extensive testing on modern computing architectures to ensure performance and scalability is delivered to the libMesh community."
"1835747","Collaborative Research: Elements: Software: Software Health Monitoring and Improvement Framework","OAC","Software Institutes","11/01/2018","08/31/2018","Marouane Kessentini","MI","University of Michigan Ann Arbor","Standard Grant","Stefan Robila","10/31/2021","$299,534.00","","marouane@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Software underpins every aspects of modern life, with significant impact in society. Poor quality software can cause huge financial losses, even threatening people's lives. Software quality is even more critical within the scientific community. The reproducibility of research results and sustainability of the research itself, heavily depend on the quality of the software developed by scientists, who usually acquire basics of software programming but are not aware of the best design practices. As a consequence, several existing open access scientific software packages are known to be hard to use and evolve due to their poor quality, as highlighted in recent studies. This project will integrate and enhance recent advances in software issue detection and refactoring techniques, created by the PIs and sponsored by NSF, in order to serve diverse scientific and engineering domains, detecting and fixing software quality issues effectively. <br/><br/>This proposal seeks to bridge the gap between software engineering community and other science and engineering community in general. It will provide quantitative comparisons of software projects against an industrial benchmark, enable users to pinpoint software issues responsible for high maintenance costs, visualize the severity of the detected issues, and refactor them using the proposed interactive refactoring framework. The proposed framework will bring together software users and software developers by enabling non software experts to post software challenges for the software community to solve, which will, in turn, boost the research and advances in software research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1615114","Petaflops simulation and design of nanoscale materials and devices","OAC","PETASCALE - TRACK 1","05/01/2016","04/29/2016","Jerzy Bernholc","NC","North Carolina State University","Standard Grant","Edward Walker","04/30/2019","$29,232.00","Carl Kelley, Wenchang Lu, Miroslav Hodak, Emil Briggs","bernholc@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","7781","","$0.00","The President's Materials Genome initiative has amply recognized<br/>the potential of simulation for helping to design new materials ""twice as fast and at a fraction<br/>of the cost."" In the quest for novel and breakthrough properties, nanoscale science and technology<br/>is particularly promising, because at the nanoscale the quantum properties of matter manifest<br/>themselves in unexpected ways, leading to non-linear, novel behavior that cannot be predicted<br/>by simple extrapolation from either the molecular or the bulk limits. Furthermore, experiments<br/>are very difficult at these scales, since they require atomic or nearly-atomic resolution,<br/>yet must also account for the emergent properties that involve thousands of atoms. Advanced<br/>simulations can supplant much of the tedious experimentation, allowing researchers to step through<br/>different options in the experimental design space. More importantly, the atomic-scale design principles<br/>leading to the desired materials or device characteristics can be uncovered through in-depth<br/>analysis of computational results, leading to accelerated progress and optimal design for<br/>a given application. <br/><br/>The goal of this project is to investigate fundamental properties of nano and nano-bio structures<br/>with potential applications in biomolecular sensors and nano-scale electronics for beyond-Moore's-law<br/>era. Specifically, the project will investigate (i) carbon nanotube-based nanocircuits that can monitor<br/>DNA replication and potentially enable high-throughput electrical sequencing of DNA, and (ii)<br/>carbon-nanoribbon-based devices for nanoscale electronics and spintronics. The proposed simulations are ambitious<br/>and potentially transformative.<br/><br/>This proposal aims to apply highly optimized quantum simulation codes to Blue Waters and to<br/>use them in two projects of high current interest: electrical detection of DNA sequence and<br/>nanoribbon-based electronics for beyond-Moore's-law era. <br/>In DNA sequencing, the proposed methodology based on electrical readout of the DNA sequence would constitute a major breakthrough<br/>and result in much faster and cheaper sequencing. It would allow for sequencing of a large<br/>fraction of population, enable truly personalized medicine and lead to thorough mapping and<br/>understanding of genetic diseases. Additionally, graphene and graphene nanoribbons are major candidates<br/>for future nanoscale devices for beyond-Moore's-law era. By systematically investigating the<br/>transport properties of nanoribbon-based electronic devices, the project will allow us to understand<br/>device performance at a quantum level and help to design the new generation of transistors.<br/><br/>Access to cheap and broadly available DNA sequencing would revolutionize health-care and treatment.<br/>The computational design and discovery of appropriate nucleotides to enable electrical DNA<br/>sequencing would lead to new technologies and new manufacturing. Furthermore, the beyond<br/>Moore's law era is of great interest to computer and computational scientists, engineers and<br/>the general public. Finally, the project will have significant educational impact to the project's local institution<br/>by involving undergraduate and graduate students, as well as postdoctoral fellows,<br/>in leading-edge computational research, with special effort being made to ensure participation<br/>of members of underrepresented groups."
"1440534","SI2-SSE: Solving Polynomial Systems with PHCpack and phcpy","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS","10/01/2014","08/01/2014","Jan Verschelde","IL","University of Illinois at Chicago","Standard Grant","Bogdan Mihaila","09/30/2019","$464,352.00","","jan@math.uic.edu","809 S. Marshfield Avenue","CHICAGO","IL","606124305","3129962862","CSE","1253, 8004, 8069","7433, 8005, 8251","$0.00","Solving polynomial systems is a fundamental problem in mathematics with applications to various fields of science and engineering. The free and open source software PHCpack applies symbolic-numeric and polyhedral methods to solve polynomial systems. As a new interface to PHCpack written in the Python scripting language, phcpy improves the functionality of PHCpack. The implementation on parallel computers to compensate for the cost overhead of multi-precision arithmetic will enable scientists and engineers to solve larger systems faster and more accurately. A web server developed with phcpy will give anyone with an internet connection access to the developed software.<br/><br/>The solvers in PHCpack apply homotopy continuation methods, blending symbolic-numeric with polyhedral algorithms. Numerical approximations to solutions are computed with Newton's method. Solutions are approximated symbolically by Puiseux series, which originate at initial forms defined by the Newton polytopes of the polynomials in the system. The design of phcpy gives a flexible interactive scripting interface, without sacrificing efficiency as compiled code in PHCpack is executed. The package phcpy will provide the tools for a scalable compute server to serve requests submitted to the web server. Multithreaded implementations on multicore processors accelerated by graphics processing units compensate for the cost overhead of double double and quad double arithmetic."
"1827211","CC* Integration: End-to-End Software-Defined Cyberinfrastruture for Smart Agriculture and Transportation","OAC","CISE RESEARCH RESOURCES","10/01/2018","07/23/2018","Hongwei Zhang","IA","Iowa State University","Standard Grant","Deepankar Medhi","09/30/2020","$999,919.00","Patrick Schnable, Arun Somani, Ahmed Kamal, Anuj Sharma","hongwei@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","2890","9102","$0.00","Imaging and other sensor-based understanding of plant behavior is becoming key<br/>to new discoveries in plant genotypes leading to more productive and environment-friendly farming.<br/>Similarly, distributed sensing is seen as a key component of a safe, efficient, and sustainable autonomous transportation systems.<br/>Existing research and education in agriculture and transportation systems are constrained by the lack of connectivity between field-deployed testbed equipment and computing infrastructure. To realize that connectivity, this project proposes to deploy CyNet wireless networks to<br/>connect experimental science testbeds to high-performance cloud computing infrastructures.<br/><br/>The CyNet project will:<br/>1) deploy Predictable, Reliable, Real-time, and high-Throughput (PRRT) wireless networking solutions using the standards-compliant, open-source Open Air Interface software framework and commodity Universal Software Radio Peripheral (USRP) hardware;<br/>2) integrate these wireless networks with software defined networks to seamlessly integrate outdoor cameras, sensors, and autonomous vehicles, and connect these components to high performance cloud computing systems;<br/>3) implement an infrastructure virtualization system that partitions CyNet into programmable, isolated experiments; and<br/>4) create an infrastructure management system that performs admission and access control and establishes specified resource allocation policies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1404995","CDS&E: Compiler/Runtime Support for Developing Scalable Parallel Multi-Scale Multi-Physics","OAC","CDS&E","07/01/2014","07/31/2014","Ponnuswamy Sadayappan","OH","Ohio State University","Standard Grant","Vipin Chaudhary","06/30/2019","$544,347.00","Atanas Rountev","sadayappan.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","8084","7433, 8084","$0.00","The dramatic strides in computer speed and performance over the last few decades make it feasible to accurately model increasingly complex phenomena. However, achieving high performance on massively parallel supercomputers is an extremely challenging task. With deepening memory hierarchies, significantly higher degrees of per-chip multi-core parallelism, the task of programming compute-intensive engineering applications to attain high performance on a large scale cluster system has become increasingly difficult. It is often the case that the time and effort required to develop effective and efficient software has become the bottleneck in advancing many areas of science and engineering. This challenge can be overcome by advances in compile-time/runtime systems that can ease the burden on the programmer while delivering a high performance portable instantiation of the particular application on modern and emerging high performance platforms.<br/><br/>To address this challenge, this project is developing a novel framework for transforming irregular scientific/engineering applications in a global address space framework. The research is grounded in a very different and complementary research direction to most current efforts in addressing the challenge of enhancing programmer productivity, maintaining portability, and achieving good performance on scalable distributed-memory parallel systems. The project will advance compiler/runtime techniques so that users can develop annotated sequential programs, to be automatically transformed by our system for efficient execution on distributed-memory parallel systems. This approach is motivated by the success of the popular OpenMP and OpenACC pragma based approaches to transforming annotated sequential programs for parallel execution on multicore and GPU/accelerator systems, respectively. An annotation based OpenAPP (APP - Asynchronous Partitioned Parallelism) framework is proposed for source-to-source transformation of an important class of scientific/engineering programs using the inspector/executor paradigm for execution on distributed-memory parallel systems. The proposed framework will be validated using several medium to large scale applications.<br/><br/>The project seeks to significantly lower the entry barrier associated with effective use of scalable distributed-memory computers, which are essential if more than 100x performance improvement over sequential codes is sought. A successful outcome of this project will be transformative for computational and domain scientists and engineers who seek to use next generation parallel systems for their simulation and modeling. The developed tools will be made publicly available to the community under an open source license. The project will also organize workshops that bring together compiler/runtime experts and computational scientists developing massively parallel scientific/engineering applications."
"1663636","SI2-SSI: Sustainable Open-Source Quantum Dynamics and Spectroscopy Software","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes","09/01/2017","05/08/2017","Xiaosong Li","WA","University of Washington","Standard Grant","Vipin Chaudhary","08/31/2021","$1,500,000.00","Anne McCoy, Eitan Geva, Albert DePrince","xsli@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1253, 8004","7433, 8004, 8009","$0.00","Advances in experimental techniques that use multiple light sources to probe chemical systems   provide unprecedented ability to understand the evolution of important and interesting chemical processes.   Theory and simulation are essential to extract the maximum amount of information from these experiments.   Thus, there is a strong need for user-friendly computer programs to model these processes and to provide physical insights to experimentalists.  To this end, this project is developing an innovative software package (ChronusQ) capable of modeling many types of experimental measurements that involve matter interacting with multiple incident light sources. The physical insights gleaned through application of ChronusQ  will be useful for the advancement of renewable energy and information technologies. In addition, given the nature of the project, students and postdocs will have unique opportunities to gain experience in high performance computing software development through direct collaborations with engineers from industrial partnerships <br/><br/>The overarching goal of the proposed activity is to develop an innovative software platform, namely Chronus Quantum (ChronusQ), which is capable of modeling different types of time resolved multidimensional spectral signals using quantum electronic and nuclear dynamics. ChronusQ performs quantum dynamic simulations of the same light and matter interactions that occur in time resolved multidimensional spectroscopies directly in the time domain.  The time correlated experimental observables required to model multidimensional spectra can then be extracted from these simulations. By providing a time dependent, state specific interpretation for the chemical dynamics encoded in multidimensional spectra, the proposed development will aid in the design of new molecules and materials that exhibit the desirable optical characteristics, with the potential for transformative impact in the broader scientific community and beyond. The ChronusQ software represents the next frontier for innovations in computational spectroscopy, which will have a far reaching impact on education and research in multidisciplinary scientific communities including chemistry, physics, nanoscience and surface science, and other fields relying on these cutting edge spectroscopic methods."
"1740282","SI2-SSE: Collaborative Research: Integrated Tools for DNA Nanostructure Design and Simulation","OAC","Software Institutes, CDS&E, DMREF","09/01/2017","08/29/2017","Shawn Douglas","CA","University of California-San Francisco","Standard Grant","Vipin Chaudhary","08/31/2020","$250,000.00","","Shawn.Douglas@ucsf.edu","1855 Folsom St Ste 425","San Francisco","CA","941034249","4154762977","CSE","8004, 8084, 8292","026Z, 067E, 084E, 7433, 8004, 8005, 9263","$0.00","Nanotechnology could one day revolutionize several activities of great importance to our national interest, including how we manufacture consumer products, how we diagnose and treat disease, and how we detect and neutralize threats to our defense. One promising approach to atomically precise construction is adapting molecular building blocks from living organisms such as DNA, RNA, and proteins, and repurposing them to self-assemble into prescribed shapes, devices, and materials. A key bottleneck to progress is the complexity of designing, building, and testing nanostructures comprised of thousands or millions of atoms. The goal of this project is to accelerate development of bio-inspired nanostructures by integrating two widely adopted software tools used in bio-nanostructure design and physics-based molecular simulation. The products of this effort will enhance our fundamental capability to understand and precisely engineer self-assembled biomolecular nanostructures, which, when coupled with experimental validation in the laboratory, will enable future demand-meeting applications of bionanotechnology.<br/><br/>Toward realizing the goal of programming matter with nanoscale precision, this project will develop software interfaces between two classes of molecular design programs that, until now, have been evolving independently from one another. A widely adopted DNA structure design program, Cadnano, will be extended to utilize the results of physics-based microscopic simulations, enabling an iterative structure design process. A leading molecular graphics program, VMD (Visual Molecular Dynamics), will be developed to seamlessly visualize Cadnano designs, provide their structural interpretation, and enable further modification of the structures using an arsenal of computational structural biology and nanotechnology tools. Both developments will utilize recent advances in cloud computing technologies, making the DNA structure design software available anywhere and to anyone in a platform-independent manner.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Civil, Mechanical and Manufacturing Innovation in the Directorate of Engineering."
"1566049","CRII: ACI: Algorithms and Tools to Facilitate the Development of High Fidelity Reactive Molecular Dynamics Models","OAC","CRII CISE Research Initiation","06/15/2016","06/20/2016","Hasan Metin Aktulga","MI","Michigan State University","Standard Grant","Sushil Prasad","05/31/2019","$175,000.00","","hma@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","026Y","7361, 8228","$0.00","Atomistic simulations enable understanding, analysis, and design of complex systems at spatio-temporal scales not easily accessible to experimental observation. Conventional molecular dynamics techniques have been applied with great success in application domains ranging from materials design to biophysical systems. Recent developments in reactive molecular dynamics have significantly extended the application scope of these techniques to systems that involve chemical bond activity. The core of these reactive atomistic simulations is a model for atomic interactions that describes the chemical structure of the domain, as well as its time evolution. This project aims to build a cyberinfrastructure to facilitate the development of highly resolved models of atomic interactions, along with their associated parameterizations. It will accomplish these goals through novel algorithms and software tools, reference datasets, and comprehensive validation in diverse application domains. The resulting infrastructure can enable computational scientists to develop high fidelity reactive force fields in a time and cost effective way. As a result, the accuracy and scope of reactive atomistic simulations have the potential to be increased significantly. The intellectual contributions of the project are complemented by education and outreach efforts that focus on interdisciplinary education through development and dissemination of instructional modules, publications, and presentations. Outreach efforts focus on recruitment of underrepresented minorities at the graduate level, and integration of undergraduate students into research efforts at an early stage.  Therefore, this research aligns with the NSF mission to promote the progress of science and to advance the national health, prosperity and welfare.<br/><br/>This project focuses on algorithms and software for the development and optimization of force fields for reactive molecular dynamics (MD) techniques. The fidelity of these simulations is critically dependent on the functional forms and parameterizations of the inter-atomic potentials. Traditionally, the development of inter-atomic potentials has involved significant domain expertise, and is highly time and labor-intensive. This project aims to design a powerful new cyberinfrastructure that integrates extraction of reference datasets, model selection and optimization algorithms, and comprehensive validation, to enable rapid prototyping and deployment of complex reactive atomistic models. In particular, reference datasets are selected for coverage as well as their impact on the inter-atomic potential, using novel procedures to minimize the requirements on quantum mechanical screenings. The process of optimizing the force field leverages this reference set and introduces a histogram based optimization technique for parameter sampling and selection. Finally, a novel API is proposed to enable the translation of suitable force fields from high-level mathematical descriptions into efficient parallel software. By automating each labor-intensive step along the way, the proposed framework aims to form a first-of-its-kind environment for development of high fidelity reactive MD models. Given the applicability of reactive MD models from materials modeling to biophysical simulations, the proposed cyberinfrastructure can help advance the state-of-the-art in advance materials design and drug discovery."
"1740212","SI2-SSE: Collaborative Research: Integrated Tools for DNA Nanostructure Design and Simulation","OAC","Software Institutes, CDS&E, DMREF","09/01/2017","08/29/2017","Aleksei Aksimentiev","IL","University of Illinois at Urbana-Champaign","Standard Grant","Vipin Chaudhary","08/31/2020","$249,994.00","","aksiment@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8004, 8084, 8292","026Z, 067E, 084E, 7433, 8004, 8005, 9263","$0.00","Nanotechnology could one day revolutionize several activities of great importance to our national interest, including how we manufacture consumer products, how we diagnose and treat disease, and how we detect and neutralize threats to our defense. One promising approach to atomically precise construction is adapting molecular building blocks from living organisms such as DNA, RNA, and proteins, and repurposing them to self-assemble into prescribed shapes, devices, and materials. A key bottleneck to progress is the complexity of designing, building, and testing nanostructures comprised of thousands or millions of atoms. The goal of this project is to accelerate development of bio-inspired nanostructures by integrating two widely adopted software tools used in bio-nanostructure design and physics-based molecular simulation. The products of this effort will enhance our fundamental capability to understand and precisely engineer self-assembled biomolecular nanostructures, which, when coupled with experimental validation in the laboratory, will enable future demand-meeting applications of bionanotechnology.<br/><br/>Toward realizing the goal of programming matter with nanoscale precision, this project will develop software interfaces between two classes of molecular design programs that, until now, have been evolving independently from one another. A widely adopted DNA structure design program, Cadnano, will be extended to utilize the results of physics-based microscopic simulations, enabling an iterative structure design process. A leading molecular graphics program, VMD (Visual Molecular Dynamics), will be developed to seamlessly visualize Cadnano designs, provide their structural interpretation, and enable further modification of the structures using an arsenal of computational structural biology and nanotechnology tools. Both developments will utilize recent advances in cloud computing technologies, making the DNA structure design software available anywhere and to anyone in a platform-independent manner.<br/><br/>This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Civil, Mechanical and Manufacturing Innovation in the Directorate of Engineering."
"1835838","Element:Software:Enabling Millisecond-Scale Biomolecular Dynamics","OAC","SPECIAL INITIATIVES, Software Institutes","10/01/2018","09/12/2018","Erik Santiso","NC","North Carolina State University","Standard Grant","Vipin Chaudhary","09/30/2021","$599,998.00","Carol Hall, Stefano Menegatti","eesantis@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","1642, 8004","026Z, 077Z, 7923, 8004","$0.00","Computer simulation methods based on Molecular Dynamics (MD) have been used for decades to understand chemical and biochemical phenomena at the molecular level. MD is a very powerful tool that has enabled scientists to understand the behavior of molecules crucial for life such as proteins and nucleic acids. MD has also been used to understand diseases and develop new drugs. However, MD is limited in both the size of the systems that can be studied and the amount of time that can be simulated. Many complex phenomena relevant to life involve systems too large to study with MD, or require following the system for much longer times. An alternative to traditional MD called discontinuous molecular dynamics (DMD) has been shown to be much more efficient to study biomolecular processes. To date, however, the use of DMD has been limited due to its inability to take advantage of modern parallel computers. This project will develop a next-generation parallel DMD software that will enable the study of complex molecular phenomena involving larger systems and longer time scales. Detailed knowledge of such processes will considerably advance the development of new materials and drugs, and human health.  The project team combines the computational and experimental expertise to successfully develop and validate a robust parallel DMD software framework. The software and results will be actively shared both with the computational simulation community and with the scientific and engineering community at large, contributing to the capability, capacity, and cohesiveness of the national cyber-infrastructure ecosystem. Furthermore, the results of this project will be used in outreach efforts geared toward the education and inclusion of minorities traditionally underrepresented in higher STEM education.<br/><br/><br/>This project aims to develop an open software framework that enables multi-millisecond dynamic simulations of peptides and peptide-mimetics by implementing a parallel discontinuous molecular dynamics (DMD) package. Unlike current molecular dynamics (MD), which features limited simulation timescales, discontinuous molecular dynamics (DMD) assumes ballistic motion of the particles between interaction events and enables the study of phenomena across much longer time scales. To demonstrate the approach, the project will (1) develop a parallel version of existing serial DMD codes to enable extending simulation times from hundreds of microseconds to several milliseconds; (2) extend and improve the available DMD peptide force field, adding parameters for non-natural peptides and peptoids; and (3) develop software for translating interaction potentials from traditional MD to DMD. The project team possesses the complementary expertise necessary for this project, including coarse-grained models and force fields for complex polymers and peptoids, MD simulation of protein self-assembly and peptide-protein binding processes, synthesis of protein-binding peptides and peptoids, and measurement of thermodynamic and kinetic binding parameters. The tools resulting from this research will allow the scientific and engineering community to model and study very long time-scale phenomena, such as biopolymer folding, aggregation and inhibition of aggregation, fibril formation, and protein-binding. This toolbox shows great promise to not only accelerate innovation in the computational design of biomaterials, but also to impact the molecular simulation community focusing on highly complex systems, up to cell-level dynamics. Notably, this project is ideal for the National Science Foundation's Cyber-infrastructure for Sustained Scientific Innovation (CSSI), as it (i) contributes to the capability, capacity, and cohesiveness of the national cyberinfrastructure ecosystem by providing user-friendly open-source computational tools, (ii) actively engages CI experts and testers of our toolbox, who would potentially be its ultimate users, (iii) advances our current capabilities in developing bioactive peptides and peptoids, (iv) establishes plans and metrics that encourage measurement of progress and certify the quality of shared tools and results, and (iv) devise strategies to combine wide-access with long-term community-driven development and progress.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1713685","High Resolution Earth System Modeling for International Climate Assessment","OAC","PETASCALE - TRACK 1","09/01/2017","05/03/2017","Ryan Sriver","IL","University of Illinois at Urbana-Champaign","Standard Grant","Edward Walker","08/31/2019","$24,973.00","","rsriver@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7781","","$0.00","This collaborative research between the University of Illinois and the National Center for Atmospheric Research (NCAR) is aimed at using the Blue Waters petascale resources to address key uncertainties associated with the numerical modeling of the Earth's climate system and the ability to accurately analyze past and projected future changes in climate. The project brings together a team of scientists to address these issues with well-recognized expertise in the study of past and future projections of climate and extensive experience in national and international assessments of climate change, as well as experts in computer science and information technology.  The petascale computing capabilities will allow the project to pursue climate extremes and analysis of uncertainties in ways that were impossible before by allowing for completion of very high resolution atmosphere and ocean fully-coupled climate simulations, a task that requires a large computing allocation only achievable on petascale systems like Blue Waters. <br/><br/>This project has two purposes distinguished by the model resolution to be employed. Both climate model simulations will be conducted with an advanced version of the NSF-DOE Community Earth System Model (CESM).  CESM, one of the world's best models of Earth's climate system, has been developed by NCAR in coordination with a community of scientists at universities and national laboratories. The first purpose is aimed at better quantifying future regional climate change, focusing on climate extremes, by running century-long, high-resolution (0.25 degree atmosphere/land, 1 degree ocean/sea ice) global coupled climate simulations, including multiple ensemble members of 20th Century and future scenarios. The second purpose will be specific studies that grow upon the project's existing Blue Water's allocation to further clarify the effects of small-scale regional features and interactions across spatial scales in climate through even higher-resolution climate simulations (0.25 degree atmosphere/land, 0.1 degree ocean/sea ice) that will push the state-of-the-art for such analysis."
"1835414","Software Elements: NUPACK: Molecular Programming in the Cloud","OAC","SPECIAL STUDIES AND ANALYSES, Software Institutes","11/01/2018","09/07/2018","Niles Pierce","CA","California Institute of Technology","Standard Grant","Vipin Chaudhary","10/31/2021","$600,000.00","","niles@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","1385, 8004","026Z, 077Z, 7923, 8004","$0.00","Life is orchestrated by programmable biomolecules (DNA, RNA, and proteins) that interact within complex molecular machines and biological circuits to grow, regulate, and repair organisms. These biological proofs-of-principle inspire diverse engineering efforts within the new fields of molecular programming, nucleic acid nanotechnology, and synthetic biology. Over the coming decades, these fields are poised to generate transformative programmable molecular and cellular technologies addressing challenges to science and society ranging from neuroscience and development, to diagnosis and treatment, and from renewable energy to sustainable manufacturing. To support these engineering efforts, the PI is engaged in a multi-decade effort to develop NUPACK (Nucleic Acid Package), a growing software suite for analyzing and designing nucleic acid structures, devices, and systems. Launched in 2007, NUPACK usage has grown to the point where the NUPACK compute resource is frequently overwhelmed by the research community. With the proposed work, the NUPACK web application will be re-architected from the ground up to run in the cloud, enabling the resource to scale dynamically in response to spikes in researcher demand and to growth year-over-year. The NUPACK user interface will be substantially expanded to allow users to harness next-generation analysis and design tools. Additionally, the re-architected web application will benefit from a complete re-write of the NUPACK scientific code base (moving from NUPACK 3.2 to 4.0) to achieve dramatic computational speed-ups and exploit enhanced physical models. With NUPACK in the cloud, users will be able to perform calculations far beyond current capabilities both in terms of scale and scientific scope, enabling exploration of a growing frontier of programmable molecular technologies.<br/><br/>NUPACK is a growing software suite for the analysis and design of nucleic acid structures, devices, and systems serving the needs of researchers in the emerging disciplines of molecular programming, nucleic acid nanotechnology, and synthetic biology. NUPACK algorithms are unique in treating complex and test tube ensembles containing arbitrary numbers of interacting strand species, providing crucial tools for capturing concentration effects essential to analyzing and designing the intermolecular interactions that are a hallmark of these new fields. Usage has increased to the point where the NUPACK compute cluster is frequently overwhelmed. With the proposed work, the NUPACK web application will be re-architected to enable deployment on the cloud, containerizing the dozens to thousands of jobs that are launched by a single click, and enabling the scale of the resource to vary dynamically minute-to-minute and year-over-year. To move to a sustainable model for NUPACK compute hardware and engineering support, NUPACK user accounts will be created that enable users to view and retrieve old jobs, to seamlessly pay for the cloud compute cycles that are used for their jobs, and to provide incremental support for the NUPACK Software Engineer proportional to their usage of this non-profit academic resource. The user interface will be substantially expanded to allow users to harness the new capabilities of the enhanced NUPACK backend, including kinetic analysis for complex and test tube ensembles, kinetic design for test tube ensembles, equilibrium design for large-scale pseudo-knotted structures in test tube ensembles, and use of new computationally parameterized physical models generated for custom experimental conditions. The re-architected web application will also benefit from a complete re-write of the NUPACK scientific code base, featuring improved implementations, reduced-complexity algorithms, overflow-safe evaluation algebras, and expanded physical models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1651577","CAREER: Bridging Geometric Design and Aerodynamic Simulation of Turbomachinery: An Integrative Design-Through-Analysis Framework Enabled by Embedded Domain Methods","OAC","CAREER: FACULTY EARLY CAR DEV","02/01/2017","12/28/2016","Dominik Schillinger","MN","University of Minnesota-Twin Cities","Standard Grant","Sushil Prasad","01/31/2022","$500,000.00","","dominik@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1045","1045","$0.00","The transfer from computer-aided geometric design to physics-based computer simulation requires geometry processing and mesh generation procedures, which often constitute significant barriers to creating rapid design-through-analysis workflows.  The objective of this project within the Faculty Early Career Development (CAREER) Program is to remove these barriers by developing new computational methodologies that enable seamless integration and automation.  Turbomachinery is a key enabling energy technology, e.g., for electricity production and air transport.  The new design-through-analysis methodologies envisioned in this project aims to fundamentally facilitate simulation-based turbomachinery design and optimization.  These can lead to critical improvements in design procedures that serve the national interest, promote national prosperity, and enable unconventional turbomachinery designs that improve sustainability and energy efficiency.  The project results also promote the progress of computational science, in particular by contributing significant methodological advances to the current drive towards unsteady full-wheel simulations.  The project includes an education component that combines a new summer camp with research internship opportunities for high school and undergraduate students.  It provides opportunities for energizing K-12 and undergraduate experiences that motivate participants to join advanced degree programs in engineering, with particular emphasis on minorities and underrepresented groups.  This will directly strengthen the infrastructure for STEM outreach at the University of Minnesota and contribute to developing and maintaining a well-trained STEM workforce, which is important for the future competitiveness of related industries in the United States.<br/><br/>The research approach revolves around the idea of combining parametric geometry modeling with new embedded domain finite element methods for integrated computational aerodynamics.  While parametric modeling techniques are available in commercial design tools, embedded domain methods remain plagued by serious technology gaps that prevent their application for high-fidelity aerodynamics.  Examples include the detrimental impact of elements with small cuts on conditioning and time step size, low-order accuracy of quadrature rules in cut elements, and algorithms that are unsuitable for emerging heterogeneous computing architectures.  The research approach focuses on devising new techniques that effectively close these gaps, incorporate key computational aerodynamics paradigms such as the Discontinuous Galerkin concept and the variational multiscale method for large eddy turbulence modeling, and enable high-order accuracy beyond established second-order codes.  The superior design-through-analysis capabilities are demonstrated for multistage unsteady aerodynamic simulations of turbine components, involving complex blade geometries, turbulent high-Reynolds-number flows and moving fluid domains."
"1817573","NSCI: Advancing U.S. Competitiveness through Public-Private Partnerships for Advanced Computing","OAC","Software Institutes","05/01/2018","04/23/2018","Charles Evans","DC","Council on Competitiveness","Standard Grant","Vipin Chaudhary","04/30/2019","$453,997.00","","cevans@compete.org","900 17th ST NW","Washington","DC","200062515","2026824292","CSE","8004","026Z, 8004, 8005","$0.00","Advanced computing is a foundational technology that has an enormous and growing impact on America's science, security and economic interests - all of which are interrelated. This project aims to: identify strategic public-private research areas for American leadership in advanced computing, optimize the way the public and private sector collaborate so research advances are deployed in meaningful ways that serve America's security and prosperity, and engage many stakeholder groups and help them prepare a diverse community of Americans to lead this shift in computing. By helping the United States field a more competitive advance computing ecosystem, the project would also advance scientific knowledge that underpins that same security and prosperity, and build the resulting industry of the future - along with the accompanying jobs - in the United States.<br/><br/>High performance computing represents a foundational technology and business asset for improving United States? competitiveness. However, the United States, a long-time leader in the supercomputing realm, has been falling behind other nations in the development of advanced computing resources potentially ceding leadership in this space and as the global innovation leader to other countries investing in the resources to power innovation. This project will improve and expand collaboration between public and private sector groups to use the limited resources available to the high-performance computing community effectively, and strengthen the ability of firms to leverage advanced computing for economic advantage by developing skills within the American workforce, new software partnerships, and greater access to advanced computing resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835292","Collaborative Research: Elements: Software: Software Health Monitoring and Improvement Framework","OAC","Software Institutes","11/01/2018","08/31/2018","Yuanfang Cai","PA","Drexel University","Standard Grant","Stefan Robila","10/31/2021","$299,679.00","","yfcai@cs.drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","8004","026Z, 077Z, 7923, 8004","$0.00","Software underpins every aspects of modern life, with significant impact in society. Poor quality software can cause huge financial losses, even threatening people's lives. Software quality is even more critical within the scientific community. The reproducibility of research results and sustainability of the research itself, heavily depend on the quality of the software developed by scientists, who usually acquire basics of software programming but are not aware of the best design practices. As a consequence, several existing open access scientific software packages are known to be hard to use and evolve due to their poor quality, as highlighted in recent studies. This project will integrate and enhance recent advances in software issue detection and refactoring techniques, created by the PIs and sponsored by NSF, in order to serve diverse scientific and engineering domains, detecting and fixing software quality issues effectively. <br/><br/>This proposal seeks to bridge the gap between software engineering community and other science and engineering community in general. It will provide quantitative comparisons of software projects against an industrial benchmark, enable users to pinpoint software issues responsible for high maintenance costs, visualize the severity of the detected issues, and refactor them using the proposed interactive refactoring framework. The proposed framework will bring together software users and software developers by enabling non software experts to post software challenges for the software community to solve, which will, in turn, boost the research and advances in software research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1724679","Student Support: 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid 2017)","OAC","INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE","05/01/2017","01/30/2018","Forough Ghahramani","NJ","Rutgers University New Brunswick","Standard Grant","Sushil K Prasad","04/30/2020","$20,000.00","","forough.ghahramani@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1640, 7361","7556, 9102","$0.00","Engaging students in premiere scientific conferences is extremely important as we seek to increase participation in STEM and to identify and develop the next generation of scientists by engaging students in premier scientific conferences. The International Symposium on Cluster, Cloud and Grid Computing (CCGrid) is a successful series of conferences that serves as the major international forum for presenting and sharing recent research results and technological developments in the fields of Cluster, Cloud and Grid computing. The CCGrid conference series has been emphasizing a comprehensive experience for students, to better prepare students attending the conference for their professional career. The CCGrid program provides a comprehensive set of events and activities that can expose students to the state of the art in research and technologies, as well as to the leading researchers and thinkers in the field. The student program is a critical aspect of CCGrid and is anchored in this technical program and exposes students to the state of the art in research and technologies, as well as to the leading researchers and thinkers in the field. Traditionally, students have participated in all aspects of this program, including being authors and presenters on papers and posters, presented demos, and been competitors in the SCALE challenge. Additionally, the CCGrid program includes the annual CCGrid Doctoral Symposium, which provides students researchers with visibility, mentoring, and valuable feedback for their research. Participating in the conference will provide students with valuable opportunities to improve their overall research skills and planning, and thus aligned with NSF's overall mission of promoting science and advancing national prosperity.<br/><br/>This project will support the participation of US-based students in CCGrid 2017. Supported students will present papers or posters describing their ongoing research to the conference audience, participate in the student program and related activities, discuss their research with experts from academia and industry during the conference, and make useful contacts. Travel grants will encourage the research interests and the involvement of students in the field who are not well funded and those who are just beginning their participation in the field or are interested in entering it. Particular effort will be made to solicit applications for the travel support from female students and students from under-represented communities."
"1341349","SC Conference Experiencing HPC for Undergraduates Program","OAC","RSCH EXPER FOR UNDERGRAD SITES, INFORMATION TECHNOLOGY RESEARC, EDUCATION AND WORKFORCE, DATANET, PETASCALE - TRACK 1, HIGH-PERFORMANCE COMPUTING, Software Institutes","07/15/2013","02/15/2019","Jeff Hollingsworth","NY","Association Computing Machinery","Standard Grant","Sushil K Prasad","06/30/2019","$166,990.00","Alan Sussman","hollings@cs.umd.edu","2 Penn Plaza","New York","NY","101210799","2128697440","CSE","1139, 1640, 7361, 7726, 7781, 7942, 8004","026Z, 1640, 7433, 7556, 7726, 7781, 7942, 8004, 9102","$0.00","The purpose of this proposal is to fund travel for undergraduate students to participate in the Experiencing HPC for Undergraduates program at the SC13, SC14 and SC15 conferences. The primary goal of this program is to lead the students into research as an undergraduate and then encourage them to attend graduate school in HPC topics in Computer and Computational Science. The program makes use of several existing parts of the SC technical program, with additional activities specific to the program. The unique content for the participants in this program will include an HPC boot camp session, talks by well known researchers in the field, a panel featuring current graduate students, and a panel on academic and industry career opportunities in the HPC field.<br/><br/>The key idea of this program is that the best way to get people excited about HPC is to visit and participate in a major technical conference in the field. SC is an ideal venue for such a program since it combines elements of a high quality technical meeting (papers, posters, tutorials) with a major industry trade show (commercial and research exhibits, vendor briefings, birds of a feather sessions). By providing sophomore and junior undergraduates an opportunity to see what the field is about, it is believed that we can excite them about the field in time for them to apply to graduate school or decide on which industry to enter when they complete their bachelor?s degrees.<br/><br/>The goal of this program is to expand the High Performance Computing workforce by encouraging talented undergraduates to consider graduate studies and careers in the field of HPC. The program will make a special effort to recruit participants from under represented groups and minority serving institutions. HPC is critical to many national goals from scientific innovation to product development. Currently there is a shortage of new students entering the field. This program is designed to try to help meet those national needs."
"1845962","CAREER: A Parallel and Efficient Computational Framework for Unified Volumetric Meshing in Large-Scale 3D/4D Anisotropy","OAC","CAREER: FACULTY EARLY CAR DEV","03/15/2019","02/11/2019","Zichun Zhong","MI","Wayne State University","Continuing grant","Sushil K Prasad","02/29/2024","$293,169.00","","zichunzhong@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","1045","1045","$0.00","This proposal develops a computational framework that helps the domain scientists who employ advanced cyberinfrastructure ecosystem (e.g., for engineering, manufacturing, healthcare, etc.) to realistically and efficiently reconstruct, visualize, and analyze 3D and 4D (space-time) volumetric objects with complex geometric structures and highly anisotropic properties (such properties are characterized by the presence of specified orientations and aspect ratios in the system). For example, in mechanical engineering, it is necessary to interactively design and model mechanical parts with user-required high-quality measures and standards. The computational framework enables fabrication of such mechanical parts with specified microstructure that can be efficiently produced to sustain much stronger stress and strain compared with those without endowing such properties, which leads to significant impact on the next-generation mechanical component design. As an integral part of the PI's career development, the educational plan emphasizes on the integration of education and research in different aspects through the PI's new ""3D hands-on"" education philosophy for K-12, undergraduate and graduate students. This project thus serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare.<br/> <br/>The research goal of this project focuses on a computational framework for anisotropic volumetric meshing, a foundational as well as translational research impacting a broad range of scientific domains.  The capability and usability of the meshing framework are evaluated by investigating fabrication of objects with internal microstructures and construction of anisotropic volumetric models to capture the organ and tissue shape. This work has the following primary components: (1) Computing high-dimensional geometric embedding based on Nash theorem in parallel: the computational realization of high-dimensional geometric embedding makes modeling complex objects with multiple tensor features being built and solved in parallel in a large linear system. (2) Modeling multi-shape of mesh element in a unified particle framework: the particle system flexibly and effectively generates high-quality honeycomb, tetrahedral, and hexahedral (grid) patterns, which are exactly designed for meshing structure. The optimization procedure is easily formulated for parallelism in the high-dimensional space. (3) Generating 3D/4D anisotropic mesh in parallel: the final multi-shape anisotropic meshes are computed in parallel in the high-dimensional space with simple Euclidean computations under the isotropic metric. The primary outcome of this project is a 3D/4D-ParaAnisoMesh system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1350454","CAREER: Software Abstractions for Stochastic Embedding in Predictive Simulations on Extreme-Scale Cyberinfrastructure","OAC","CAREER: FACULTY EARLY CAR DEV, INFORMATION TECHNOLOGY RESEARC","02/01/2014","08/17/2017","Onkar Sahni","NY","Rensselaer Polytechnic Institute","Standard Grant","Sushil K Prasad","01/31/2020","$513,409.00","","sahni@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1045, 1640","019Z, 1045, 9179","$0.00","Computer simulations for scientific problems are now considered as the third pillar of scientific inquiry, where simulation-based prediction for increasingly complex real-world problems has been matched with the growth in computing power. These physical problems can be described mathematically through models that encompass complex stochastic multiscale systems. In such models several terms and parameters are uncertain and not accounting for these uncertainties in the system-level prediction can lead to significant inaccuracies and futile predictions. For reliable predictions, the uncertainties must be statistically quantified to understand their effects on quantities being evaluated by the simulation. This need has given rise to several simulation tools that have been applied to tackle challenging problems. However, these simulation tools are often integrated in an ad-hoc fashion leading to under utilization of the hardware, or limited applicability in terms of the problem at hand, or both. Therefore, algorithmic and software elements and abstractions are needed that can re-use existing components, and support creation of new ones, such that they can be integrated with ease to construct effective tools for stochastic simulations.<br/><br/>To achieve this goal, this project is investigating novel abstractions based on a rigorous and systematic approach to stochastic embedding techniques. Stochastic embedding implies insertion of uncertainty propagation loops/samples in the calculations within the physics engine. The idea of embedding is to increase the computational efficiency. With embedding, different software components become aware of the stochastic discretization and account for it not only in the underlying floating-point operations but also in parallelization and communications. The focus of this project is on generalizations that target different physics analysis codes and a broad range of stochastic discretization techniques including adaptive collocation, low-rank separated representation, and stochastic Galerkin. The ultimate goal is to achieve tremendously efficient and new levels of reliable predictive simulations on next-generation computing platforms and cyberinfrastructure, where the size of the overall stochastic problem is enormous (e.g., with many trillions of degrees-of-freedom in the joint spatiotemporal-stochastic space).<br/><br/>The research goal is to provide remarkable improvements in our ability to reliably predict and control the performance of complex stochastic multiscale systems, which in-turn will have great scientific, economic and social impacts (e.g., in making energy generation and management systems highly efficient and reliable). The resulting techniques are expected to be applicable to other broad research areas such as large-scale parametric studies, optimization and inverse problems. This project builds on a comprehensive three-pronged education plan that includes K-12, undergraduate and graduate students as well as broader community (including industry). The idea is to educate and grow the next generation of researchers focused on advanced computing and computational science. This will be done through summer camps, courses and workshops. In order to have the maximum impact, results from this research will be disseminated via a variety of methods such as conference presentations, journal papers, software documents, and tutorials."
"1842623","EAGER: Measuring Real World Application Performance on Next-Generation Computing Systems","OAC","ETF","10/01/2018","08/03/2018","Robert Henschel","IN","Indiana University","Standard Grant","Edward Walker","03/31/2020","$300,000.00","Rudolf Eigenmann, Sunita Chandrasekaran","henschel@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7476","7916","$0.00","The project proposes to create a benchmark suite using real-world scientific applications. The benchmark suite will be developed jointly with the High-Performance Group (HPG) of the Standard Performance Evaluation Corporation (SPEC), a non-profit organization with the goal of creating and maintaining standardized benchmarks to evaluate performance and energy efficiency for the newest generation of computing systems. The project will leverage SPEC infrastructure to ensure continuous maintenance of the benchmark suite, as well as support for result submission, review, and publication on the SPEC website. Furthermore, the project will use the published results of the benchmark suite to create a public ranking of High-Performance Computing (HPC) systems deployed across the world. Partnership with SPEC will ensure that the results of the project will be disseminated to industry.<br/><br/>The creation of a new real-world application benchmark suite jointly with SPEC/HPG, as well as the development of performance metrics suitable for application benchmarks, will lead to improved HPC system design and enable the better understanding of how next-generation computing systems must evolve. In addition to evaluating HPC systems, the proposed application benchmark suite will also enable the comparative performance evaluation of novel software and hardware ecosystems.  Moreover, the proposed work builds on NSF investment in domain science application development by including these applications in the suite. Importantly, the project will be an opportunity to bring the benchmark community together to develop and define performance and throughput metrics for real world scientific applications. Finally, the project will deliver a benchmark suite that will be freely available to non-profit organizations and sustained by SPEC.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1450904","IRNC-BackBone- TransPAC4 - Pragmatic Application-Driven International Networking","OAC","INTERNATIONAL RES NET CONNECT","03/01/2015","10/04/2018","Jennifer Schopf","IN","Indiana University","Cooperative Agreement","Kevin Thompson","02/29/2020","$4,800,000.00","Andrew Lee","jmschopf@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","8128550516","CSE","7369","5921, 5924, 5942, 7369","$0.00","TransPAC4 is a cross-organizational approach to provide services and bandwidth connecting scientific researchers in the US with their counterparts in Asia. This connectivity has an immediate impact on the research environment, and also anticipates future application and technology advances.<br/><br/>Indiana University (IU) leads the TransPAC4 Collaboration in a cooperative partnership with Asian regional partnerships including: Asia Pacific Advanced Network (APAN), the Trans-Eurasia Information Network (TEIN), and the Delivery of Advanced Network Technology to Europe (DANTE) project. Research and Education networking partners include the Japanese National Institute of Information and Communications Technology (NICT), the Japanese National Institute of Informatics (NII), the Singapore Advanced Research and Education Network (SingAREN), and the Taiwan National Network. Domestic partners include Internet2, the Energy Sciences network (ESnet), and Pacific Wave's international peering facility.<br/><br/>In addition to providing high performance network capacity in the form of 10G-100G circuits and associated services, the project focuses on increasing the partnerships needed to take advantage of these links, including but not limited to growing communities in geosciences, bioinformatics, and particle physics. The TransPAC4 Collaboration expands and enhances end user engagement and operational integration in Asia-Pacific. Close cooperation among participants serves to maximize US-Asia connectivity, increase collaborations, deploy dynamic circuit services to meet the needs of high-bandwidth applications, and increase reliability through alternative circuits. The project also extends the use of Software Defined Networking (SDN)and network acceleration services."
"1338051","MRI: Acquisition of SuperMIC -- A Heterogeneous Computing Environment to Enable Transformation of Computational Research and Education in the State of Louisiana","OAC","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE, DATANET, EPSCoR Co-Funding","10/01/2013","07/20/2018","Honggao Liu","LA","Louisiana State University & Agricultural and Mechanical College","Standard Grant","Edward Walker","09/30/2019","$3,924,181.00","Joel Tohline, Susanne Brenner, Jagannathan Ramanujam, Qin Chen, Mark Jarrell","honggao@tamu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","1189, 1640, 7231, 7726, 9150","1189, 9150","$0.00","This is an award to acquire a compute cluster at LSU.  The computer is a heterogeneous HPC cluster named SuperMIC containing both Intel Xeon Phi and NVIDIA Kepler K20X GPU (graphics processing unit) accelerators. The intent is to conduct research on programming such clusters while advancing projects that are dependent on HPC. The efforts range from modeling conditions which threaten coastal environments and test mitigation techniques; to simulating the motions of tumors/organs in cancer patients due to respiratory actions to aid radiotherapy planning and management. The burden of learning highly complex hybrid programming models presents an enormous software development crisis and demands a better solution. SuperMIC will serve as the development platform to extend current programming frameworks, such as Cactus, by incorporating GPU and Xeon Phi methods. Such frameworks allow users to move seamlessly from serial to multi-core to distributed parallel platforms without changing their applications, and yet achieve high performance. The SuperMIC project will include training and education at all levels, from a Beowulf boot camp for high school students to more than 20 annual LSU workshops and computational sciences distance learning courses for students at LONI (Louisiana Optical Network Initiative) and LA-SiGMA (Louisiana Alliance for Simulation-Guided Materials Applications) member institutions. These include Southern University, Xavier University, and Grambling State University - all historically black colleges and universities (HBCU) which have large underrepresented minority enrollments. The SuperMIC cluster will be used in the LSU and LA-SiGMA REU and RET programs. It will impact the national HPC community through resources committed to the NSF XSEDE program and the Southeastern Universities Research Association SURAgrid.  The SuperMIC will commit 40% of the usage of the machine to the XSEDE XRAC allocation committee."
"1836797","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2017","09/19/2018","Matthew Knepley","NY","SUNY at Buffalo","Standard Grant","Stefan Robila","07/31/2020","$215,715.00","","knepley@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1664142","Collaborative Research: SI2-SSI: EVOLVE: Enhancing the Open MPI Software for Next Generation Architectures and Applications","OAC","Software Institutes","06/01/2017","05/26/2017","George Bosilca","TN","University of Tennessee Knoxville","Standard Grant","Stefan Robila","05/31/2021","$1,566,215.00","Thomas Herault, Aurelien Bouteiller","bosilca@icl.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","8004","026Z, 7433, 7942, 8004, 8009","$0.00","For nearly two decades, the Message Passing Interface (MPI) has been an essential part of the High-Performance Computing ecosystem and consequently a key enabler for important scientific breakthroughs. It is a fundamental building block for most large-scale simulations from physics, chemistry, biology, material sciences as engineering.  Open MPI is an open source implementation of the MPI specification, widely used and adopted by the research community as well as industry. The Open MPI library is jointly developed and maintained by a consortium of academic institutions, national labs and industrial partners. It is installed on virtually all large-scale computer systems in the US as well as in the rest of the world. The goal of this project is to enhance and modernize the Open MPI library in the context of the ongoing evolution of modern computer systems, and to ensure its future operability on all upcoming architectures. We aim at implementing fundamental software techniques that can be used in many-core systems to execute MPI-based parallel applications more efficiently, and to tolerate process and memory failures at all scales, from current systems, up to the extreme scales expected before the end of the decade.<br/><br/>Open MPI is an open source implementation of the Message Passing Interface (MPI) specification. The MPI API is currently being extended to consider the needs of application developers in terms of efficiency, productivity and resilience. The project will also support academic involvement in the design, development and evaluation of the Open MPI software, and ensure academic presence in the MPI Forum. The goal of this proposal is to enhance the Open MPI software library, focusing on two aspects: (1) Extend Open MPI to support new features of the MPI specification. Open MPI will continue to support all new features of current and upcoming MPI specifications. The two most significant areas within the context of this proposal are (a) extensions to better support hybrid programming models and (b) support for fault tolerance in MPI applications. To improve support for hybrid programming models, the MPI Forum is currently considering introducing the notion of MPI Endpoints, which could be used by different threads of an MPI rank to instantiate multiple separate communication contexts. The goal within this project is to develop an implementation of endpoints to support effective hybrid programming model, and to extend the concept to other aspects of parallel applications such as File I/O operations. One of the project partners (UTK) leads the current proposal in the MPI Forum to expose failures and ensure the continuation of the execution of MPI applications. In the context of this SSI proposal, the goal is to harden, improve, and expand the support of the existing ULFM implementation in Open MPI and thus enable end-users to design application-specific resilience approaches for future platforms. (2) Enhance the Open MPI core to support new architectures and improve scalability. While Open MPI has demonstrated very good scalability in the past, there is significant work to be done to ensure similarly good performance on future architectures. Specifically, we propose a groundbreaking rework of the startup environment that will improve process launch scalability, increase support for asynchronous progress of operations, enable support for accelerators, and reduce sensitivity to system noise. The project would also enhance the support for File I/O operations as part of the Open MPI package by expanding our work on highly scalable collective I/O operations through delegation and exploring the utilization of burst buffers as temporary storage."
"1663887","Collaborative Research: SI2-SSI: EVOLVE: Enhancing the Open MPI Software for Next Generation Architectures and Applications","OAC","Software Institutes","06/01/2017","05/26/2017","Edgar Gabriel","TX","University of Houston","Standard Grant","Stefan Robila","05/31/2021","$308,785.00","","gabriel@cs.uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","8004","026Z, 7433, 7942, 8004, 8009","$0.00","For nearly two decades, the Message Passing Interface (MPI) has been an essential part of the High-Performance Computing ecosystem and consequently a key enabler for important scientific breakthroughs. It is a fundamental building block for most large-scale simulations from physics, chemistry, biology, material sciences as engineering.  Open MPI is an open source implementation of the MPI specification, widely used and adopted by the research community as well as industry. The Open MPI library is jointly developed and maintained by a consortium of academic institutions, national labs and industrial partners. It is installed on virtually all large-scale computer systems in the US as well as in the rest of the world. The goal of this project is to enhance and modernize the Open MPI library in the context of the ongoing evolution of modern computer systems, and to ensure its future operability on all upcoming architectures. We aim at implementing fundamental software techniques that can be used in many-core systems to execute MPI-based parallel applications more efficiently, and to tolerate process and memory failures at all scales, from current systems, up to the extreme scales expected before the end of the decade.<br/><br/>Open MPI is an open source implementation of the Message Passing Interface (MPI) specification. The MPI API is currently being extended to consider the needs of application developers in terms of efficiency, productivity and resilience. The project will also support academic involvement in the design, development and evaluation of the Open MPI software, and ensure academic presence in the MPI Forum. The goal of this proposal is to enhance the Open MPI software library, focusing on two aspects: (1) Extend Open MPI to support new features of the MPI specification. Open MPI will continue to support all new features of current and upcoming MPI specifications. The two most significant areas within the context of this proposal are (a) extensions to better support hybrid programming models and (b) support for fault tolerance in MPI applications. To improve support for hybrid programming models, the MPI Forum is currently considering introducing the notion of MPI Endpoints, which could be used by different threads of an MPI rank to instantiate multiple separate communication contexts. The goal within this project is to develop an implementation of endpoints to support effective hybrid programming model, and to extend the concept to other aspects of parallel applications such as File I/O operations. One of the project partners (UTK) leads the current proposal in the MPI Forum to expose failures and ensure the continuation of the execution of MPI applications. In the context of this SSI proposal, the goal is to harden, improve, and expand the support of the existing ULFM implementation in Open MPI and thus enable end-users to design application-specific resilience approaches for future platforms. (2) Enhance the Open MPI core to support new architectures and improve scalability. While Open MPI has demonstrated very good scalability in the past, there is significant work to be done to ensure similarly good performance on future architectures. Specifically, we propose a groundbreaking rework of the startup environment that will improve process launch scalability, increase support for asynchronous progress of operations, enable support for accelerators, and reduce sensitivity to system noise. The project would also enhance the support for File I/O operations as part of the Open MPI package by expanding our work on highly scalable collective I/O operations through delegation and exploring the utilization of burst buffers as temporary storage."
"1450374","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2015","08/11/2015","Neelesh Patankar","IL","Northwestern University","Standard Grant","Micah Beck","07/31/2020","$512,966.00","","n-patankar@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1450327","SI2-SSI: Collaborative Research:  Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","OAC","OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","08/01/2015","06/09/2017","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Standard Grant","Micah Beck","07/31/2020","$940,056.00","Robert O'Bara","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1253, 8004, 8069, 8084","7433, 8004, 8009, 8084, 9251","$0.00","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.<br/><br/>The immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development."
"1807622","Collaborative Research: CDS&E: ReaxFF2:  Efficient and Scalable Methods for Long-time Reactive Molecular Dynamics Simulations","OAC","DMR SHORT TERM SUPPORT, Theory, Models, Comput. Method, CDS&E","09/01/2018","08/31/2018","Hasan Metin Aktulga","MI","Michigan State University","Standard Grant","Vipin Chaudhary","08/31/2021","$250,818.00","","hma@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1712, 6881, 8084","026Z, 054Z, 7926, 8084, 9216, 9263","$0.00","This project aims to enable long-time simulations of reactive molecular systems through efficient<br/>and scalable techniques. Long-time reactive simulations are critical for several scientific problems<br/>such as catalysis, battery interfaces, biological simulations involving water, and emerging<br/>areas like surface oxidation and chemical vapor deposition (CVD) growth. However, progress on<br/>these fronts is limited because long-time simulations of large-scale systems are very difficult, if<br/>not impossible, to perform using existing methods. The Reactive Force Field (ReaxFF) method is in principle  <br/>ideally suited for this purpose. However, the short time steps required in current ReaxFF simulations <br/>and the computationally expensive force field formulation limit ReaxFF's temporal capabilities to <br/>narrow simulation time ranges. This project aims to overcome such limitations by creating ReaxFF2,<br/>which will extend time scales by one to two orders of magnitude - thus making large-scale, <br/>long-time RMD simulations accessible to a wide community. Codes developed will be made publicly <br/>available and results from this project will be highlighted on a dedicated website, and they will also be <br/>incorporated into workshops by the PIs.<br/><br/>In creating ReaxFF2, the PIs will enhance the Reax force field formulation significantly, and develop <br/>innovative algorithms and software implementations for scalable simulations. More specifically, <br/>alternative ReaxFF interactions will be formulated to eliminate sharp derivatives in energy terms <br/>and enhance ReaxFF time step lengths by at least a factor of four. To accelerate the dynamic charge <br/>distribution models needed in RMD, scalable parallel preconditioning techniques for the iterative <br/>solvers will be developed. A task parallel approach to compute interactions, hierarchical problem <br/>decomposition, vectorization of the key kernels, and use of mixed precision arithmetics constitute<br/>the main techniques that will be utilized to fully leverage the performance capabilities of large<br/>computer clusters. Finally, capabilities of accelerated RMD concepts in the proposed ReaxFF2 <br/>formulation will be evaluated and inlined trajectory analysis tools for RMD will be developed <br/>to facilitate the study of long-time RMD simulations. This project will significantly enhance the PIs' <br/>software development, community building,  and sustenance efforts for the RMD community. Codes, <br/>functional forms, and parameter sets developed will be made publicly available, enabling fast and <br/>accurate modeling of diverse reactive systems beyond the scope of this project. For community outreach, <br/>results from this project will be highlighted on a dedicated website, and they will also be incorporated <br/>into workshops by the PIs.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research <br/>and the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807740","Collaborative Research: CDS&E: ReaxFF2: Efficient and Scalable Methods for Long-time Reactive Molecular Dynamics Simulations","OAC","DMR SHORT TERM SUPPORT, Theory, Models, Comput. Method, CDS&E","09/01/2018","08/31/2018","Adri van Duin","PA","Pennsylvania State Univ University Park","Standard Grant","Vipin Chaudhary","08/31/2021","$200,000.00","","acv13@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1712, 6881, 8084","026Z, 054Z, 7926, 8084, 9216, 9263","$0.00","This project aims to enable long-time simulations of reactive molecular systems through efficient<br/>and scalable techniques. Long-time reactive simulations are critical for several scientific problems<br/>such as catalysis, battery interfaces, biological simulations involving water, and emerging<br/>areas like surface oxidation and chemical vapor deposition (CVD) growth. However, progress on<br/>these fronts is limited because long-time simulations of large-scale systems are very difficult, if<br/>not impossible, to perform using existing methods. The Reactive Force Field (ReaxFF) method is in principle  <br/>ideally suited for this purpose. However, the short time steps required in current ReaxFF simulations <br/>and the computationally expensive force field formulation limit ReaxFF's temporal capabilities to <br/>narrow simulation time ranges. This project aims to overcome such limitations by creating ReaxFF2,<br/>which will extend time scales by one to two orders of magnitude - thus making large-scale, <br/>long-time RMD simulations accessible to a wide community. Codes developed will be made publicly <br/>available and results from this project will be highlighted on a dedicated website, and they will also be <br/>incorporated into workshops by the PIs.<br/><br/>In creating ReaxFF2, the PIs will enhance the Reax force field formulation significantly, and develop <br/>innovative algorithms and software implementations for scalable simulations. More specifically, <br/>alternative ReaxFF interactions will be formulated to eliminate sharp derivatives in energy terms <br/>and enhance ReaxFF time step lengths by at least a factor of four. To accelerate the dynamic charge <br/>distribution models needed in RMD, scalable parallel preconditioning techniques for the iterative <br/>solvers will be developed. A task parallel approach to compute interactions, hierarchical problem <br/>decomposition, vectorization of the key kernels, and use of mixed precision arithmetics constitute<br/>the main techniques that will be utilized to fully leverage the performance capabilities of large<br/>computer clusters. Finally, capabilities of accelerated RMD concepts in the proposed ReaxFF2 <br/>formulation will be evaluated and inlined trajectory analysis tools for RMD will be developed <br/>to facilitate the study of long-time RMD simulations. This project will significantly enhance the PIs' <br/>software development, community building,  and sustenance efforts for the RMD community. Codes, <br/>functional forms, and parameter sets developed will be made publicly available, enabling fast and <br/>accurate modeling of diverse reactive systems beyond the scope of this project. For community outreach, <br/>results from this project will be highlighted on a dedicated website, and they will also be incorporated <br/>into workshops by the PIs.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research <br/>and the Division of Chemistry within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1725755","SPX:   Collaborative Research:   SANDY:   Sparsification-based Approach for Analyzing Network Dynamics","OAC","SPX: Scalable Parallelism in t, INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF, IIS SPECIAL PROJECTS","09/01/2017","09/20/2018","Sajal Das","MO","Missouri University of Science and Technology","Continuing grant","Vipin Chaudhary","08/31/2020","$232,073.00","","sdas@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","042Y, 1640, 1714, 2878, 7484","026Z, 9150","$0.00","The goal of this three-year project, Sparsification-based Approach for Analyzing Network Dynamics (SANDY), is to develop a suite of scalable parallel algorithms for updating dynamic networks for different problems that can be executed on a wide range of HPC platforms. Dynamic network analysis will enable researchers to study the evolution of complex systems in diverse disciplines, such as bioinformatics, social sciences, and epidemiology. The SANDY project is expected to initiate a new direction of research in developing parallel dynamic network algorithms that will benefit multiple analysis objectives (e.g., motif finding and network alignment) and application domains (e.g., epidemiology, health care). Research findings will be integrated into courses on network analysis, parallel algorithms, and bioinformatics offered at the three collaborating institutions. The PIs will collaborate with high schools to deliver talks on network theory, and encourage women and minority students to pursue IT-related careers. <br/><br/> <br/>To develop efficient and scalable parallel algorithms, the PIs propose to use an elegant technique, called graph sparsification, that expresses graph algorithms in a reduction-like fashion. The formal steps to parallelization, as guided by the graph sparsification framework, provide a template for creating provably correct parallel algorithms for dynamic networks. The proposed algorithms will address the dual needs of portability and performance optimization. The framework will further provide a mechanism for combining high level (e.g., static and dynamic graph partitioning) and low level (e.g., dataflow algorithms) tuning strategies to ensure high performance and scalability for various parallel architectures by considering such factors as scalability, time, memory, and energy efficiency."
